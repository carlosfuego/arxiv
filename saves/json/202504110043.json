[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06813v1",
                "updated": "2025-04-09T12:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "title": "Introducing the Arm-membench Throughput Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Arm-membench Throughput Benchmark"
                },
                "summary": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA."
                },
                "authors": [
                    {
                        "name": "Cyrill Burth"
                    },
                    {
                        "name": "Markus Velten"
                    },
                    {
                        "name": "Robert Schöne"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schöne"
                },
                "author": "Robert Schöne",
                "arxiv_doi": "10.1007/978-3-031-85697-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85697-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures, published in Parallel Processing and Applied\n  Mathematics (PPAM 2024), see https://doi.org/10.1007/978-3-031-85697-6_7",
                "arxiv_journal_ref": "Parallel Processing and Applied Mathematics. PPAM 2024. Lecture\n  Notes in Computer Science, vol 15579. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v5",
                "updated": "2025-04-09T03:49:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    49,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06419v1",
                "updated": "2025-04-08T20:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "title": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests."
                },
                "authors": [
                    {
                        "name": "Sanjit Neelam"
                    },
                    {
                        "name": "Daniel Heinlein"
                    },
                    {
                        "name": "Vaclav Cvicek"
                    },
                    {
                        "name": "Akshay Mishra"
                    },
                    {
                        "name": "Reiner Pope"
                    }
                ],
                "author_detail": {
                    "name": "Reiner Pope"
                },
                "author": "Reiner Pope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v1",
                "updated": "2025-04-08T20:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation."
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-André Noël"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-André Noël"
                },
                "author": "Pierre-André Noël",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17692v2",
                "updated": "2025-04-08T19:26:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    26,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-26T20:44:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    20,
                    44,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation"
                },
                "summary": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity."
                },
                "authors": [
                    {
                        "name": "Michael Czekanski"
                    },
                    {
                        "name": "Benjamin Faber"
                    },
                    {
                        "name": "Margaret Fairborn"
                    },
                    {
                        "name": "Adelle Wright"
                    },
                    {
                        "name": "David Bindel"
                    }
                ],
                "author_detail": {
                    "name": "David Bindel"
                },
                "author": "David Bindel",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v2",
                "updated": "2025-04-08T12:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    46,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06319v1",
                "updated": "2025-04-08T09:17:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching"
                },
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines."
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05718v1",
                "updated": "2025-04-08T06:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:38:27Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor"
                },
                "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core."
                },
                "authors": [
                    {
                        "name": "Christopher Reinwardt"
                    },
                    {
                        "name": "Robert Balas"
                    },
                    {
                        "name": "Alessandro Ottaviano"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "8 pages, 7 figures, accepted at the 22nd ACM International Conference\n  on Computing Frontiers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03661v2",
                "updated": "2025-04-08T04:34:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    34,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T13:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    32,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization"
                },
                "summary": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION."
                },
                "authors": [
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    },
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Gezi Li"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "7 pages, 7 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05646v1",
                "updated": "2025-04-08T03:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases."
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v3",
                "updated": "2025-04-07T22:48:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    22,
                    48,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v2",
                "updated": "2025-04-07T20:52:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    20,
                    52,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05097v1",
                "updated": "2025-04-07T14:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Tuning: State-based Test-Time Scaling on RWKV-7"
                },
                "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v1",
                "updated": "2025-04-07T08:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00414v3",
                "updated": "2025-04-07T06:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    27,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering"
                },
                "summary": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments."
                },
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v1",
                "updated": "2025-04-07T03:22:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v2",
                "updated": "2025-04-07T01:35:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    1,
                    35,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v2",
                "updated": "2025-04-06T12:20:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    12,
                    20,
                    25,
                    6,
                    96,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v1",
                "updated": "2025-04-05T00:59:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_comment": "7 pages, 14 figures. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03632v1",
                "updated": "2025-04-04T17:56:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:56:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs"
                },
                "summary": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems."
                },
                "authors": [
                    {
                        "name": "Huda Ibeid"
                    },
                    {
                        "name": "Vikram Narayana"
                    },
                    {
                        "name": "Jeongnim Kim"
                    },
                    {
                        "name": "Anthony Nguyen"
                    },
                    {
                        "name": "Vitali Morozov"
                    },
                    {
                        "name": "Ye Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ye Luo"
                },
                "author": "Ye Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02073v2",
                "updated": "2025-04-04T15:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    30,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2023-07-05T07:30:53Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    7,
                    30,
                    53,
                    2,
                    186,
                    0
                ],
                "title": "Performance Modeling of Data Storage Systems using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Modeling of Data Storage Systems using Generative Models"
                },
                "summary": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning."
                },
                "authors": [
                    {
                        "name": "Abdalaziz Rashid Al-Maeeni"
                    },
                    {
                        "name": "Aziz Temirkhanov"
                    },
                    {
                        "name": "Artem Ryzhikov"
                    },
                    {
                        "name": "Mikhail Hushchyn"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Hushchyn"
                },
                "author": "Mikhail Hushchyn",
                "arxiv_doi": "10.1109/ACCESS.2025.3552409",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3552409",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.02073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 2025 ( Volume: 13) 49643 - 49658",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03499v1",
                "updated": "2025-04-04T14:55:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:55:27Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "title": "Optimistic Learning for Communication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic Learning for Communication Networks"
                },
                "summary": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach."
                },
                "authors": [
                    {
                        "name": "George Iosifidis"
                    },
                    {
                        "name": "Naram Mhaisen"
                    },
                    {
                        "name": "Douglas J. Leith"
                    }
                ],
                "author_detail": {
                    "name": "Douglas J. Leith"
                },
                "author": "Douglas J. Leith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v3",
                "updated": "2025-04-04T13:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    27,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v1",
                "updated": "2025-04-04T03:30:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16444v3",
                "updated": "2025-04-03T22:49:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2024-05-26T06:00:17Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    0,
                    17,
                    6,
                    147,
                    0
                ],
                "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion"
                },
                "summary": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03048v1",
                "updated": "2025-04-03T21:53:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:53:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "LLM Library Learning Fails: A LEGO-Prover Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Library Learning Fails: A LEGO-Prover Case Study"
                },
                "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines."
                },
                "authors": [
                    {
                        "name": "Ian Berlot-Attwell"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Xujie Si"
                    }
                ],
                "author_detail": {
                    "name": "Xujie Si"
                },
                "author": "Xujie Si",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02976v1",
                "updated": "2025-04-03T18:54:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:54:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching"
                },
                "summary": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates."
                },
                "authors": [
                    {
                        "name": "Nooshin Bahador"
                    }
                ],
                "author_detail": {
                    "name": "Nooshin Bahador"
                },
                "author": "Nooshin Bahador",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02972v1",
                "updated": "2025-04-03T18:47:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:47:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "Improved Compact Genetic Algorithms with Efficient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Compact Genetic Algorithms with Efficient Caching"
                },
                "summary": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy."
                },
                "authors": [
                    {
                        "name": "Prasanta Dutta"
                    },
                    {
                        "name": "Anirban Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Mukhopadhyay"
                },
                "author": "Anirban Mukhopadhyay",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02921v1",
                "updated": "2025-04-03T17:08:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:08:42Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service."
                },
                "authors": [
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Seo Jin Park"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03775v1",
                "updated": "2025-04-03T08:58:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:58:05Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling"
                },
                "summary": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs."
                },
                "authors": [
                    {
                        "name": "Weiqing Li"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Xiangyong Ding"
                    },
                    {
                        "name": "Zhangcheng Tao"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02268v1",
                "updated": "2025-04-03T04:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data"
                },
                "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Justin Cechmanek"
                    },
                    {
                        "name": "Tyler Hutcherson"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Jen Agarwal"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    },
                    {
                        "name": "Manvinder Singh"
                    },
                    {
                        "name": "Benoit Dion"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Dion"
                },
                "arxiv_affiliation": "Redis",
                "author": "Benoit Dion",
                "arxiv_comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02220v1",
                "updated": "2025-04-03T02:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T02:24:21Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "title": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations"
                },
                "summary": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Helen Mayer"
                    },
                    {
                        "name": "James Richards"
                    }
                ],
                "author_detail": {
                    "name": "James Richards"
                },
                "author": "James Richards",
                "arxiv_comment": "International Conference on Computing Technologies and Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v2",
                "updated": "2025-04-03T01:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    1,
                    23,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v2",
                "updated": "2025-04-02T18:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    51,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v4",
                "updated": "2025-04-02T04:57:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    57,
                    15,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v1",
                "updated": "2025-04-02T01:49:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "11 pages, 6 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01157v1",
                "updated": "2025-04-01T19:48:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T19:48:17Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB"
                },
                "summary": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden."
                },
                "authors": [
                    {
                        "name": "Anas Dorbani"
                    },
                    {
                        "name": "Sunny Yasser"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01104v1",
                "updated": "2025-04-01T18:21:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:21:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Fundamentals of Caching Layered Data objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of Caching Layered Data objects"
                },
                "summary": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers."
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "arxiv_comment": "An abridged version of this paper has been accepted at the 45th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01084v1",
                "updated": "2025-04-01T18:00:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:00:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Surfactants Screen Slide Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfactants Screen Slide Electrification"
                },
                "summary": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop."
                },
                "authors": [
                    {
                        "name": "Xiaomei Li"
                    },
                    {
                        "name": "Zhongyuan Ni"
                    },
                    {
                        "name": "Xiaoteng Zhou"
                    },
                    {
                        "name": "Lisa S. Bauer"
                    },
                    {
                        "name": "Diego Diaz"
                    },
                    {
                        "name": "Gabriele Schäfer"
                    },
                    {
                        "name": "Hans-Jürgen Butt"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Jürgen Butt"
                },
                "author": "Hans-Jürgen Butt",
                "arxiv_comment": "13 pages, 4 figures, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00999v1",
                "updated": "2025-04-01T17:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization"
                },
                "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Luyuan Zhang"
                    },
                    {
                        "name": "Zedong Wang"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "CVPR2025 (in process for more analysis and extension)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v1",
                "updated": "2025-04-01T17:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00726v1",
                "updated": "2025-04-01T12:34:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T12:34:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning"
                },
                "summary": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Weibo He"
                    },
                    {
                        "name": "Wanglei Feng"
                    },
                    {
                        "name": "Zhenyu Wen"
                    },
                    {
                        "name": "Bin Qian"
                    },
                    {
                        "name": "Blesson Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Blesson Varghese"
                },
                "author": "Blesson Varghese",
                "arxiv_comment": "Poster accepted at IEEE ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00557v1",
                "updated": "2025-04-01T09:10:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T09:10:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features"
                },
                "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity."
                },
                "authors": [
                    {
                        "name": "Jewon Lee"
                    },
                    {
                        "name": "Ki-Ung Song"
                    },
                    {
                        "name": "Seungmin Yang"
                    },
                    {
                        "name": "Donguk Lim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "accepted at CVPR 2025 Workshop on ELVM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00474v1",
                "updated": "2025-04-01T07:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T07:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "title": "High specific impulse electrospray propulsion with small capillary\n  emitters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High specific impulse electrospray propulsion with small capillary\n  emitters"
                },
                "summary": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$."
                },
                "authors": [
                    {
                        "name": "Manel Caballero-Pérez"
                    },
                    {
                        "name": "Marc Galobardes-Esteban"
                    },
                    {
                        "name": "Manuel Gamero-Castaño"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Gamero-Castaño"
                },
                "author": "Manuel Gamero-Castaño",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v1",
                "updated": "2025-03-31T12:32:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v1",
                "updated": "2025-03-31T11:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v2",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23897v1",
                "updated": "2025-03-31T09:46:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model"
                },
                "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."
                },
                "authors": [
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Bihan Wen"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v4",
                "updated": "2025-03-31T03:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    3,
                    28,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v2",
                "updated": "2025-03-31T02:19:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    19,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v2",
                "updated": "2025-03-30T11:14:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    14,
                    17,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages,fix figure mistake(inv/fwd skipping) in fig2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23397v1",
                "updated": "2025-03-30T11:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T11:09:06Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "title": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update"
                },
                "summary": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Wenhai Li"
                    },
                    {
                        "name": "Lingfeng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Deng"
                },
                "author": "Lingfeng Deng",
                "arxiv_comment": "14 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23388v1",
                "updated": "2025-03-30T10:34:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T10:34:45Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation"
                },
                "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
                },
                "authors": [
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Faisal Nadeem Khan"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v2",
                "updated": "2025-03-30T09:46:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    46,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v2",
                "updated": "2025-03-30T09:19:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    19,
                    53,
                    6,
                    89,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12820v2",
                "updated": "2025-03-30T08:13:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    13,
                    50,
                    6,
                    89,
                    0
                ],
                "published": "2024-07-01T13:05:42Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    5,
                    42,
                    0,
                    183,
                    0
                ],
                "title": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference"
                },
                "summary": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding."
                },
                "authors": [
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Xiaodong Ji"
                    },
                    {
                        "name": "Yilin Chen"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23294v1",
                "updated": "2025-03-30T03:20:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T03:20:34Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "title": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference"
                },
                "summary": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the Design, Automation, and Test in Europe 2025 (DATE\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v2",
                "updated": "2025-03-30T02:45:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    2,
                    45,
                    0,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v2",
                "updated": "2025-03-29T23:00:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    23,
                    0,
                    27,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v2",
                "updated": "2025-03-29T04:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    43,
                    11,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22796v1",
                "updated": "2025-03-28T18:00:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T18:00:12Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers"
                },
                "summary": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity."
                },
                "authors": [
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Rundong Su"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Mingzhu Shen Yibo Fan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v3",
                "updated": "2025-03-28T16:15:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    15,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v3",
                "updated": "2025-03-28T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    11,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22329v1",
                "updated": "2025-03-28T11:08:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:08:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "A Refined Analysis of Massive Activations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refined Analysis of Massive Activations in LLMs"
                },
                "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations."
                },
                "authors": [
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Fabian Güra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Güra"
                },
                "author": "Fabian Güra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22196v1",
                "updated": "2025-03-28T07:26:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T07:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices"
                },
                "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22017v1",
                "updated": "2025-03-27T22:16:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T22:16:57Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "title": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype"
                },
                "summary": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device."
                },
                "authors": [
                    {
                        "name": "Jianping Zeng"
                    },
                    {
                        "name": "Shuyi Pei"
                    },
                    {
                        "name": "Da Zhang"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Amir Beygi"
                    },
                    {
                        "name": "Xuebin Yao"
                    },
                    {
                        "name": "Ramdas Kachare"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Zongwang Li"
                    },
                    {
                        "name": "Marie Nguyen"
                    },
                    {
                        "name": "Rekha Pitchumani"
                    },
                    {
                        "name": "Yang Soek Ki"
                    },
                    {
                        "name": "Changhee Jung"
                    }
                ],
                "author_detail": {
                    "name": "Changhee Jung"
                },
                "author": "Changhee Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v1",
                "updated": "2025-03-27T17:37:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v4",
                "updated": "2025-03-27T15:21:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    21,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v2",
                "updated": "2025-03-27T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    11,
                    37,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v3",
                "updated": "2025-03-27T12:14:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    14,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v3",
                "updated": "2025-03-27T11:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    46,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v2",
                "updated": "2025-03-27T09:53:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    53,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/TVLSI.2025.3527225",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2025.3527225",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems (\n  Volume: 33, Issue: 4, April 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v2",
                "updated": "2025-03-27T07:02:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    2,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding"
                },
                "summary": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03708v1",
                "updated": "2025-03-27T00:10:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    0,
                    10,
                    40,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T00:10:40Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    0,
                    10,
                    40,
                    3,
                    86,
                    0
                ],
                "title": "Solving AI Foundational Model Latency with Telco Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving AI Foundational Model Latency with Telco Infrastructure"
                },
                "summary": "Latency remains a critical bottleneck for deploying foundational artificial\nintelligence (AI) models, such as large language models (LLMs), in\ncustomer-facing, real-time applications. While cloud-based inference offers\nscalability, it frequently introduces delays unacceptable for interactive\nexperiences, such as semantic search, personalized recommendations, or\nconversational interfaces. Telecommunications operators, historically adept at\nsolving content latency challenges through partnerships with providers like\nGoogle and Facebook, now have a unique opportunity to address similar AI\nlatency concerns. This paper presents a technical framework leveraging Telco\ninfrastructure-spanning regional data centers, existing content delivery\nnetwork (CDN) nodes, and near-radio access network (RAN) sites-as hierarchical\n\"AI edges\" for caching and partial inference. We explore the architectural\nfeasibility of embedding semantic and vector-based AI inference caches within\nexisting Telco assets, proposing tiered caching strategies and split-inference\narchitectures that significantly reduce latency and compute costs.\nAdditionally, we address technical challenges specific to Telcos, such as cache\nsynchronization, model distribution, privacy, and hardware acceleration\nconsiderations. Finally, we discuss viable partnership models between telcos\nand AI providers, highlighting how this innovative use of telco infrastructure\ncan unlock both improved AI user experience and new revenue streams.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency remains a critical bottleneck for deploying foundational artificial\nintelligence (AI) models, such as large language models (LLMs), in\ncustomer-facing, real-time applications. While cloud-based inference offers\nscalability, it frequently introduces delays unacceptable for interactive\nexperiences, such as semantic search, personalized recommendations, or\nconversational interfaces. Telecommunications operators, historically adept at\nsolving content latency challenges through partnerships with providers like\nGoogle and Facebook, now have a unique opportunity to address similar AI\nlatency concerns. This paper presents a technical framework leveraging Telco\ninfrastructure-spanning regional data centers, existing content delivery\nnetwork (CDN) nodes, and near-radio access network (RAN) sites-as hierarchical\n\"AI edges\" for caching and partial inference. We explore the architectural\nfeasibility of embedding semantic and vector-based AI inference caches within\nexisting Telco assets, proposing tiered caching strategies and split-inference\narchitectures that significantly reduce latency and compute costs.\nAdditionally, we address technical challenges specific to Telcos, such as cache\nsynchronization, model distribution, privacy, and hardware acceleration\nconsiderations. Finally, we discuss viable partnership models between telcos\nand AI providers, highlighting how this innovative use of telco infrastructure\ncan unlock both improved AI user experience and new revenue streams."
                },
                "authors": [
                    {
                        "name": "Sebastian Barros"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Barros"
                },
                "author": "Sebastian Barros",
                "arxiv_comment": "39 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v2",
                "updated": "2025-03-26T15:08:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    8,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qingxiang Lin"
                    },
                    {
                        "name": "Jingwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v3",
                "updated": "2025-03-26T13:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    59,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs"
                },
                "summary": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_doi": "10.1145/3710848.3710863",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3710848.3710863",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20481v1",
                "updated": "2025-03-26T12:10:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Analyzing Modern NVIDIA GPU cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Modern NVIDIA GPU cores"
                },
                "summary": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area."
                },
                "authors": [
                    {
                        "name": "Rodrigo Huerta"
                    },
                    {
                        "name": "Mojtaba Abaie Shoushtary"
                    },
                    {
                        "name": "José-Lorenzo Cruz"
                    },
                    {
                        "name": "Antonio González"
                    }
                ],
                "author_detail": {
                    "name": "Antonio González"
                },
                "author": "Antonio González",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v2",
                "updated": "2025-03-26T11:08:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    8,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20174v1",
                "updated": "2025-03-26T02:58:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T02:58:41Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "title": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration"
                },
                "summary": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Shihao Zhou"
                    },
                    {
                        "name": "Dayu Li"
                    },
                    {
                        "name": "Jinshan Pan"
                    },
                    {
                        "name": "Juncheng Zhou"
                    },
                    {
                        "name": "Jinglei Shi"
                    },
                    {
                        "name": "Jufeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Yang"
                },
                "author": "Jufeng Yang",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v2",
                "updated": "2025-03-26T01:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    58,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.07097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07097v1",
                "updated": "2025-04-09T17:59:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    59,
                    42,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:59:42Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    59,
                    42,
                    2,
                    99,
                    0
                ],
                "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual\n  Learning"
                },
                "summary": "Continual learning in large language models (LLMs) is prone to catastrophic\nforgetting, where adapting to new tasks significantly degrades performance on\npreviously learned ones. Existing methods typically rely on low-rank,\nparameter-efficient updates that limit the model's expressivity and introduce\nadditional parameters per task, leading to scalability issues. To address these\nlimitations, we propose a novel continual full fine-tuning approach leveraging\nadaptive singular value decomposition (SVD). Our method dynamically identifies\ntask-specific low-rank parameter subspaces and constrains updates to be\northogonal to critical directions associated with prior tasks, thus effectively\nminimizing interference without additional parameter overhead or storing\nprevious task gradients. We evaluate our approach extensively on standard\ncontinual learning benchmarks using both encoder-decoder (T5-Large) and\ndecoder-only (LLaMA-2 7B) models, spanning diverse tasks including\nclassification, generation, and reasoning. Empirically, our method achieves\nstate-of-the-art results, up to 7% higher average accuracy than recent\nbaselines like O-LoRA, and notably maintains the model's general linguistic\ncapabilities, instruction-following accuracy, and safety throughout the\ncontinual learning process by reducing forgetting to near-negligible levels.\nOur adaptive SVD framework effectively balances model plasticity and knowledge\nretention, providing a practical, theoretically grounded, and computationally\nscalable solution for continual learning scenarios in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning in large language models (LLMs) is prone to catastrophic\nforgetting, where adapting to new tasks significantly degrades performance on\npreviously learned ones. Existing methods typically rely on low-rank,\nparameter-efficient updates that limit the model's expressivity and introduce\nadditional parameters per task, leading to scalability issues. To address these\nlimitations, we propose a novel continual full fine-tuning approach leveraging\nadaptive singular value decomposition (SVD). Our method dynamically identifies\ntask-specific low-rank parameter subspaces and constrains updates to be\northogonal to critical directions associated with prior tasks, thus effectively\nminimizing interference without additional parameter overhead or storing\nprevious task gradients. We evaluate our approach extensively on standard\ncontinual learning benchmarks using both encoder-decoder (T5-Large) and\ndecoder-only (LLaMA-2 7B) models, spanning diverse tasks including\nclassification, generation, and reasoning. Empirically, our method achieves\nstate-of-the-art results, up to 7% higher average accuracy than recent\nbaselines like O-LoRA, and notably maintains the model's general linguistic\ncapabilities, instruction-following accuracy, and safety throughout the\ncontinual learning process by reducing forgetting to near-negligible levels.\nOur adaptive SVD framework effectively balances model plasticity and knowledge\nretention, providing a practical, theoretically grounded, and computationally\nscalable solution for continual learning scenarios in large language models."
                },
                "authors": [
                    {
                        "name": "Nikhil Shivakumar Nayak"
                    },
                    {
                        "name": "Krishnateja Killamsetty"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Abhishek Bhandwaldar"
                    },
                    {
                        "name": "Prateek Chanda"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Aldo Pareja"
                    },
                    {
                        "name": "Oleg Silkin"
                    },
                    {
                        "name": "Mustafa Eyceoz"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "arxiv_comment": "25 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07089v1",
                "updated": "2025-04-09T17:58:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    58,
                    58,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:58:58Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    58,
                    58,
                    2,
                    99,
                    0
                ],
                "title": "OmniCaptioner: One Captioner to Rule Them All",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCaptioner: One Captioner to Rule Them All"
                },
                "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities."
                },
                "authors": [
                    {
                        "name": "Yiting Lu"
                    },
                    {
                        "name": "Jiakang Yuan"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Shitian Zhao"
                    },
                    {
                        "name": "Qi Qin"
                    },
                    {
                        "name": "Xinyue Li"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Dongyang Liu"
                    },
                    {
                        "name": "Yuewen Cao"
                    },
                    {
                        "name": "Xiangchao Yan"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Zhibo Chen"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Peng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Peng Gao"
                },
                "author": "Peng Gao",
                "arxiv_comment": "More visualizations on Homepage:\n  https://alpha-innovator.github.io/OmniCaptioner-project-page and Official\n  code: https://github.com/Alpha-Innovator/OmniCaptioner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07087v1",
                "updated": "2025-04-09T17:58:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    58,
                    47,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:58:47Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    58,
                    47,
                    2,
                    99,
                    0
                ],
                "title": "KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on\n  Textualized Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on\n  Textualized Knowledge Graphs"
                },
                "summary": "Knowledge graphs have emerged as a popular method for injecting up-to-date,\nfactual knowledge into large language models (LLMs). This is typically achieved\nby converting the knowledge graph into text that the LLM can process in\ncontext. While multiple methods of encoding knowledge graphs have been\nproposed, the impact of this textualization process on LLM performance remains\nunder-explored. We introduce KG-LLM-Bench, a comprehensive and extensible\nbenchmark spanning five knowledge graph understanding tasks, and evaluate how\ndifferent encoding strategies affect performance across various base models.\nOur extensive experiments with seven language models and five textualization\nstrategies provide insights for optimizing LLM performance on KG reasoning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs have emerged as a popular method for injecting up-to-date,\nfactual knowledge into large language models (LLMs). This is typically achieved\nby converting the knowledge graph into text that the LLM can process in\ncontext. While multiple methods of encoding knowledge graphs have been\nproposed, the impact of this textualization process on LLM performance remains\nunder-explored. We introduce KG-LLM-Bench, a comprehensive and extensible\nbenchmark spanning five knowledge graph understanding tasks, and evaluate how\ndifferent encoding strategies affect performance across various base models.\nOur extensive experiments with seven language models and five textualization\nstrategies provide insights for optimizing LLM performance on KG reasoning\ntasks."
                },
                "authors": [
                    {
                        "name": "Elan Markowitz"
                    },
                    {
                        "name": "Krupa Galiya"
                    },
                    {
                        "name": "Greg Ver Steeg"
                    },
                    {
                        "name": "Aram Galstyan"
                    }
                ],
                "author_detail": {
                    "name": "Aram Galstyan"
                },
                "author": "Aram Galstyan",
                "arxiv_comment": "To be presented at NAACL-HLT, KnowledgeNLP Workshop (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07084v1",
                "updated": "2025-04-09T17:56:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    58,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:56:58Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    58,
                    2,
                    99,
                    0
                ],
                "title": "A geometric ensemble method for Bayesian inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A geometric ensemble method for Bayesian inference"
                },
                "summary": "Conventional approximations to Bayesian inference rely on either\napproximations by statistics such as mean and covariance or by point particles.\nRecent advances such as the ensemble Gaussian mixture filter have generalized\nthese notions to sums of parameterized distributions. This work presents a new\nmethodology for approximating Bayesian inference by sums of uniform\ndistributions on convex polytopes. The methodology presented herein is\ndeveloped from the simplest convex polytope filter that takes advantage of\nuniform prior and measurement uncertainty, to an operationally viable ensemble\nfilter with Kalmanized approximations to updating convex polytopes. Numerical\nresults on the Ikeda map show the viability of this methodology in the\nlow-dimensional setting, and numerical results on the Lorenz '96 equations\nsimilarly show viability in the high-dimensional setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional approximations to Bayesian inference rely on either\napproximations by statistics such as mean and covariance or by point particles.\nRecent advances such as the ensemble Gaussian mixture filter have generalized\nthese notions to sums of parameterized distributions. This work presents a new\nmethodology for approximating Bayesian inference by sums of uniform\ndistributions on convex polytopes. The methodology presented herein is\ndeveloped from the simplest convex polytope filter that takes advantage of\nuniform prior and measurement uncertainty, to an operationally viable ensemble\nfilter with Kalmanized approximations to updating convex polytopes. Numerical\nresults on the Ikeda map show the viability of this methodology in the\nlow-dimensional setting, and numerical results on the Lorenz '96 equations\nsimilarly show viability in the high-dimensional setting."
                },
                "authors": [
                    {
                        "name": "Andrey A Popov"
                    }
                ],
                "author_detail": {
                    "name": "Andrey A Popov"
                },
                "author": "Andrey A Popov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60G25, 62L12, 62M20, 93E11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07726v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07726v3",
                "updated": "2025-04-09T17:54:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    54,
                    25,
                    2,
                    99,
                    0
                ],
                "published": "2024-06-11T21:09:45Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    21,
                    9,
                    45,
                    1,
                    163,
                    0
                ],
                "title": "A Concise Mathematical Description of Active Inference in Discrete Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Concise Mathematical Description of Active Inference in Discrete Time"
                },
                "summary": "In this paper we present a concise mathematical description of active\ninference in discrete time. The main part of the paper serves as a basic\nintroduction to the topic, including a detailed example of the action selection\nmechanism. The appendix discusses the more subtle mathematical details,\ntargeting readers who have already studied the active inference literature but\nstruggle to make sense of the mathematical details and derivations. Throughout,\nwe emphasize precise and standard mathematical notation, ensuring consistency\nwith existing texts and linking all equations to widely used references on\nactive inference. Additionally, we provide Python code that implements the\naction selection and learning mechanisms described in this paper and is\ncompatible with pymdp environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we present a concise mathematical description of active\ninference in discrete time. The main part of the paper serves as a basic\nintroduction to the topic, including a detailed example of the action selection\nmechanism. The appendix discusses the more subtle mathematical details,\ntargeting readers who have already studied the active inference literature but\nstruggle to make sense of the mathematical details and derivations. Throughout,\nwe emphasize precise and standard mathematical notation, ensuring consistency\nwith existing texts and linking all equations to widely used references on\nactive inference. Additionally, we provide Python code that implements the\naction selection and learning mechanisms described in this paper and is\ncompatible with pymdp environments."
                },
                "authors": [
                    {
                        "name": "Jesse van Oostrum"
                    },
                    {
                        "name": "Carlotta Langer"
                    },
                    {
                        "name": "Nihat Ay"
                    }
                ],
                "author_detail": {
                    "name": "Nihat Ay"
                },
                "author": "Nihat Ay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07726v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07726v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07081v1",
                "updated": "2025-04-09T17:54:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    54,
                    22,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:54:22Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    54,
                    22,
                    2,
                    99,
                    0
                ],
                "title": "Self-Steering Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Steering Language Models"
                },
                "summary": "While test-time reasoning enables language models to tackle complex tasks,\nsearching or planning in natural language can be slow, costly, and error-prone.\nBut even when LMs struggle to emulate the precise reasoning steps needed to\nsolve a problem, they often excel at describing its abstract structure--both\nhow to verify solutions and how to search for them. This paper introduces\nDisCIPL, a method for \"self-steering\" LMs where a Planner model generates a\ntask-specific inference program that is executed by a population of Follower\nmodels. Our approach equips LMs with the ability to write recursive search\nprocedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models,\nincluding GPT-4o and o1, on challenging constrained generation tasks. In\ndecoupling planning from execution, our work opens up a design space of\nhighly-parallelized Monte Carlo inference strategies that outperform standard\nbest-of-N sampling, require no finetuning, and can be implemented automatically\nby existing LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While test-time reasoning enables language models to tackle complex tasks,\nsearching or planning in natural language can be slow, costly, and error-prone.\nBut even when LMs struggle to emulate the precise reasoning steps needed to\nsolve a problem, they often excel at describing its abstract structure--both\nhow to verify solutions and how to search for them. This paper introduces\nDisCIPL, a method for \"self-steering\" LMs where a Planner model generates a\ntask-specific inference program that is executed by a population of Follower\nmodels. Our approach equips LMs with the ability to write recursive search\nprocedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models,\nincluding GPT-4o and o1, on challenging constrained generation tasks. In\ndecoupling planning from execution, our work opens up a design space of\nhighly-parallelized Monte Carlo inference strategies that outperform standard\nbest-of-N sampling, require no finetuning, and can be implemented automatically\nby existing LMs."
                },
                "authors": [
                    {
                        "name": "Gabriel Grand"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Vikash K. Mansinghka"
                    },
                    {
                        "name": "Alexander K. Lew"
                    },
                    {
                        "name": "Jacob Andreas"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Andreas"
                },
                "author": "Jacob Andreas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07080v1",
                "updated": "2025-04-09T17:53:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    53,
                    55,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:53:55Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    53,
                    55,
                    2,
                    99,
                    0
                ],
                "title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning"
                },
                "summary": "Despite great performance on Olympiad-level reasoning problems, frontier\nlarge language models can still struggle on high school math when presented\nwith novel problems outside standard benchmarks. Going beyond final accuracy,\nwe propose a deductive consistency metric to analyze chain-of-thought output\nfrom language models (LMs).Formally, deductive reasoning involves two subtasks:\nunderstanding a set of input premises and inferring the conclusions that follow\nfrom them. The proposed metric studies LMs' performance on these subtasks, with\nthe goal of explaining LMs' reasoning errors on novel problems: how well do LMs\nunderstand input premises with increasing context lengths, and how well can\nthey infer conclusions over multiple reasoning hops? Since existing benchmarks\nmay be memorized, we develop a pipeline to evaluate LMs' deductive consistency\non novel, perturbed versions of benchmark problems. On novel grade school math\nproblems (GSM-8k), we find that LMs are fairly robust to increasing number of\ninput premises, but suffer significant accuracy decay as the number of\nreasoning hops is increased. Interestingly, these errors are masked in the\noriginal benchmark as all models achieve near 100% accuracy. As we increase the\nnumber of solution steps using a synthetic dataset, prediction over multiple\nhops still remains the major source of error compared to understanding input\npremises. Other factors, such as shifts in language style or natural\npropagation of early errors do not explain the trends. Our analysis provides a\nnew view to characterize LM reasoning -- as computations over a window of input\npremises and reasoning hops -- that can provide unified evaluation across\nproblem domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite great performance on Olympiad-level reasoning problems, frontier\nlarge language models can still struggle on high school math when presented\nwith novel problems outside standard benchmarks. Going beyond final accuracy,\nwe propose a deductive consistency metric to analyze chain-of-thought output\nfrom language models (LMs).Formally, deductive reasoning involves two subtasks:\nunderstanding a set of input premises and inferring the conclusions that follow\nfrom them. The proposed metric studies LMs' performance on these subtasks, with\nthe goal of explaining LMs' reasoning errors on novel problems: how well do LMs\nunderstand input premises with increasing context lengths, and how well can\nthey infer conclusions over multiple reasoning hops? Since existing benchmarks\nmay be memorized, we develop a pipeline to evaluate LMs' deductive consistency\non novel, perturbed versions of benchmark problems. On novel grade school math\nproblems (GSM-8k), we find that LMs are fairly robust to increasing number of\ninput premises, but suffer significant accuracy decay as the number of\nreasoning hops is increased. Interestingly, these errors are masked in the\noriginal benchmark as all models achieve near 100% accuracy. As we increase the\nnumber of solution steps using a synthetic dataset, prediction over multiple\nhops still remains the major source of error compared to understanding input\npremises. Other factors, such as shifts in language style or natural\npropagation of early errors do not explain the trends. Our analysis provides a\nnew view to characterize LM reasoning -- as computations over a window of input\npremises and reasoning hops -- that can provide unified evaluation across\nproblem domains."
                },
                "authors": [
                    {
                        "name": "Atharva Pandey"
                    },
                    {
                        "name": "Kshitij Dubey"
                    },
                    {
                        "name": "Rahul Sharma"
                    },
                    {
                        "name": "Amit Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Amit Sharma"
                },
                "author": "Amit Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07070v1",
                "updated": "2025-04-09T17:39:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    39,
                    58,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:39:58Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    39,
                    58,
                    2,
                    99,
                    0
                ],
                "title": "A Survey on Personalized and Pluralistic Preference Alignment in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Personalized and Pluralistic Preference Alignment in Large\n  Language Models"
                },
                "summary": "Personalized preference alignment for large language models (LLMs), the\nprocess of tailoring LLMs to individual users' preferences, is an emerging\nresearch direction spanning the area of NLP and personalization. In this\nsurvey, we present an analysis of works on personalized alignment and modeling\nfor LLMs. We introduce a taxonomy of preference alignment techniques, including\ntraining time, inference time, and additionally, user-modeling based methods.\nWe provide analysis and discussion on the strengths and limitations of each\ngroup of techniques and then cover evaluation, benchmarks, as well as open\nproblems in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized preference alignment for large language models (LLMs), the\nprocess of tailoring LLMs to individual users' preferences, is an emerging\nresearch direction spanning the area of NLP and personalization. In this\nsurvey, we present an analysis of works on personalized alignment and modeling\nfor LLMs. We introduce a taxonomy of preference alignment techniques, including\ntraining time, inference time, and additionally, user-modeling based methods.\nWe provide analysis and discussion on the strengths and limitations of each\ngroup of techniques and then cover evaluation, benchmarks, as well as open\nproblems in the field."
                },
                "authors": [
                    {
                        "name": "Zhouhang Xie"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yiran Shen"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Xintong Li"
                    },
                    {
                        "name": "Aaron Chang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Sachin Kumar"
                    },
                    {
                        "name": "Bodhisattwa Prasad Majumder"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Prithviraj Ammanabrolu"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07069v1",
                "updated": "2025-04-09T17:39:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    39,
                    41,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:39:41Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    39,
                    41,
                    2,
                    99,
                    0
                ],
                "title": "HalluciNot: Hallucination Detection Through Context and Common Knowledge\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HalluciNot: Hallucination Detection Through Context and Common Knowledge\n  Verification"
                },
                "summary": "This paper introduces a comprehensive system for detecting hallucinations in\nlarge language model (LLM) outputs in enterprise settings. We present a novel\ntaxonomy of LLM responses specific to hallucination in enterprise applications,\ncategorizing them into context-based, common knowledge, enterprise-specific,\nand innocuous statements. Our hallucination detection model HDM-2 validates LLM\nresponses with respect to both context and generally known facts (common\nknowledge). It provides both hallucination scores and word-level annotations,\nenabling precise identification of problematic content. To evaluate it on\ncontext-based and common-knowledge hallucinations, we introduce a new dataset\nHDMBench. Experimental results demonstrate that HDM-2 out-performs existing\napproaches across RagTruth, TruthfulQA, and HDMBench datasets. This work\naddresses the specific challenges of enterprise deployment, including\ncomputational efficiency, domain specialization, and fine-grained error\nidentification. Our evaluation dataset, model weights, and inference code are\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a comprehensive system for detecting hallucinations in\nlarge language model (LLM) outputs in enterprise settings. We present a novel\ntaxonomy of LLM responses specific to hallucination in enterprise applications,\ncategorizing them into context-based, common knowledge, enterprise-specific,\nand innocuous statements. Our hallucination detection model HDM-2 validates LLM\nresponses with respect to both context and generally known facts (common\nknowledge). It provides both hallucination scores and word-level annotations,\nenabling precise identification of problematic content. To evaluate it on\ncontext-based and common-knowledge hallucinations, we introduce a new dataset\nHDMBench. Experimental results demonstrate that HDM-2 out-performs existing\napproaches across RagTruth, TruthfulQA, and HDMBench datasets. This work\naddresses the specific challenges of enterprise deployment, including\ncomputational efficiency, domain specialization, and fine-grained error\nidentification. Our evaluation dataset, model weights, and inference code are\npublicly available."
                },
                "authors": [
                    {
                        "name": "Bibek Paudel"
                    },
                    {
                        "name": "Alexander Lyzhov"
                    },
                    {
                        "name": "Preetam Joshi"
                    },
                    {
                        "name": "Puneet Anand"
                    }
                ],
                "author_detail": {
                    "name": "Puneet Anand"
                },
                "author": "Puneet Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02511v2",
                "updated": "2025-04-09T17:34:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    34,
                    52,
                    2,
                    99,
                    0
                ],
                "published": "2024-06-20T01:24:30Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    1,
                    24,
                    30,
                    3,
                    172,
                    0
                ],
                "title": "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on\n  Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on\n  Path Planning"
                },
                "summary": "Path planning is a fundamental scientific problem in robotics and autonomous\nnavigation, requiring the derivation of efficient routes from starting to\ndestination points while avoiding obstacles. Traditional algorithms like A* and\nits variants are capable of ensuring path validity but suffer from significant\ncomputational and memory inefficiencies as the state space grows. Conversely,\nlarge language models (LLMs) excel in broader environmental analysis through\ncontextual understanding, providing global insights into environments. However,\nthey fall short in detailed spatial and temporal reasoning, often leading to\ninvalid or inefficient routes. In this work, we propose LLM-A*, an new LLM\nbased route planning method that synergistically combines the precise\npathfinding capabilities of A* with the global reasoning capability of LLMs.\nThis hybrid approach aims to enhance pathfinding efficiency in terms of time\nand space complexity while maintaining the integrity of path validity,\nespecially in large-scale scenarios. By integrating the strengths of both\nmethodologies, LLM-A* addresses the computational and memory limitations of\nconventional algorithms without compromising on the validity required for\neffective pathfinding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path planning is a fundamental scientific problem in robotics and autonomous\nnavigation, requiring the derivation of efficient routes from starting to\ndestination points while avoiding obstacles. Traditional algorithms like A* and\nits variants are capable of ensuring path validity but suffer from significant\ncomputational and memory inefficiencies as the state space grows. Conversely,\nlarge language models (LLMs) excel in broader environmental analysis through\ncontextual understanding, providing global insights into environments. However,\nthey fall short in detailed spatial and temporal reasoning, often leading to\ninvalid or inefficient routes. In this work, we propose LLM-A*, an new LLM\nbased route planning method that synergistically combines the precise\npathfinding capabilities of A* with the global reasoning capability of LLMs.\nThis hybrid approach aims to enhance pathfinding efficiency in terms of time\nand space complexity while maintaining the integrity of path validity,\nespecially in large-scale scenarios. By integrating the strengths of both\nmethodologies, LLM-A* addresses the computational and memory limitations of\nconventional algorithms without compromising on the validity required for\neffective pathfinding."
                },
                "authors": [
                    {
                        "name": "Silin Meng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Cheng-Fu Yang"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "arxiv_comment": "Findings of the Association for Computational Linguistics: EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07055v1",
                "updated": "2025-04-09T17:16:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    16,
                    23,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:16:23Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    16,
                    23,
                    2,
                    99,
                    0
                ],
                "title": "$Π$-NeSy: A Possibilistic Neuro-Symbolic Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$Π$-NeSy: A Possibilistic Neuro-Symbolic Approach"
                },
                "summary": "In this article, we introduce a neuro-symbolic approach that combines a\nlow-level perception task performed by a neural network with a high-level\nreasoning task performed by a possibilistic rule-based system. The goal is to\nbe able to derive for each input instance the degree of possibility that it\nbelongs to a target (meta-)concept. This (meta-)concept is connected to\nintermediate concepts by a possibilistic rule-based system. The probability of\neach intermediate concept for the input instance is inferred using a neural\nnetwork. The connection between the low-level perception task and the\nhigh-level reasoning task lies in the transformation of neural network outputs\nmodeled by probability distributions (through softmax activation) into\npossibility distributions. The use of intermediate concepts is valuable for the\nexplanation purpose: using the rule-based system, the classification of an\ninput instance as an element of the (meta-)concept can be justified by the fact\nthat intermediate concepts have been recognized.\n  From the technical side, our contribution consists of the design of efficient\nmethods for defining the matrix relation and the equation system associated\nwith a possibilistic rule-based system. The corresponding matrix and equation\nare key data structures used to perform inferences from a possibilistic\nrule-based system and to learn the values of the rule parameters in such a\nsystem according to a training data sample. Furthermore, leveraging recent\nresults on the handling of inconsistent systems of fuzzy relational equations,\nan approach for learning rule parameters according to multiple training data\nsamples is presented. Experiments carried out on the MNIST addition problems\nand the MNIST Sudoku puzzles problems highlight the effectiveness of our\napproach compared with state-of-the-art neuro-symbolic ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we introduce a neuro-symbolic approach that combines a\nlow-level perception task performed by a neural network with a high-level\nreasoning task performed by a possibilistic rule-based system. The goal is to\nbe able to derive for each input instance the degree of possibility that it\nbelongs to a target (meta-)concept. This (meta-)concept is connected to\nintermediate concepts by a possibilistic rule-based system. The probability of\neach intermediate concept for the input instance is inferred using a neural\nnetwork. The connection between the low-level perception task and the\nhigh-level reasoning task lies in the transformation of neural network outputs\nmodeled by probability distributions (through softmax activation) into\npossibility distributions. The use of intermediate concepts is valuable for the\nexplanation purpose: using the rule-based system, the classification of an\ninput instance as an element of the (meta-)concept can be justified by the fact\nthat intermediate concepts have been recognized.\n  From the technical side, our contribution consists of the design of efficient\nmethods for defining the matrix relation and the equation system associated\nwith a possibilistic rule-based system. The corresponding matrix and equation\nare key data structures used to perform inferences from a possibilistic\nrule-based system and to learn the values of the rule parameters in such a\nsystem according to a training data sample. Furthermore, leveraging recent\nresults on the handling of inconsistent systems of fuzzy relational equations,\nan approach for learning rule parameters according to multiple training data\nsamples is presented. Experiments carried out on the MNIST addition problems\nand the MNIST Sudoku puzzles problems highlight the effectiveness of our\napproach compared with state-of-the-art neuro-symbolic ones."
                },
                "authors": [
                    {
                        "name": "Ismaïl Baaj"
                    },
                    {
                        "name": "Pierre Marquis"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Marquis"
                },
                "author": "Pierre Marquis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04277v2",
                "updated": "2025-04-09T17:15:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    15,
                    47,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-05T20:35:54Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    20,
                    35,
                    54,
                    5,
                    95,
                    0
                ],
                "title": "Beyond the Hype: Embeddings vs. Prompting for Multiclass Classification\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Hype: Embeddings vs. Prompting for Multiclass Classification\n  Tasks"
                },
                "summary": "Are traditional classification approaches irrelevant in this era of AI hype?\nWe show that there are multiclass classification problems where predictive\nmodels holistically outperform LLM prompt-based frameworks. Given text and\nimages from home-service project descriptions provided by Thumbtack customers,\nwe build embeddings-based softmax models that predict the professional category\n(e.g., handyman, bathroom remodeling) associated with each problem description.\nWe then compare against prompts that ask state-of-the-art LLM models to solve\nthe same problem. We find that the embeddings approach outperforms the best LLM\nprompts in terms of accuracy, calibration, latency, and financial cost. In\nparticular, the embeddings approach has 49.5% higher accuracy than the\nprompting approach, and its superiority is consistent across text-only,\nimage-only, and text-image problem descriptions. Furthermore, it yields\nwell-calibrated probabilities, which we later use as confidence signals to\nprovide contextualized user experience during deployment. On the contrary,\nprompting scores are overly uninformative. Finally, the embeddings approach is\n14 and 81 times faster than prompting in processing images and text\nrespectively, while under realistic deployment assumptions, it can be up to 10\ntimes cheaper. Based on these results, we deployed a variation of the\nembeddings approach, and through A/B testing we observed performance consistent\nwith our offline analysis. Our study shows that for multiclass classification\nproblems that can leverage proprietary datasets, an embeddings-based approach\nmay yield unequivocally better results. Hence, scientists, practitioners,\nengineers, and business leaders can use our study to go beyond the hype and\nconsider appropriate predictive models for their classification use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are traditional classification approaches irrelevant in this era of AI hype?\nWe show that there are multiclass classification problems where predictive\nmodels holistically outperform LLM prompt-based frameworks. Given text and\nimages from home-service project descriptions provided by Thumbtack customers,\nwe build embeddings-based softmax models that predict the professional category\n(e.g., handyman, bathroom remodeling) associated with each problem description.\nWe then compare against prompts that ask state-of-the-art LLM models to solve\nthe same problem. We find that the embeddings approach outperforms the best LLM\nprompts in terms of accuracy, calibration, latency, and financial cost. In\nparticular, the embeddings approach has 49.5% higher accuracy than the\nprompting approach, and its superiority is consistent across text-only,\nimage-only, and text-image problem descriptions. Furthermore, it yields\nwell-calibrated probabilities, which we later use as confidence signals to\nprovide contextualized user experience during deployment. On the contrary,\nprompting scores are overly uninformative. Finally, the embeddings approach is\n14 and 81 times faster than prompting in processing images and text\nrespectively, while under realistic deployment assumptions, it can be up to 10\ntimes cheaper. Based on these results, we deployed a variation of the\nembeddings approach, and through A/B testing we observed performance consistent\nwith our offline analysis. Our study shows that for multiclass classification\nproblems that can leverage proprietary datasets, an embeddings-based approach\nmay yield unequivocally better results. Hence, scientists, practitioners,\nengineers, and business leaders can use our study to go beyond the hype and\nconsider appropriate predictive models for their classification use cases."
                },
                "authors": [
                    {
                        "name": "Marios Kokkodis"
                    },
                    {
                        "name": "Richard Demsyn-Jones"
                    },
                    {
                        "name": "Vijay Raghavan"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Raghavan"
                },
                "author": "Vijay Raghavan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07053v1",
                "updated": "2025-04-09T17:14:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    14,
                    33,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:14:33Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    14,
                    33,
                    2,
                    99,
                    0
                ],
                "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken\n  Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken\n  Language Modeling"
                },
                "summary": "Large Language Models (LLMs) excel in text-based natural language processing\ntasks but remain constrained by their reliance on textual inputs and outputs.\nTo enable more natural human-LLM interaction, recent progress have focused on\nderiving a spoken language model (SLM) that can not only listen but also\ngenerate speech. To achieve this, a promising direction is to conduct\nspeech-text joint modeling. However, recent SLM still lag behind text LLM due\nto the modality mismatch. One significant mismatch can be the sequence lengths\nbetween speech and text tokens. To address this, we introduce Text-Aligned\nSpeech Tokenization and Embedding (TASTE), a method that directly addresses the\nmodality gap by aligning speech token with the corresponding text transcription\nduring the tokenization stage. We propose a method that can achieve this\nthrough the special aggregation mechanism and with speech reconstruction as the\ntraining objective. We conduct extensive experiments and show that TASTE can\npreserve essential paralinguistic information while dramatically reducing the\ntoken sequence length. Furthermore, by leveraging TASTE, we can adapt\ntext-based LLMs into effective SLMs with parameter-efficient fine-tuning\ntechniques such as Low-Rank Adaptation (LoRA). Experimental results on\nbenchmark tasks, including SALMON and StoryCloze, demonstrate that TASTE-based\nSLMs perform similarly to previous full-finetuning methods. To our knowledge,\nTASTE is the first end-to-end approach that utilizes a reconstruction objective\nto automatically learn a text-aligned speech tokenization and embedding\nsuitable for spoken language modeling. Our demo, code, and models are publicly\navailable at https://github.com/mtkresearch/TASTE-SpokenLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in text-based natural language processing\ntasks but remain constrained by their reliance on textual inputs and outputs.\nTo enable more natural human-LLM interaction, recent progress have focused on\nderiving a spoken language model (SLM) that can not only listen but also\ngenerate speech. To achieve this, a promising direction is to conduct\nspeech-text joint modeling. However, recent SLM still lag behind text LLM due\nto the modality mismatch. One significant mismatch can be the sequence lengths\nbetween speech and text tokens. To address this, we introduce Text-Aligned\nSpeech Tokenization and Embedding (TASTE), a method that directly addresses the\nmodality gap by aligning speech token with the corresponding text transcription\nduring the tokenization stage. We propose a method that can achieve this\nthrough the special aggregation mechanism and with speech reconstruction as the\ntraining objective. We conduct extensive experiments and show that TASTE can\npreserve essential paralinguistic information while dramatically reducing the\ntoken sequence length. Furthermore, by leveraging TASTE, we can adapt\ntext-based LLMs into effective SLMs with parameter-efficient fine-tuning\ntechniques such as Low-Rank Adaptation (LoRA). Experimental results on\nbenchmark tasks, including SALMON and StoryCloze, demonstrate that TASTE-based\nSLMs perform similarly to previous full-finetuning methods. To our knowledge,\nTASTE is the first end-to-end approach that utilizes a reconstruction objective\nto automatically learn a text-aligned speech tokenization and embedding\nsuitable for spoken language modeling. Our demo, code, and models are publicly\navailable at https://github.com/mtkresearch/TASTE-SpokenLM."
                },
                "authors": [
                    {
                        "name": "Liang-Hsuan Tseng"
                    },
                    {
                        "name": "Yi-Chang Chen"
                    },
                    {
                        "name": "Kuan-Yi Lee"
                    },
                    {
                        "name": "Da-Shan Shiu"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Preprint. Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.20331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.20331v2",
                "updated": "2025-04-09T17:13:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    13,
                    27,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-29T17:59:53Z",
                "published_parsed": [
                    2024,
                    3,
                    29,
                    17,
                    59,
                    53,
                    4,
                    89,
                    0
                ],
                "title": "Unsolvable Problem Detection: Robust Understanding Evaluation for Large\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsolvable Problem Detection: Robust Understanding Evaluation for Large\n  Multimodal Models"
                },
                "summary": "This paper introduces a novel task to evaluate the robust understanding\ncapability of Large Multimodal Models (LMMs), termed $\\textbf{Unsolvable\nProblem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely\nused to assess the understanding capability of LMMs, but it does not guarantee\nthat LMMs truly comprehend the answer. UPD assesses the LMM's ability to\nwithhold answers when encountering unsolvable problems of MCQA, verifying\nwhether the model truly understands the answer. UPD encompasses three problems:\nAbsent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and\nIncompatible Visual Question Detection (IVQD), covering unsolvable cases like\nanswer-lacking or incompatible choices and image-question mismatches. For the\nevaluation, we introduce the MM-UPD Bench, a benchmark for assessing\nperformance across various ability dimensions. Our experiments reveal that even\nmost LMMs, which demonstrate adequate performance on existing benchmarks,\nstruggle significantly with MM-UPD, underscoring a novel aspect of\ntrustworthiness that current benchmarks have overlooked. A detailed analysis\nshows that LMMs have different bottlenecks and chain-of-thought and\nself-reflection improved performance for LMMs with the bottleneck in their LLM\ncapability. We hope our insights will enhance the broader understanding and\ndevelopment of more reliable LMMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel task to evaluate the robust understanding\ncapability of Large Multimodal Models (LMMs), termed $\\textbf{Unsolvable\nProblem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely\nused to assess the understanding capability of LMMs, but it does not guarantee\nthat LMMs truly comprehend the answer. UPD assesses the LMM's ability to\nwithhold answers when encountering unsolvable problems of MCQA, verifying\nwhether the model truly understands the answer. UPD encompasses three problems:\nAbsent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and\nIncompatible Visual Question Detection (IVQD), covering unsolvable cases like\nanswer-lacking or incompatible choices and image-question mismatches. For the\nevaluation, we introduce the MM-UPD Bench, a benchmark for assessing\nperformance across various ability dimensions. Our experiments reveal that even\nmost LMMs, which demonstrate adequate performance on existing benchmarks,\nstruggle significantly with MM-UPD, underscoring a novel aspect of\ntrustworthiness that current benchmarks have overlooked. A detailed analysis\nshows that LMMs have different bottlenecks and chain-of-thought and\nself-reflection improved performance for LMMs with the bottleneck in their LLM\ncapability. We hope our insights will enhance the broader understanding and\ndevelopment of more reliable LMMs."
                },
                "authors": [
                    {
                        "name": "Atsuyuki Miyai"
                    },
                    {
                        "name": "Jingkang Yang"
                    },
                    {
                        "name": "Jingyang Zhang"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Qing Yu"
                    },
                    {
                        "name": "Go Irie"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Hai Li"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kiyoharu Aizawa"
                    }
                ],
                "author_detail": {
                    "name": "Kiyoharu Aizawa"
                },
                "author": "Kiyoharu Aizawa",
                "arxiv_comment": "Code: https://github.com/AtsuMiyai/UPD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.20331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.20331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07052v1",
                "updated": "2025-04-09T17:12:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    12,
                    49,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:12:49Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    12,
                    49,
                    2,
                    99,
                    0
                ],
                "title": "To Backtrack or Not to Backtrack: When Sequential Search Limits Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Backtrack or Not to Backtrack: When Sequential Search Limits Model\n  Reasoning"
                },
                "summary": "Recent advancements in large language models have significantly improved\ntheir reasoning abilities, particularly through techniques involving search and\nbacktracking. Backtracking naturally scales test-time compute by enabling\nsequential, linearized exploration via long chain-of-thought (CoT) generation.\nHowever, this is not the only strategy for scaling test-time compute: parallel\nsampling with best-of-n selection provides an alternative that generates\ndiverse solutions simultaneously. Despite the growing adoption of sequential\nsearch, its advantages over parallel sampling--especially under a fixed compute\nbudget remain poorly understood. In this paper, we systematically compare these\ntwo approaches on two challenging reasoning tasks: CountDown and Sudoku.\nSurprisingly, we find that sequential search underperforms parallel sampling on\nCountDown but outperforms it on Sudoku, suggesting that backtracking is not\nuniversally beneficial. We identify two factors that can cause backtracking to\ndegrade performance: (1) training on fixed search traces can lock models into\nsuboptimal strategies, and (2) explicit CoT supervision can discourage\n\"implicit\" (non-verbalized) reasoning. Extending our analysis to reinforcement\nlearning (RL), we show that models with backtracking capabilities benefit\nsignificantly from RL fine-tuning, while models without backtracking see\nlimited, mixed gains. Together, these findings challenge the assumption that\nbacktracking universally enhances LLM reasoning, instead revealing a complex\ninteraction between task structure, training data, model scale, and learning\nparadigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models have significantly improved\ntheir reasoning abilities, particularly through techniques involving search and\nbacktracking. Backtracking naturally scales test-time compute by enabling\nsequential, linearized exploration via long chain-of-thought (CoT) generation.\nHowever, this is not the only strategy for scaling test-time compute: parallel\nsampling with best-of-n selection provides an alternative that generates\ndiverse solutions simultaneously. Despite the growing adoption of sequential\nsearch, its advantages over parallel sampling--especially under a fixed compute\nbudget remain poorly understood. In this paper, we systematically compare these\ntwo approaches on two challenging reasoning tasks: CountDown and Sudoku.\nSurprisingly, we find that sequential search underperforms parallel sampling on\nCountDown but outperforms it on Sudoku, suggesting that backtracking is not\nuniversally beneficial. We identify two factors that can cause backtracking to\ndegrade performance: (1) training on fixed search traces can lock models into\nsuboptimal strategies, and (2) explicit CoT supervision can discourage\n\"implicit\" (non-verbalized) reasoning. Extending our analysis to reinforcement\nlearning (RL), we show that models with backtracking capabilities benefit\nsignificantly from RL fine-tuning, while models without backtracking see\nlimited, mixed gains. Together, these findings challenge the assumption that\nbacktracking universally enhances LLM reasoning, instead revealing a complex\ninteraction between task structure, training data, model scale, and learning\nparadigm."
                },
                "authors": [
                    {
                        "name": "Tian Qin"
                    },
                    {
                        "name": "David Alvarez-Melis"
                    },
                    {
                        "name": "Samy Jelassi"
                    },
                    {
                        "name": "Eran Malach"
                    }
                ],
                "author_detail": {
                    "name": "Eran Malach"
                },
                "author": "Eran Malach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21936v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21936v3",
                "updated": "2025-04-09T16:49:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    49,
                    15,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-27T19:26:33Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    19,
                    26,
                    33,
                    3,
                    86,
                    0
                ],
                "title": "Binarity at LOw Metallicity (BLOeM): Enhanced multiplicity of early\n  B-type dwarfs and giants at $Z=0.2\\,{\\rm Z}_\\odot$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binarity at LOw Metallicity (BLOeM): Enhanced multiplicity of early\n  B-type dwarfs and giants at $Z=0.2\\,{\\rm Z}_\\odot$"
                },
                "summary": "Early B-type stars ($M_i=8-15$ M$_\\odot$) are frequently in multiple systems,\nas evidenced by spectroscopic campaigns in the Milky Way (MW) and the Large\nMagellanic Cloud (LMC). Previous studies have shown no strong metallicity\ndependence in the close-binary (a>10 au) fraction or orbital-period\ndistributions between the MW's solar metallicity (Z$_\\odot$) and that of the\nLMC (Z=0.5 Z$_\\odot$). However, similar analyses in more metal-poor\nenvironments are still scarce. We focus on 309 early B-type stars (luminosity\nclasses III-V) from the Binarity at LOw Metallicity campaign in the Small\nMagellanic Cloud (SMC, Z=0.2 Z$_\\odot$) using VLT/FLAMES multi-epoch\nspectroscopy. By applying binary detection criteria consistent with previous\nworks, we identify 153 stars (91 SB1, 59 SB2, 3 SB3) exhibiting significant\nradial-velocity (RV) variations, resulting in an observed multiplicity fraction\nof $f^{obs}_{mult}=50\\pm3\\%$. Using Monte Carlo simulations to account for\nobservational biases, we infer an intrinsic close-binary fraction of\n$f_{mult}=80\\pm8\\%$. A Markov chain Monte Carlo analysis of the peak-to-peak RV\ndistribution ($\\Delta{\\rm RV}_{\\rm max}$) confirms a high multiplicity fraction\nof $f_{mult}=78\\pm5\\%$. These findings suggest a possible anti-correlation\nbetween metallicity and the fraction of close B-type binaries, with the SMC\nmultiplicity fraction significantly exceeding previous measurements in the LMC\nand MW. The enhanced fraction of close binaries at SMC's low metallicity may\nhave broad implications for massive-star evolution in the early Universe. More\nfrequent mass transfer and envelope stripping could boost the production of\nexotic transients, stripped supernovae, gravitational-wave progenitors, and\nsustained UV ionising flux, potentially affecting cosmic reionisation.\nTheoretical predictions of binary evolution under metal-poor conditions will\nprovide a key test of our results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early B-type stars ($M_i=8-15$ M$_\\odot$) are frequently in multiple systems,\nas evidenced by spectroscopic campaigns in the Milky Way (MW) and the Large\nMagellanic Cloud (LMC). Previous studies have shown no strong metallicity\ndependence in the close-binary (a>10 au) fraction or orbital-period\ndistributions between the MW's solar metallicity (Z$_\\odot$) and that of the\nLMC (Z=0.5 Z$_\\odot$). However, similar analyses in more metal-poor\nenvironments are still scarce. We focus on 309 early B-type stars (luminosity\nclasses III-V) from the Binarity at LOw Metallicity campaign in the Small\nMagellanic Cloud (SMC, Z=0.2 Z$_\\odot$) using VLT/FLAMES multi-epoch\nspectroscopy. By applying binary detection criteria consistent with previous\nworks, we identify 153 stars (91 SB1, 59 SB2, 3 SB3) exhibiting significant\nradial-velocity (RV) variations, resulting in an observed multiplicity fraction\nof $f^{obs}_{mult}=50\\pm3\\%$. Using Monte Carlo simulations to account for\nobservational biases, we infer an intrinsic close-binary fraction of\n$f_{mult}=80\\pm8\\%$. A Markov chain Monte Carlo analysis of the peak-to-peak RV\ndistribution ($\\Delta{\\rm RV}_{\\rm max}$) confirms a high multiplicity fraction\nof $f_{mult}=78\\pm5\\%$. These findings suggest a possible anti-correlation\nbetween metallicity and the fraction of close B-type binaries, with the SMC\nmultiplicity fraction significantly exceeding previous measurements in the LMC\nand MW. The enhanced fraction of close binaries at SMC's low metallicity may\nhave broad implications for massive-star evolution in the early Universe. More\nfrequent mass transfer and envelope stripping could boost the production of\nexotic transients, stripped supernovae, gravitational-wave progenitors, and\nsustained UV ionising flux, potentially affecting cosmic reionisation.\nTheoretical predictions of binary evolution under metal-poor conditions will\nprovide a key test of our results."
                },
                "authors": [
                    {
                        "name": "J. I. Villaseñor"
                    },
                    {
                        "name": "H. Sana"
                    },
                    {
                        "name": "L. Mahy"
                    },
                    {
                        "name": "T. Shenar"
                    },
                    {
                        "name": "J. Bodensteiner"
                    },
                    {
                        "name": "N. Britavskiy"
                    },
                    {
                        "name": "D. J. Lennon"
                    },
                    {
                        "name": "M. Moe"
                    },
                    {
                        "name": "L. R. Patrick"
                    },
                    {
                        "name": "M. Pawlak"
                    },
                    {
                        "name": "D. M. Bowman"
                    },
                    {
                        "name": "P. A. Crowther"
                    },
                    {
                        "name": "S. E. de Mink"
                    },
                    {
                        "name": "K. Deshmukh"
                    },
                    {
                        "name": "C. J. Evans"
                    },
                    {
                        "name": "M. Fabry"
                    },
                    {
                        "name": "M. Fouesneau"
                    },
                    {
                        "name": "G. Holgado"
                    },
                    {
                        "name": "N. Langer"
                    },
                    {
                        "name": "J. Maíz Apellániz"
                    },
                    {
                        "name": "I. Mandel"
                    },
                    {
                        "name": "L. M. Oskinova"
                    },
                    {
                        "name": "D. Pauli"
                    },
                    {
                        "name": "V. Ramachandran"
                    },
                    {
                        "name": "M. Renzo"
                    },
                    {
                        "name": "H. -W. Rix"
                    },
                    {
                        "name": "D. F. Rocha"
                    },
                    {
                        "name": "A. A. C. S. Sander"
                    },
                    {
                        "name": "F. R. N. Schneider"
                    },
                    {
                        "name": "K. Sen"
                    },
                    {
                        "name": "S. Simón-Díaz"
                    },
                    {
                        "name": "J. Th. van Loon"
                    },
                    {
                        "name": "S. Toonen"
                    },
                    {
                        "name": "J. S. Vink"
                    }
                ],
                "author_detail": {
                    "name": "J. S. Vink"
                },
                "author": "J. S. Vink",
                "arxiv_comment": "16 pages, 18 figures (additional 10 pages and 7 figures in\n  appendices). Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21936v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21936v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07029v1",
                "updated": "2025-04-09T16:44:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    44,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T16:44:19Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    44,
                    19,
                    2,
                    99,
                    0
                ],
                "title": "Distilling Textual Priors from LLM to Efficient Image Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Textual Priors from LLM to Efficient Image Fusion"
                },
                "summary": "Multi-modality image fusion aims to synthesize a single, comprehensive image\nfrom multiple source inputs. Traditional approaches, such as CNNs and GANs,\noffer efficiency but struggle to handle low-quality or complex inputs. Recent\nadvances in text-guided methods leverage large model priors to overcome these\nlimitations, but at the cost of significant computational overhead, both in\nmemory and inference time. To address this challenge, we propose a novel\nframework for distilling large model priors, eliminating the need for text\nguidance during inference while dramatically reducing model size. Our framework\nutilizes a teacher-student architecture, where the teacher network incorporates\nlarge model priors and transfers this knowledge to a smaller student network\nvia a tailored distillation process. Additionally, we introduce spatial-channel\ncross-fusion module to enhance the model's ability to leverage textual priors\nacross both spatial and channel dimensions. Our method achieves a favorable\ntrade-off between computational efficiency and fusion quality. The distilled\nnetwork, requiring only 10\\% of the parameters and inference time of the\nteacher network, retains 90\\% of its performance and outperforms existing SOTA\nmethods. Extensive experiments demonstrate the effectiveness of our approach.\nThe implementation will be made publicly available as an open-source resource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modality image fusion aims to synthesize a single, comprehensive image\nfrom multiple source inputs. Traditional approaches, such as CNNs and GANs,\noffer efficiency but struggle to handle low-quality or complex inputs. Recent\nadvances in text-guided methods leverage large model priors to overcome these\nlimitations, but at the cost of significant computational overhead, both in\nmemory and inference time. To address this challenge, we propose a novel\nframework for distilling large model priors, eliminating the need for text\nguidance during inference while dramatically reducing model size. Our framework\nutilizes a teacher-student architecture, where the teacher network incorporates\nlarge model priors and transfers this knowledge to a smaller student network\nvia a tailored distillation process. Additionally, we introduce spatial-channel\ncross-fusion module to enhance the model's ability to leverage textual priors\nacross both spatial and channel dimensions. Our method achieves a favorable\ntrade-off between computational efficiency and fusion quality. The distilled\nnetwork, requiring only 10\\% of the parameters and inference time of the\nteacher network, retains 90\\% of its performance and outperforms existing SOTA\nmethods. Extensive experiments demonstrate the effectiveness of our approach.\nThe implementation will be made publicly available as an open-source resource."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Xuanhua He"
                    },
                    {
                        "name": "Ke Cao"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Man Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18389v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18389v2",
                "updated": "2025-04-09T16:40:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    40,
                    21,
                    2,
                    99,
                    0
                ],
                "published": "2025-02-25T17:33:20Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    33,
                    20,
                    1,
                    56,
                    0
                ],
                "title": "Monte Carlo Temperature: a robust sampling strategy for LLM's\n  uncertainty quantification methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Temperature: a robust sampling strategy for LLM's\n  uncertainty quantification methods"
                },
                "summary": "Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential\nfor their safe and reliable deployment, particularly in critical applications\nwhere incorrect outputs can have serious consequences. Current UQ methods\ntypically rely on querying the model multiple times using non-zero temperature\nsampling to generate diverse outputs for uncertainty estimation. However, the\nimpact of selecting a given temperature parameter is understudied, and our\nanalysis reveals that temperature plays a fundamental role in the quality of\nuncertainty estimates. The conventional approach of identifying optimal\ntemperature values requires expensive hyperparameter optimization (HPO) that\nmust be repeated for each new model-dataset combination. We propose Monte Carlo\nTemperature (MCT), a robust sampling strategy that eliminates the need for\ntemperature calibration. Our analysis reveals that: 1) MCT provides more robust\nuncertainty estimates across a wide range of temperatures, 2) MCT improves the\nperformance of UQ methods by replacing fixed-temperature strategies that do not\nrely on HPO, and 3) MCT achieves statistical parity with oracle temperatures,\nwhich represent the ideal outcome of a well-tuned but computationally expensive\nHPO process. These findings demonstrate that effective UQ can be achieved\nwithout the computational burden of temperature parameter calibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential\nfor their safe and reliable deployment, particularly in critical applications\nwhere incorrect outputs can have serious consequences. Current UQ methods\ntypically rely on querying the model multiple times using non-zero temperature\nsampling to generate diverse outputs for uncertainty estimation. However, the\nimpact of selecting a given temperature parameter is understudied, and our\nanalysis reveals that temperature plays a fundamental role in the quality of\nuncertainty estimates. The conventional approach of identifying optimal\ntemperature values requires expensive hyperparameter optimization (HPO) that\nmust be repeated for each new model-dataset combination. We propose Monte Carlo\nTemperature (MCT), a robust sampling strategy that eliminates the need for\ntemperature calibration. Our analysis reveals that: 1) MCT provides more robust\nuncertainty estimates across a wide range of temperatures, 2) MCT improves the\nperformance of UQ methods by replacing fixed-temperature strategies that do not\nrely on HPO, and 3) MCT achieves statistical parity with oracle temperatures,\nwhich represent the ideal outcome of a well-tuned but computationally expensive\nHPO process. These findings demonstrate that effective UQ can be achieved\nwithout the computational burden of temperature parameter calibration."
                },
                "authors": [
                    {
                        "name": "Nicola Cecere"
                    },
                    {
                        "name": "Andrea Bacciu"
                    },
                    {
                        "name": "Ignacio Fernández Tobías"
                    },
                    {
                        "name": "Amin Mantrach"
                    }
                ],
                "author_detail": {
                    "name": "Amin Mantrach"
                },
                "author": "Amin Mantrach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18389v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18389v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07022v1",
                "updated": "2025-04-09T16:37:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    37,
                    3,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T16:37:03Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    37,
                    3,
                    2,
                    99,
                    0
                ],
                "title": "Evaluating Retrieval Augmented Generative Models for Document Queries in\n  Transportation Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Retrieval Augmented Generative Models for Document Queries in\n  Transportation Safety"
                },
                "summary": "Applications of generative Large Language Models LLMs are rapidly expanding\nacross various domains, promising significant improvements in workflow\nefficiency and information retrieval. However, their implementation in\nspecialized, high-stakes domains such as hazardous materials transportation is\nchallenging due to accuracy and reliability concerns. This study evaluates the\nperformance of three fine-tuned generative models, ChatGPT, Google's Vertex AI,\nand ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in\nretrieving regulatory information essential for hazardous material\ntransportation compliance in the United States. Utilizing approximately 40\npublicly available federal and state regulatory documents, we developed 100\nrealistic queries relevant to route planning and permitting requirements.\nResponses were qualitatively rated based on accuracy, detail, and relevance,\ncomplemented by quantitative assessments of semantic similarity between model\noutputs. Results demonstrated that the RAG-augmented LLaMA models significantly\noutperformed Vertex AI and ChatGPT, providing more detailed and generally\naccurate information, despite occasional inconsistencies. This research\nintroduces the first known application of RAG in transportation safety,\nemphasizing the need for domain-specific fine-tuning and rigorous evaluation\nmethodologies to ensure reliability and minimize the risk of inaccuracies in\nhigh-stakes environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications of generative Large Language Models LLMs are rapidly expanding\nacross various domains, promising significant improvements in workflow\nefficiency and information retrieval. However, their implementation in\nspecialized, high-stakes domains such as hazardous materials transportation is\nchallenging due to accuracy and reliability concerns. This study evaluates the\nperformance of three fine-tuned generative models, ChatGPT, Google's Vertex AI,\nand ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in\nretrieving regulatory information essential for hazardous material\ntransportation compliance in the United States. Utilizing approximately 40\npublicly available federal and state regulatory documents, we developed 100\nrealistic queries relevant to route planning and permitting requirements.\nResponses were qualitatively rated based on accuracy, detail, and relevance,\ncomplemented by quantitative assessments of semantic similarity between model\noutputs. Results demonstrated that the RAG-augmented LLaMA models significantly\noutperformed Vertex AI and ChatGPT, providing more detailed and generally\naccurate information, despite occasional inconsistencies. This research\nintroduces the first known application of RAG in transportation safety,\nemphasizing the need for domain-specific fine-tuning and rigorous evaluation\nmethodologies to ensure reliability and minimize the risk of inaccuracies in\nhigh-stakes environments."
                },
                "authors": [
                    {
                        "name": "Chad Melton"
                    },
                    {
                        "name": "Alex Sorokine"
                    },
                    {
                        "name": "Steve Peterson"
                    }
                ],
                "author_detail": {
                    "name": "Steve Peterson"
                },
                "author": "Steve Peterson",
                "arxiv_comment": "14 pages, 3 Figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07015v1",
                "updated": "2025-04-09T16:32:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    32,
                    13,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T16:32:13Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    32,
                    13,
                    2,
                    99,
                    0
                ],
                "title": "LLM-IFT: LLM-Powered Information Flow Tracking for Secure Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-IFT: LLM-Powered Information Flow Tracking for Secure Hardware"
                },
                "summary": "As modern hardware designs grow in complexity and size, ensuring security\nacross the confidentiality, integrity, and availability (CIA) triad becomes\nincreasingly challenging. Information flow tracking (IFT) is a widely-used\napproach to tracing data propagation, identifying unauthorized activities that\nmay compromise confidentiality or/and integrity in hardware. However,\ntraditional IFT methods struggle with scalability and adaptability,\nparticularly in high-density and interconnected architectures, leading to\ntracing bottlenecks that limit applicability in large-scale hardware. To\naddress these limitations and show the potential of transformer-based models in\nintegrated circuit (IC) design, this paper introduces LLM-IFT that integrates\nlarge language models (LLM) for the realization of the IFT process in hardware.\nLLM-IFT exploits LLM-driven structured reasoning to perform hierarchical\ndependency analysis, systematically breaking down even the most complex\ndesigns. Through a multi-step LLM invocation, the framework analyzes both\nintra-module and inter-module dependencies, enabling comprehensive IFT\nassessment. By focusing on a set of Trust-Hub vulnerability test cases at both\nthe IP level and the SoC level, our experiments demonstrate a 100\\% success\nrate in accurate IFT analysis for confidentiality and integrity checks in\nhardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As modern hardware designs grow in complexity and size, ensuring security\nacross the confidentiality, integrity, and availability (CIA) triad becomes\nincreasingly challenging. Information flow tracking (IFT) is a widely-used\napproach to tracing data propagation, identifying unauthorized activities that\nmay compromise confidentiality or/and integrity in hardware. However,\ntraditional IFT methods struggle with scalability and adaptability,\nparticularly in high-density and interconnected architectures, leading to\ntracing bottlenecks that limit applicability in large-scale hardware. To\naddress these limitations and show the potential of transformer-based models in\nintegrated circuit (IC) design, this paper introduces LLM-IFT that integrates\nlarge language models (LLM) for the realization of the IFT process in hardware.\nLLM-IFT exploits LLM-driven structured reasoning to perform hierarchical\ndependency analysis, systematically breaking down even the most complex\ndesigns. Through a multi-step LLM invocation, the framework analyzes both\nintra-module and inter-module dependencies, enabling comprehensive IFT\nassessment. By focusing on a set of Trust-Hub vulnerability test cases at both\nthe IP level and the SoC level, our experiments demonstrate a 100\\% success\nrate in accurate IFT analysis for confidentiality and integrity checks in\nhardware."
                },
                "authors": [
                    {
                        "name": "Nowfel Mashnoor"
                    },
                    {
                        "name": "Mohammad Akyash"
                    },
                    {
                        "name": "Hadi Kamali"
                    },
                    {
                        "name": "Kimia Azar"
                    }
                ],
                "author_detail": {
                    "name": "Kimia Azar"
                },
                "author": "Kimia Azar",
                "arxiv_comment": "This paper is presented at IEEE VLSI Test Symposium (VTS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03916v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03916v3",
                "updated": "2025-04-09T16:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    27,
                    2,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-07T16:31:10Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    31,
                    10,
                    1,
                    7,
                    0
                ],
                "title": "Dolphin: Moving Towards Closed-loop Auto-research through Thinking,\n  Practice, and Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dolphin: Moving Towards Closed-loop Auto-research through Thinking,\n  Practice, and Feedback"
                },
                "summary": "The scientific research paradigm is undergoing a profound transformation\nowing to the development of Artificial Intelligence (AI). Recent works\ndemonstrate that various AI-assisted research methods can largely improve\nresearch efficiency by improving data analysis, accelerating computation, and\nfostering novel idea generation. To further move towards the ultimate goal\n(i.e., automatic scientific research), in this paper, we introduce Dolphin, a\nclosed-loop LLM-driven framework to enhance the automation level of scientific\nresearch. Dolphin first generates novel ideas based on feedback from previous\nexperiments and relevant papers ranked by the topic and task attributes. Then,\nthe generated ideas can be implemented using a code template refined and\ndebugged with the designed exception-traceback-guided local code structure.\nFinally, Dolphin automatically analyzes the results of each idea and feeds the\nresults back to the next round of idea generation. Experiments are conducted on\nthe benchmark datasets of different topics and a subset of MLE-bench. Results\nshow that Dolphin can continuously improve the performance of the input topic\nin a loop. We highlight that Dolphin can automatically propose methods that are\ncomparable to the state-of-the-art in some tasks such as 3D point\nclassification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scientific research paradigm is undergoing a profound transformation\nowing to the development of Artificial Intelligence (AI). Recent works\ndemonstrate that various AI-assisted research methods can largely improve\nresearch efficiency by improving data analysis, accelerating computation, and\nfostering novel idea generation. To further move towards the ultimate goal\n(i.e., automatic scientific research), in this paper, we introduce Dolphin, a\nclosed-loop LLM-driven framework to enhance the automation level of scientific\nresearch. Dolphin first generates novel ideas based on feedback from previous\nexperiments and relevant papers ranked by the topic and task attributes. Then,\nthe generated ideas can be implemented using a code template refined and\ndebugged with the designed exception-traceback-guided local code structure.\nFinally, Dolphin automatically analyzes the results of each idea and feeds the\nresults back to the next round of idea generation. Experiments are conducted on\nthe benchmark datasets of different topics and a subset of MLE-bench. Results\nshow that Dolphin can continuously improve the performance of the input topic\nin a loop. We highlight that Dolphin can automatically propose methods that are\ncomparable to the state-of-the-art in some tasks such as 3D point\nclassification."
                },
                "authors": [
                    {
                        "name": "Jiakang Yuan"
                    },
                    {
                        "name": "Xiangchao Yan"
                    },
                    {
                        "name": "Shiyang Feng"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "21 pages, 12 figures, and our homepage:\n  https://alpha-innovator.github.io/Dolphin-project-page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03916v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03916v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09358v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09358v3",
                "updated": "2025-04-09T16:26:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    26,
                    17,
                    2,
                    99,
                    0
                ],
                "published": "2024-04-14T21:10:57Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    21,
                    10,
                    57,
                    6,
                    105,
                    0
                ],
                "title": "Two-stage Estimators for Spatial Confounding with Point-Referenced Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-stage Estimators for Spatial Confounding with Point-Referenced Data"
                },
                "summary": "Public health data are often spatially dependent, but standard spatial\nregression methods can suffer from bias and invalid inference when the\nindependent variable is associated with spatially-correlated residuals. This\ncould occur if, for example, there is an unmeasured environmental contaminant\nassociated with the independent and outcome variables in a spatial regression\nanalysis. Geoadditive structural equation modeling (gSEM), in which an\nestimated spatial trend is removed from both the explanatory and response\nvariables before estimating the parameters of interest, has previously been\nproposed as a solution, but there has been little investigation of gSEM's\nproperties with point-referenced data. We link gSEM to results on double\nmachine learning and semiparametric regression based on two-stage procedures.\nWe propose using these semiparametric estimators for spatial regression using\nGaussian processes with Mat\\`ern covariance to estimate the spatial trends, and\nterm this class of estimators Double Spatial Regression (DSR). We derive\nregularity conditions for root-$n$ asymptotic normality and consistency and\nclosed-form variance estimation, and show that in simulations where standard\nspatial regression estimators are highly biased and have poor coverage, DSR can\nmitigate bias more effectively than competitors and obtain nominal coverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public health data are often spatially dependent, but standard spatial\nregression methods can suffer from bias and invalid inference when the\nindependent variable is associated with spatially-correlated residuals. This\ncould occur if, for example, there is an unmeasured environmental contaminant\nassociated with the independent and outcome variables in a spatial regression\nanalysis. Geoadditive structural equation modeling (gSEM), in which an\nestimated spatial trend is removed from both the explanatory and response\nvariables before estimating the parameters of interest, has previously been\nproposed as a solution, but there has been little investigation of gSEM's\nproperties with point-referenced data. We link gSEM to results on double\nmachine learning and semiparametric regression based on two-stage procedures.\nWe propose using these semiparametric estimators for spatial regression using\nGaussian processes with Mat\\`ern covariance to estimate the spatial trends, and\nterm this class of estimators Double Spatial Regression (DSR). We derive\nregularity conditions for root-$n$ asymptotic normality and consistency and\nclosed-form variance estimation, and show that in simulations where standard\nspatial regression estimators are highly biased and have poor coverage, DSR can\nmitigate bias more effectively than competitors and obtain nominal coverage."
                },
                "authors": [
                    {
                        "name": "Nate Wiecha"
                    },
                    {
                        "name": "Jane A. Hoppin"
                    },
                    {
                        "name": "Brian J. Reich"
                    }
                ],
                "author_detail": {
                    "name": "Brian J. Reich"
                },
                "author": "Brian J. Reich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09358v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09358v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11283v2",
                "updated": "2025-04-09T16:09:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    9,
                    27,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-15T05:05:56Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    5,
                    56,
                    1,
                    289,
                    0
                ],
                "title": "AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor\n  Generator Against LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor\n  Generator Against LLM Alignment"
                },
                "summary": "With the growing adoption of reinforcement learning with human feedback\n(RLHF) for aligning large language models (LLMs), the risk of backdoor\ninstallation during alignment has increased, leading to unintended and harmful\nbehaviors. Existing backdoor triggers are typically limited to fixed word\npatterns, making them detectable during data cleaning and easily removable\npost-poisoning. In this work, we explore the use of prompt-specific paraphrases\nas backdoor triggers, enhancing their stealth and resistance to removal during\nLLM alignment. We propose AdvBDGen, an adversarially fortified generative\nfine-tuning framework that automatically generates prompt-specific backdoors\nthat are effective, stealthy, and transferable across models. AdvBDGen employs\na generator-discriminator pair, fortified by an adversary, to ensure the\ninstallability and stealthiness of backdoors. It enables the crafting and\nsuccessful installation of complex triggers using as little as 3% of the\nfine-tuning data. Once installed, these backdoors can jailbreak LLMs during\ninference, demonstrate improved stability against perturbations compared to\ntraditional constant triggers, and are more challenging to remove. These\nfindings underscore an urgent need for the research community to develop more\nrobust defenses against adversarial backdoor threats in LLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing adoption of reinforcement learning with human feedback\n(RLHF) for aligning large language models (LLMs), the risk of backdoor\ninstallation during alignment has increased, leading to unintended and harmful\nbehaviors. Existing backdoor triggers are typically limited to fixed word\npatterns, making them detectable during data cleaning and easily removable\npost-poisoning. In this work, we explore the use of prompt-specific paraphrases\nas backdoor triggers, enhancing their stealth and resistance to removal during\nLLM alignment. We propose AdvBDGen, an adversarially fortified generative\nfine-tuning framework that automatically generates prompt-specific backdoors\nthat are effective, stealthy, and transferable across models. AdvBDGen employs\na generator-discriminator pair, fortified by an adversary, to ensure the\ninstallability and stealthiness of backdoors. It enables the crafting and\nsuccessful installation of complex triggers using as little as 3% of the\nfine-tuning data. Once installed, these backdoors can jailbreak LLMs during\ninference, demonstrate improved stability against perturbations compared to\ntraditional constant triggers, and are more challenging to remove. These\nfindings underscore an urgent need for the research community to develop more\nrobust defenses against adversarial backdoor threats in LLM alignment."
                },
                "authors": [
                    {
                        "name": "Pankayaraj Pathmanathan"
                    },
                    {
                        "name": "Udari Madhushani Sehwag"
                    },
                    {
                        "name": "Michael-Andrei Panaitescu-Liess"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang",
                "arxiv_comment": "Published at the Neurips Safe Generative AI Workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01999v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01999v4",
                "updated": "2025-04-09T15:52:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    52,
                    59,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-02T20:04:02Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    20,
                    4,
                    2,
                    2,
                    276,
                    0
                ],
                "title": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding &\n  Reasoning Capabilities of CodeLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding &\n  Reasoning Capabilities of CodeLLMs"
                },
                "summary": "Recent advances in Code Large Language Models (CodeLLMs) have primarily\nfocused on open-ended code generation, often overlooking the crucial aspect of\ncode understanding and reasoning. To bridge this gap, we introduce CodeMMLU, a\ncomprehensive multiple-choice benchmark designed to evaluate the depth of\nsoftware and code comprehension in LLMs. CodeMMLU includes nearly 20,000\nquestions spanning diverse domains, including code analysis, defect detection,\nand software engineering principles across multiple programming languages.\nUnlike traditional benchmarks that emphasize code generation, CodeMMLU assesses\na model's ability to reason about programs across a wide-range of tasks such as\ncode repair, execution reasoning, and fill-in-the-blank challenges. Our\nextensive evaluation reveals that even state-of-the-art models struggle with\nCodeMMLU, highlighting significant gaps in comprehension beyond generation. By\nemphasizing the essential connection between code understanding and effective\nAI-assisted development, CodeMMLU provides a critical resource for advancing\nmore reliable and capable coding assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Code Large Language Models (CodeLLMs) have primarily\nfocused on open-ended code generation, often overlooking the crucial aspect of\ncode understanding and reasoning. To bridge this gap, we introduce CodeMMLU, a\ncomprehensive multiple-choice benchmark designed to evaluate the depth of\nsoftware and code comprehension in LLMs. CodeMMLU includes nearly 20,000\nquestions spanning diverse domains, including code analysis, defect detection,\nand software engineering principles across multiple programming languages.\nUnlike traditional benchmarks that emphasize code generation, CodeMMLU assesses\na model's ability to reason about programs across a wide-range of tasks such as\ncode repair, execution reasoning, and fill-in-the-blank challenges. Our\nextensive evaluation reveals that even state-of-the-art models struggle with\nCodeMMLU, highlighting significant gaps in comprehension beyond generation. By\nemphasizing the essential connection between code understanding and effective\nAI-assisted development, CodeMMLU provides a critical resource for advancing\nmore reliable and capable coding assistants."
                },
                "authors": [
                    {
                        "name": "Dung Nguyen Manh"
                    },
                    {
                        "name": "Thang Phan Chau"
                    },
                    {
                        "name": "Nam Le Hai"
                    },
                    {
                        "name": "Thong T. Doan"
                    },
                    {
                        "name": "Nam V. Nguyen"
                    },
                    {
                        "name": "Quang Pham"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01999v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01999v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06982v1",
                "updated": "2025-04-09T15:38:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    38,
                    18,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T15:38:18Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    38,
                    18,
                    2,
                    99,
                    0
                ],
                "title": "SIGMAN:Scaling 3D Human Gaussian Generation with Millions of Assets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIGMAN:Scaling 3D Human Gaussian Generation with Millions of Assets"
                },
                "summary": "3D human digitization has long been a highly pursued yet challenging task.\nExisting methods aim to generate high-quality 3D digital humans from single or\nmultiple views, but remain primarily constrained by current paradigms and the\nscarcity of 3D human assets. Specifically, recent approaches fall into several\nparadigms: optimization-based and feed-forward (both single-view regression and\nmulti-view generation with reconstruction). However, they are limited by slow\nspeed, low quality, cascade reasoning, and ambiguity in mapping low-dimensional\nplanes to high-dimensional space due to occlusion and invisibility,\nrespectively. Furthermore, existing 3D human assets remain small-scale,\ninsufficient for large-scale training. To address these challenges, we propose\na latent space generation paradigm for 3D human digitization, which involves\ncompressing multi-view images into Gaussians via a UV-structured VAE, along\nwith DiT-based conditional generation, we transform the ill-posed\nlow-to-high-dimensional mapping problem into a learnable distribution shift,\nwhich also supports end-to-end inference. In addition, we employ the multi-view\noptimization approach combined with synthetic data to construct the HGS-1M\ndataset, which contains $1$ million 3D Gaussian assets to support the\nlarge-scale training. Experimental results demonstrate that our paradigm,\npowered by large-scale training, produces high-quality 3D human Gaussians with\nintricate textures, facial details, and loose clothing deformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D human digitization has long been a highly pursued yet challenging task.\nExisting methods aim to generate high-quality 3D digital humans from single or\nmultiple views, but remain primarily constrained by current paradigms and the\nscarcity of 3D human assets. Specifically, recent approaches fall into several\nparadigms: optimization-based and feed-forward (both single-view regression and\nmulti-view generation with reconstruction). However, they are limited by slow\nspeed, low quality, cascade reasoning, and ambiguity in mapping low-dimensional\nplanes to high-dimensional space due to occlusion and invisibility,\nrespectively. Furthermore, existing 3D human assets remain small-scale,\ninsufficient for large-scale training. To address these challenges, we propose\na latent space generation paradigm for 3D human digitization, which involves\ncompressing multi-view images into Gaussians via a UV-structured VAE, along\nwith DiT-based conditional generation, we transform the ill-posed\nlow-to-high-dimensional mapping problem into a learnable distribution shift,\nwhich also supports end-to-end inference. In addition, we employ the multi-view\noptimization approach combined with synthetic data to construct the HGS-1M\ndataset, which contains $1$ million 3D Gaussian assets to support the\nlarge-scale training. Experimental results demonstrate that our paradigm,\npowered by large-scale training, produces high-quality 3D human Gaussians with\nintricate textures, facial details, and loose clothing deformation."
                },
                "authors": [
                    {
                        "name": "Yuhang Yang"
                    },
                    {
                        "name": "Fengqi Liu"
                    },
                    {
                        "name": "Yixing Lu"
                    },
                    {
                        "name": "Qin Zhao"
                    },
                    {
                        "name": "Pingyu Wu"
                    },
                    {
                        "name": "Wei Zhai"
                    },
                    {
                        "name": "Ran Yi"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Lizhuang Ma"
                    },
                    {
                        "name": "Zheng-Jun Zha"
                    },
                    {
                        "name": "Junting Dong"
                    }
                ],
                "author_detail": {
                    "name": "Junting Dong"
                },
                "author": "Junting Dong",
                "arxiv_comment": "project page:https://yyvhang.github.io/SIGMAN_3D/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06973v1",
                "updated": "2025-04-09T15:28:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    28,
                    36,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T15:28:36Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    28,
                    36,
                    2,
                    99,
                    0
                ],
                "title": "HIP 15429: a newborn Be star on an eccentric binary orbit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIP 15429: a newborn Be star on an eccentric binary orbit"
                },
                "summary": "We identified a new post-interaction binary, HIP 15429, consisting of a\nstripped star and a recently formed, rapidly rotating Be star companion ($v\n\\sin i \\approx 270$ km/s) sharing many similarities with recently identified\nbloated stripped stars. From orbital fitting of multi-epoch radial velocities\nwe find a 221-day period. We also find an eccentricity of $e=0.52$, which is\nunexpectedly high as tides are expected to have circularised the orbit\nefficiently during the presumed recent mass transfer. The formation of a\ncircumbinary disk during the mass transfer phase or the presence of an unseen\ntertiary companion might explain the orbit's high eccentricity. We determined\nphysical parameters for both stars by fitting the spectra of the disentangled\nbinary components and multi-band photometry. The stripped nature of the donor\nstar is affirmed by its high luminosity at a low inferred mass ($\\lesssim 1\n\\mathrm{M}_\\odot$) and imprints of CNO-processed material in the surface\nabundances. The donor's relatively large radius and cool temperature\n($T_{\\mathrm{eff}} = 13.5 \\pm 0.5$ kK) suggest that it has only recently ceased\nmass transfer. Evolutionary models assuming a 5-6 $\\mathrm{M}_\\odot$ progenitor\ncan reproduce these parameters and imply that the binary is currently evolving\ntowards a stage where the donor becomes a subdwarf orbiting a Be star. The\nremarkably high eccentricity of HIP 15429 challenges standard tidal evolution\nmodels, suggesting either inefficient tidal dissipation or external influences,\nsuch as a tertiary companion or circumbinary disk. This underscores the need to\nidentify and characterise more post-mass transfer binaries to benchmark and\nrefine theoretical models of binary evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We identified a new post-interaction binary, HIP 15429, consisting of a\nstripped star and a recently formed, rapidly rotating Be star companion ($v\n\\sin i \\approx 270$ km/s) sharing many similarities with recently identified\nbloated stripped stars. From orbital fitting of multi-epoch radial velocities\nwe find a 221-day period. We also find an eccentricity of $e=0.52$, which is\nunexpectedly high as tides are expected to have circularised the orbit\nefficiently during the presumed recent mass transfer. The formation of a\ncircumbinary disk during the mass transfer phase or the presence of an unseen\ntertiary companion might explain the orbit's high eccentricity. We determined\nphysical parameters for both stars by fitting the spectra of the disentangled\nbinary components and multi-band photometry. The stripped nature of the donor\nstar is affirmed by its high luminosity at a low inferred mass ($\\lesssim 1\n\\mathrm{M}_\\odot$) and imprints of CNO-processed material in the surface\nabundances. The donor's relatively large radius and cool temperature\n($T_{\\mathrm{eff}} = 13.5 \\pm 0.5$ kK) suggest that it has only recently ceased\nmass transfer. Evolutionary models assuming a 5-6 $\\mathrm{M}_\\odot$ progenitor\ncan reproduce these parameters and imply that the binary is currently evolving\ntowards a stage where the donor becomes a subdwarf orbiting a Be star. The\nremarkably high eccentricity of HIP 15429 challenges standard tidal evolution\nmodels, suggesting either inefficient tidal dissipation or external influences,\nsuch as a tertiary companion or circumbinary disk. This underscores the need to\nidentify and characterise more post-mass transfer binaries to benchmark and\nrefine theoretical models of binary evolution."
                },
                "authors": [
                    {
                        "name": "Johanna Müller-Horn"
                    },
                    {
                        "name": "Kareem El-Badry"
                    },
                    {
                        "name": "Hans-Walter Rix"
                    },
                    {
                        "name": "Tomer Shenar"
                    },
                    {
                        "name": "Rhys Seeburger"
                    },
                    {
                        "name": "Jaime Villasenor"
                    },
                    {
                        "name": "Julia Bodensteiner"
                    },
                    {
                        "name": "W. David Latham"
                    },
                    {
                        "name": "Allyson Bieryla"
                    },
                    {
                        "name": "A. Lars Buchhave"
                    },
                    {
                        "name": "Howard Isaacson"
                    },
                    {
                        "name": "W. Andrew Howard"
                    }
                ],
                "author_detail": {
                    "name": "W. Andrew Howard"
                },
                "author": "W. Andrew Howard",
                "arxiv_comment": "submitted to Astronomy & Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13149v2",
                "updated": "2025-04-09T15:28:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    28,
                    20,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-22T19:00:00Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    19,
                    0,
                    0,
                    2,
                    22,
                    0
                ],
                "title": "Galaxy infall models for arbitrary velocity directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy infall models for arbitrary velocity directions"
                },
                "summary": "For most galaxies in the cosmos, our knowledge of their motion is limited to\nline-of-sight velocities from redshift observations. To determine the radial\nvelocity between two galaxies the minor and major infall models were\nestablished by Karachentsev and Kashibadze (2006). Regardless of the background\ncosmology, our derivations reveal that these infall models approximate the\ntotal radial velocity between two galaxies by two different projections\nemploying different information about the system. For galaxies having small\nangular separations $\\theta$, all infall models agree that the radial velocity\nis the difference of their line-of-sight components. Applying these models to\nca. $500$ halos of the Illustris-3 simulation, we find the perpendicular and\ntangential velocity parts to be non-negligible for more than 90% of all, more\nthan 5000 infalling subhalos. Thus, even for $\\theta < 10$ deg, the\ninfall-model velocities deviate from the true radial velocity. Only for 30% we\nfound the true one lay between the minor and major infall velocity. However,\nthe infall models yield robust upper and lower bounds to the true radial\nvelocity dispersion. Observed under $\\theta < 10$ deg the velocity dispersion\ninferred from the sole difference of line-of-sight velocity components even\ncoincides with the true one, justifying this approach for high-redshift groups\nand clusters. Based on these findings, we predict the radial velocity\ndispersion of the M81-group from the minor infall model (upper bound)\n$\\sigma_{\\mathrm{r,min}} = (193 \\pm 42)~\\mbox{km}/\\mbox{s}$, from the major\ninfall model (lower bound) $\\sigma_{\\mathrm{r,maj}} = (129 \\pm 64)\n~\\mbox{km}/\\mbox{s}$ and $\\sigma_\\mathrm{r,\\Delta v} = (102 \\pm\n36)~\\mbox{km}/\\mbox{s}$ from the line-of-sight-velocity difference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For most galaxies in the cosmos, our knowledge of their motion is limited to\nline-of-sight velocities from redshift observations. To determine the radial\nvelocity between two galaxies the minor and major infall models were\nestablished by Karachentsev and Kashibadze (2006). Regardless of the background\ncosmology, our derivations reveal that these infall models approximate the\ntotal radial velocity between two galaxies by two different projections\nemploying different information about the system. For galaxies having small\nangular separations $\\theta$, all infall models agree that the radial velocity\nis the difference of their line-of-sight components. Applying these models to\nca. $500$ halos of the Illustris-3 simulation, we find the perpendicular and\ntangential velocity parts to be non-negligible for more than 90% of all, more\nthan 5000 infalling subhalos. Thus, even for $\\theta < 10$ deg, the\ninfall-model velocities deviate from the true radial velocity. Only for 30% we\nfound the true one lay between the minor and major infall velocity. However,\nthe infall models yield robust upper and lower bounds to the true radial\nvelocity dispersion. Observed under $\\theta < 10$ deg the velocity dispersion\ninferred from the sole difference of line-of-sight velocity components even\ncoincides with the true one, justifying this approach for high-redshift groups\nand clusters. Based on these findings, we predict the radial velocity\ndispersion of the M81-group from the minor infall model (upper bound)\n$\\sigma_{\\mathrm{r,min}} = (193 \\pm 42)~\\mbox{km}/\\mbox{s}$, from the major\ninfall model (lower bound) $\\sigma_{\\mathrm{r,maj}} = (129 \\pm 64)\n~\\mbox{km}/\\mbox{s}$ and $\\sigma_\\mathrm{r,\\Delta v} = (102 \\pm\n36)~\\mbox{km}/\\mbox{s}$ from the line-of-sight-velocity difference."
                },
                "authors": [
                    {
                        "name": "Jenny Wagner"
                    },
                    {
                        "name": "David Benisty"
                    }
                ],
                "author_detail": {
                    "name": "David Benisty"
                },
                "author": "David Benisty",
                "arxiv_comment": "15 pages, 11 figures; simulations were incorporated;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06969v1",
                "updated": "2025-04-09T15:26:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    26,
                    0,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T15:26:00Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    26,
                    0,
                    2,
                    99,
                    0
                ],
                "title": "Towards LLMs Robustness to Changes in Prompt Format Styles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLMs Robustness to Changes in Prompt Format Styles"
                },
                "summary": "Large language models (LLMs) have gained popularity in recent years for their\nutility in various applications. However, they are sensitive to non-semantic\nchanges in prompt formats, where small changes in the prompt format can lead to\nsignificant performance fluctuations. In the literature, this problem is\ncommonly referred to as prompt brittleness. Previous research on prompt\nengineering has focused mainly on developing techniques for identifying the\noptimal prompt for specific tasks. Some studies have also explored the issue of\nprompt brittleness and proposed methods to quantify performance variations;\nhowever, no simple solution has been found to address this challenge. We\npropose Mixture of Formats (MOF), a simple and efficient technique for\naddressing prompt brittleness in LLMs by diversifying the styles used in the\nprompt few-shot examples. MOF was inspired by computer vision techniques that\nutilize diverse style datasets to prevent models from associating specific\nstyles with the target variable. Empirical results show that our proposed\ntechnique reduces style-induced prompt brittleness in various LLMs while also\nenhancing overall performance across prompt variations and different datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained popularity in recent years for their\nutility in various applications. However, they are sensitive to non-semantic\nchanges in prompt formats, where small changes in the prompt format can lead to\nsignificant performance fluctuations. In the literature, this problem is\ncommonly referred to as prompt brittleness. Previous research on prompt\nengineering has focused mainly on developing techniques for identifying the\noptimal prompt for specific tasks. Some studies have also explored the issue of\nprompt brittleness and proposed methods to quantify performance variations;\nhowever, no simple solution has been found to address this challenge. We\npropose Mixture of Formats (MOF), a simple and efficient technique for\naddressing prompt brittleness in LLMs by diversifying the styles used in the\nprompt few-shot examples. MOF was inspired by computer vision techniques that\nutilize diverse style datasets to prevent models from associating specific\nstyles with the target variable. Empirical results show that our proposed\ntechnique reduces style-induced prompt brittleness in various LLMs while also\nenhancing overall performance across prompt variations and different datasets."
                },
                "authors": [
                    {
                        "name": "Lilian Ngweta"
                    },
                    {
                        "name": "Kiran Kate"
                    },
                    {
                        "name": "Jason Tsay"
                    },
                    {
                        "name": "Yara Rizk"
                    }
                ],
                "author_detail": {
                    "name": "Yara Rizk"
                },
                "author": "Yara Rizk",
                "arxiv_comment": "NAACL Student Research Workshop (SRW) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02916v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02916v3",
                "updated": "2025-04-09T15:20:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    20,
                    33,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-03T19:07:53Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    19,
                    7,
                    53,
                    3,
                    277,
                    0
                ],
                "title": "LLM Safeguard is a Double-Edged Sword: Exploiting False Positives for\n  Denial-of-Service Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Safeguard is a Double-Edged Sword: Exploiting False Positives for\n  Denial-of-Service Attacks"
                },
                "summary": "Safety is a paramount concern for large language models (LLMs) in open\ndeployment, motivating the development of safeguard methods that enforce\nethical and responsible use through safety alignment or guardrail mechanisms.\nJailbreak attacks that exploit the \\emph{false negatives} of safeguard methods\nhave emerged as a prominent research focus in the field of LLM security.\nHowever, we found that the malicious attackers could also exploit false\npositives of safeguards, i.e., fooling the safeguard model to block safe\ncontent mistakenly, leading to a denial-of-service (DoS) affecting LLM users.\nTo bridge the knowledge gap of this overlooked threat, we explore multiple\nattack methods that include inserting a short adversarial prompt into user\nprompt templates and corrupting the LLM on the server by poisoned fine-tuning.\nIn both ways, the attack triggers safeguard rejections of user requests from\nthe client. Our evaluation demonstrates the severity of this threat across\nmultiple scenarios. For instance, in the scenario of white-box adversarial\nprompt injection, the attacker can use our optimization process to\nautomatically generate seemingly safe adversarial prompts, approximately only\n30 characters long, that universally block over 97% of user requests on Llama\nGuard 3. These findings reveal a new dimension in LLM safeguard evaluation --\nadversarial robustness to false positives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety is a paramount concern for large language models (LLMs) in open\ndeployment, motivating the development of safeguard methods that enforce\nethical and responsible use through safety alignment or guardrail mechanisms.\nJailbreak attacks that exploit the \\emph{false negatives} of safeguard methods\nhave emerged as a prominent research focus in the field of LLM security.\nHowever, we found that the malicious attackers could also exploit false\npositives of safeguards, i.e., fooling the safeguard model to block safe\ncontent mistakenly, leading to a denial-of-service (DoS) affecting LLM users.\nTo bridge the knowledge gap of this overlooked threat, we explore multiple\nattack methods that include inserting a short adversarial prompt into user\nprompt templates and corrupting the LLM on the server by poisoned fine-tuning.\nIn both ways, the attack triggers safeguard rejections of user requests from\nthe client. Our evaluation demonstrates the severity of this threat across\nmultiple scenarios. For instance, in the scenario of white-box adversarial\nprompt injection, the attacker can use our optimization process to\nautomatically generate seemingly safe adversarial prompts, approximately only\n30 characters long, that universally block over 97% of user requests on Llama\nGuard 3. These findings reveal a new dimension in LLM safeguard evaluation --\nadversarial robustness to false positives."
                },
                "authors": [
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Ziyang Xiong"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02916v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02916v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02166v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02166v4",
                "updated": "2025-04-09T15:17:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    17,
                    5,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-04T01:04:14Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    1,
                    4,
                    14,
                    1,
                    63,
                    0
                ],
                "title": "The impact of local noise recorded at the ET candidate sites on the\n  signal to noise ratio of CBC gravitational wave signals for the ET triangle\n  configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of local noise recorded at the ET candidate sites on the\n  signal to noise ratio of CBC gravitational wave signals for the ET triangle\n  configuration"
                },
                "summary": "We present an evaluation of how site dependent noise can affect the signal to\nnoise ratio (SNR) of compact binary coalescence (CBC) signals in the future 3rd\ngeneration gravitational wave (GW) detector Einstein Telescope (ET). The design\nof ET is currently pushing the scientific community to study its scientific\npotential with respect to known, and possibly unexpected, GW signals using its\ndesign sensitivity curves. However, local ambient noise may have an impact on\nthe ET sensitivity at low frequency and therefore affect the SNR of CBC signals\nat low frequency. Therefore, we study the impact of ambient noise on the ET\nsensitivity curve at the two sites candidate to host ET - Sardinia, in Italy,\nand the Euregio Meuse-Rhine (EMR) at the Netherlands-Belgium border - and infer\nthe impact on the ET sensitivity curve and how the SNR of CBC signals at low\nfrequencies is affected. We find that Sardinia shows results which are on par,\nif not better, than the design case. On the other hand, ambient noise for the\ncurrent EMR sensitivity curve in Terziet causes a higher degradation of the SNR\nperformances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an evaluation of how site dependent noise can affect the signal to\nnoise ratio (SNR) of compact binary coalescence (CBC) signals in the future 3rd\ngeneration gravitational wave (GW) detector Einstein Telescope (ET). The design\nof ET is currently pushing the scientific community to study its scientific\npotential with respect to known, and possibly unexpected, GW signals using its\ndesign sensitivity curves. However, local ambient noise may have an impact on\nthe ET sensitivity at low frequency and therefore affect the SNR of CBC signals\nat low frequency. Therefore, we study the impact of ambient noise on the ET\nsensitivity curve at the two sites candidate to host ET - Sardinia, in Italy,\nand the Euregio Meuse-Rhine (EMR) at the Netherlands-Belgium border - and infer\nthe impact on the ET sensitivity curve and how the SNR of CBC signals at low\nfrequencies is affected. We find that Sardinia shows results which are on par,\nif not better, than the design case. On the other hand, ambient noise for the\ncurrent EMR sensitivity curve in Terziet causes a higher degradation of the SNR\nperformances."
                },
                "authors": [
                    {
                        "name": "Matteo Di Giovanni"
                    },
                    {
                        "name": "Davide Rozza"
                    },
                    {
                        "name": "Rosario De Rosa"
                    },
                    {
                        "name": "Enrico Calloni"
                    },
                    {
                        "name": "Domenico D'Urso"
                    },
                    {
                        "name": "Luca Naticchioni"
                    },
                    {
                        "name": "Annalisa Allocca"
                    },
                    {
                        "name": "Giovanni Luca Cardello"
                    },
                    {
                        "name": "Alessandro Cardini"
                    },
                    {
                        "name": "Andrea Contu"
                    },
                    {
                        "name": "Giovanni Diaferia"
                    },
                    {
                        "name": "Luciano Errico"
                    },
                    {
                        "name": "Carlo Giunchi"
                    },
                    {
                        "name": "Jan Harms"
                    },
                    {
                        "name": "Irene Molinari"
                    },
                    {
                        "name": "Marco Olivieri"
                    },
                    {
                        "name": "Piero Rapagnani"
                    },
                    {
                        "name": "Fulvio Ricci"
                    },
                    {
                        "name": "Valeria Sipala"
                    },
                    {
                        "name": "Lucia Trozzo"
                    }
                ],
                "author_detail": {
                    "name": "Lucia Trozzo"
                },
                "author": "Lucia Trozzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02166v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02166v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23653v2",
                "updated": "2025-04-09T15:14:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    14,
                    53,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-31T01:35:50Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    1,
                    35,
                    50,
                    0,
                    90,
                    0
                ],
                "title": "Scalable Geometric Learning with Correlation-Based Functional Brain\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Geometric Learning with Correlation-Based Functional Brain\n  Networks"
                },
                "summary": "The correlation matrix is a central representation of functional brain\nnetworks in neuroimaging. Traditional analyses often treat pairwise\ninteractions independently in a Euclidean setting, overlooking the intrinsic\ngeometry of correlation matrices. While earlier attempts have embraced the\nquotient geometry of the correlation manifold, they remain limited by\ncomputational inefficiency and numerical instability, particularly in\nhigh-dimensional contexts. This paper presents a novel geometric framework that\nemploys diffeomorphic transformations to embed correlation matrices into a\nEuclidean space, preserving salient manifold properties and enabling\nlarge-scale analyses. The proposed method integrates with established learning\nalgorithms - regression, dimensionality reduction, and clustering - and extends\nnaturally to population-level inference of brain networks. Simulation studies\ndemonstrate both improved computational speed and enhanced accuracy compared to\nconventional manifold-based approaches. Moreover, applications in real\nneuroimaging scenarios illustrate the framework's utility, enhancing behavior\nscore prediction, subject fingerprinting in resting-state fMRI, and hypothesis\ntesting in electroencephalogram data. An open-source MATLAB toolbox is provided\nto facilitate broader adoption and advance the application of correlation\ngeometry in functional brain network research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The correlation matrix is a central representation of functional brain\nnetworks in neuroimaging. Traditional analyses often treat pairwise\ninteractions independently in a Euclidean setting, overlooking the intrinsic\ngeometry of correlation matrices. While earlier attempts have embraced the\nquotient geometry of the correlation manifold, they remain limited by\ncomputational inefficiency and numerical instability, particularly in\nhigh-dimensional contexts. This paper presents a novel geometric framework that\nemploys diffeomorphic transformations to embed correlation matrices into a\nEuclidean space, preserving salient manifold properties and enabling\nlarge-scale analyses. The proposed method integrates with established learning\nalgorithms - regression, dimensionality reduction, and clustering - and extends\nnaturally to population-level inference of brain networks. Simulation studies\ndemonstrate both improved computational speed and enhanced accuracy compared to\nconventional manifold-based approaches. Moreover, applications in real\nneuroimaging scenarios illustrate the framework's utility, enhancing behavior\nscore prediction, subject fingerprinting in resting-state fMRI, and hypothesis\ntesting in electroencephalogram data. An open-source MATLAB toolbox is provided\nto facilitate broader adoption and advance the application of correlation\ngeometry in functional brain network research."
                },
                "authors": [
                    {
                        "name": "Kisung You"
                    },
                    {
                        "name": "Yelim Lee"
                    },
                    {
                        "name": "Hae-Jeong Park"
                    }
                ],
                "author_detail": {
                    "name": "Hae-Jeong Park"
                },
                "author": "Hae-Jeong Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07991v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07991v5",
                "updated": "2025-04-09T15:05:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    5,
                    27,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-10T14:48:57Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    48,
                    57,
                    3,
                    284,
                    0
                ],
                "title": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets"
                },
                "summary": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems."
                },
                "authors": [
                    {
                        "name": "Tommaso Giorgi"
                    },
                    {
                        "name": "Lorenzo Cima"
                    },
                    {
                        "name": "Tiziano Fagni"
                    },
                    {
                        "name": "Marco Avvenuti"
                    },
                    {
                        "name": "Stefano Cresci"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Cresci"
                },
                "author": "Stefano Cresci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07991v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07991v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11176v2",
                "updated": "2025-04-09T14:54:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    54,
                    12,
                    2,
                    99,
                    0
                ],
                "published": "2025-02-16T15:54:53Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    15,
                    54,
                    53,
                    6,
                    47,
                    0
                ],
                "title": "LogiDynamics: Unraveling the Dynamics of Logical Inference in Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogiDynamics: Unraveling the Dynamics of Logical Inference in Large\n  Language Model Reasoning"
                },
                "summary": "Modern large language models (LLMs) employ various forms of logical\ninference, both implicitly and explicitly, when addressing reasoning tasks.\nUnderstanding how to optimally leverage these inference paradigms is critical\nfor advancing LLMs' reasoning capabilities. This paper adopts an exploratory\napproach by introducing a controlled evaluation environment for analogical\nreasoning -- a fundamental cognitive task -- that is systematically\nparameterized across three dimensions: modality (textual, visual, symbolic),\ndifficulty (easy, medium, hard), and task format (multiple-choice or free-text\ngeneration). We analyze the comparative dynamics of inductive, abductive, and\ndeductive inference pipelines across these dimensions, and demonstrate that our\nfindings generalize to broader in-context learning tasks. Additionally, we\ninvestigate advanced paradigms such as hypothesis selection, verification, and\nrefinement, revealing their potential to scale up logical inference in LLM\nreasoning. This exploratory study provides a foundation for future research in\nenhancing LLM reasoning through systematic logical inference strategies.\nResources are available at https://github.com/HKUST-KnowComp/LogiDynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) employ various forms of logical\ninference, both implicitly and explicitly, when addressing reasoning tasks.\nUnderstanding how to optimally leverage these inference paradigms is critical\nfor advancing LLMs' reasoning capabilities. This paper adopts an exploratory\napproach by introducing a controlled evaluation environment for analogical\nreasoning -- a fundamental cognitive task -- that is systematically\nparameterized across three dimensions: modality (textual, visual, symbolic),\ndifficulty (easy, medium, hard), and task format (multiple-choice or free-text\ngeneration). We analyze the comparative dynamics of inductive, abductive, and\ndeductive inference pipelines across these dimensions, and demonstrate that our\nfindings generalize to broader in-context learning tasks. Additionally, we\ninvestigate advanced paradigms such as hypothesis selection, verification, and\nrefinement, revealing their potential to scale up logical inference in LLM\nreasoning. This exploratory study provides a foundation for future research in\nenhancing LLM reasoning through systematic logical inference strategies.\nResources are available at https://github.com/HKUST-KnowComp/LogiDynamics."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Jiayang Cheng"
                    },
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Haochen Shi"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Ginny Y. Wong"
                    },
                    {
                        "name": "Simon See"
                    }
                ],
                "author_detail": {
                    "name": "Simon See"
                },
                "author": "Simon See",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06943v1",
                "updated": "2025-04-09T14:51:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    51,
                    2,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T14:51:02Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    51,
                    2,
                    2,
                    99,
                    0
                ],
                "title": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations,\n  Architectural Components, and Cognitive Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations,\n  Architectural Components, and Cognitive Integration"
                },
                "summary": "Agents powered by Large Language Models (LLMs) have recently demonstrated\nimpressive capabilities in various tasks. Still, they face limitations in tasks\nrequiring specific, structured knowledge, flexibility, or accountable\ndecision-making. While agents are capable of perceiving their environments,\nforming inferences, planning, and executing actions towards goals, they often\nface issues such as hallucinations and lack of contextual memory across\ninteractions. This paper explores how Case-Based Reasoning (CBR), a strategy\nthat solves new problems by referencing past experiences, can be integrated\ninto LLM agent frameworks. This integration allows LLMs to leverage explicit\nknowledge, enhancing their effectiveness. We systematically review the\ntheoretical foundations of these enhanced agents, identify critical framework\ncomponents, and formulate a mathematical model for the CBR processes of case\nretrieval, adaptation, and learning. We also evaluate CBR-enhanced agents\nagainst other methods like Chain-of-Thought reasoning and standard\nRetrieval-Augmented Generation, analyzing their relative strengths. Moreover,\nwe explore how leveraging CBR's cognitive dimensions (including\nself-reflection, introspection, and curiosity) via goal-driven autonomy\nmechanisms can further enhance the LLM agent capabilities. Contributing to the\nongoing research on neuro-symbolic hybrid systems, this work posits CBR as a\nviable technique for enhancing the reasoning skills and cognitive aspects of\nautonomous LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents powered by Large Language Models (LLMs) have recently demonstrated\nimpressive capabilities in various tasks. Still, they face limitations in tasks\nrequiring specific, structured knowledge, flexibility, or accountable\ndecision-making. While agents are capable of perceiving their environments,\nforming inferences, planning, and executing actions towards goals, they often\nface issues such as hallucinations and lack of contextual memory across\ninteractions. This paper explores how Case-Based Reasoning (CBR), a strategy\nthat solves new problems by referencing past experiences, can be integrated\ninto LLM agent frameworks. This integration allows LLMs to leverage explicit\nknowledge, enhancing their effectiveness. We systematically review the\ntheoretical foundations of these enhanced agents, identify critical framework\ncomponents, and formulate a mathematical model for the CBR processes of case\nretrieval, adaptation, and learning. We also evaluate CBR-enhanced agents\nagainst other methods like Chain-of-Thought reasoning and standard\nRetrieval-Augmented Generation, analyzing their relative strengths. Moreover,\nwe explore how leveraging CBR's cognitive dimensions (including\nself-reflection, introspection, and curiosity) via goal-driven autonomy\nmechanisms can further enhance the LLM agent capabilities. Contributing to the\nongoing research on neuro-symbolic hybrid systems, this work posits CBR as a\nviable technique for enhancing the reasoning skills and cognitive aspects of\nautonomous LLM agents."
                },
                "authors": [
                    {
                        "name": "Kostas Hatalis"
                    },
                    {
                        "name": "Despina Christou"
                    },
                    {
                        "name": "Vyshnavi Kondapalli"
                    }
                ],
                "author_detail": {
                    "name": "Vyshnavi Kondapalli"
                },
                "author": "Vyshnavi Kondapalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06939v1",
                "updated": "2025-04-09T14:43:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    43,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T14:43:08Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    43,
                    8,
                    2,
                    99,
                    0
                ],
                "title": "FeedbackEval: A Benchmark for Evaluating Large Language Models in\n  Feedback-Driven Code Repair Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FeedbackEval: A Benchmark for Evaluating Large Language Models in\n  Feedback-Driven Code Repair Tasks"
                },
                "summary": "Code repair is a fundamental task in software development, facilitating\nefficient bug resolution and software maintenance. Although large language\nmodels (LLMs) have demonstrated considerable potential in automated code\nrepair, their ability to comprehend and effectively leverage diverse types of\nfeedback remains insufficiently understood. To bridge this gap, we introduce\nFeedbackEval, a systematic benchmark for evaluating LLMs' feedback\ncomprehension and performance in code repair tasks. We conduct a comprehensive\nempirical study on five state-of-the-art LLMs, including GPT-4o, Claude-3.5,\nGemini-1.5, GLM-4, and Qwen2.5, to evaluate their behavior under both\nsingle-iteration and iterative code repair settings. Our results show that\nstructured feedback, particularly in the form of test feedback, leads to the\nhighest repair success rates, while unstructured feedback proves significantly\nless effective. Iterative feedback further enhances repair performance, though\nthe marginal benefit diminishes after two or three rounds. Moreover, prompt\nstructure is shown to be critical: incorporating docstrings, contextual\ninformation, and explicit guidelines substantially improves outcomes, whereas\npersona-based, chain-of-thought, and few-shot prompting strategies offer\nlimited benefits in single-iteration scenarios. This work introduces a robust\nbenchmark and delivers practical insights to advance the understanding and\ndevelopment of feedback-driven code repair using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code repair is a fundamental task in software development, facilitating\nefficient bug resolution and software maintenance. Although large language\nmodels (LLMs) have demonstrated considerable potential in automated code\nrepair, their ability to comprehend and effectively leverage diverse types of\nfeedback remains insufficiently understood. To bridge this gap, we introduce\nFeedbackEval, a systematic benchmark for evaluating LLMs' feedback\ncomprehension and performance in code repair tasks. We conduct a comprehensive\nempirical study on five state-of-the-art LLMs, including GPT-4o, Claude-3.5,\nGemini-1.5, GLM-4, and Qwen2.5, to evaluate their behavior under both\nsingle-iteration and iterative code repair settings. Our results show that\nstructured feedback, particularly in the form of test feedback, leads to the\nhighest repair success rates, while unstructured feedback proves significantly\nless effective. Iterative feedback further enhances repair performance, though\nthe marginal benefit diminishes after two or three rounds. Moreover, prompt\nstructure is shown to be critical: incorporating docstrings, contextual\ninformation, and explicit guidelines substantially improves outcomes, whereas\npersona-based, chain-of-thought, and few-shot prompting strategies offer\nlimited benefits in single-iteration scenarios. This work introduces a robust\nbenchmark and delivers practical insights to advance the understanding and\ndevelopment of feedback-driven code repair using LLMs."
                },
                "authors": [
                    {
                        "name": "Dekun Dai"
                    },
                    {
                        "name": "MingWei Liu"
                    },
                    {
                        "name": "Anji Li"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06923v1",
                "updated": "2025-04-09T14:30:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    30,
                    30,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T14:30:30Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    30,
                    30,
                    2,
                    99,
                    0
                ],
                "title": "The Importance of Being Discrete: Measuring the Impact of Discretization\n  in End-to-End Differentially Private Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Importance of Being Discrete: Measuring the Impact of Discretization\n  in End-to-End Differentially Private Synthetic Data"
                },
                "summary": "Differentially Private (DP) generative marginal models are often used in the\nwild to release synthetic tabular datasets in lieu of sensitive data while\nproviding formal privacy guarantees. These models approximate low-dimensional\nmarginals or query workloads; crucially, they require the training data to be\npre-discretized, i.e., continuous values need to first be partitioned into\nbins. However, as the range of values (or their domain) is often inferred\ndirectly from the training data, with the number of bins and bin edges\ntypically defined arbitrarily, this approach can ultimately break end-to-end DP\nguarantees and may not always yield optimal utility.\n  In this paper, we present an extensive measurement study of four\ndiscretization strategies in the context of DP marginal generative models. More\nprecisely, we design DP versions of three discretizers (uniform, quantile, and\nk-means) and reimplement the PrivTree algorithm. We find that optimizing both\nthe choice of discretizer and bin count can improve utility, on average, by\nalmost 30% across six DP marginal models, compared to the default strategy and\nnumber of bins, with PrivTree being the best-performing discretizer in the\nmajority of cases. We demonstrate that, while DP generative models with\nnon-private discretization remain vulnerable to membership inference attacks,\napplying DP during discretization effectively mitigates this risk. Finally, we\npropose an optimized approach for automatically selecting the optimal number of\nbins, achieving high utility while reducing both privacy budget consumption and\ncomputational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private (DP) generative marginal models are often used in the\nwild to release synthetic tabular datasets in lieu of sensitive data while\nproviding formal privacy guarantees. These models approximate low-dimensional\nmarginals or query workloads; crucially, they require the training data to be\npre-discretized, i.e., continuous values need to first be partitioned into\nbins. However, as the range of values (or their domain) is often inferred\ndirectly from the training data, with the number of bins and bin edges\ntypically defined arbitrarily, this approach can ultimately break end-to-end DP\nguarantees and may not always yield optimal utility.\n  In this paper, we present an extensive measurement study of four\ndiscretization strategies in the context of DP marginal generative models. More\nprecisely, we design DP versions of three discretizers (uniform, quantile, and\nk-means) and reimplement the PrivTree algorithm. We find that optimizing both\nthe choice of discretizer and bin count can improve utility, on average, by\nalmost 30% across six DP marginal models, compared to the default strategy and\nnumber of bins, with PrivTree being the best-performing discretizer in the\nmajority of cases. We demonstrate that, while DP generative models with\nnon-private discretization remain vulnerable to membership inference attacks,\napplying DP during discretization effectively mitigates this risk. Finally, we\npropose an optimized approach for automatically selecting the optimal number of\nbins, achieving high utility while reducing both privacy budget consumption and\ncomputational overhead."
                },
                "authors": [
                    {
                        "name": "Georgi Ganev"
                    },
                    {
                        "name": "Meenatchi Sundaram Muthu Selva Annamalai"
                    },
                    {
                        "name": "Sofiane Mahiou"
                    },
                    {
                        "name": "Emiliano De Cristofaro"
                    }
                ],
                "author_detail": {
                    "name": "Emiliano De Cristofaro"
                },
                "author": "Emiliano De Cristofaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06915v1",
                "updated": "2025-04-09T14:23:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    23,
                    4,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T14:23:04Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    23,
                    4,
                    2,
                    99,
                    0
                ],
                "title": "An Analysis of Temporal Dropout in Earth Observation Time Series for\n  Regression Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Analysis of Temporal Dropout in Earth Observation Time Series for\n  Regression Tasks"
                },
                "summary": "Missing instances in time series data impose a significant challenge to deep\nlearning models, particularly in regression tasks. In the Earth Observation\nfield, satellite failure or cloud occlusion frequently results in missing\ntime-steps, introducing uncertainties in the predicted output and causing a\ndecline in predictive performance. While many studies address missing\ntime-steps through data augmentation to improve model robustness, the\nuncertainty arising at the input level is commonly overlooked. To address this\ngap, we introduce Monte Carlo Temporal Dropout (MC-TD), a method that\nexplicitly accounts for input-level uncertainty by randomly dropping time-steps\nduring inference using a predefined dropout ratio, thereby simulating the\neffect of missing data. To bypass the need for costly searches for the optimal\ndropout ratio, we extend this approach with Monte Carlo Concrete Temporal\nDropout (MC-ConcTD), a method that learns the optimal dropout distribution\ndirectly. Both MC-TD and MC-ConcTD are applied during inference, leveraging\nMonte Carlo sampling for uncertainty quantification. Experiments on three EO\ntime-series datasets demonstrate that MC-ConcTD improves predictive performance\nand uncertainty calibration compared to existing approaches. Additionally, we\nhighlight the advantages of adaptive dropout tuning over manual selection,\nmaking uncertainty quantification more robust and accessible for EO\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing instances in time series data impose a significant challenge to deep\nlearning models, particularly in regression tasks. In the Earth Observation\nfield, satellite failure or cloud occlusion frequently results in missing\ntime-steps, introducing uncertainties in the predicted output and causing a\ndecline in predictive performance. While many studies address missing\ntime-steps through data augmentation to improve model robustness, the\nuncertainty arising at the input level is commonly overlooked. To address this\ngap, we introduce Monte Carlo Temporal Dropout (MC-TD), a method that\nexplicitly accounts for input-level uncertainty by randomly dropping time-steps\nduring inference using a predefined dropout ratio, thereby simulating the\neffect of missing data. To bypass the need for costly searches for the optimal\ndropout ratio, we extend this approach with Monte Carlo Concrete Temporal\nDropout (MC-ConcTD), a method that learns the optimal dropout distribution\ndirectly. Both MC-TD and MC-ConcTD are applied during inference, leveraging\nMonte Carlo sampling for uncertainty quantification. Experiments on three EO\ntime-series datasets demonstrate that MC-ConcTD improves predictive performance\nand uncertainty calibration compared to existing approaches. Additionally, we\nhighlight the advantages of adaptive dropout tuning over manual selection,\nmaking uncertainty quantification more robust and accessible for EO\napplications."
                },
                "authors": [
                    {
                        "name": "Miro Miranda"
                    },
                    {
                        "name": "Francisco Mena"
                    },
                    {
                        "name": "Andreas Dengel"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Dengel"
                },
                "author": "Andreas Dengel",
                "arxiv_comment": "Accepted at Symposium on Intelligent Data Analysis (IDA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15050v2",
                "updated": "2025-04-09T14:18:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    18,
                    47,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-19T09:39:32Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    39,
                    32,
                    2,
                    78,
                    0
                ],
                "title": "Studying and Understanding the Effectiveness and Failures of\n  Conversational LLM-Based Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studying and Understanding the Effectiveness and Failures of\n  Conversational LLM-Based Repair"
                },
                "summary": "Automated program repair (APR) is designed to automate the process of\nbug-fixing. In recent years, thanks to the rapid development of large language\nmodels (LLMs), automated repair has achieved remarkable progress. Advanced APR\ntechniques powered by conversational LLMs, most notably ChatGPT, have exhibited\nimpressive repair abilities and gained increasing popularity due to the\ncapabilities of the underlying LLMs in providing repair feedback and performing\niterative patch improvement. Despite the superiority, conversational APR\ntechniques still fail to repair a large number of bugs. For example, a\nstate-of-the-art conversational technique ChatRepair does not correctly repair\nover half of the single-function bugs in the Defects4J dataset. To understand\nthe effectiveness and failures of conversational LLM-based repair and provide\npossible directions for improvement, we studied the exemplary ChatRepair with a\nfocus on comparing the effectiveness of its cloze-style and full function\nrepair strategies, assessing its key iterative component for patch improvement,\nand analyzing the repair failures. Our study has led to a series of findings,\nwhich we believe provide key implications for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated program repair (APR) is designed to automate the process of\nbug-fixing. In recent years, thanks to the rapid development of large language\nmodels (LLMs), automated repair has achieved remarkable progress. Advanced APR\ntechniques powered by conversational LLMs, most notably ChatGPT, have exhibited\nimpressive repair abilities and gained increasing popularity due to the\ncapabilities of the underlying LLMs in providing repair feedback and performing\niterative patch improvement. Despite the superiority, conversational APR\ntechniques still fail to repair a large number of bugs. For example, a\nstate-of-the-art conversational technique ChatRepair does not correctly repair\nover half of the single-function bugs in the Defects4J dataset. To understand\nthe effectiveness and failures of conversational LLM-based repair and provide\npossible directions for improvement, we studied the exemplary ChatRepair with a\nfocus on comparing the effectiveness of its cloze-style and full function\nrepair strategies, assessing its key iterative component for patch improvement,\nand analyzing the repair failures. Our study has led to a series of findings,\nwhich we believe provide key implications for future research."
                },
                "authors": [
                    {
                        "name": "Aolin Chen"
                    },
                    {
                        "name": "Haojun Wu"
                    },
                    {
                        "name": "Qi Xin"
                    },
                    {
                        "name": "Steven P. Reiss"
                    },
                    {
                        "name": "Jifeng Xuan"
                    }
                ],
                "author_detail": {
                    "name": "Jifeng Xuan"
                },
                "author": "Jifeng Xuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06910v1",
                "updated": "2025-04-09T14:14:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    14,
                    42,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T14:14:42Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    14,
                    42,
                    2,
                    99,
                    0
                ],
                "title": "Identifying Aspects in Peer Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Aspects in Peer Reviews"
                },
                "summary": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspect sets from review forms and guidelines of major NLP venues, yet\ndata-driven methods for aspect identification are largely underexplored. To\naddress this gap, our work takes a bottom-up approach: we propose an\noperational definition of aspect and develop a data-driven schema for deriving\nfine-grained aspects from a corpus of peer reviews. We introduce a dataset of\npeer reviews augmented with aspects and show how it can be used for\ncommunity-level review analysis. We further show how the choice of aspects can\nimpact downstream applications, such as LLM-generated review detection. Our\nresults lay a foundation for a principled and data-driven investigation of\nreview aspects, and pave the path for new applications of NLP to support peer\nreview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspect sets from review forms and guidelines of major NLP venues, yet\ndata-driven methods for aspect identification are largely underexplored. To\naddress this gap, our work takes a bottom-up approach: we propose an\noperational definition of aspect and develop a data-driven schema for deriving\nfine-grained aspects from a corpus of peer reviews. We introduce a dataset of\npeer reviews augmented with aspects and show how it can be used for\ncommunity-level review analysis. We further show how the choice of aspects can\nimpact downstream applications, such as LLM-generated review detection. Our\nresults lay a foundation for a principled and data-driven investigation of\nreview aspects, and pave the path for new applications of NLP to support peer\nreview."
                },
                "authors": [
                    {
                        "name": "Sheng Lu"
                    },
                    {
                        "name": "Ilia Kuznetsov"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16615v2",
                "updated": "2025-04-09T14:08:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    8,
                    58,
                    2,
                    99,
                    0
                ],
                "published": "2024-12-21T13:19:15Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    19,
                    15,
                    5,
                    356,
                    0
                ],
                "title": "Large Language Model Can Be a Foundation for Hidden Rationale-Based\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Can Be a Foundation for Hidden Rationale-Based\n  Retrieval"
                },
                "summary": "Despite the recent advancement in Retrieval-Augmented Generation (RAG)\nsystems, most retrieval methodologies are often developed for factual\nretrieval, which assumes query and positive documents are semantically similar.\nIn this paper, we instead propose and study a more challenging type of\nretrieval task, called hidden rationale retrieval, in which query and document\nare not similar but can be inferred by reasoning chains, logic relationships,\nor empirical experiences. To address such problems, an instruction-tuned Large\nlanguage model (LLM) with a cross-encoder architecture could be a reasonable\nchoice. To further strengthen pioneering LLM-based retrievers, we design a\nspecial instruction that transforms the retrieval task into a generative task\nby prompting LLM to answer a binary-choice question. The model can be\nfine-tuned with direct preference optimization (DPO). The framework is also\noptimized for computational efficiency with no performance degradation. We name\nthis retrieval framework by RaHoRe and verify its zero-shot and fine-tuned\nperformance superiority on Emotional Support Conversation (ESC), compared with\nprevious retrieval works. Our study suggests the potential to employ LLM as a\nfoundation for a wider scope of retrieval tasks. Our codes, models, and\ndatasets are available on https://github.com/flyfree5/LaHoRe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent advancement in Retrieval-Augmented Generation (RAG)\nsystems, most retrieval methodologies are often developed for factual\nretrieval, which assumes query and positive documents are semantically similar.\nIn this paper, we instead propose and study a more challenging type of\nretrieval task, called hidden rationale retrieval, in which query and document\nare not similar but can be inferred by reasoning chains, logic relationships,\nor empirical experiences. To address such problems, an instruction-tuned Large\nlanguage model (LLM) with a cross-encoder architecture could be a reasonable\nchoice. To further strengthen pioneering LLM-based retrievers, we design a\nspecial instruction that transforms the retrieval task into a generative task\nby prompting LLM to answer a binary-choice question. The model can be\nfine-tuned with direct preference optimization (DPO). The framework is also\noptimized for computational efficiency with no performance degradation. We name\nthis retrieval framework by RaHoRe and verify its zero-shot and fine-tuned\nperformance superiority on Emotional Support Conversation (ESC), compared with\nprevious retrieval works. Our study suggests the potential to employ LLM as a\nfoundation for a wider scope of retrieval tasks. Our codes, models, and\ndatasets are available on https://github.com/flyfree5/LaHoRe."
                },
                "authors": [
                    {
                        "name": "Luo Ji"
                    },
                    {
                        "name": "Feixiang Guo"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Ningyuan Xi"
                    },
                    {
                        "name": "Yihong Wang"
                    },
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Hongyang Lei"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yong Chen"
                },
                "author": "Yong Chen",
                "arxiv_comment": "10 pages, 3 figures, ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06895v1",
                "updated": "2025-04-09T13:55:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    55,
                    32,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T13:55:32Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    55,
                    32,
                    2,
                    99,
                    0
                ],
                "title": "ColorizeDiffusion v2: Enhancing Reference-based Sketch Colorization\n  Through Separating Utilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ColorizeDiffusion v2: Enhancing Reference-based Sketch Colorization\n  Through Separating Utilities"
                },
                "summary": "Reference-based sketch colorization methods have garnered significant\nattention due to their potential applications in the animation production\nindustry. However, most existing methods are trained with image triplets of\nsketch, reference, and ground truth that are semantically and spatially\nwell-aligned, while real-world references and sketches often exhibit\nsubstantial misalignment. This mismatch in data distribution between training\nand inference leads to overfitting, consequently resulting in spatial artifacts\nand significant degradation in overall colorization quality, limiting potential\napplications of current methods for general purposes. To address this\nlimitation, we conduct an in-depth analysis of the \\textbf{carrier}, defined as\nthe latent representation facilitating information transfer from reference to\nsketch. Based on this analysis, we propose a novel workflow that dynamically\nadapts the carrier to optimize distinct aspects of colorization. Specifically,\nfor spatially misaligned artifacts, we introduce a split cross-attention\nmechanism with spatial masks, enabling region-specific reference injection\nwithin the diffusion process. To mitigate semantic neglect of sketches, we\nemploy dedicated background and style encoders to transfer detailed reference\ninformation in the latent feature space, achieving enhanced spatial control and\nricher detail synthesis. Furthermore, we propose character-mask merging and\nbackground bleaching as preprocessing steps to improve foreground-background\nintegration and background generation. Extensive qualitative and quantitative\nevaluations, including a user study, demonstrate the superior performance of\nour proposed method compared to existing approaches. An ablation study further\nvalidates the efficacy of each proposed component.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference-based sketch colorization methods have garnered significant\nattention due to their potential applications in the animation production\nindustry. However, most existing methods are trained with image triplets of\nsketch, reference, and ground truth that are semantically and spatially\nwell-aligned, while real-world references and sketches often exhibit\nsubstantial misalignment. This mismatch in data distribution between training\nand inference leads to overfitting, consequently resulting in spatial artifacts\nand significant degradation in overall colorization quality, limiting potential\napplications of current methods for general purposes. To address this\nlimitation, we conduct an in-depth analysis of the \\textbf{carrier}, defined as\nthe latent representation facilitating information transfer from reference to\nsketch. Based on this analysis, we propose a novel workflow that dynamically\nadapts the carrier to optimize distinct aspects of colorization. Specifically,\nfor spatially misaligned artifacts, we introduce a split cross-attention\nmechanism with spatial masks, enabling region-specific reference injection\nwithin the diffusion process. To mitigate semantic neglect of sketches, we\nemploy dedicated background and style encoders to transfer detailed reference\ninformation in the latent feature space, achieving enhanced spatial control and\nricher detail synthesis. Furthermore, we propose character-mask merging and\nbackground bleaching as preprocessing steps to improve foreground-background\nintegration and background generation. Extensive qualitative and quantitative\nevaluations, including a user study, demonstrate the superior performance of\nour proposed method compared to existing approaches. An ablation study further\nvalidates the efficacy of each proposed component."
                },
                "authors": [
                    {
                        "name": "Dingkun Yan"
                    },
                    {
                        "name": "Xinrui Wang"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Suguru Saito"
                    },
                    {
                        "name": "Jiaxian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxian Guo"
                },
                "author": "Jiaxian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06204v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06204v3",
                "updated": "2025-04-09T13:54:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    54,
                    59,
                    2,
                    99,
                    0
                ],
                "published": "2024-06-26T16:34:33Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    16,
                    34,
                    33,
                    2,
                    178,
                    0
                ],
                "title": "A Survey on Mixture of Experts in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Mixture of Experts in Large Language Models"
                },
                "summary": "Large language models (LLMs) have garnered unprecedented advancements across\ndiverse fields, ranging from natural language processing to computer vision and\nbeyond. The prowess of LLMs is underpinned by their substantial model size,\nextensive and diverse datasets, and the vast computational power harnessed\nduring training, all of which contribute to the emergent abilities of LLMs\n(e.g., in-context learning) that are not present in small models. Within this\ncontext, the mixture of experts (MoE) has emerged as an effective method for\nsubstantially scaling up model capacity with minimal computation overhead,\ngaining significant attention from academia and industry. Despite its growing\nprevalence, there lacks a systematic and comprehensive review of the literature\non MoE. This survey seeks to bridge that gap, serving as an essential resource\nfor researchers delving into the intricacies of MoE. We first briefly introduce\nthe structure of the MoE layer, followed by proposing a new taxonomy of MoE.\nNext, we overview the core designs for various MoE models including both\nalgorithmic and systemic aspects, alongside collections of available\nopen-source implementations, hyperparameter configurations and empirical\nevaluations. Furthermore, we delineate the multifaceted applications of MoE in\npractice, and outline some potential directions for future research. To\nfacilitate ongoing updates and the sharing of cutting-edge advances in MoE\nresearch, we have established a resource repository at\nhttps://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have garnered unprecedented advancements across\ndiverse fields, ranging from natural language processing to computer vision and\nbeyond. The prowess of LLMs is underpinned by their substantial model size,\nextensive and diverse datasets, and the vast computational power harnessed\nduring training, all of which contribute to the emergent abilities of LLMs\n(e.g., in-context learning) that are not present in small models. Within this\ncontext, the mixture of experts (MoE) has emerged as an effective method for\nsubstantially scaling up model capacity with minimal computation overhead,\ngaining significant attention from academia and industry. Despite its growing\nprevalence, there lacks a systematic and comprehensive review of the literature\non MoE. This survey seeks to bridge that gap, serving as an essential resource\nfor researchers delving into the intricacies of MoE. We first briefly introduce\nthe structure of the MoE layer, followed by proposing a new taxonomy of MoE.\nNext, we overview the core designs for various MoE models including both\nalgorithmic and systemic aspects, alongside collections of available\nopen-source implementations, hyperparameter configurations and empirical\nevaluations. Furthermore, we delineate the multifaceted applications of MoE in\npractice, and outline some potential directions for future research. To\nfacilitate ongoing updates and the sharing of cutting-edge advances in MoE\nresearch, we have established a resource repository at\nhttps://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs."
                },
                "authors": [
                    {
                        "name": "Weilin Cai"
                    },
                    {
                        "name": "Juyong Jiang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Jing Tang"
                    },
                    {
                        "name": "Sunghun Kim"
                    },
                    {
                        "name": "Jiayi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Huang"
                },
                "author": "Jiayi Huang",
                "arxiv_doi": "10.1109/TKDE.2025.3554028",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TKDE.2025.3554028",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.06204v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06204v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The first three authors contributed equally to this work; Accepted by\n  TKDE",
                "arxiv_journal_ref": "IEEE Transactions on Knowledge and Data Engineering (TKDE) 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06017v2",
                "updated": "2025-04-09T13:54:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    54,
                    18,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T13:22:09Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    22,
                    9,
                    1,
                    98,
                    0
                ],
                "title": "CAI: An Open, Bug Bounty-Ready Cybersecurity AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAI: An Open, Bug Bounty-Ready Cybersecurity AI"
                },
                "summary": "By 2028 most cybersecurity actions will be autonomous, with humans\nteleoperating. We present the first classification of autonomy levels in\ncybersecurity and introduce Cybersecurity AI (CAI), an open-source framework\nthat democratizes advanced security testing through specialized AI agents.\nThrough rigorous empirical evaluation, we demonstrate that CAI consistently\noutperforms state-of-the-art results in CTF benchmarks, solving challenges\nacross diverse categories with significantly greater efficiency -up to 3,600x\nfaster than humans in specific tasks and averaging 11x faster overall. CAI\nachieved first place among AI teams and secured a top-20 position worldwide in\nthe \"AI vs Human\" CTF live Challenge, earning a monetary reward of $750. Based\non our results, we argue against LLM-vendor claims about limited security\ncapabilities. Beyond cybersecurity competitions, CAI demonstrates real-world\neffectiveness, reaching top-30 in Spain and top-500 worldwide on Hack The Box\nwithin a week, while dramatically reducing security testing costs by an average\nof 156x. Our framework transcends theoretical benchmarks by enabling\nnon-professionals to discover significant security bugs (CVSS 4.3-7.5) at rates\ncomparable to experts during bug bounty exercises. By combining modular agent\ndesign with seamless tool integration and human oversight (HITL), CAI addresses\ncritical market gaps, offering organizations of all sizes access to AI-powered\nbug bounty security testing previously available only to well-resourced firms\n-thereby challenging the oligopolistic ecosystem currently dominated by major\nbug bounty platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By 2028 most cybersecurity actions will be autonomous, with humans\nteleoperating. We present the first classification of autonomy levels in\ncybersecurity and introduce Cybersecurity AI (CAI), an open-source framework\nthat democratizes advanced security testing through specialized AI agents.\nThrough rigorous empirical evaluation, we demonstrate that CAI consistently\noutperforms state-of-the-art results in CTF benchmarks, solving challenges\nacross diverse categories with significantly greater efficiency -up to 3,600x\nfaster than humans in specific tasks and averaging 11x faster overall. CAI\nachieved first place among AI teams and secured a top-20 position worldwide in\nthe \"AI vs Human\" CTF live Challenge, earning a monetary reward of $750. Based\non our results, we argue against LLM-vendor claims about limited security\ncapabilities. Beyond cybersecurity competitions, CAI demonstrates real-world\neffectiveness, reaching top-30 in Spain and top-500 worldwide on Hack The Box\nwithin a week, while dramatically reducing security testing costs by an average\nof 156x. Our framework transcends theoretical benchmarks by enabling\nnon-professionals to discover significant security bugs (CVSS 4.3-7.5) at rates\ncomparable to experts during bug bounty exercises. By combining modular agent\ndesign with seamless tool integration and human oversight (HITL), CAI addresses\ncritical market gaps, offering organizations of all sizes access to AI-powered\nbug bounty security testing previously available only to well-resourced firms\n-thereby challenging the oligopolistic ecosystem currently dominated by major\nbug bounty platforms."
                },
                "authors": [
                    {
                        "name": "Víctor Mayoral-Vilches"
                    },
                    {
                        "name": "Luis Javier Navarrete-Lozano"
                    },
                    {
                        "name": "María Sanz-Gómez"
                    },
                    {
                        "name": "Lidia Salas Espejo"
                    },
                    {
                        "name": "Martiño Crespo-Álvarez"
                    },
                    {
                        "name": "Francisco Oca-Gonzalez"
                    },
                    {
                        "name": "Francesco Balassone"
                    },
                    {
                        "name": "Alfonso Glera-Picón"
                    },
                    {
                        "name": "Unai Ayucar-Carbajo"
                    },
                    {
                        "name": "Jon Ander Ruiz-Alcalde"
                    },
                    {
                        "name": "Stefan Rass"
                    },
                    {
                        "name": "Martin Pinzger"
                    },
                    {
                        "name": "Endika Gil-Uriarte"
                    }
                ],
                "author_detail": {
                    "name": "Endika Gil-Uriarte"
                },
                "author": "Endika Gil-Uriarte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06891v1",
                "updated": "2025-04-09T13:51:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    51,
                    42,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T13:51:42Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    51,
                    42,
                    2,
                    99,
                    0
                ],
                "title": "Helioseismic inference of the solar radiative opacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Helioseismic inference of the solar radiative opacity"
                },
                "summary": "The Sun is the most studied of all stars, and thus constitutes a benchmark\nfor stellar models. However, our vision of the Sun is still incomplete, as\nillustrated by the current debate on its chemical composition. The problem\nreaches far beyond chemical abundances and is intimately linked to microscopic\nand macroscopic physical ingredients of solar models such as radiative opacity,\nfor which experimental results have been recently measured that still await\ntheoretical explanations. We present opacity profiles derived from helioseismic\ninferences and compare them with detailed theoretical computations of\nindividual element contributions using three different opacity computation\ncodes, in a complementary way to experimental results. We find that our seismic\nopacity is about 10% higher than theoretical values used in current solar\nmodels around 2 million degrees, but lower by 35% than some recent available\ntheoretical values. Using the Sun as a laboratory of fundamental physics, we\nshow that quantitative comparisons between various opacity tables are required\nto understand the origin of the discrepancies between reported helioseismic,\ntheoretical and experimental opacity values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Sun is the most studied of all stars, and thus constitutes a benchmark\nfor stellar models. However, our vision of the Sun is still incomplete, as\nillustrated by the current debate on its chemical composition. The problem\nreaches far beyond chemical abundances and is intimately linked to microscopic\nand macroscopic physical ingredients of solar models such as radiative opacity,\nfor which experimental results have been recently measured that still await\ntheoretical explanations. We present opacity profiles derived from helioseismic\ninferences and compare them with detailed theoretical computations of\nindividual element contributions using three different opacity computation\ncodes, in a complementary way to experimental results. We find that our seismic\nopacity is about 10% higher than theoretical values used in current solar\nmodels around 2 million degrees, but lower by 35% than some recent available\ntheoretical values. Using the Sun as a laboratory of fundamental physics, we\nshow that quantitative comparisons between various opacity tables are required\nto understand the origin of the discrepancies between reported helioseismic,\ntheoretical and experimental opacity values."
                },
                "authors": [
                    {
                        "name": "Gaël Buldgen"
                    },
                    {
                        "name": "Jean-Christophe Pain"
                    },
                    {
                        "name": "Philippe Cossé"
                    },
                    {
                        "name": "Christophe Blancard"
                    },
                    {
                        "name": "Franck Gilleron"
                    },
                    {
                        "name": "Anil Pradhan"
                    },
                    {
                        "name": "Christopher J. Fontes"
                    },
                    {
                        "name": "James Colgan"
                    },
                    {
                        "name": "Arlette Noels"
                    },
                    {
                        "name": "Joergen Christensen-Dalsgaard"
                    },
                    {
                        "name": "Morgan Deal"
                    },
                    {
                        "name": "Sergey V. Ayukov"
                    },
                    {
                        "name": "Vladimir A. Baturin"
                    },
                    {
                        "name": "Anna V. Oreshina"
                    },
                    {
                        "name": "Richard Scuflaire"
                    },
                    {
                        "name": "Charly Pinçon"
                    },
                    {
                        "name": "Yveline Lebreton"
                    },
                    {
                        "name": "Thierry Corbard"
                    },
                    {
                        "name": "Patrick Eggenberger"
                    },
                    {
                        "name": "Peter Hakel"
                    },
                    {
                        "name": "David P. Kilcrease"
                    }
                ],
                "author_detail": {
                    "name": "David P. Kilcrease"
                },
                "author": "David P. Kilcrease",
                "arxiv_doi": "10.1038/s41467-024-54793-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41467-024-54793-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Nature Communications on January 27. The Arxiv version\n  is the version including supplementary figures as part of the Methods Section",
                "arxiv_journal_ref": "Nat Commun 16, 693 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15209v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15209v2",
                "updated": "2025-04-09T13:46:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    46,
                    27,
                    2,
                    99,
                    0
                ],
                "published": "2024-11-20T10:32:22Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    32,
                    22,
                    2,
                    325,
                    0
                ],
                "title": "Quantized symbolic time series approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized symbolic time series approximation"
                },
                "summary": "Time series are ubiquitous in numerous science and engineering domains, e.g.,\nsignal processing, bioinformatics, and astronomy. Previous work has verified\nthe efficacy of symbolic time series representation in a variety of engineering\napplications due to its storage efficiency and numerosity reduction. The most\nrecent symbolic aggregate approximation technique, ABBA, has been shown to\npreserve essential shape information of time series and improve downstream\napplications, e.g., neural network inference regarding prediction and anomaly\ndetection in time series.\n  Motivated by the emergence of high-performance hardware which enables\nefficient computation for low bit-width representations, we present a new\nquantization-based ABBA symbolic approximation technique, QABBA, which exhibits\nimproved storage efficiency while retaining the original speed and accuracy of\nsymbolic reconstruction. We prove an upper bound for the error arising from\nquantization and discuss how the number of bits should be chosen to balance\nthis with other errors.\n  An application of QABBA with large language models (LLMs) for time series\nregression is also presented, and its utility is investigated. By representing\nthe symbolic chain of patterns on time series, QABBA not only avoids the\ntraining of embedding from scratch, but also achieves a new state-of-the-art on\nMonash regression dataset. The symbolic approximation to the time series offers\na more efficient way to fine-tune LLMs on the time series regression task which\ncontains various application domains. We further present a set of extensive\nexperiments performed across various well-established datasets to demonstrate\nthe advantages of the QABBA method for symbolic approximation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series are ubiquitous in numerous science and engineering domains, e.g.,\nsignal processing, bioinformatics, and astronomy. Previous work has verified\nthe efficacy of symbolic time series representation in a variety of engineering\napplications due to its storage efficiency and numerosity reduction. The most\nrecent symbolic aggregate approximation technique, ABBA, has been shown to\npreserve essential shape information of time series and improve downstream\napplications, e.g., neural network inference regarding prediction and anomaly\ndetection in time series.\n  Motivated by the emergence of high-performance hardware which enables\nefficient computation for low bit-width representations, we present a new\nquantization-based ABBA symbolic approximation technique, QABBA, which exhibits\nimproved storage efficiency while retaining the original speed and accuracy of\nsymbolic reconstruction. We prove an upper bound for the error arising from\nquantization and discuss how the number of bits should be chosen to balance\nthis with other errors.\n  An application of QABBA with large language models (LLMs) for time series\nregression is also presented, and its utility is investigated. By representing\nthe symbolic chain of patterns on time series, QABBA not only avoids the\ntraining of embedding from scratch, but also achieves a new state-of-the-art on\nMonash regression dataset. The symbolic approximation to the time series offers\na more efficient way to fine-tune LLMs on the time series regression task which\ncontains various application domains. We further present a set of extensive\nexperiments performed across various well-established datasets to demonstrate\nthe advantages of the QABBA method for symbolic approximation."
                },
                "authors": [
                    {
                        "name": "Erin Carson"
                    },
                    {
                        "name": "Xinye Chen"
                    },
                    {
                        "name": "Cheng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Kang"
                },
                "author": "Cheng Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15209v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15209v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13230v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13230v2",
                "updated": "2025-04-09T13:38:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    38,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-08-23T17:11:04Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    11,
                    4,
                    4,
                    236,
                    0
                ],
                "title": "Amortized Bayesian Multilevel Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized Bayesian Multilevel Models"
                },
                "summary": "Multilevel models (MLMs) are a central building block of the Bayesian\nworkflow. They enable joint, interpretable modeling of data across hierarchical\nlevels and provide a fully probabilistic quantification of uncertainty. Despite\ntheir well-recognized advantages, MLMs pose significant computational\nchallenges, often rendering their estimation and evaluation intractable within\nreasonable time constraints. Recent advances in simulation-based inference\noffer promising solutions for addressing complex probabilistic models using\ndeep generative networks. However, the utility and reliability of deep learning\nmethods for estimating Bayesian MLMs remains largely unexplored, especially\nwhen compared with gold-standard samplers. To this end, we explore a family of\nneural network architectures that leverage the probabilistic factorization of\nmultilevel models to facilitate efficient neural network training and\nsubsequent near-instant posterior inference on unseen datasets. We test our\nmethod on several real-world case studies and provide comprehensive comparisons\nto Stan's gold standard sampler, where possible. Finally, we provide an\nopen-source implementation of our methods to stimulate further research in the\nnascent field of amortized Bayesian inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilevel models (MLMs) are a central building block of the Bayesian\nworkflow. They enable joint, interpretable modeling of data across hierarchical\nlevels and provide a fully probabilistic quantification of uncertainty. Despite\ntheir well-recognized advantages, MLMs pose significant computational\nchallenges, often rendering their estimation and evaluation intractable within\nreasonable time constraints. Recent advances in simulation-based inference\noffer promising solutions for addressing complex probabilistic models using\ndeep generative networks. However, the utility and reliability of deep learning\nmethods for estimating Bayesian MLMs remains largely unexplored, especially\nwhen compared with gold-standard samplers. To this end, we explore a family of\nneural network architectures that leverage the probabilistic factorization of\nmultilevel models to facilitate efficient neural network training and\nsubsequent near-instant posterior inference on unseen datasets. We test our\nmethod on several real-world case studies and provide comprehensive comparisons\nto Stan's gold standard sampler, where possible. Finally, we provide an\nopen-source implementation of our methods to stimulate further research in the\nnascent field of amortized Bayesian inference."
                },
                "authors": [
                    {
                        "name": "Daniel Habermann"
                    },
                    {
                        "name": "Marvin Schmitt"
                    },
                    {
                        "name": "Lars Kühmichel"
                    },
                    {
                        "name": "Andreas Bulling"
                    },
                    {
                        "name": "Stefan T. Radev"
                    },
                    {
                        "name": "Paul-Christian Bürkner"
                    }
                ],
                "author_detail": {
                    "name": "Paul-Christian Bürkner"
                },
                "author": "Paul-Christian Bürkner",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13230v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13230v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12172v2",
                "updated": "2025-04-09T13:30:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    30,
                    18,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-15T15:29:05Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    15,
                    29,
                    5,
                    5,
                    74,
                    0
                ],
                "title": "SEAL: Semantic Aware Image Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Semantic Aware Image Watermarking"
                },
                "summary": "Generative models have rapidly evolved to generate realistic outputs.\nHowever, their synthetic outputs increasingly challenge the clear distinction\nbetween natural and AI-generated content, necessitating robust watermarking\ntechniques. Watermarks are typically expected to preserve the integrity of the\ntarget image, withstand removal attempts, and prevent unauthorized replication\nonto unrelated images. To address this need, recent methods embed persistent\nwatermarks into images produced by diffusion models using the initial noise.\nYet, to do so, they either distort the distribution of generated images or rely\non searching through a long dictionary of used keys for detection.\n  In this paper, we propose a novel watermarking method that embeds semantic\ninformation about the generated image directly into the watermark, enabling a\ndistortion-free watermark that can be verified without requiring a database of\nkey patterns. Instead, the key pattern can be inferred from the semantic\nembedding of the image using locality-sensitive hashing. Furthermore,\nconditioning the watermark detection on the original image content improves\nrobustness against forgery attacks. To demonstrate that, we consider two\nlargely overlooked attack strategies: (i) an attacker extracting the initial\nnoise and generating a novel image with the same pattern; (ii) an attacker\ninserting an unrelated (potentially harmful) object into a watermarked image,\npossibly while preserving the watermark. We empirically validate our method's\nincreased robustness to these attacks. Taken together, our results suggest that\ncontent-aware watermarks can mitigate risks arising from image-generative\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have rapidly evolved to generate realistic outputs.\nHowever, their synthetic outputs increasingly challenge the clear distinction\nbetween natural and AI-generated content, necessitating robust watermarking\ntechniques. Watermarks are typically expected to preserve the integrity of the\ntarget image, withstand removal attempts, and prevent unauthorized replication\nonto unrelated images. To address this need, recent methods embed persistent\nwatermarks into images produced by diffusion models using the initial noise.\nYet, to do so, they either distort the distribution of generated images or rely\non searching through a long dictionary of used keys for detection.\n  In this paper, we propose a novel watermarking method that embeds semantic\ninformation about the generated image directly into the watermark, enabling a\ndistortion-free watermark that can be verified without requiring a database of\nkey patterns. Instead, the key pattern can be inferred from the semantic\nembedding of the image using locality-sensitive hashing. Furthermore,\nconditioning the watermark detection on the original image content improves\nrobustness against forgery attacks. To demonstrate that, we consider two\nlargely overlooked attack strategies: (i) an attacker extracting the initial\nnoise and generating a novel image with the same pattern; (ii) an attacker\ninserting an unrelated (potentially harmful) object into a watermarked image,\npossibly while preserving the watermark. We empirically validate our method's\nincreased robustness to these attacks. Taken together, our results suggest that\ncontent-aware watermarks can mitigate risks arising from image-generative\nmodels."
                },
                "authors": [
                    {
                        "name": "Kasra Arabi"
                    },
                    {
                        "name": "R. Teal Witter"
                    },
                    {
                        "name": "Chinmay Hegde"
                    },
                    {
                        "name": "Niv Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Niv Cohen"
                },
                "author": "Niv Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07413v2",
                "updated": "2025-04-09T13:29:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    29,
                    9,
                    2,
                    99,
                    0
                ],
                "published": "2024-11-11T22:36:33Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    22,
                    36,
                    33,
                    0,
                    316,
                    0
                ],
                "title": "ODEStream: A Buffer-Free Online Learning Framework with ODE-based\n  Adaptor for Streaming Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ODEStream: A Buffer-Free Online Learning Framework with ODE-based\n  Adaptor for Streaming Time Series Forecasting"
                },
                "summary": "Addressing the challenges of irregularity and concept drift in streaming time\nseries is crucial for real-world predictive modelling. Previous studies in time\nseries continual learning often propose models that require buffering long\nsequences, potentially restricting the responsiveness of the inference system.\nMoreover, these models are typically designed for regularly sampled data, an\nunrealistic assumption in real-world scenarios. This paper introduces\nODEStream, a novel buffer-free continual learning framework that incorporates a\ntemporal isolation layer to capture temporal dependencies within the data.\nSimultaneously, it leverages the capability of neural ordinary differential\nequations to process irregular sequences and generate a continuous data\nrepresentation, enabling seamless adaptation to changing dynamics in a data\nstreaming scenario. Our approach focuses on learning how the dynamics and\ndistribution of historical data change over time, facilitating direct\nprocessing of streaming sequences. Evaluations on benchmark real-world datasets\ndemonstrate that ODEStream outperforms the state-of-the-art online learning and\nstreaming analysis baseline models, providing accurate predictions over\nextended periods while minimising performance degradation over time by learning\nhow the sequence dynamics change. The implementation of ODEStream is available\nat: https://github.com/FtoonAbushaqra/ODEStream.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the challenges of irregularity and concept drift in streaming time\nseries is crucial for real-world predictive modelling. Previous studies in time\nseries continual learning often propose models that require buffering long\nsequences, potentially restricting the responsiveness of the inference system.\nMoreover, these models are typically designed for regularly sampled data, an\nunrealistic assumption in real-world scenarios. This paper introduces\nODEStream, a novel buffer-free continual learning framework that incorporates a\ntemporal isolation layer to capture temporal dependencies within the data.\nSimultaneously, it leverages the capability of neural ordinary differential\nequations to process irregular sequences and generate a continuous data\nrepresentation, enabling seamless adaptation to changing dynamics in a data\nstreaming scenario. Our approach focuses on learning how the dynamics and\ndistribution of historical data change over time, facilitating direct\nprocessing of streaming sequences. Evaluations on benchmark real-world datasets\ndemonstrate that ODEStream outperforms the state-of-the-art online learning and\nstreaming analysis baseline models, providing accurate predictions over\nextended periods while minimising performance degradation over time by learning\nhow the sequence dynamics change. The implementation of ODEStream is available\nat: https://github.com/FtoonAbushaqra/ODEStream.git."
                },
                "authors": [
                    {
                        "name": "Futoon M. Abushaqra"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Yongli Ren"
                    },
                    {
                        "name": "Flora D. Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora D. Salim"
                },
                "author": "Flora D. Salim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.15787v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.15787v3",
                "updated": "2025-04-09T13:27:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    27,
                    11,
                    2,
                    99,
                    0
                ],
                "published": "2023-06-27T20:34:21Z",
                "published_parsed": [
                    2023,
                    6,
                    27,
                    20,
                    34,
                    21,
                    1,
                    178,
                    0
                ],
                "title": "Network inference via approximate Bayesian computation. Illustration on\n  a stochastic multi-population neural mass model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network inference via approximate Bayesian computation. Illustration on\n  a stochastic multi-population neural mass model"
                },
                "summary": "In this article, we propose an adapted sequential Monte Carlo approximate\nBayesian computation (SMC-ABC) algorithm for network inference in coupled\nstochastic differential equations (SDEs) used for multivariate time series\nmodeling. Our approach is motivated by neuroscience, specifically the challenge\nof estimating brain connectivity before and during epileptic seizures. To this\nend, we make four key contributions. First, we introduce a 6N-dimensional SDE\nto model the activity of N coupled neuronal populations, extending the\n(single-population) stochastic Jansen and Rit neural mass model used to\ndescribe human electroencephalography (EEG) rhythms, particularly epileptic\nactivity. Second, we construct a reliable and efficient numerical splitting\nscheme for the model simulation. Third, we apply the proposed adapted SMC-ABC\nalgorithm to the neural mass model and validate it on different types of\nsimulated data. Compared to standard SMC-ABC, our approach significantly\nreduces computational cost by requiring fewer model simulations to reach the\ndesired posterior region, thanks to the inclusion of binary parameters\ndescribing the presence or absence of coupling directions. Finally, we apply\nour method to real multi-channel EEG data, uncovering potential similarities in\npatients' brain activities across different epileptic seizures, as well as\ndifferences between pre-seizure and seizure periods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we propose an adapted sequential Monte Carlo approximate\nBayesian computation (SMC-ABC) algorithm for network inference in coupled\nstochastic differential equations (SDEs) used for multivariate time series\nmodeling. Our approach is motivated by neuroscience, specifically the challenge\nof estimating brain connectivity before and during epileptic seizures. To this\nend, we make four key contributions. First, we introduce a 6N-dimensional SDE\nto model the activity of N coupled neuronal populations, extending the\n(single-population) stochastic Jansen and Rit neural mass model used to\ndescribe human electroencephalography (EEG) rhythms, particularly epileptic\nactivity. Second, we construct a reliable and efficient numerical splitting\nscheme for the model simulation. Third, we apply the proposed adapted SMC-ABC\nalgorithm to the neural mass model and validate it on different types of\nsimulated data. Compared to standard SMC-ABC, our approach significantly\nreduces computational cost by requiring fewer model simulations to reach the\ndesired posterior region, thanks to the inclusion of binary parameters\ndescribing the presence or absence of coupling directions. Finally, we apply\nour method to real multi-channel EEG data, uncovering potential similarities in\npatients' brain activities across different epileptic seizures, as well as\ndifferences between pre-seizure and seizure periods."
                },
                "authors": [
                    {
                        "name": "Susanne Ditlevsen"
                    },
                    {
                        "name": "Massimiliano Tamborrino"
                    },
                    {
                        "name": "Irene Tubikanec"
                    }
                ],
                "author_detail": {
                    "name": "Irene Tubikanec"
                },
                "author": "Irene Tubikanec",
                "arxiv_comment": "37 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.15787v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.15787v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06870v1",
                "updated": "2025-04-09T13:21:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    21,
                    20,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T13:21:20Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    21,
                    20,
                    2,
                    99,
                    0
                ],
                "title": "Bayesian Component Separation for DESI LAE Automated Spectroscopic\n  Redshifts and Photometric Targeting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Component Separation for DESI LAE Automated Spectroscopic\n  Redshifts and Photometric Targeting"
                },
                "summary": "Lyman Alpha Emitters (LAEs) are valuable high-redshift cosmological probes\ntraditionally identified using specialized narrow-band photometric surveys. In\nground-based spectroscopy, it can be difficult to distinguish the sharp LAE\npeak from residual sky emission lines using automated methods, leading to\nmisclassified redshifts. We present a Bayesian spectral component separation\ntechnique to automatically determine spectroscopic redshifts for LAEs while\nmarginalizing over sky residuals. We use visually inspected spectra of LAEs\nobtained using the Dark Energy Spectroscopic Instrument (DESI) to create a\ndata-driven prior and can determine redshift by jointly inferring sky residual,\nLAE, and residual components for each individual spectrum. We demonstrate this\nmethod on 910 spectroscopically observed $z = 2-4$ DESI LAE candidate spectra\nand determine their redshifts with $>$90% accuracy when validated against\nvisually inspected redshifts. Using the $\\Delta \\chi^2$ value from our pipeline\nas a proxy for detection confidence, we then explore potential survey design\nchoices and implications for targeting LAEs with medium-band photometry. This\nmethod allows for scalability and accuracy in determining redshifts from DESI\nspectra, and the results provide recommendations for LAE targeting in\nanticipation of future high-redshift spectroscopic surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lyman Alpha Emitters (LAEs) are valuable high-redshift cosmological probes\ntraditionally identified using specialized narrow-band photometric surveys. In\nground-based spectroscopy, it can be difficult to distinguish the sharp LAE\npeak from residual sky emission lines using automated methods, leading to\nmisclassified redshifts. We present a Bayesian spectral component separation\ntechnique to automatically determine spectroscopic redshifts for LAEs while\nmarginalizing over sky residuals. We use visually inspected spectra of LAEs\nobtained using the Dark Energy Spectroscopic Instrument (DESI) to create a\ndata-driven prior and can determine redshift by jointly inferring sky residual,\nLAE, and residual components for each individual spectrum. We demonstrate this\nmethod on 910 spectroscopically observed $z = 2-4$ DESI LAE candidate spectra\nand determine their redshifts with $>$90% accuracy when validated against\nvisually inspected redshifts. Using the $\\Delta \\chi^2$ value from our pipeline\nas a proxy for detection confidence, we then explore potential survey design\nchoices and implications for targeting LAEs with medium-band photometry. This\nmethod allows for scalability and accuracy in determining redshifts from DESI\nspectra, and the results provide recommendations for LAE targeting in\nanticipation of future high-redshift spectroscopic surveys."
                },
                "authors": [
                    {
                        "name": "Ana Sofía M. Uzsoy"
                    },
                    {
                        "name": "Andrew K. Saydjari"
                    },
                    {
                        "name": "Arjun Dey"
                    },
                    {
                        "name": "Anand Raichoor"
                    },
                    {
                        "name": "Douglas P. Finkbeiner"
                    },
                    {
                        "name": "Eric Gawiser"
                    },
                    {
                        "name": "Kyoung-Soo Lee"
                    },
                    {
                        "name": "Steven Ahlen"
                    },
                    {
                        "name": "Davide Bianchi"
                    },
                    {
                        "name": "David Brooks"
                    },
                    {
                        "name": "Todd Claybaugh"
                    },
                    {
                        "name": "Andrei Cuceu"
                    },
                    {
                        "name": "Axel de la Macorra"
                    },
                    {
                        "name": "Peter Doel"
                    },
                    {
                        "name": "Andreu Font-Ribera"
                    },
                    {
                        "name": "Jaime E. Forero-Romero"
                    },
                    {
                        "name": "Enrique Gaztañaga"
                    },
                    {
                        "name": "Satya Gontcho A Gontcho"
                    },
                    {
                        "name": "Gaston Gutierrez"
                    },
                    {
                        "name": "Mustapha Ishak"
                    },
                    {
                        "name": "Robert Kehoe"
                    },
                    {
                        "name": "David Kirkby"
                    },
                    {
                        "name": "Anthony Kremin"
                    },
                    {
                        "name": "Martin Landriau"
                    },
                    {
                        "name": "Laurent Le Guillou"
                    },
                    {
                        "name": "Aaron Meisner"
                    },
                    {
                        "name": "Ramon Miquel"
                    },
                    {
                        "name": "John Moustakas"
                    },
                    {
                        "name": "Nathalie Palanque-Delabrouille"
                    },
                    {
                        "name": "Francisco Prada"
                    },
                    {
                        "name": "Ignasi Pérez-Ràfols"
                    },
                    {
                        "name": "Graziano Rossi"
                    },
                    {
                        "name": "Eusebio Sanchez"
                    },
                    {
                        "name": "David Schlegel"
                    },
                    {
                        "name": "Michael Schubnell"
                    },
                    {
                        "name": "Hee-Jong Seo"
                    },
                    {
                        "name": "David Sprayberry"
                    },
                    {
                        "name": "Gregory Tarlé"
                    },
                    {
                        "name": "Benjamin Alan Weaver"
                    },
                    {
                        "name": "Hu Zou"
                    }
                ],
                "author_detail": {
                    "name": "Hu Zou"
                },
                "author": "Hu Zou",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06861v1",
                "updated": "2025-04-09T13:11:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    11,
                    9,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T13:11:09Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    11,
                    9,
                    2,
                    99,
                    0
                ],
                "title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for\n  Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for\n  Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation"
                },
                "summary": "Zero-shot, training-free, image-based text-to-video generation is an emerging\narea that aims to generate videos using existing image-based diffusion models.\nCurrent methods in this space require specific architectural changes to image\ngeneration models, which limit their adaptability and scalability. In contrast\nto such methods, we provide a model-agnostic approach. We use intersections in\ndiffusion trajectories, working only with the latent values. We could not\nobtain localized frame-wise coherence and diversity using only the intersection\nof trajectories. Thus, we instead use a grid-based approach. An in-context\ntrained LLM is used to generate coherent frame-wise prompts; another is used to\nidentify differences between frames. Based on these, we obtain a CLIP-based\nattention mask that controls the timing of switching the prompts for each grid\ncell. Earlier switching results in higher variance, while later switching\nresults in more coherence. Therefore, our approach can ensure appropriate\ncontrol between coherence and variance for the frames. Our approach results in\nstate-of-the-art performance while being more flexible when working with\ndiverse image-generation models. The empirical analysis using quantitative\nmetrics and user studies confirms our model's superior temporal consistency,\nvisual fidelity and user satisfaction, thus providing a novel way to obtain\ntraining-free, image-based text-to-video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot, training-free, image-based text-to-video generation is an emerging\narea that aims to generate videos using existing image-based diffusion models.\nCurrent methods in this space require specific architectural changes to image\ngeneration models, which limit their adaptability and scalability. In contrast\nto such methods, we provide a model-agnostic approach. We use intersections in\ndiffusion trajectories, working only with the latent values. We could not\nobtain localized frame-wise coherence and diversity using only the intersection\nof trajectories. Thus, we instead use a grid-based approach. An in-context\ntrained LLM is used to generate coherent frame-wise prompts; another is used to\nidentify differences between frames. Based on these, we obtain a CLIP-based\nattention mask that controls the timing of switching the prompts for each grid\ncell. Earlier switching results in higher variance, while later switching\nresults in more coherence. Therefore, our approach can ensure appropriate\ncontrol between coherence and variance for the frames. Our approach results in\nstate-of-the-art performance while being more flexible when working with\ndiverse image-generation models. The empirical analysis using quantitative\nmetrics and user studies confirms our model's superior temporal consistency,\nvisual fidelity and user satisfaction, thus providing a novel way to obtain\ntraining-free, image-based text-to-video generation."
                },
                "authors": [
                    {
                        "name": "Diljeet Jagpal"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Vinay P. Namboodiri"
                    }
                ],
                "author_detail": {
                    "name": "Vinay P. Namboodiri"
                },
                "author": "Vinay P. Namboodiri",
                "arxiv_comment": "Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05523v2",
                "updated": "2025-04-09T13:09:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    9,
                    6,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-07T21:51:32Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    21,
                    51,
                    32,
                    0,
                    97,
                    0
                ],
                "title": "Pretraining Language Models for Diachronic Linguistic Change Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining Language Models for Diachronic Linguistic Change Discovery"
                },
                "summary": "Large language models (LLMs) have shown potential as tools for scientific\ndiscovery. This has engendered growing interest in their use in humanistic\ndisciplines, such as historical linguistics and literary studies. These fields\noften construct arguments on the basis of delineations like genre, or more\ninflexibly, time period. Although efforts have been made to restrict inference\nto specific domains via fine-tuning or model editing, we posit that the only\ntrue guarantee is domain-restricted pretraining -- typically, a data- and\ncompute-expensive proposition.\n  We show that efficient pretraining techniques can produce useful models over\ncorpora too large for easy manual inspection but too small for \"typical\" LLM\napproaches. We employ a novel date-attribution pipeline in order to obtain a\ntemporally-segmented dataset of five 10-million-word slices. We train two\ncorresponding five-model batteries over these corpus segments, efficient\npretraining and Llama3-8B parameter efficiently finetuned.\n  We find that the pretrained models are faster to train than the finetuned\nbaselines and that they better respect the historical divisions of our corpus.\nEmphasizing speed and precision over a-historical comprehensiveness enables a\nnumber of novel approaches to hypothesis discovery and testing in our target\nfields. Taking up diachronic linguistics as a testbed, we show that our method\nenables the detection of a diverse set of phenomena, including en masse lexical\nchange, non-lexical (grammatical and morphological) change, and word sense\nintroduction/obsolescence. We provide a ready-to-use pipeline that allows\nextension of our approach to other target fields with only minimal adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown potential as tools for scientific\ndiscovery. This has engendered growing interest in their use in humanistic\ndisciplines, such as historical linguistics and literary studies. These fields\noften construct arguments on the basis of delineations like genre, or more\ninflexibly, time period. Although efforts have been made to restrict inference\nto specific domains via fine-tuning or model editing, we posit that the only\ntrue guarantee is domain-restricted pretraining -- typically, a data- and\ncompute-expensive proposition.\n  We show that efficient pretraining techniques can produce useful models over\ncorpora too large for easy manual inspection but too small for \"typical\" LLM\napproaches. We employ a novel date-attribution pipeline in order to obtain a\ntemporally-segmented dataset of five 10-million-word slices. We train two\ncorresponding five-model batteries over these corpus segments, efficient\npretraining and Llama3-8B parameter efficiently finetuned.\n  We find that the pretrained models are faster to train than the finetuned\nbaselines and that they better respect the historical divisions of our corpus.\nEmphasizing speed and precision over a-historical comprehensiveness enables a\nnumber of novel approaches to hypothesis discovery and testing in our target\nfields. Taking up diachronic linguistics as a testbed, we show that our method\nenables the detection of a diverse set of phenomena, including en masse lexical\nchange, non-lexical (grammatical and morphological) change, and word sense\nintroduction/obsolescence. We provide a ready-to-use pipeline that allows\nextension of our approach to other target fields with only minimal adaptation."
                },
                "authors": [
                    {
                        "name": "Elisabeth Fittschen"
                    },
                    {
                        "name": "Sabrina Li"
                    },
                    {
                        "name": "Tom Lippincott"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Craig Messner"
                    }
                ],
                "author_detail": {
                    "name": "Craig Messner"
                },
                "author": "Craig Messner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06845v1",
                "updated": "2025-04-09T13:02:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    2,
                    33,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T13:02:33Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    2,
                    33,
                    2,
                    99,
                    0
                ],
                "title": "Probability density function for dispersion measure of fast radio burst\n  from extragalactic medium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probability density function for dispersion measure of fast radio burst\n  from extragalactic medium"
                },
                "summary": "Fast Radio Bursts (FRBs) have emerged as powerful probes in cosmology. An\noptimized method for extracting the cosmic baryon density from localized FRBs,\nbased on maximizing the joint likelihood function of the extragalactic\ndispersion measure ($\\mathrm{DM}_{\\mathrm{ext}}$), was proposed by Macquart et\nal. [Nature 581, 391 (2020)]. In this Letter, we identify a crucial term that\nwas omitted in their derivation of the probability density function (PDF) for\n$\\mathrm{DM}_{\\mathrm{ext}}$. Using simulated FRB data, we demonstrate that\nneglecting this term leads to a systematic bias in the inferred cosmic baryon\ndensity, with deviations exceeding the $1\\sigma$ confidence level. This\nhighlights the importance of the missing term for the reliable cosmological\napplication of FRBs. Furthermore, employing a sample of 88 real localized FRBs,\nwe find that the baryon density derived using the original PDF by Macquart et\nal. is inconsistent with the Planck 2018 CMB data, while our corrected PDF\nyields a result in excellent agreement. We conclude that the omitted term is\nessential and must be included in order to obtain accurate cosmological\nconstraints from FRB observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Radio Bursts (FRBs) have emerged as powerful probes in cosmology. An\noptimized method for extracting the cosmic baryon density from localized FRBs,\nbased on maximizing the joint likelihood function of the extragalactic\ndispersion measure ($\\mathrm{DM}_{\\mathrm{ext}}$), was proposed by Macquart et\nal. [Nature 581, 391 (2020)]. In this Letter, we identify a crucial term that\nwas omitted in their derivation of the probability density function (PDF) for\n$\\mathrm{DM}_{\\mathrm{ext}}$. Using simulated FRB data, we demonstrate that\nneglecting this term leads to a systematic bias in the inferred cosmic baryon\ndensity, with deviations exceeding the $1\\sigma$ confidence level. This\nhighlights the importance of the missing term for the reliable cosmological\napplication of FRBs. Furthermore, employing a sample of 88 real localized FRBs,\nwe find that the baryon density derived using the original PDF by Macquart et\nal. is inconsistent with the Planck 2018 CMB data, while our corrected PDF\nyields a result in excellent agreement. We conclude that the omitted term is\nessential and must be included in order to obtain accurate cosmological\nconstraints from FRB observations."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Hongwei Yu"
                    },
                    {
                        "name": "Puxun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Puxun Wu"
                },
                "author": "Puxun Wu",
                "arxiv_comment": "15 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06835v1",
                "updated": "2025-04-09T12:51:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    51,
                    10,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:51:10Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    51,
                    10,
                    2,
                    99,
                    0
                ],
                "title": "LVC: A Lightweight Compression Framework for Enhancing VLMs in Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVC: A Lightweight Compression Framework for Enhancing VLMs in Long\n  Video Understanding"
                },
                "summary": "Long video understanding is a complex task that requires both spatial detail\nand temporal awareness. While Vision-Language Models (VLMs) obtain frame-level\nunderstanding capabilities through multi-frame input, they suffer from\ninformation loss due to the sparse sampling strategy. In contrast, Video Large\nLanguage Models (Video-LLMs) capture temporal relationships within visual\nfeatures but are limited by the scarcity of high-quality video-text datasets.\nTo transfer long video understanding capabilities to VLMs with minimal data and\ncomputational cost, we propose Lightweight Video Compression (LVC), a novel\nmethod featuring the Query-Attention Video Compression mechanism, which\neffectively tackles the sparse sampling problem in VLMs. By training only the\nalignment layer with 10k short video-text pairs, LVC significantly enhances the\ntemporal reasoning abilities of VLMs. Extensive experiments show that LVC\nprovides consistent performance improvements across various models, including\nthe InternVL2 series and Phi-3.5-Vision. Notably, the InternVL2-40B-LVC\nachieves scores of 68.2 and 65.9 on the long video understanding benchmarks\nMLVU and Video-MME, respectively, with relative improvements of 14.6% and 7.7%.\nThe enhanced models and code will be publicly available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding is a complex task that requires both spatial detail\nand temporal awareness. While Vision-Language Models (VLMs) obtain frame-level\nunderstanding capabilities through multi-frame input, they suffer from\ninformation loss due to the sparse sampling strategy. In contrast, Video Large\nLanguage Models (Video-LLMs) capture temporal relationships within visual\nfeatures but are limited by the scarcity of high-quality video-text datasets.\nTo transfer long video understanding capabilities to VLMs with minimal data and\ncomputational cost, we propose Lightweight Video Compression (LVC), a novel\nmethod featuring the Query-Attention Video Compression mechanism, which\neffectively tackles the sparse sampling problem in VLMs. By training only the\nalignment layer with 10k short video-text pairs, LVC significantly enhances the\ntemporal reasoning abilities of VLMs. Extensive experiments show that LVC\nprovides consistent performance improvements across various models, including\nthe InternVL2 series and Phi-3.5-Vision. Notably, the InternVL2-40B-LVC\nachieves scores of 68.2 and 65.9 on the long video understanding benchmarks\nMLVU and Video-MME, respectively, with relative improvements of 14.6% and 7.7%.\nThe enhanced models and code will be publicly available soon."
                },
                "authors": [
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Haoran Wu"
                    },
                    {
                        "name": "Yiming Rong"
                    },
                    {
                        "name": "Deyang Jiang"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yunlong Zhao"
                    },
                    {
                        "name": "Shuang Xu"
                    },
                    {
                        "name": "Bo XU"
                    }
                ],
                "author_detail": {
                    "name": "Bo XU"
                },
                "author": "Bo XU",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23462v2",
                "updated": "2025-04-09T12:35:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    35,
                    29,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-30T14:37:21Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    14,
                    37,
                    21,
                    6,
                    89,
                    0
                ],
                "title": "Accelerated Stein Variational Gradient Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated Stein Variational Gradient Flow"
                },
                "summary": "Stein variational gradient descent (SVGD) is a kernel-based particle method\nfor sampling from a target distribution, e.g., in generative modeling and\nBayesian inference. SVGD does not require estimating the gradient of the\nlog-density, which is called score estimation. In practice, SVGD can be slow\ncompared to score-estimation based sampling algorithms. To design fast and\nefficient high-dimensional sampling algorithms, we introduce ASVGD, an\naccelerated SVGD, based on an accelerated gradient flow in a metric space of\nprobability densities following Nesterov's method. We then derive a\nmomentum-based discrete-time sampling algorithm, which evolves a set of\nparticles deterministically. To stabilize the particles' momentum update, we\nalso study a Wasserstein metric regularization. For the generalized bilinear\nkernel and the Gaussian kernel, toy numerical examples with varied target\ndistributions demonstrate the effectiveness of ASVGD compared to SVGD and other\npopular sampling methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stein variational gradient descent (SVGD) is a kernel-based particle method\nfor sampling from a target distribution, e.g., in generative modeling and\nBayesian inference. SVGD does not require estimating the gradient of the\nlog-density, which is called score estimation. In practice, SVGD can be slow\ncompared to score-estimation based sampling algorithms. To design fast and\nefficient high-dimensional sampling algorithms, we introduce ASVGD, an\naccelerated SVGD, based on an accelerated gradient flow in a metric space of\nprobability densities following Nesterov's method. We then derive a\nmomentum-based discrete-time sampling algorithm, which evolves a set of\nparticles deterministically. To stabilize the particles' momentum update, we\nalso study a Wasserstein metric regularization. For the generalized bilinear\nkernel and the Gaussian kernel, toy numerical examples with varied target\ndistributions demonstrate the effectiveness of ASVGD compared to SVGD and other\npopular sampling methods."
                },
                "authors": [
                    {
                        "name": "Viktor Stein"
                    },
                    {
                        "name": "Wuchen Li"
                    }
                ],
                "author_detail": {
                    "name": "Wuchen Li"
                },
                "author": "Wuchen Li",
                "arxiv_comment": "Submitted to GSI'25, 9 pages, 2 figures, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46N10 (Primary) 46E22 94A15 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06823v1",
                "updated": "2025-04-09T12:31:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    31,
                    25,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:31:25Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    31,
                    25,
                    2,
                    99,
                    0
                ],
                "title": "Open Problems and a Hypothetical Path Forward in LLM Knowledge Paradigms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Problems and a Hypothetical Path Forward in LLM Knowledge Paradigms"
                },
                "summary": "Knowledge is fundamental to the overall capabilities of Large Language Models\n(LLMs). The knowledge paradigm of a model, which dictates how it encodes and\nutilizes knowledge, significantly affects its performance. Despite the\ncontinuous development of LLMs under existing knowledge paradigms, issues\nwithin these frameworks continue to constrain model potential.\n  This blog post highlight three critical open problems limiting model\ncapabilities: (1) challenges in knowledge updating for LLMs, (2) the failure of\nreverse knowledge generalization (the reversal curse), and (3) conflicts in\ninternal knowledge. We review recent progress made in addressing these issues\nand discuss potential general solutions. Based on observations in these areas,\nwe propose a hypothetical paradigm based on Contextual Knowledge Scaling, and\nfurther outline implementation pathways that remain feasible within\ncontemporary techniques. Evidence suggests this approach holds potential to\naddress current shortcomings, serving as our vision for future model paradigms.\n  This blog post aims to provide researchers with a brief overview of progress\nin LLM knowledge systems, while provide inspiration for the development of\nnext-generation model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge is fundamental to the overall capabilities of Large Language Models\n(LLMs). The knowledge paradigm of a model, which dictates how it encodes and\nutilizes knowledge, significantly affects its performance. Despite the\ncontinuous development of LLMs under existing knowledge paradigms, issues\nwithin these frameworks continue to constrain model potential.\n  This blog post highlight three critical open problems limiting model\ncapabilities: (1) challenges in knowledge updating for LLMs, (2) the failure of\nreverse knowledge generalization (the reversal curse), and (3) conflicts in\ninternal knowledge. We review recent progress made in addressing these issues\nand discuss potential general solutions. Based on observations in these areas,\nwe propose a hypothetical paradigm based on Contextual Knowledge Scaling, and\nfurther outline implementation pathways that remain feasible within\ncontemporary techniques. Evidence suggests this approach holds potential to\naddress current shortcomings, serving as our vision for future model paradigms.\n  This blog post aims to provide researchers with a brief overview of progress\nin LLM knowledge systems, while provide inspiration for the development of\nnext-generation model architectures."
                },
                "authors": [
                    {
                        "name": "Xiaotian Ye"
                    },
                    {
                        "name": "Mengqi Zhang"
                    },
                    {
                        "name": "Shu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shu Wu"
                },
                "author": "Shu Wu",
                "arxiv_comment": "Blog post preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06803v1",
                "updated": "2025-04-09T11:48:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    48,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T11:48:37Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    48,
                    37,
                    2,
                    99,
                    0
                ],
                "title": "DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation"
                },
                "summary": "Diffusion Transformer (DiT), an emerging diffusion model for visual\ngeneration, has demonstrated superior performance but suffers from substantial\ncomputational costs. Our investigations reveal that these costs primarily stem\nfrom the \\emph{static} inference paradigm, which inevitably introduces\nredundant computation in certain \\emph{diffusion timesteps} and \\emph{spatial\nregions}. To overcome this inefficiency, we propose \\textbf{Dy}namic\n\\textbf{Di}ffusion \\textbf{T}ransformer (DyDiT), an architecture that\n\\emph{dynamically} adjusts its computation along both \\emph{timestep} and\n\\emph{spatial} dimensions. Specifically, we introduce a \\emph{Timestep-wise\nDynamic Width} (TDW) approach that adapts model width conditioned on the\ngeneration timesteps. In addition, we design a \\emph{Spatial-wise Dynamic\nToken} (SDT) strategy to avoid redundant computation at unnecessary spatial\nlocations. TDW and SDT can be seamlessly integrated into DiT and significantly\naccelerates the generation process. Building on these designs, we further\nenhance DyDiT in three key aspects. First, DyDiT is integrated seamlessly with\nflow matching-based generation, enhancing its versatility. Furthermore, we\nenhance DyDiT to tackle more complex visual generation tasks, including video\ngeneration and text-to-image generation, thereby broadening its real-world\napplications. Finally, to address the high cost of full fine-tuning and\ndemocratize technology access, we investigate the feasibility of training DyDiT\nin a parameter-efficient manner and introduce timestep-based dynamic LoRA\n(TD-LoRA). Extensive experiments on diverse visual generation models, including\nDiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT), an emerging diffusion model for visual\ngeneration, has demonstrated superior performance but suffers from substantial\ncomputational costs. Our investigations reveal that these costs primarily stem\nfrom the \\emph{static} inference paradigm, which inevitably introduces\nredundant computation in certain \\emph{diffusion timesteps} and \\emph{spatial\nregions}. To overcome this inefficiency, we propose \\textbf{Dy}namic\n\\textbf{Di}ffusion \\textbf{T}ransformer (DyDiT), an architecture that\n\\emph{dynamically} adjusts its computation along both \\emph{timestep} and\n\\emph{spatial} dimensions. Specifically, we introduce a \\emph{Timestep-wise\nDynamic Width} (TDW) approach that adapts model width conditioned on the\ngeneration timesteps. In addition, we design a \\emph{Spatial-wise Dynamic\nToken} (SDT) strategy to avoid redundant computation at unnecessary spatial\nlocations. TDW and SDT can be seamlessly integrated into DiT and significantly\naccelerates the generation process. Building on these designs, we further\nenhance DyDiT in three key aspects. First, DyDiT is integrated seamlessly with\nflow matching-based generation, enhancing its versatility. Furthermore, we\nenhance DyDiT to tackle more complex visual generation tasks, including video\ngeneration and text-to-image generation, thereby broadening its real-world\napplications. Finally, to address the high cost of full fine-tuning and\ndemocratize technology access, we investigate the feasibility of training DyDiT\nin a parameter-efficient manner and introduce timestep-based dynamic LoRA\n(TD-LoRA). Extensive experiments on diverse visual generation models, including\nDiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT."
                },
                "authors": [
                    {
                        "name": "Wangbo Zhao"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Yibing Song"
                    },
                    {
                        "name": "Gao Huang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "arxiv_comment": "Extended journal version for ICLR. arXiv admin note: substantial text\n  overlap with arXiv:2410.03456",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06792v1",
                "updated": "2025-04-09T11:34:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    34,
                    6,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T11:34:06Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    34,
                    6,
                    2,
                    99,
                    0
                ],
                "title": "Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot\n  Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot\n  Demonstrations"
                },
                "summary": "Mixture-of-Experts (MoE) models achieve a favorable trade-off between\nperformance and inference efficiency by activating only a subset of experts.\nHowever, the memory overhead of storing all experts remains a major limitation,\nespecially in large-scale MoE models such as DeepSeek-R1 (671B). In this study,\nwe investigate domain specialization and expert redundancy in large-scale MoE\nmodels and uncover a consistent behavior we term few-shot expert localization,\nwith only a few demonstrations, the model consistently activates a sparse and\nstable subset of experts. Building on this observation, we propose a simple yet\neffective pruning framework, EASY-EP, that leverages a few domain-specific\ndemonstrations to identify and retain only the most relevant experts. EASY-EP\ncomprises two key components: output-aware expert importance assessment and\nexpert-level token contribution estimation. The former evaluates the importance\nof each expert for the current token by considering the gating scores and\nmagnitudes of the outputs of activated experts, while the latter assesses the\ncontribution of tokens based on representation similarities after and before\nrouted experts. Experiments show that our method can achieve comparable\nperformances and $2.99\\times$ throughput under the same memory budget with full\nDeepSeek-R1 with only half the experts. Our code is available at\nhttps://github.com/RUCAIBox/EASYEP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models achieve a favorable trade-off between\nperformance and inference efficiency by activating only a subset of experts.\nHowever, the memory overhead of storing all experts remains a major limitation,\nespecially in large-scale MoE models such as DeepSeek-R1 (671B). In this study,\nwe investigate domain specialization and expert redundancy in large-scale MoE\nmodels and uncover a consistent behavior we term few-shot expert localization,\nwith only a few demonstrations, the model consistently activates a sparse and\nstable subset of experts. Building on this observation, we propose a simple yet\neffective pruning framework, EASY-EP, that leverages a few domain-specific\ndemonstrations to identify and retain only the most relevant experts. EASY-EP\ncomprises two key components: output-aware expert importance assessment and\nexpert-level token contribution estimation. The former evaluates the importance\nof each expert for the current token by considering the gating scores and\nmagnitudes of the outputs of activated experts, while the latter assesses the\ncontribution of tokens based on representation similarities after and before\nrouted experts. Experiments show that our method can achieve comparable\nperformances and $2.99\\times$ throughput under the same memory budget with full\nDeepSeek-R1 with only half the experts. Our code is available at\nhttps://github.com/RUCAIBox/EASYEP."
                },
                "authors": [
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Dong Wu"
                    },
                    {
                        "name": "Feng Xiao"
                    },
                    {
                        "name": "Zhifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhifeng Wang"
                },
                "author": "Zhifeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01313v2",
                "updated": "2025-04-09T11:30:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    30,
                    46,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-02T02:55:35Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    2,
                    55,
                    35,
                    2,
                    92,
                    0
                ],
                "title": "Analyzing Functional Data with a Mixture of Covariance Structures Using\n  a Curve-Based Sampling Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Functional Data with a Mixture of Covariance Structures Using\n  a Curve-Based Sampling Scheme"
                },
                "summary": "Motivated by distinct walking patterns in real-world free-living gait data,\nthis paper proposes an innovative curve-based sampling scheme for the analysis\nof functional data characterized by a mixture of covariance structures.\nTraditional approaches often fail to adequately capture inherent complexities\narising from heterogeneous covariance patterns across distinct subsets of the\ndata. We introduce a unified Bayesian framework that integrates a nonlinear\nregression function with a continuous-time hidden Markov model, enabling the\nidentification and utilization of varying covariance structures. One of the key\ncontributions is the development of a computationally efficient curve-based\nsampling scheme for hidden state estimation, addressing the sampling\ncomplexities associated with high-dimensional, conditionally dependent data.\nThis paper details the Bayesian inference procedure, examines the asymptotic\nproperties to ensure the structural consistency of the model, and demonstrates\nits effectiveness through simulated and real-world examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by distinct walking patterns in real-world free-living gait data,\nthis paper proposes an innovative curve-based sampling scheme for the analysis\nof functional data characterized by a mixture of covariance structures.\nTraditional approaches often fail to adequately capture inherent complexities\narising from heterogeneous covariance patterns across distinct subsets of the\ndata. We introduce a unified Bayesian framework that integrates a nonlinear\nregression function with a continuous-time hidden Markov model, enabling the\nidentification and utilization of varying covariance structures. One of the key\ncontributions is the development of a computationally efficient curve-based\nsampling scheme for hidden state estimation, addressing the sampling\ncomplexities associated with high-dimensional, conditionally dependent data.\nThis paper details the Bayesian inference procedure, examines the asymptotic\nproperties to ensure the structural consistency of the model, and demonstrates\nits effectiveness through simulated and real-world examples."
                },
                "authors": [
                    {
                        "name": "Yian Yu"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Jian Qing Shi"
                    }
                ],
                "author_detail": {
                    "name": "Jian Qing Shi"
                },
                "author": "Jian Qing Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06785v1",
                "updated": "2025-04-09T11:19:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    19,
                    17,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T11:19:17Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    19,
                    17,
                    2,
                    99,
                    0
                ],
                "title": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement\n  Monitoring"
                },
                "summary": "Effective and rapid evaluation of pavement surface condition is critical for\nprioritizing maintenance, ensuring transportation safety, and minimizing\nvehicle wear and tear. While conventional manual inspections suffer from\nsubjectivity, existing machine learning-based methods are constrained by their\nreliance on large and high-quality labeled datasets, which require significant\nresources and limit adaptability across varied road conditions. The\nrevolutionary advancements in Large Language Models (LLMs) present significant\npotential for overcoming these challenges. In this study, we propose an\ninnovative automated zero-shot learning approach that leverages the image\nrecognition and natural language understanding capabilities of LLMs to assess\nroad conditions effectively. Multiple LLM-based assessment models were\ndeveloped, employing prompt engineering strategies aligned with the Pavement\nSurface Condition Index (PSCI) standards. These models' accuracy and\nreliability were evaluated against official PSCI results, with an optimized\nmodel ultimately selected. Extensive tests benchmarked the optimized model\nagainst evaluations from various levels experts using Google Street View road\nimages. The results reveal that the LLM-based approach can effectively assess\nroad conditions, with the optimized model -employing comprehensive and\nstructured prompt engineering strategies -outperforming simpler configurations\nby achieving high accuracy and consistency, even surpassing expert evaluations.\nMoreover, successfully applying the optimized model to Google Street View\nimages demonstrates its potential for future city-scale deployments. These\nfindings highlight the transformative potential of LLMs in automating road\ndamage evaluations and underscore the pivotal role of detailed prompt\nengineering in achieving reliable assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and rapid evaluation of pavement surface condition is critical for\nprioritizing maintenance, ensuring transportation safety, and minimizing\nvehicle wear and tear. While conventional manual inspections suffer from\nsubjectivity, existing machine learning-based methods are constrained by their\nreliance on large and high-quality labeled datasets, which require significant\nresources and limit adaptability across varied road conditions. The\nrevolutionary advancements in Large Language Models (LLMs) present significant\npotential for overcoming these challenges. In this study, we propose an\ninnovative automated zero-shot learning approach that leverages the image\nrecognition and natural language understanding capabilities of LLMs to assess\nroad conditions effectively. Multiple LLM-based assessment models were\ndeveloped, employing prompt engineering strategies aligned with the Pavement\nSurface Condition Index (PSCI) standards. These models' accuracy and\nreliability were evaluated against official PSCI results, with an optimized\nmodel ultimately selected. Extensive tests benchmarked the optimized model\nagainst evaluations from various levels experts using Google Street View road\nimages. The results reveal that the LLM-based approach can effectively assess\nroad conditions, with the optimized model -employing comprehensive and\nstructured prompt engineering strategies -outperforming simpler configurations\nby achieving high accuracy and consistency, even surpassing expert evaluations.\nMoreover, successfully applying the optimized model to Google Street View\nimages demonstrates its potential for future city-scale deployments. These\nfindings highlight the transformative potential of LLMs in automating road\ndamage evaluations and underscore the pivotal role of detailed prompt\nengineering in achieving reliable assessments."
                },
                "authors": [
                    {
                        "name": "Shuoshuo Xu"
                    },
                    {
                        "name": "Kai Zhao"
                    },
                    {
                        "name": "James Loney"
                    },
                    {
                        "name": "Zili Li"
                    },
                    {
                        "name": "Andrea Visentin"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Visentin"
                },
                "author": "Andrea Visentin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06779v1",
                "updated": "2025-04-09T11:02:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    2,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T11:02:19Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    2,
                    19,
                    2,
                    99,
                    0
                ],
                "title": "What if we find nothing? Bayesian analysis of the statistical\n  information of null results in future exoplanet habitability and biosignature\n  surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What if we find nothing? Bayesian analysis of the statistical\n  information of null results in future exoplanet habitability and biosignature\n  surveys"
                },
                "summary": "Future telescopes will survey temperate, terrestrial exoplanets to estimate\nthe frequency of habitable ($\\eta_{\\text{Hab}}$) or inhabited\n($\\eta_{\\text{Life}}$) planets. This study aims to determine the minimum number\nof planets ($N$) required to draw statistically significant conclusions,\nparticularly in the case of a null result (i.e., no detections). Using a\nBayesian framework, we analyzed surveys of up to $N=100$ planets to infer the\nfrequency of a binary observable feature ($\\eta_{\\text{obs}}$) after null\nresults. Posterior best fits and upper limits were derived for various survey\nsizes and compared with predicted yields from missions like the Large\nInterferometer for Exoplanets (LIFE) and the Habitable Worlds Observatory\n(HWO). Our findings indicate that $N=20-50$ ``perfect'' observations (100\\%\nconfidence in detecting or excluding the feature) yield conclusions relatively\nindependent of priors. To achieve 99.9\\% upper limits of $\\eta_{\\text{obs}}\n\\leq 0.2/0.1$, approximately $N \\simeq 40/80$ observations are needed. For\n``imperfect'' observations, uncertainties in interpretation and sample biases\nbecome limiting factors. We show that LIFE and HWO aim for sufficiently large\nsurvey sizes to provide statistically meaningful estimates of habitable\nenvironments and life prevalence under these assumptions. However, robust\nconclusions require careful sample selection and high-confidence detection or\nexclusion of features in each observation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future telescopes will survey temperate, terrestrial exoplanets to estimate\nthe frequency of habitable ($\\eta_{\\text{Hab}}$) or inhabited\n($\\eta_{\\text{Life}}$) planets. This study aims to determine the minimum number\nof planets ($N$) required to draw statistically significant conclusions,\nparticularly in the case of a null result (i.e., no detections). Using a\nBayesian framework, we analyzed surveys of up to $N=100$ planets to infer the\nfrequency of a binary observable feature ($\\eta_{\\text{obs}}$) after null\nresults. Posterior best fits and upper limits were derived for various survey\nsizes and compared with predicted yields from missions like the Large\nInterferometer for Exoplanets (LIFE) and the Habitable Worlds Observatory\n(HWO). Our findings indicate that $N=20-50$ ``perfect'' observations (100\\%\nconfidence in detecting or excluding the feature) yield conclusions relatively\nindependent of priors. To achieve 99.9\\% upper limits of $\\eta_{\\text{obs}}\n\\leq 0.2/0.1$, approximately $N \\simeq 40/80$ observations are needed. For\n``imperfect'' observations, uncertainties in interpretation and sample biases\nbecome limiting factors. We show that LIFE and HWO aim for sufficiently large\nsurvey sizes to provide statistically meaningful estimates of habitable\nenvironments and life prevalence under these assumptions. However, robust\nconclusions require careful sample selection and high-confidence detection or\nexclusion of features in each observation."
                },
                "authors": [
                    {
                        "name": "Daniel Angerhausen"
                    },
                    {
                        "name": "Amedeo Balbi"
                    },
                    {
                        "name": "Andjelka B. Kovačević"
                    },
                    {
                        "name": "Emily O. Garvin"
                    },
                    {
                        "name": "Sascha P. Quanz"
                    }
                ],
                "author_detail": {
                    "name": "Sascha P. Quanz"
                },
                "author": "Sascha P. Quanz",
                "arxiv_doi": "10.3847/1538-3881/adb96d",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-3881/adb96d",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06778v1",
                "updated": "2025-04-09T10:58:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    58,
                    54,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T10:58:54Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    58,
                    54,
                    2,
                    99,
                    0
                ],
                "title": "Controllable Automatic Foley Artist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Automatic Foley Artist"
                },
                "summary": "Foley is a key element in video production, refers to the process of adding\nan audio signal to a silent video while ensuring semantic and temporal\nalignment. In recent years, the rise of personalized content creation and\nadvancements in automatic video-to-audio models have increased the demand for\ngreater user control in the process. One possible approach is to incorporate\ntext to guide audio generation. While supported by existing methods, challenges\nremain in ensuring compatibility between modalities, particularly when the text\nintroduces additional information or contradicts the sounds naturally inferred\nfrom the visuals. In this work, we introduce CAFA (Controllable Automatic Foley\nArtist) a video-and-text-to-audio model that generates semantically and\ntemporally aligned audio for a given video, guided by text input. CAFA is built\nupon a text-to-audio model and integrates video information through a modality\nadapter mechanism. By incorporating text, users can refine semantic details and\nintroduce creative variations, guiding the audio synthesis beyond the expected\nvideo contextual cues. Experiments show that besides its superior quality in\nterms of semantic alignment and audio-visual synchronization the proposed\nmethod enable high textual controllability as demonstrated in subjective and\nobjective evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foley is a key element in video production, refers to the process of adding\nan audio signal to a silent video while ensuring semantic and temporal\nalignment. In recent years, the rise of personalized content creation and\nadvancements in automatic video-to-audio models have increased the demand for\ngreater user control in the process. One possible approach is to incorporate\ntext to guide audio generation. While supported by existing methods, challenges\nremain in ensuring compatibility between modalities, particularly when the text\nintroduces additional information or contradicts the sounds naturally inferred\nfrom the visuals. In this work, we introduce CAFA (Controllable Automatic Foley\nArtist) a video-and-text-to-audio model that generates semantically and\ntemporally aligned audio for a given video, guided by text input. CAFA is built\nupon a text-to-audio model and integrates video information through a modality\nadapter mechanism. By incorporating text, users can refine semantic details and\nintroduce creative variations, guiding the audio synthesis beyond the expected\nvideo contextual cues. Experiments show that besides its superior quality in\nterms of semantic alignment and audio-visual synchronization the proposed\nmethod enable high textual controllability as demonstrated in subjective and\nobjective evaluations."
                },
                "authors": [
                    {
                        "name": "Roi Benita"
                    },
                    {
                        "name": "Michael Finkelson"
                    },
                    {
                        "name": "Tavi Halperin"
                    },
                    {
                        "name": "Gleb Sterkin"
                    },
                    {
                        "name": "Yossi Adi"
                    }
                ],
                "author_detail": {
                    "name": "Yossi Adi"
                },
                "author": "Yossi Adi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.07294v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.07294v4",
                "updated": "2025-04-09T10:45:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    45,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2024-01-14T14:05:33Z",
                "published_parsed": [
                    2024,
                    1,
                    14,
                    14,
                    5,
                    33,
                    6,
                    14,
                    0
                ],
                "title": "Multilevel Metamodels: Enhancing Inference, Interpretability, and\n  Generalizability in Monte Carlo Simulation Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilevel Metamodels: Enhancing Inference, Interpretability, and\n  Generalizability in Monte Carlo Simulation Studies"
                },
                "summary": "Metamodels, or the regression analysis of Monte Carlo simulation results,\nprovide a powerful tool to summarize simulation findings. However, an\nunderutilized approach is the multilevel metamodel (MLMM) that accounts for the\ndependent data structure that arises from fitting multiple models to the same\nsimulated data set. In this study, we articulate the theoretical rationale for\nthe MLMM and illustrate how it can improve the interpretability of simulation\nresults, better account for complex simulation designs, and provide new\ninsights into the generalizability of simulation findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metamodels, or the regression analysis of Monte Carlo simulation results,\nprovide a powerful tool to summarize simulation findings. However, an\nunderutilized approach is the multilevel metamodel (MLMM) that accounts for the\ndependent data structure that arises from fitting multiple models to the same\nsimulated data set. In this study, we articulate the theoretical rationale for\nthe MLMM and illustrate how it can improve the interpretability of simulation\nresults, better account for complex simulation designs, and provide new\ninsights into the generalizability of simulation findings."
                },
                "authors": [
                    {
                        "name": "Joshua Gilbert"
                    },
                    {
                        "name": "Luke Miratrix"
                    }
                ],
                "author_detail": {
                    "name": "Luke Miratrix"
                },
                "author": "Luke Miratrix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.07294v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.07294v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05345v2",
                "updated": "2025-04-09T10:43:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    43,
                    24,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-07T11:39:01Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    11,
                    39,
                    1,
                    4,
                    66,
                    0
                ],
                "title": "Characterizing planetary material accreted by cool helium atmosphere\n  white dwarfs using an exponentially decaying disc model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing planetary material accreted by cool helium atmosphere\n  white dwarfs using an exponentially decaying disc model"
                },
                "summary": "We present Keck High Resolution Echelle Spectrometer (HIRES) observations and\nmodel atmosphere analysis for two nearby, cool, helium-dominated atmosphere\nwhite dwarfs that have been polluted by accretion: WD J1927-0355 and WD\nJ2141-3300. Detected elements common to both white dwarfs are Mg, Ca, Ti, Cr,\nFe, and Ni, with additional detections of Na, Al, Si and Sr in WD J2141-3300.\nWe present an approach for inferring the composition of the accreted material,\nby adopting a physically motivated model in which the mass accretion rate\ndecays exponentially with time, which provides constraints on the time since\nthe start of the accretion event. The accretion events were most likely to have\nbegan at least 1 Myr ago, however the characteristic disc lifetime could not be\nconstrained due to degeneracies. Both white dwarfs were found to have accreted\nbulk planetary material with compositions similar to that of both bulk Earth\nand chondritic meteorites. The parent bodies causing pollution in both cases\nwere inferred to be the mass of a small moon or dwarf planet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Keck High Resolution Echelle Spectrometer (HIRES) observations and\nmodel atmosphere analysis for two nearby, cool, helium-dominated atmosphere\nwhite dwarfs that have been polluted by accretion: WD J1927-0355 and WD\nJ2141-3300. Detected elements common to both white dwarfs are Mg, Ca, Ti, Cr,\nFe, and Ni, with additional detections of Na, Al, Si and Sr in WD J2141-3300.\nWe present an approach for inferring the composition of the accreted material,\nby adopting a physically motivated model in which the mass accretion rate\ndecays exponentially with time, which provides constraints on the time since\nthe start of the accretion event. The accretion events were most likely to have\nbegan at least 1 Myr ago, however the characteristic disc lifetime could not be\nconstrained due to degeneracies. Both white dwarfs were found to have accreted\nbulk planetary material with compositions similar to that of both bulk Earth\nand chondritic meteorites. The parent bodies causing pollution in both cases\nwere inferred to be the mass of a small moon or dwarf planet."
                },
                "authors": [
                    {
                        "name": "Mairi W. O'Brien"
                    },
                    {
                        "name": "Pier-Emmanuel Tremblay"
                    },
                    {
                        "name": "Beth L. Klein"
                    },
                    {
                        "name": "Carl Melis"
                    },
                    {
                        "name": "Detlev Koester"
                    },
                    {
                        "name": "Andrew M. Buchan"
                    },
                    {
                        "name": "Dimitri Veras"
                    },
                    {
                        "name": "Alexandra E. Doyle"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra E. Doyle"
                },
                "author": "Alexandra E. Doyle",
                "arxiv_doi": "10.1093/mnras/staf398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Monthly Notices of the Royal Astronomical Society, Volume 539,\n  Issue 1, May 2025, Pages 171-184",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06766v1",
                "updated": "2025-04-09T10:42:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    42,
                    36,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T10:42:36Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    42,
                    36,
                    2,
                    99,
                    0
                ],
                "title": "FamilyTool: A Multi-hop Personalized Tool Use Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FamilyTool: A Multi-hop Personalized Tool Use Benchmark"
                },
                "summary": "The integration of tool learning with Large Language Models (LLMs) has\nexpanded their capabilities in handling complex tasks by leveraging external\ntools. However, existing benchmarks for tool learning inadequately address\ncritical real-world personalized scenarios, particularly those requiring\nmulti-hop reasoning and inductive knowledge adaptation in dynamic environments.\nTo bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a\nfamily-based knowledge graph (KG) that simulates personalized, multi-hop tool\nuse scenarios. FamilyTool challenges LLMs with queries spanning 1 to 3\nrelational hops (e.g., inferring familial connections and preferences) and\nincorporates an inductive KG setting where models must adapt to unseen user\npreferences and relationships without re-training, a common limitation in prior\napproaches that compromises generalization. We further propose KGETool: a\nsimple KG-augmented evaluation pipeline to systematically assess LLMs' tool use\nability in these settings. Experiments reveal significant performance gaps in\nstate-of-the-art LLMs, with accuracy dropping sharply as hop complexity\nincreases and inductive scenarios exposing severe generalization deficits.\nThese findings underscore the limitations of current LLMs in handling\npersonalized, evolving real-world contexts and highlight the urgent need for\nadvancements in tool-learning frameworks. FamilyTool serves as a critical\nresource for evaluating and advancing LLM agents' reasoning, adaptability, and\nscalability in complex, dynamic environments. Code and dataset are available at\nGithub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of tool learning with Large Language Models (LLMs) has\nexpanded their capabilities in handling complex tasks by leveraging external\ntools. However, existing benchmarks for tool learning inadequately address\ncritical real-world personalized scenarios, particularly those requiring\nmulti-hop reasoning and inductive knowledge adaptation in dynamic environments.\nTo bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a\nfamily-based knowledge graph (KG) that simulates personalized, multi-hop tool\nuse scenarios. FamilyTool challenges LLMs with queries spanning 1 to 3\nrelational hops (e.g., inferring familial connections and preferences) and\nincorporates an inductive KG setting where models must adapt to unseen user\npreferences and relationships without re-training, a common limitation in prior\napproaches that compromises generalization. We further propose KGETool: a\nsimple KG-augmented evaluation pipeline to systematically assess LLMs' tool use\nability in these settings. Experiments reveal significant performance gaps in\nstate-of-the-art LLMs, with accuracy dropping sharply as hop complexity\nincreases and inductive scenarios exposing severe generalization deficits.\nThese findings underscore the limitations of current LLMs in handling\npersonalized, evolving real-world contexts and highlight the urgent need for\nadvancements in tool-learning frameworks. FamilyTool serves as a critical\nresource for evaluating and advancing LLM agents' reasoning, adaptability, and\nscalability in complex, dynamic environments. Code and dataset are available at\nGithub."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Yiran Guo"
                    },
                    {
                        "name": "Yining Zheng"
                    },
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17975v2",
                "updated": "2025-04-09T10:21:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    21,
                    7,
                    2,
                    99,
                    0
                ],
                "published": "2024-04-27T18:28:10Z",
                "published_parsed": [
                    2024,
                    4,
                    27,
                    18,
                    28,
                    10,
                    5,
                    118,
                    0
                ],
                "title": "Automating Customer Needs Analysis: A Comparative Study of Large\n  Language Models in the Travel Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Customer Needs Analysis: A Comparative Study of Large\n  Language Models in the Travel Industry"
                },
                "summary": "In the rapidly evolving landscape of Natural Language Processing (NLP), Large\nLanguage Models (LLMs) have emerged as powerful tools for many tasks, such as\nextracting valuable insights from vast amounts of textual data. In this study,\nwe conduct a comparative analysis of LLMs for the extraction of travel customer\nneeds from TripAdvisor and Reddit posts. Leveraging a diverse range of models,\nincluding both open-source and proprietary ones such as GPT-4 and Gemini, we\naim to elucidate their strengths and weaknesses in this specialized domain.\nThrough an evaluation process involving metrics such as BERTScore, ROUGE, and\nBLEU, we assess the performance of each model in accurately identifying and\nsummarizing customer needs. Our findings highlight the efficacy of opensource\nLLMs, particularly Mistral 7B, in achieving comparable performance to larger\nclosed models while offering affordability and customization benefits.\nAdditionally, we underscore the importance of considering factors such as model\nsize, resource requirements, and performance metrics when selecting the most\nsuitable LLM for customer needs analysis tasks. Overall, this study contributes\nvaluable insights for businesses seeking to leverage advanced NLP techniques to\nenhance customer experience and drive operational efficiency in the travel\nindustry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving landscape of Natural Language Processing (NLP), Large\nLanguage Models (LLMs) have emerged as powerful tools for many tasks, such as\nextracting valuable insights from vast amounts of textual data. In this study,\nwe conduct a comparative analysis of LLMs for the extraction of travel customer\nneeds from TripAdvisor and Reddit posts. Leveraging a diverse range of models,\nincluding both open-source and proprietary ones such as GPT-4 and Gemini, we\naim to elucidate their strengths and weaknesses in this specialized domain.\nThrough an evaluation process involving metrics such as BERTScore, ROUGE, and\nBLEU, we assess the performance of each model in accurately identifying and\nsummarizing customer needs. Our findings highlight the efficacy of opensource\nLLMs, particularly Mistral 7B, in achieving comparable performance to larger\nclosed models while offering affordability and customization benefits.\nAdditionally, we underscore the importance of considering factors such as model\nsize, resource requirements, and performance metrics when selecting the most\nsuitable LLM for customer needs analysis tasks. Overall, this study contributes\nvaluable insights for businesses seeking to leverage advanced NLP techniques to\nenhance customer experience and drive operational efficiency in the travel\nindustry."
                },
                "authors": [
                    {
                        "name": "Simone Barandoni"
                    },
                    {
                        "name": "Filippo Chiarello"
                    },
                    {
                        "name": "Lorenzo Cascone"
                    },
                    {
                        "name": "Emiliano Marrale"
                    },
                    {
                        "name": "Salvatore Puccio"
                    }
                ],
                "author_detail": {
                    "name": "Salvatore Puccio"
                },
                "author": "Salvatore Puccio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06748v1",
                "updated": "2025-04-09T10:09:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    9,
                    29,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T10:09:29Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    9,
                    29,
                    2,
                    99,
                    0
                ],
                "title": "Efficient Deployment of Spiking Neural Networks on SpiNNaker2 for DVS\n  Gesture Recognition Using Neuromorphic Intermediate Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Deployment of Spiking Neural Networks on SpiNNaker2 for DVS\n  Gesture Recognition Using Neuromorphic Intermediate Representation"
                },
                "summary": "Spiking Neural Networks (SNNs) are highly energy-efficient during inference,\nmaking them particularly suitable for deployment on neuromorphic hardware.\nTheir ability to process event-driven inputs, such as data from dynamic vision\nsensors (DVS), further enhances their applicability to edge computing tasks.\nHowever, the resource constraints of edge hardware necessitate techniques like\nweight quantization, which reduce the memory footprint of SNNs while preserving\naccuracy. Despite its importance, existing quantization methods typically focus\non synaptic weights quantization without taking account of other critical\nparameters, such as scaling neuron firing thresholds.\n  To address this limitation, we present the first benchmark for the DVS\ngesture recognition task using SNNs optimized for the many-core neuromorphic\nchip SpiNNaker2. Our study evaluates two quantization pipelines for fixed-point\ncomputations. The first approach employs post training quantization (PTQ) with\npercentile-based threshold scaling, while the second uses quantization aware\ntraining (QAT) with adaptive threshold scaling. Both methods achieve accurate\n8-bit on-chip inference, closely approximating 32-bit floating-point\nperformance. Additionally, our baseline SNNs perform competitively against\npreviously reported results without specialized techniques. These models are\ndeployed on SpiNNaker2 using the neuromorphic intermediate representation\n(NIR). Ultimately, we achieve 94.13% classification accuracy on-chip,\ndemonstrating the SpiNNaker2's potential for efficient, low-energy neuromorphic\ncomputing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) are highly energy-efficient during inference,\nmaking them particularly suitable for deployment on neuromorphic hardware.\nTheir ability to process event-driven inputs, such as data from dynamic vision\nsensors (DVS), further enhances their applicability to edge computing tasks.\nHowever, the resource constraints of edge hardware necessitate techniques like\nweight quantization, which reduce the memory footprint of SNNs while preserving\naccuracy. Despite its importance, existing quantization methods typically focus\non synaptic weights quantization without taking account of other critical\nparameters, such as scaling neuron firing thresholds.\n  To address this limitation, we present the first benchmark for the DVS\ngesture recognition task using SNNs optimized for the many-core neuromorphic\nchip SpiNNaker2. Our study evaluates two quantization pipelines for fixed-point\ncomputations. The first approach employs post training quantization (PTQ) with\npercentile-based threshold scaling, while the second uses quantization aware\ntraining (QAT) with adaptive threshold scaling. Both methods achieve accurate\n8-bit on-chip inference, closely approximating 32-bit floating-point\nperformance. Additionally, our baseline SNNs perform competitively against\npreviously reported results without specialized techniques. These models are\ndeployed on SpiNNaker2 using the neuromorphic intermediate representation\n(NIR). Ultimately, we achieve 94.13% classification accuracy on-chip,\ndemonstrating the SpiNNaker2's potential for efficient, low-energy neuromorphic\ncomputing."
                },
                "authors": [
                    {
                        "name": "Sirine Arfa"
                    },
                    {
                        "name": "Bernhard Vogginger"
                    },
                    {
                        "name": "Chen Liu"
                    },
                    {
                        "name": "Johannes Partzsch"
                    },
                    {
                        "name": "Mark Schone"
                    },
                    {
                        "name": "Christian Mayr"
                    }
                ],
                "author_detail": {
                    "name": "Christian Mayr"
                },
                "author": "Christian Mayr",
                "arxiv_comment": "8 pages, 3 figures, 8 tables, Conference-2025 Neuro Inspired\n  Computational Elements (NICE)",
                "arxiv_journal_ref": "2025 Neuro Inspired Computational Elements (NICE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24206v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24206v2",
                "updated": "2025-04-09T09:45:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    45,
                    55,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-31T15:24:05Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    24,
                    5,
                    0,
                    90,
                    0
                ],
                "title": "Synthetic News Generation for Fake News Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic News Generation for Fake News Classification"
                },
                "summary": "This study explores the generation and evaluation of synthetic fake news\nthrough fact based manipulations using large language models (LLMs). We\nintroduce a novel methodology that extracts key facts from real articles,\nmodifies them, and regenerates content to simulate fake news while maintaining\ncoherence. To assess the quality of the generated content, we propose a set of\nevaluation metrics coherence, dissimilarity, and correctness. The research also\ninvestigates the application of synthetic data in fake news classification,\ncomparing traditional machine learning models with transformer based models\nsuch as BERT. Our experiments demonstrate that transformer models, especially\nBERT, effectively leverage synthetic data for fake news detection, showing\nimprovements with smaller proportions of synthetic data. Additionally, we find\nthat fact verification features, which focus on identifying factual\ninconsistencies, provide the most promising results in distinguishing synthetic\nfake news. The study highlights the potential of synthetic data to enhance fake\nnews detection systems, offering valuable insights for future research and\nsuggesting that targeted improvements in synthetic data generation can further\nstrengthen detection models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the generation and evaluation of synthetic fake news\nthrough fact based manipulations using large language models (LLMs). We\nintroduce a novel methodology that extracts key facts from real articles,\nmodifies them, and regenerates content to simulate fake news while maintaining\ncoherence. To assess the quality of the generated content, we propose a set of\nevaluation metrics coherence, dissimilarity, and correctness. The research also\ninvestigates the application of synthetic data in fake news classification,\ncomparing traditional machine learning models with transformer based models\nsuch as BERT. Our experiments demonstrate that transformer models, especially\nBERT, effectively leverage synthetic data for fake news detection, showing\nimprovements with smaller proportions of synthetic data. Additionally, we find\nthat fact verification features, which focus on identifying factual\ninconsistencies, provide the most promising results in distinguishing synthetic\nfake news. The study highlights the potential of synthetic data to enhance fake\nnews detection systems, offering valuable insights for future research and\nsuggesting that targeted improvements in synthetic data generation can further\nstrengthen detection models."
                },
                "authors": [
                    {
                        "name": "Abdul Sittar"
                    },
                    {
                        "name": "Luka Golob"
                    },
                    {
                        "name": "Mateja Smiljanic"
                    }
                ],
                "author_detail": {
                    "name": "Mateja Smiljanic"
                },
                "author": "Mateja Smiljanic",
                "arxiv_comment": "One of the authors objected to submit the paper because he was not\n  aware of that and he likes to modify the paper before submitting to arXiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24206v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24206v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06730v1",
                "updated": "2025-04-09T09:38:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    38,
                    45,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T09:38:45Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    38,
                    45,
                    2,
                    99,
                    0
                ],
                "title": "PETNet -- Coincident Particle Event Detection using Spiking Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PETNet -- Coincident Particle Event Detection using Spiking Neural\n  Networks"
                },
                "summary": "Spiking neural networks (SNN) hold the promise of being a more biologically\nplausible, low-energy alternative to conventional artificial neural networks.\nTheir time-variant nature makes them particularly suitable for processing\ntime-resolved, sparse binary data. In this paper, we investigate the potential\nof leveraging SNNs for the detection of photon coincidences in positron\nemission tomography (PET) data. PET is a medical imaging technique based on\ninjecting a patient with a radioactive tracer and detecting the emitted\nphotons. One central post-processing task for inferring an image of the tracer\ndistribution is the filtering of invalid hits occurring due to e.g. absorption\nor scattering processes. Our approach, coined PETNet, interprets the detector\nhits as a binary-valued spike train and learns to identify photon coincidence\npairs in a supervised manner. We introduce a dedicated multi-objective loss\nfunction and demonstrate the effects of explicitly modeling the detector\ngeometry on simulation data for two use-cases. Our results show that PETNet can\noutperform the state-of-the-art classical algorithm with a maximal coincidence\ndetection $F_1$ of 95.2%. At the same time, PETNet is able to predict photon\ncoincidences up to 36 times faster than the classical approach, highlighting\nthe great potential of SNNs in particle physics applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking neural networks (SNN) hold the promise of being a more biologically\nplausible, low-energy alternative to conventional artificial neural networks.\nTheir time-variant nature makes them particularly suitable for processing\ntime-resolved, sparse binary data. In this paper, we investigate the potential\nof leveraging SNNs for the detection of photon coincidences in positron\nemission tomography (PET) data. PET is a medical imaging technique based on\ninjecting a patient with a radioactive tracer and detecting the emitted\nphotons. One central post-processing task for inferring an image of the tracer\ndistribution is the filtering of invalid hits occurring due to e.g. absorption\nor scattering processes. Our approach, coined PETNet, interprets the detector\nhits as a binary-valued spike train and learns to identify photon coincidence\npairs in a supervised manner. We introduce a dedicated multi-objective loss\nfunction and demonstrate the effects of explicitly modeling the detector\ngeometry on simulation data for two use-cases. Our results show that PETNet can\noutperform the state-of-the-art classical algorithm with a maximal coincidence\ndetection $F_1$ of 95.2%. At the same time, PETNet is able to predict photon\ncoincidences up to 36 times faster than the classical approach, highlighting\nthe great potential of SNNs in particle physics applications."
                },
                "authors": [
                    {
                        "name": "Jan Debus"
                    },
                    {
                        "name": "Charlotte Debus"
                    },
                    {
                        "name": "Günther Dissertori"
                    },
                    {
                        "name": "Markus Götz"
                    }
                ],
                "author_detail": {
                    "name": "Markus Götz"
                },
                "author": "Markus Götz",
                "arxiv_doi": "10.1109/NICE61972.2024.10549584",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/NICE61972.2024.10549584",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 Neuro Inspired Computational Elements Conference (NICE)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06722v1",
                "updated": "2025-04-09T09:23:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    23,
                    11,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T09:23:11Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    23,
                    11,
                    2,
                    99,
                    0
                ],
                "title": "Plastic tensor networks for interpretable generative modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plastic tensor networks for interpretable generative modeling"
                },
                "summary": "A structural optimization scheme for a single-layer nonnegative adaptive\ntensor tree (NATT) that models a target probability distribution is proposed.\nThe NATT scheme, by construction, has the advantage that it is interpretable as\na probabilistic graphical model. We consider the NATT scheme and a recently\nproposed Born machine adaptive tensor tree (BMATT) optimization scheme and\ndemonstrate their effectiveness on a variety of generative modeling tasks where\nthe objective is to infer the hidden structure of a provided dataset. Our\nresults show that in terms of minimizing the negative log-likelihood, the\nsingle-layer scheme has model performance comparable to the Born machine\nscheme, though not better. The tasks include deducing the structure of binary\nbitwise operations, learning the internal structure of random Bayesian networks\ngiven only visible sites, and a real-world example related to hierarchical\nclustering where a cladogram is constructed from mitochondrial DNA sequences.\nIn doing so, we also show the importance of the choice of network topology and\nthe versatility of a least-mutual information criterion in selecting a\ncandidate structure for a tensor tree, as well as discuss aspects of these\ntensor tree generative models including their information content and\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A structural optimization scheme for a single-layer nonnegative adaptive\ntensor tree (NATT) that models a target probability distribution is proposed.\nThe NATT scheme, by construction, has the advantage that it is interpretable as\na probabilistic graphical model. We consider the NATT scheme and a recently\nproposed Born machine adaptive tensor tree (BMATT) optimization scheme and\ndemonstrate their effectiveness on a variety of generative modeling tasks where\nthe objective is to infer the hidden structure of a provided dataset. Our\nresults show that in terms of minimizing the negative log-likelihood, the\nsingle-layer scheme has model performance comparable to the Born machine\nscheme, though not better. The tasks include deducing the structure of binary\nbitwise operations, learning the internal structure of random Bayesian networks\ngiven only visible sites, and a real-world example related to hierarchical\nclustering where a cladogram is constructed from mitochondrial DNA sequences.\nIn doing so, we also show the importance of the choice of network topology and\nthe versatility of a least-mutual information criterion in selecting a\ncandidate structure for a tensor tree, as well as discuss aspects of these\ntensor tree generative models including their information content and\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Katsuya O. Akamatsu"
                    },
                    {
                        "name": "Kenji Harada"
                    },
                    {
                        "name": "Tsuyoshi Okubo"
                    },
                    {
                        "name": "Naoki Kawashima"
                    }
                ],
                "author_detail": {
                    "name": "Naoki Kawashima"
                },
                "author": "Naoki Kawashima",
                "arxiv_comment": "37 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06721v1",
                "updated": "2025-04-09T09:20:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    20,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T09:20:37Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    20,
                    37,
                    2,
                    99,
                    0
                ],
                "title": "Learning global control of underactuated systems with Model-Based\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning global control of underactuated systems with Model-Based\n  Reinforcement Learning"
                },
                "summary": "This short paper describes our proposed solution for the third edition of the\n\"AI Olympics with RealAIGym\" competition, held at ICRA 2025. We employed\nMonte-Carlo Probabilistic Inference for Learning Control (MC-PILCO), an MBRL\nalgorithm recognized for its exceptional data efficiency across various\nlow-dimensional robotic tasks, including cart-pole, ball \\& plate, and Furuta\npendulum systems. MC-PILCO optimizes a system dynamics model using interaction\ndata, enabling policy refinement through simulation rather than direct system\ndata optimization. This approach has proven highly effective in physical\nsystems, offering greater data efficiency than Model-Free (MF) alternatives.\nNotably, MC-PILCO has previously won the first two editions of this\ncompetition, demonstrating its robustness in both simulated and real-world\nenvironments. Besides briefly reviewing the algorithm, we discuss the most\ncritical aspects of the MC-PILCO implementation in the tasks at hand: learning\na global policy for the pendubot and acrobot systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This short paper describes our proposed solution for the third edition of the\n\"AI Olympics with RealAIGym\" competition, held at ICRA 2025. We employed\nMonte-Carlo Probabilistic Inference for Learning Control (MC-PILCO), an MBRL\nalgorithm recognized for its exceptional data efficiency across various\nlow-dimensional robotic tasks, including cart-pole, ball \\& plate, and Furuta\npendulum systems. MC-PILCO optimizes a system dynamics model using interaction\ndata, enabling policy refinement through simulation rather than direct system\ndata optimization. This approach has proven highly effective in physical\nsystems, offering greater data efficiency than Model-Free (MF) alternatives.\nNotably, MC-PILCO has previously won the first two editions of this\ncompetition, demonstrating its robustness in both simulated and real-world\nenvironments. Besides briefly reviewing the algorithm, we discuss the most\ncritical aspects of the MC-PILCO implementation in the tasks at hand: learning\na global policy for the pendubot and acrobot systems."
                },
                "authors": [
                    {
                        "name": "Niccolò Turcato"
                    },
                    {
                        "name": "Marco Calì"
                    },
                    {
                        "name": "Alberto Dalla Libera"
                    },
                    {
                        "name": "Giulio Giacomuzzo"
                    },
                    {
                        "name": "Ruggero Carli"
                    },
                    {
                        "name": "Diego Romeres"
                    }
                ],
                "author_detail": {
                    "name": "Diego Romeres"
                },
                "author": "Diego Romeres",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2409.05811",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06720v1",
                "updated": "2025-04-09T09:19:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    19,
                    55,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T09:19:55Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    19,
                    55,
                    2,
                    99,
                    0
                ],
                "title": "Revealing the ages of metal-rich RR Lyrae via kinematic label transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the ages of metal-rich RR Lyrae via kinematic label transfer"
                },
                "summary": "RR Lyrae stars have long been considered reliable tracers of old, metal-poor\npopulations, primarily due to their prevalence in globular clusters and the\nGalactic halo. However, the discovery of a metal-rich subpopulation in the\nGalactic disc, kinematically colder and more rotationally supported, challenges\nthis classical view. Understanding the age of these metal-rich RR Lyrae stars\nis crucial for constraining their formation pathways and assessing what\nGalactic populations they are tracing. In this work, we leverage the\nunprecedented astrometric precision of Gaia DR3 to infer the age distribution\nof metal-rich RR Lyrae stars through a kinematic comparison with O-rich Mira\nvariables. Mira variables, with their well-established period-age relation,\nserve as a natural clock, allowing us to transfer age information to RR Lyrae\nstars via their phase-space properties. By applying this approach across\ndifferent metallicity bins, we find that the most metal-rich RR Lyrae stars\n($[\\rm Fe/H] > -0.5$) exhibit kinematics consistent with a population\nsignificantly younger ($\\approx 6-7$ Gyr) than typically assumed for RR Lyrae\nstars. In contrast, those with $-1 < [\\rm Fe/H] < -0.5$ show properties more\naligned with older ($\\approx 9-11$ Gyr) populations. Interestingly, we also\nfind evidence of a possible double age populations for the most metal-rich RR\nLyrae, one younger with ages between 3 and 6 Gyr, and another one older ranging\nfrom 8 to 11 Gyr. These results provide strong evidence that metal-rich RR\nLyrae stars in the Galactic field do not exclusively trace ancient populations.\nThis finding challenges the current model of RR Lyrae formation and supports\nalternative formation scenarios, such as binary evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RR Lyrae stars have long been considered reliable tracers of old, metal-poor\npopulations, primarily due to their prevalence in globular clusters and the\nGalactic halo. However, the discovery of a metal-rich subpopulation in the\nGalactic disc, kinematically colder and more rotationally supported, challenges\nthis classical view. Understanding the age of these metal-rich RR Lyrae stars\nis crucial for constraining their formation pathways and assessing what\nGalactic populations they are tracing. In this work, we leverage the\nunprecedented astrometric precision of Gaia DR3 to infer the age distribution\nof metal-rich RR Lyrae stars through a kinematic comparison with O-rich Mira\nvariables. Mira variables, with their well-established period-age relation,\nserve as a natural clock, allowing us to transfer age information to RR Lyrae\nstars via their phase-space properties. By applying this approach across\ndifferent metallicity bins, we find that the most metal-rich RR Lyrae stars\n($[\\rm Fe/H] > -0.5$) exhibit kinematics consistent with a population\nsignificantly younger ($\\approx 6-7$ Gyr) than typically assumed for RR Lyrae\nstars. In contrast, those with $-1 < [\\rm Fe/H] < -0.5$ show properties more\naligned with older ($\\approx 9-11$ Gyr) populations. Interestingly, we also\nfind evidence of a possible double age populations for the most metal-rich RR\nLyrae, one younger with ages between 3 and 6 Gyr, and another one older ranging\nfrom 8 to 11 Gyr. These results provide strong evidence that metal-rich RR\nLyrae stars in the Galactic field do not exclusively trace ancient populations.\nThis finding challenges the current model of RR Lyrae formation and supports\nalternative formation scenarios, such as binary evolution."
                },
                "authors": [
                    {
                        "name": "HanYuan Zhang"
                    },
                    {
                        "name": "Giuliano Iorio"
                    },
                    {
                        "name": "Vasily Belokurov"
                    },
                    {
                        "name": "N. Wyn Evans"
                    },
                    {
                        "name": "Alexey Bobrick"
                    },
                    {
                        "name": "Valentina D'Orazi"
                    }
                ],
                "author_detail": {
                    "name": "Valentina D'Orazi"
                },
                "author": "Valentina D'Orazi",
                "arxiv_comment": "13 pages, 9 figures, 4 appendices, submitted to MNRAS. Comments\n  welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06706v1",
                "updated": "2025-04-09T09:09:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    22,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T09:09:22Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    22,
                    2,
                    99,
                    0
                ],
                "title": "Learning-Inspired Fuzzy Logic Algorithms for Enhanced Control of\n  Oscillatory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Inspired Fuzzy Logic Algorithms for Enhanced Control of\n  Oscillatory Systems"
                },
                "summary": "The transportation of sensitive equipment often suffers from vibrations\ncaused by terrain, weather, and motion speed, leading to inefficiencies and\npotential damage. To address this challenge, this paper explores an intelligent\ncontrol framework leveraging fuzzy logic, a foundational AI technique, to\nsuppress oscillations in suspension systems. Inspired by learning based\nmethodologies, the proposed approach utilizes fuzzy inference and Gaussian\nmembership functions to emulate adaptive, human like decision making. By\nminimizing the need for explicit mathematical models, the method demonstrates\nrobustness in both linear and nonlinear systems. Experimental validation\nhighlights the controllers ability to adapt to varying suspension lengths,\nreducing oscillation amplitudes and improving stability under dynamic\nconditions. This research bridges the gap between traditional control systems\nand learning inspired techniques, offering a scalable, data efficient solution\nfor modern transportation challenges",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transportation of sensitive equipment often suffers from vibrations\ncaused by terrain, weather, and motion speed, leading to inefficiencies and\npotential damage. To address this challenge, this paper explores an intelligent\ncontrol framework leveraging fuzzy logic, a foundational AI technique, to\nsuppress oscillations in suspension systems. Inspired by learning based\nmethodologies, the proposed approach utilizes fuzzy inference and Gaussian\nmembership functions to emulate adaptive, human like decision making. By\nminimizing the need for explicit mathematical models, the method demonstrates\nrobustness in both linear and nonlinear systems. Experimental validation\nhighlights the controllers ability to adapt to varying suspension lengths,\nreducing oscillation amplitudes and improving stability under dynamic\nconditions. This research bridges the gap between traditional control systems\nand learning inspired techniques, offering a scalable, data efficient solution\nfor modern transportation challenges"
                },
                "authors": [
                    {
                        "name": "Vuong Anh Trung"
                    },
                    {
                        "name": "Thanh Son Pham"
                    },
                    {
                        "name": "Truc Thanh Tran"
                    },
                    {
                        "name": "Tran le Thang Dong"
                    },
                    {
                        "name": "Tran Thuan Hoang"
                    }
                ],
                "author_detail": {
                    "name": "Tran Thuan Hoang"
                },
                "author": "Tran Thuan Hoang",
                "arxiv_comment": "4 pages, 5 figures, conference",
                "arxiv_journal_ref": "10th ACIS International Conference on Big Data, Cloud Computing,\n  and Data Science (BCD 2025-Winter)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00850v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00850v3",
                "updated": "2025-04-09T09:09:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    11,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-30T11:16:04Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    16,
                    4,
                    2,
                    304,
                    0
                ],
                "title": "GWQ: Gradient-Aware Weight Quantization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GWQ: Gradient-Aware Weight Quantization for Large Language Models"
                },
                "summary": "Large language models (LLMs) show impressive performance in solving complex\nlanguage tasks. However, its large number of parameters presents significant\nchallenges for the deployment. So, compressing LLMs to low bits can enable to\ndeploy on resource-constrained devices. To address this problem, we propose\ngradient-aware weight quantization (GWQ), the first quantization approach for\nlow-bit weight quantization that leverages gradients to localize outliers,\nrequiring only a minimal amount of calibration data for outlier detection. GWQ\nretains the top 1\\% outliers preferentially at FP16 precision, while the\nremaining non-outlier weights are stored in a low-bit. We widely evaluate GWQ\non different task include language modeling, grounding detection, massive\nmultitask language understanding and vision-language question and answering.\nResults show that models quantified by GWQ performs better than other\nquantization method. During quantization process, GWQ only need one calibration\nset to realize effective quant. Also, GWQ achieves 1.2x inference speedup in\ncomparison to the original model and effectively reduces the inference memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show impressive performance in solving complex\nlanguage tasks. However, its large number of parameters presents significant\nchallenges for the deployment. So, compressing LLMs to low bits can enable to\ndeploy on resource-constrained devices. To address this problem, we propose\ngradient-aware weight quantization (GWQ), the first quantization approach for\nlow-bit weight quantization that leverages gradients to localize outliers,\nrequiring only a minimal amount of calibration data for outlier detection. GWQ\nretains the top 1\\% outliers preferentially at FP16 precision, while the\nremaining non-outlier weights are stored in a low-bit. We widely evaluate GWQ\non different task include language modeling, grounding detection, massive\nmultitask language understanding and vision-language question and answering.\nResults show that models quantified by GWQ performs better than other\nquantization method. During quantization process, GWQ only need one calibration\nset to realize effective quant. Also, GWQ achieves 1.2x inference speedup in\ncomparison to the original model and effectively reduces the inference memory."
                },
                "authors": [
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Haiyang Liu"
                    },
                    {
                        "name": "Zijian Ling"
                    },
                    {
                        "name": "Minxi Yan"
                    },
                    {
                        "name": "Ziyang Yan"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Jingcai Guo"
                    },
                    {
                        "name": "Ling Shao"
                    },
                    {
                        "name": "Hao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Tang"
                },
                "author": "Hao Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00850v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12946v2",
                "updated": "2025-04-09T08:59:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    59,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2024-11-20T00:31:23Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    31,
                    23,
                    2,
                    325,
                    0
                ],
                "title": "A Flexible Large Language Models Guardrail Development Methodology\n  Applied to Off-Topic Prompt Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Flexible Large Language Models Guardrail Development Methodology\n  Applied to Off-Topic Prompt Detection"
                },
                "summary": "Large Language Models (LLMs) are prone to off-topic misuse, where users may\nprompt these models to perform tasks beyond their intended scope. Current\nguardrails, which often rely on curated examples or custom classifiers, suffer\nfrom high false-positive rates, limited adaptability, and the impracticality of\nrequiring real-world data that is not available in pre-production. In this\npaper, we introduce a flexible, data-free guardrail development methodology\nthat addresses these challenges. By thoroughly defining the problem space\nqualitatively and passing this to an LLM to generate diverse prompts, we\nconstruct a synthetic dataset to benchmark and train off-topic guardrails that\noutperform heuristic approaches. Additionally, by framing the task as\nclassifying whether the user prompt is relevant with respect to the system\nprompt, our guardrails effectively generalize to other misuse categories,\nincluding jailbreak and harmful prompts. Lastly, we further contribute to the\nfield by open-sourcing both the synthetic dataset and the off-topic guardrail\nmodels, providing valuable resources for developing guardrails in\npre-production environments and supporting future research and development in\nLLM safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to off-topic misuse, where users may\nprompt these models to perform tasks beyond their intended scope. Current\nguardrails, which often rely on curated examples or custom classifiers, suffer\nfrom high false-positive rates, limited adaptability, and the impracticality of\nrequiring real-world data that is not available in pre-production. In this\npaper, we introduce a flexible, data-free guardrail development methodology\nthat addresses these challenges. By thoroughly defining the problem space\nqualitatively and passing this to an LLM to generate diverse prompts, we\nconstruct a synthetic dataset to benchmark and train off-topic guardrails that\noutperform heuristic approaches. Additionally, by framing the task as\nclassifying whether the user prompt is relevant with respect to the system\nprompt, our guardrails effectively generalize to other misuse categories,\nincluding jailbreak and harmful prompts. Lastly, we further contribute to the\nfield by open-sourcing both the synthetic dataset and the off-topic guardrail\nmodels, providing valuable resources for developing guardrails in\npre-production environments and supporting future research and development in\nLLM safety."
                },
                "authors": [
                    {
                        "name": "Gabriel Chua"
                    },
                    {
                        "name": "Shing Yee Chan"
                    },
                    {
                        "name": "Shaun Khoo"
                    }
                ],
                "author_detail": {
                    "name": "Shaun Khoo"
                },
                "author": "Shaun Khoo",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06688v1",
                "updated": "2025-04-09T08:49:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    49,
                    24,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T08:49:24Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    49,
                    24,
                    2,
                    99,
                    0
                ],
                "title": "Efficient Timestamping for Sampling-based Race Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Timestamping for Sampling-based Race Detection"
                },
                "summary": "Dynamic race detection based on the happens before (HB) partial order has now\nbecome the de facto approach to quickly identify data races in multi-threaded\nsoftware. Most practical implementations for detecting these races use\ntimestamps to infer causality between events and detect races based on these\ntimestamps. Such an algorithm updates timestamps (stored in vector clocks) at\nevery event in the execution, and is known to induce excessive overhead. Random\nsampling has emerged as a promising algorithmic paradigm to offset this\noverhead. It offers the promise of making sound race detection scalable. In\nthis work we consider the task of designing an efficient sampling based race\ndetector with low overhead for timestamping when the number of sampled events\nis much smaller than the total events in an execution. To solve this problem,\nwe propose (1) a new notion of freshness timestamp, (2) a new data structure to\nstore timestamps, and (3) an algorithm that uses a combination of them to\nreduce the cost of timestamping in sampling based race detection. Further, we\nprove that our algorithm is close to optimal -- the number of vector clock\ntraversals is bounded by the number of sampled events and number of threads,\nand further, on any given dynamic execution, the cost of timestamping due to\nour algorithm is close to the amount of work any timestamping-based algorithm\nmust perform on that execution, that is it is instance optimal. Our evaluation\non real world benchmarks demonstrates the effectiveness of our proposed\nalgorithm over prior timestamping algorithms that are agnostic to sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic race detection based on the happens before (HB) partial order has now\nbecome the de facto approach to quickly identify data races in multi-threaded\nsoftware. Most practical implementations for detecting these races use\ntimestamps to infer causality between events and detect races based on these\ntimestamps. Such an algorithm updates timestamps (stored in vector clocks) at\nevery event in the execution, and is known to induce excessive overhead. Random\nsampling has emerged as a promising algorithmic paradigm to offset this\noverhead. It offers the promise of making sound race detection scalable. In\nthis work we consider the task of designing an efficient sampling based race\ndetector with low overhead for timestamping when the number of sampled events\nis much smaller than the total events in an execution. To solve this problem,\nwe propose (1) a new notion of freshness timestamp, (2) a new data structure to\nstore timestamps, and (3) an algorithm that uses a combination of them to\nreduce the cost of timestamping in sampling based race detection. Further, we\nprove that our algorithm is close to optimal -- the number of vector clock\ntraversals is bounded by the number of sampled events and number of threads,\nand further, on any given dynamic execution, the cost of timestamping due to\nour algorithm is close to the amount of work any timestamping-based algorithm\nmust perform on that execution, that is it is instance optimal. Our evaluation\non real world benchmarks demonstrates the effectiveness of our proposed\nalgorithm over prior timestamping algorithms that are agnostic to sampling."
                },
                "authors": [
                    {
                        "name": "Minjian Zhang"
                    },
                    {
                        "name": "Daniel Wee Soong Lim"
                    },
                    {
                        "name": "Mosaad Al Thokair"
                    },
                    {
                        "name": "Umang Mathur"
                    },
                    {
                        "name": "Mahesh Viswanathan"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Viswanathan"
                },
                "author": "Mahesh Viswanathan",
                "arxiv_comment": "To appear at PLDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20936v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20936v5",
                "updated": "2025-04-09T08:42:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    42,
                    34,
                    2,
                    99,
                    0
                ],
                "published": "2024-05-31T15:34:36Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    15,
                    34,
                    36,
                    4,
                    152,
                    0
                ],
                "title": "Bayesian Deep Generative Models for Multiplex Networks with Multiscale\n  Overlapping Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Deep Generative Models for Multiplex Networks with Multiscale\n  Overlapping Clusters"
                },
                "summary": "Our interest is in multiplex network data with multiple network samples\nobserved across the same set of nodes. Examples originate from a variety of\nfields, including brain connectivity, international trade networks, and social\nnetworks, among others. Our goal is to infer a hierarchical structure of the\nnodes at a population level, while performing multi-resolution clustering of\nthe individual replicates. To accomplish this, we propose a Bayesian\nhierarchical model, provide theoretical support in terms of identifiability and\nposterior consistency, and design efficient methods for posterior computation.\nWe provide novel technical tools for proving model identifiability, which are\nof independent interest. Our proposed methodology is demonstrated through\nnumerical simulation and an application to brain connectome data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our interest is in multiplex network data with multiple network samples\nobserved across the same set of nodes. Examples originate from a variety of\nfields, including brain connectivity, international trade networks, and social\nnetworks, among others. Our goal is to infer a hierarchical structure of the\nnodes at a population level, while performing multi-resolution clustering of\nthe individual replicates. To accomplish this, we propose a Bayesian\nhierarchical model, provide theoretical support in terms of identifiability and\nposterior consistency, and design efficient methods for posterior computation.\nWe provide novel technical tools for proving model identifiability, which are\nof independent interest. Our proposed methodology is demonstrated through\nnumerical simulation and an application to brain connectome data."
                },
                "authors": [
                    {
                        "name": "Yuren Zhou"
                    },
                    {
                        "name": "Yuqi Gu"
                    },
                    {
                        "name": "David B. Dunson"
                    }
                ],
                "author_detail": {
                    "name": "David B. Dunson"
                },
                "author": "David B. Dunson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20936v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20936v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12848v2",
                "updated": "2025-04-09T08:38:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    38,
                    44,
                    2,
                    99,
                    0
                ],
                "published": "2024-12-17T12:22:44Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    22,
                    44,
                    1,
                    352,
                    0
                ],
                "title": "ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical\n  Insights from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical\n  Insights from Large Language Models"
                },
                "summary": "With the rise and widespread use of Large Language Models (LLMs), ensuring\ntheir safety is crucial to prevent harm to humans and promote ethical\nbehaviors. However, directly assessing value valence (i.e., support or oppose)\nby leveraging large-scale data training is untrustworthy and inexplainable. We\nassume that emulating humans to rely on social norms to make moral decisions\ncan help LLMs understand and predict moral judgment. However, capturing human\nvalues remains a challenge, as multiple related norms might conflict in\nspecific contexts. Consider norms that are upheld by the majority and promote\nthe well-being of society are more likely to be accepted and widely adopted\n(e.g., \"don't cheat,\"). Therefore, it is essential for LLM to identify the\nappropriate norms for a given scenario before making moral decisions. To this\nend, we introduce a novel moral judgment approach called \\textit{ClarityEthic}\nthat leverages LLMs' reasoning ability and contrastive learning to uncover\nrelevant social norms for human actions from different perspectives and select\nthe most reliable one to enhance judgment accuracy. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art approaches in moral\njudgment tasks. Moreover, human evaluations confirm that the generated social\nnorms provide plausible explanations that support the judgments. This suggests\nthat modeling human moral judgment with the emulating humans moral strategy is\npromising for improving the ethical behaviors of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise and widespread use of Large Language Models (LLMs), ensuring\ntheir safety is crucial to prevent harm to humans and promote ethical\nbehaviors. However, directly assessing value valence (i.e., support or oppose)\nby leveraging large-scale data training is untrustworthy and inexplainable. We\nassume that emulating humans to rely on social norms to make moral decisions\ncan help LLMs understand and predict moral judgment. However, capturing human\nvalues remains a challenge, as multiple related norms might conflict in\nspecific contexts. Consider norms that are upheld by the majority and promote\nthe well-being of society are more likely to be accepted and widely adopted\n(e.g., \"don't cheat,\"). Therefore, it is essential for LLM to identify the\nappropriate norms for a given scenario before making moral decisions. To this\nend, we introduce a novel moral judgment approach called \\textit{ClarityEthic}\nthat leverages LLMs' reasoning ability and contrastive learning to uncover\nrelevant social norms for human actions from different perspectives and select\nthe most reliable one to enhance judgment accuracy. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art approaches in moral\njudgment tasks. Moreover, human evaluations confirm that the generated social\nnorms provide plausible explanations that support the judgments. This suggests\nthat modeling human moral judgment with the emulating humans moral strategy is\npromising for improving the ethical behaviors of LLMs."
                },
                "authors": [
                    {
                        "name": "Yuxi Sun"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenxuan Zhang"
                },
                "author": "Wenxuan Zhang",
                "arxiv_comment": "We have noticed that this version of our experiment and method\n  description isn't quite complete or accurate. To make sure we present our\n  best work, we think it would be a good idea to withdraw the manuscript for\n  now and take some time to revise and reformat it",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01705v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01705v2",
                "updated": "2025-04-09T08:36:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    36,
                    10,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-03T09:04:45Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    4,
                    45,
                    4,
                    3,
                    0
                ],
                "title": "The Essence of Contextual Understanding in Theory of Mind: A Study on\n  Question Answering with Story Characters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Essence of Contextual Understanding in Theory of Mind: A Study on\n  Question Answering with Story Characters"
                },
                "summary": "Theory-of-Mind (ToM) is a fundamental psychological capability that allows\nhumans to understand and interpret the mental states of others. Humans infer\nothers' thoughts by integrating causal cues and indirect clues from broad\ncontextual information, often derived from past interactions. In other words,\nhuman ToM heavily relies on the understanding about the backgrounds and life\nstories of others. Unfortunately, this aspect is largely overlooked in existing\nbenchmarks for evaluating machines' ToM capabilities, due to their usage of\nshort narratives without global context, especially personal background of\ncharacters. In this paper, we verify the importance of comprehensive contextual\nunderstanding about personal backgrounds in ToM and assess the performance of\nLLMs in such complex scenarios. To achieve this, we introduce CharToM\nbenchmark, comprising 1,035 ToM questions based on characters from classic\nnovels. Our human study reveals a significant disparity in performance: the\nsame group of educated participants performs dramatically better when they have\nread the novels compared to when they have not. In parallel, our experiments on\nstate-of-the-art LLMs, including the very recent o1 and DeepSeek-R1 models,\nshow that LLMs still perform notably worse than humans, despite that they have\nseen these stories during pre-training. This highlights the limitations of\ncurrent LLMs in capturing the nuanced contextual information required for ToM\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory-of-Mind (ToM) is a fundamental psychological capability that allows\nhumans to understand and interpret the mental states of others. Humans infer\nothers' thoughts by integrating causal cues and indirect clues from broad\ncontextual information, often derived from past interactions. In other words,\nhuman ToM heavily relies on the understanding about the backgrounds and life\nstories of others. Unfortunately, this aspect is largely overlooked in existing\nbenchmarks for evaluating machines' ToM capabilities, due to their usage of\nshort narratives without global context, especially personal background of\ncharacters. In this paper, we verify the importance of comprehensive contextual\nunderstanding about personal backgrounds in ToM and assess the performance of\nLLMs in such complex scenarios. To achieve this, we introduce CharToM\nbenchmark, comprising 1,035 ToM questions based on characters from classic\nnovels. Our human study reveals a significant disparity in performance: the\nsame group of educated participants performs dramatically better when they have\nread the novels compared to when they have not. In parallel, our experiments on\nstate-of-the-art LLMs, including the very recent o1 and DeepSeek-R1 models,\nshow that LLMs still perform notably worse than humans, despite that they have\nseen these stories during pre-training. This highlights the limitations of\ncurrent LLMs in capturing the nuanced contextual information required for ToM\nreasoning."
                },
                "authors": [
                    {
                        "name": "Chulun Zhou"
                    },
                    {
                        "name": "Qiujing Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Xiaoqian Yue"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Jiangnan Li"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Shunchi Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01705v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01705v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02514v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02514v3",
                "updated": "2025-04-09T08:33:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    33,
                    54,
                    2,
                    99,
                    0
                ],
                "published": "2025-02-04T17:33:08Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    33,
                    8,
                    1,
                    35,
                    0
                ],
                "title": "Privacy Attacks on Image AutoRegressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Attacks on Image AutoRegressive Models"
                },
                "summary": "Image autoregressive generation has emerged as a powerful new paradigm, with\nimage autoregressive models (IARs) matching state-of-the-art diffusion models\n(DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for higher\ngeneration speed. However, the privacy risks associated with IARs remain\nunexplored, raising concerns about their responsible deployment. To address\nthis gap, we conduct a comprehensive privacy analysis of IARs, comparing their\nprivacy risks to those of DMs as a reference point. Specifically, we develop a\nnovel membership inference attack (MIA) that achieves a remarkably high success\nrate in detecting training images, with a True Positive Rate at False Positive\nRate = 1% (TPR@FPR=1%) of 86.38%, compared to just 6.38% for DMs using\ncomparable attacks. We leverage our novel MIA to perform dataset inference (DI)\nfor IARs and show that it requires as few as 6 samples to detect dataset\nmembership, compared to 200 samples for DI in DMs. This confirms a higher level\nof information leakage in IARs. Finally, we are able to extract hundreds of\ntraining data points from an IAR (e.g., 698 from VAR-d30). Our results suggest\na fundamental privacy-utility trade-off: while IARs excel in image generation\nquality and speed, they are empirically significantly more vulnerable to\nprivacy attacks compared to DMs that achieve similar performance. This trend\nsuggests that incorporating techniques from DMs into IARs, such as modeling the\nper-token probability distribution using a diffusion procedure, could help\nmitigate IARs' vulnerability to privacy attacks. We make our code available at:\nhttps://github.com/sprintml/privacy_attacks_against_iars",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image autoregressive generation has emerged as a powerful new paradigm, with\nimage autoregressive models (IARs) matching state-of-the-art diffusion models\n(DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for higher\ngeneration speed. However, the privacy risks associated with IARs remain\nunexplored, raising concerns about their responsible deployment. To address\nthis gap, we conduct a comprehensive privacy analysis of IARs, comparing their\nprivacy risks to those of DMs as a reference point. Specifically, we develop a\nnovel membership inference attack (MIA) that achieves a remarkably high success\nrate in detecting training images, with a True Positive Rate at False Positive\nRate = 1% (TPR@FPR=1%) of 86.38%, compared to just 6.38% for DMs using\ncomparable attacks. We leverage our novel MIA to perform dataset inference (DI)\nfor IARs and show that it requires as few as 6 samples to detect dataset\nmembership, compared to 200 samples for DI in DMs. This confirms a higher level\nof information leakage in IARs. Finally, we are able to extract hundreds of\ntraining data points from an IAR (e.g., 698 from VAR-d30). Our results suggest\na fundamental privacy-utility trade-off: while IARs excel in image generation\nquality and speed, they are empirically significantly more vulnerable to\nprivacy attacks compared to DMs that achieve similar performance. This trend\nsuggests that incorporating techniques from DMs into IARs, such as modeling the\nper-token probability distribution using a diffusion procedure, could help\nmitigate IARs' vulnerability to privacy attacks. We make our code available at:\nhttps://github.com/sprintml/privacy_attacks_against_iars"
                },
                "authors": [
                    {
                        "name": "Antoni Kowalczuk"
                    },
                    {
                        "name": "Jan Dubiński"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Adam Dziedzic"
                    }
                ],
                "author_detail": {
                    "name": "Adam Dziedzic"
                },
                "author": "Adam Dziedzic",
                "arxiv_comment": "Code: https://github.com/sprintml/privacy_attacks_against_iars",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02514v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02514v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14026v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14026v5",
                "updated": "2025-04-09T08:23:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    23,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2024-06-20T06:46:23Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    6,
                    46,
                    23,
                    3,
                    172,
                    0
                ],
                "title": "Demystifying Language Model Forgetting with Low-rank Example\n  Associations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Language Model Forgetting with Low-rank Example\n  Associations"
                },
                "summary": "Large Language models (LLMs) suffer from forgetting of upstream data when\nfine-tuned. Despite efforts on mitigating forgetting, few have investigated\nwhether, and how forgotten upstream examples are dependent on newly learned\ntasks. Insights on such dependencies enable efficient and targeted mitigation\nof forgetting. In this paper, we empirically analyze forgetting that occurs in\n$N$ upstream examples of language modeling or instruction-tuning after\nfine-tuning LLMs on one of $M$ new tasks, visualized in $M\\times N$ matrices.\nWe show that the matrices are often well-approximated with low-rank matrices,\nindicating the dominance of simple associations between the learned tasks and\nforgotten upstream examples. Leveraging the analysis, we predict forgetting of\nupstream examples when fine-tuning on unseen tasks with matrix completion over\nthe empirical associations. This enables fast identification of most forgotten\nexamples without expensive inference on the entire upstream data. The approach,\ndespite simplicity, outperforms prior approaches that learn semantic\nrelationships of learned tasks and upstream examples with LMs for predicting\nforgetting. We demonstrate the practical utility of our analysis by showing\nstatistically significantly reduced forgetting as we upweight predicted\nexamples for replay at fine-tuning. Project page:\nhttps://inklab.usc.edu/lm-forgetting-prediction/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) suffer from forgetting of upstream data when\nfine-tuned. Despite efforts on mitigating forgetting, few have investigated\nwhether, and how forgotten upstream examples are dependent on newly learned\ntasks. Insights on such dependencies enable efficient and targeted mitigation\nof forgetting. In this paper, we empirically analyze forgetting that occurs in\n$N$ upstream examples of language modeling or instruction-tuning after\nfine-tuning LLMs on one of $M$ new tasks, visualized in $M\\times N$ matrices.\nWe show that the matrices are often well-approximated with low-rank matrices,\nindicating the dominance of simple associations between the learned tasks and\nforgotten upstream examples. Leveraging the analysis, we predict forgetting of\nupstream examples when fine-tuning on unseen tasks with matrix completion over\nthe empirical associations. This enables fast identification of most forgotten\nexamples without expensive inference on the entire upstream data. The approach,\ndespite simplicity, outperforms prior approaches that learn semantic\nrelationships of learned tasks and upstream examples with LMs for predicting\nforgetting. We demonstrate the practical utility of our analysis by showing\nstatistically significantly reduced forgetting as we upweight predicted\nexamples for replay at fine-tuning. Project page:\nhttps://inklab.usc.edu/lm-forgetting-prediction/"
                },
                "authors": [
                    {
                        "name": "Xisen Jin"
                    },
                    {
                        "name": "Xiang Ren"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Ren"
                },
                "author": "Xiang Ren",
                "arxiv_comment": "8 pages; preprint, fixed Table 5 in Appendix D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14026v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14026v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04713v2",
                "updated": "2025-04-09T08:15:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    15,
                    21,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-07T03:50:12Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    50,
                    12,
                    0,
                    97,
                    0
                ],
                "title": "Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting\n  Sequential Needles from Long Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting\n  Sequential Needles from Long Contexts"
                },
                "summary": "Evaluating the ability of large language models (LLMs) to handle extended\ncontexts is critical, particularly for retrieving information relevant to\nspecific queries embedded within lengthy inputs. We introduce Sequential-NIAH,\na benchmark specifically designed to evaluate the capability of LLMs to extract\nsequential information items (known as needles) from long contexts. The\nbenchmark comprises three types of needle generation pipelines: synthetic,\nreal, and open-domain QA. It includes contexts ranging from 8K to 128K tokens\nin length, with a dataset of 14,000 samples (2,000 reserved for testing). To\nfacilitate evaluation on this benchmark, we trained a synthetic data-driven\nevaluation model capable of evaluating answer correctness based on\nchronological or logical order, achieving an accuracy of 99.49% on synthetic\ntest data. We conducted experiments on six well-known LLMs, revealing that even\nthe best-performing model achieved a maximum accuracy of only 63.15%. Further\nanalysis highlights the growing challenges posed by increasing context lengths\nand the number of needles, underscoring substantial room for improvement.\nAdditionally, noise robustness experiments validate the reliability of the\nbenchmark, making Sequential-NIAH an important reference for advancing research\non long text extraction capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the ability of large language models (LLMs) to handle extended\ncontexts is critical, particularly for retrieving information relevant to\nspecific queries embedded within lengthy inputs. We introduce Sequential-NIAH,\na benchmark specifically designed to evaluate the capability of LLMs to extract\nsequential information items (known as needles) from long contexts. The\nbenchmark comprises three types of needle generation pipelines: synthetic,\nreal, and open-domain QA. It includes contexts ranging from 8K to 128K tokens\nin length, with a dataset of 14,000 samples (2,000 reserved for testing). To\nfacilitate evaluation on this benchmark, we trained a synthetic data-driven\nevaluation model capable of evaluating answer correctness based on\nchronological or logical order, achieving an accuracy of 99.49% on synthetic\ntest data. We conducted experiments on six well-known LLMs, revealing that even\nthe best-performing model achieved a maximum accuracy of only 63.15%. Further\nanalysis highlights the growing challenges posed by increasing context lengths\nand the number of needles, underscoring substantial room for improvement.\nAdditionally, noise robustness experiments validate the reliability of the\nbenchmark, making Sequential-NIAH an important reference for advancing research\non long text extraction capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Yifei Yu"
                    },
                    {
                        "name": "Qian-Wen Zhang"
                    },
                    {
                        "name": "Lingfeng Qiao"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Fang Li"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Zengxi Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Xiaolong Liang"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08397v2",
                "updated": "2025-04-09T08:01:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    1,
                    55,
                    2,
                    99,
                    0
                ],
                "published": "2024-11-13T07:32:58Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    7,
                    32,
                    58,
                    2,
                    318,
                    0
                ],
                "title": "CLaSP: Learning Concepts for Time-Series Signals from Natural Language\n  Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLaSP: Learning Concepts for Time-Series Signals from Natural Language\n  Supervision"
                },
                "summary": "This paper presents CLaSP, a novel model for retrieving time-series signals\nusing natural language queries that describe signal characteristics. The\nability to search time-series signals based on descriptive queries is essential\nin domains such as industrial diagnostics, where data scientists often need to\nfind signals with specific characteristics. However, existing methods rely on\nsketch-based inputs, predefined synonym dictionaries, or domain-specific manual\ndesigns, limiting their scalability and adaptability. CLaSP addresses these\nchallenges by employing contrastive learning to map time-series signals to\nnatural language descriptions. Unlike prior approaches, it eliminates the need\nfor predefined synonym dictionaries and leverages the rich contextual knowledge\nof large language models (LLMs). Using the TRUCE and SUSHI datasets, which pair\ntime-series signals with natural language descriptions, we demonstrate that\nCLaSP achieves high accuracy in retrieving a variety of time series patterns\nbased on natural language queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents CLaSP, a novel model for retrieving time-series signals\nusing natural language queries that describe signal characteristics. The\nability to search time-series signals based on descriptive queries is essential\nin domains such as industrial diagnostics, where data scientists often need to\nfind signals with specific characteristics. However, existing methods rely on\nsketch-based inputs, predefined synonym dictionaries, or domain-specific manual\ndesigns, limiting their scalability and adaptability. CLaSP addresses these\nchallenges by employing contrastive learning to map time-series signals to\nnatural language descriptions. Unlike prior approaches, it eliminates the need\nfor predefined synonym dictionaries and leverages the rich contextual knowledge\nof large language models (LLMs). Using the TRUCE and SUSHI datasets, which pair\ntime-series signals with natural language descriptions, we demonstrate that\nCLaSP achieves high accuracy in retrieving a variety of time series patterns\nbased on natural language queries."
                },
                "authors": [
                    {
                        "name": "Aoi Ito"
                    },
                    {
                        "name": "Kota Dohi"
                    },
                    {
                        "name": "Yohei Kawaguchi"
                    }
                ],
                "author_detail": {
                    "name": "Yohei Kawaguchi"
                },
                "author": "Yohei Kawaguchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06664v1",
                "updated": "2025-04-09T07:56:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    56,
                    56,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T07:56:56Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    56,
                    56,
                    2,
                    99,
                    0
                ],
                "title": "SEE: Continual Fine-tuning with Sequential Ensemble of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEE: Continual Fine-tuning with Sequential Ensemble of Experts"
                },
                "summary": "Continual fine-tuning of large language models (LLMs) suffers from\ncatastrophic forgetting. Rehearsal-based methods mitigate this problem by\nretaining a small set of old data. Nevertheless, they still suffer inevitable\nperformance loss. Although training separate experts for each task can help\nprevent forgetting, effectively assembling them remains a challenge. Some\napproaches use routers to assign tasks to experts, but in continual learning,\nthey often require retraining for optimal performance. To address these\nchallenges, we introduce the Sequential Ensemble of Experts (SEE) framework.\nSEE removes the need for an additional router, allowing each expert to\nindependently decide whether a query should be handled. The framework employs\ndistributed routing, and during continual fine-tuning, SEE only requires the\ntraining of new experts for incoming tasks rather than retraining the entire\nsystem. Experiments reveal that the SEE outperforms prior approaches, including\nmulti-task learning, in continual fine-tuning. It also demonstrates remarkable\ngeneralization ability, as the expert can effectively identify\nout-of-distribution queries, which can then be directed to a more generalized\nmodel for resolution. This work highlights the promising potential of\nintegrating routing and response mechanisms within each expert, paving the way\nfor the future of distributed model ensembling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual fine-tuning of large language models (LLMs) suffers from\ncatastrophic forgetting. Rehearsal-based methods mitigate this problem by\nretaining a small set of old data. Nevertheless, they still suffer inevitable\nperformance loss. Although training separate experts for each task can help\nprevent forgetting, effectively assembling them remains a challenge. Some\napproaches use routers to assign tasks to experts, but in continual learning,\nthey often require retraining for optimal performance. To address these\nchallenges, we introduce the Sequential Ensemble of Experts (SEE) framework.\nSEE removes the need for an additional router, allowing each expert to\nindependently decide whether a query should be handled. The framework employs\ndistributed routing, and during continual fine-tuning, SEE only requires the\ntraining of new experts for incoming tasks rather than retraining the entire\nsystem. Experiments reveal that the SEE outperforms prior approaches, including\nmulti-task learning, in continual fine-tuning. It also demonstrates remarkable\ngeneralization ability, as the expert can effectively identify\nout-of-distribution queries, which can then be directed to a more generalized\nmodel for resolution. This work highlights the promising potential of\nintegrating routing and response mechanisms within each expert, paving the way\nfor the future of distributed model ensembling."
                },
                "authors": [
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "9pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19271v2",
                "updated": "2025-04-09T07:51:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    51,
                    49,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-25T02:05:46Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    2,
                    5,
                    46,
                    1,
                    84,
                    0
                ],
                "title": "MARS: Memory-Enhanced Agents with Reflective Self-improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARS: Memory-Enhanced Agents with Reflective Self-improvement"
                },
                "summary": "Large language models (LLMs) have made significant advances in the field of\nnatural language processing, but they still face challenges such as continuous\ndecision-making, lack of long-term memory, and limited context windows in\ndynamic environments. To address these issues, this paper proposes an\ninnovative framework Memory-Enhanced Agents with Reflective Self-improvement.\nThe MARS framework comprises three agents: the User, the Assistant, and the\nChecker. By integrating iterative feedback, reflective mechanisms, and a memory\noptimization mechanism based on the Ebbinghaus forgetting curve, it\nsignificantly enhances the agents capabilities in handling multi-tasking and\nlong-span information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advances in the field of\nnatural language processing, but they still face challenges such as continuous\ndecision-making, lack of long-term memory, and limited context windows in\ndynamic environments. To address these issues, this paper proposes an\ninnovative framework Memory-Enhanced Agents with Reflective Self-improvement.\nThe MARS framework comprises three agents: the User, the Assistant, and the\nChecker. By integrating iterative feedback, reflective mechanisms, and a memory\noptimization mechanism based on the Ebbinghaus forgetting curve, it\nsignificantly enhances the agents capabilities in handling multi-tasking and\nlong-span information."
                },
                "authors": [
                    {
                        "name": "Xuechen Liang"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Yinghui Xia"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Yijin Wang"
                    },
                    {
                        "name": "Jingsong Yang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Yuantao Wang"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Xueqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqian Wang"
                },
                "author": "Xueqian Wang",
                "arxiv_comment": "We are withdrawing this version because it duplicates our previous\n  submission (arXiv:2409.00872)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06659v1",
                "updated": "2025-04-09T07:49:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    49,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T07:49:08Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    49,
                    8,
                    2,
                    99,
                    0
                ],
                "title": "Bridging the Gap Between Preference Alignment and Machine Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap Between Preference Alignment and Machine Unlearning"
                },
                "summary": "Despite advances in Preference Alignment (PA) for Large Language Models\n(LLMs), mainstream methods like Reinforcement Learning with Human Feedback\n(RLHF) face notable challenges. These approaches require high-quality datasets\nof positive preference examples, which are costly to obtain and computationally\nintensive due to training instability, limiting their use in low-resource\nscenarios. LLM unlearning technique presents a promising alternative, by\ndirectly removing the influence of negative examples. However, current research\nhas primarily focused on empirical validation, lacking systematic quantitative\nanalysis. To bridge this gap, we propose a framework to explore the\nrelationship between PA and LLM unlearning. Specifically, we introduce a\nbi-level optimization-based method to quantify the impact of unlearning\nspecific negative examples on PA performance. Our analysis reveals that not all\nnegative examples contribute equally to alignment improvement when unlearned,\nand the effect varies significantly across examples. Building on this insight,\nwe pose a crucial question: how can we optimally select and weight negative\nexamples for unlearning to maximize PA performance? To answer this, we propose\na framework called Unlearning to Align (U2A), which leverages bi-level\noptimization to efficiently select and unlearn examples for optimal PA\nperformance. We validate the proposed method through extensive experiments,\nwith results confirming its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in Preference Alignment (PA) for Large Language Models\n(LLMs), mainstream methods like Reinforcement Learning with Human Feedback\n(RLHF) face notable challenges. These approaches require high-quality datasets\nof positive preference examples, which are costly to obtain and computationally\nintensive due to training instability, limiting their use in low-resource\nscenarios. LLM unlearning technique presents a promising alternative, by\ndirectly removing the influence of negative examples. However, current research\nhas primarily focused on empirical validation, lacking systematic quantitative\nanalysis. To bridge this gap, we propose a framework to explore the\nrelationship between PA and LLM unlearning. Specifically, we introduce a\nbi-level optimization-based method to quantify the impact of unlearning\nspecific negative examples on PA performance. Our analysis reveals that not all\nnegative examples contribute equally to alignment improvement when unlearned,\nand the effect varies significantly across examples. Building on this insight,\nwe pose a crucial question: how can we optimally select and weight negative\nexamples for unlearning to maximize PA performance? To answer this, we propose\na framework called Unlearning to Align (U2A), which leverages bi-level\noptimization to efficiently select and unlearn examples for optimal PA\nperformance. We validate the proposed method through extensive experiments,\nwith results confirming its effectiveness."
                },
                "authors": [
                    {
                        "name": "Xiaohua Feng"
                    },
                    {
                        "name": "Yuyuan Li"
                    },
                    {
                        "name": "Huwei Ji"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Chaochao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chaochao Chen"
                },
                "author": "Chaochao Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06658v1",
                "updated": "2025-04-09T07:48:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    48,
                    10,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T07:48:10Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    48,
                    10,
                    2,
                    99,
                    0
                ],
                "title": "A Neuro-inspired Interpretation of Unlearning in Large Language Models\n  through Sample-level Unlearning Difficulty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Neuro-inspired Interpretation of Unlearning in Large Language Models\n  through Sample-level Unlearning Difficulty"
                },
                "summary": "Driven by privacy protection laws and regulations, unlearning in Large\nLanguage Models (LLMs) is gaining increasing attention. However, current\nresearch often neglects the interpretability of the unlearning process,\nparticularly concerning sample-level unlearning difficulty. Existing studies\ntypically assume a uniform unlearning difficulty across samples. This\nsimplification risks attributing the performance of unlearning algorithms to\nsample selection rather than the algorithm's design, potentially steering the\ndevelopment of LLM unlearning in the wrong direction. Thus, we investigate the\nrelationship between LLM unlearning and sample characteristics, with a focus on\nunlearning difficulty. Drawing inspiration from neuroscience, we propose a\nMemory Removal Difficulty ($\\mathrm{MRD}$) metric to quantify sample-level\nunlearning difficulty. Using $\\mathrm{MRD}$, we analyze the characteristics of\nhard-to-unlearn versus easy-to-unlearn samples. Furthermore, we propose an\n$\\mathrm{MRD}$-based weighted sampling method to optimize existing unlearning\nalgorithms, which prioritizes easily forgettable samples, thereby improving\nunlearning efficiency and effectiveness. We validate the proposed metric and\nmethod using public benchmarks and datasets, with results confirming its\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by privacy protection laws and regulations, unlearning in Large\nLanguage Models (LLMs) is gaining increasing attention. However, current\nresearch often neglects the interpretability of the unlearning process,\nparticularly concerning sample-level unlearning difficulty. Existing studies\ntypically assume a uniform unlearning difficulty across samples. This\nsimplification risks attributing the performance of unlearning algorithms to\nsample selection rather than the algorithm's design, potentially steering the\ndevelopment of LLM unlearning in the wrong direction. Thus, we investigate the\nrelationship between LLM unlearning and sample characteristics, with a focus on\nunlearning difficulty. Drawing inspiration from neuroscience, we propose a\nMemory Removal Difficulty ($\\mathrm{MRD}$) metric to quantify sample-level\nunlearning difficulty. Using $\\mathrm{MRD}$, we analyze the characteristics of\nhard-to-unlearn versus easy-to-unlearn samples. Furthermore, we propose an\n$\\mathrm{MRD}$-based weighted sampling method to optimize existing unlearning\nalgorithms, which prioritizes easily forgettable samples, thereby improving\nunlearning efficiency and effectiveness. We validate the proposed metric and\nmethod using public benchmarks and datasets, with results confirming its\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Xiaohua Feng"
                    },
                    {
                        "name": "Yuyuan Li"
                    },
                    {
                        "name": "Chengye Wang"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Chaochao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chaochao Chen"
                },
                "author": "Chaochao Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06650v1",
                "updated": "2025-04-09T07:37:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    37,
                    27,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T07:37:27Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    37,
                    27,
                    2,
                    99,
                    0
                ],
                "title": "ThoughtProbe: Classifier-Guided Thought Space Exploration Leveraging LLM\n  Intrinsic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThoughtProbe: Classifier-Guided Thought Space Exploration Leveraging LLM\n  Intrinsic Reasoning"
                },
                "summary": "Pre-trained large language models (LLMs) have been demonstrated to possess\nintrinsic reasoning capabilities that can emerge naturally when expanding the\nresponse space. However, the neural representation mechanisms underlying these\nintrinsic capabilities and approaches for their optimal utilization remain\ninadequately understood. In this work, we make the key discovery that a simple\nlinear classifier can effectively detect intrinsic reasoning capabilities in\nLLMs' activation space, particularly within specific representation types and\nnetwork layers. Based on this finding, we propose a classifier-guided search\nframework that strategically explore a tree-structured response space. In each\nnode expansion, the classifier serves as a scoring and ranking mechanism that\nefficiently allocates computational resources by identifying and prioritizing\nmore thoughtful reasoning directions for continuation. After completing the\ntree expansion, we collect answers from all branches to form a candidate answer\npool. We propose a branch-aggregation selection method that marginalizes over\nall supporting branches by aggregating their thoughtfulness scores, thereby\nidentifying the optimal answer from the pool. Experimental results show that\nour framework's comprehensive exploration not only covers valid reasoning\nchains but also effectively identifies them, achieving significant improvements\nacross multiple arithmetic reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained large language models (LLMs) have been demonstrated to possess\nintrinsic reasoning capabilities that can emerge naturally when expanding the\nresponse space. However, the neural representation mechanisms underlying these\nintrinsic capabilities and approaches for their optimal utilization remain\ninadequately understood. In this work, we make the key discovery that a simple\nlinear classifier can effectively detect intrinsic reasoning capabilities in\nLLMs' activation space, particularly within specific representation types and\nnetwork layers. Based on this finding, we propose a classifier-guided search\nframework that strategically explore a tree-structured response space. In each\nnode expansion, the classifier serves as a scoring and ranking mechanism that\nefficiently allocates computational resources by identifying and prioritizing\nmore thoughtful reasoning directions for continuation. After completing the\ntree expansion, we collect answers from all branches to form a candidate answer\npool. We propose a branch-aggregation selection method that marginalizes over\nall supporting branches by aggregating their thoughtfulness scores, thereby\nidentifying the optimal answer from the pool. Experimental results show that\nour framework's comprehensive exploration not only covers valid reasoning\nchains but also effectively identifies them, achieving significant improvements\nacross multiple arithmetic reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Zijian Wang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06637v1",
                "updated": "2025-04-09T07:26:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    26,
                    24,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T07:26:24Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    26,
                    24,
                    2,
                    99,
                    0
                ],
                "title": "SCI-Reason: A Dataset with Chain-of-Thought Rationales for Complex\n  Multimodal Reasoning in Academic Areas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCI-Reason: A Dataset with Chain-of-Thought Rationales for Complex\n  Multimodal Reasoning in Academic Areas"
                },
                "summary": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) demonstrate\nimpressive problem-solving skills in many tasks and domains. However, their\nability to reason with complex images in academic domains has not been\nsystematically investigated. To bridge this gap, we present SCI-Reason, a\ndataset for complex multimodel reasoning in academic areas. SCI-Reason aims to\ntest and improve the reasoning ability of large multimodal models using real\ncomplex images in academic domains. The dataset contains 12,066 images and\n12,626 question-answer pairs extracted from PubMed, divided into training,\nvalidation and test splits. Each question-answer pair also contains an accurate\nand efficient inference chain as a guide to improving the inference properties\nof the dataset. With SCI-Reason, we performed a comprehensive evaluation of 8\nwell-known models. The best performing model, Claude-3.7-Sonnet, only achieved\nan accuracy of 55.19%. Error analysis shows that more than half of the model\nfailures are due to breakdowns in multi-step inference chains rather than\nerrors in primary visual feature extraction. This finding underscores the\ninherent limitations in reasoning capabilities exhibited by current multimodal\nmodels when processing complex image analysis tasks within authentic academic\ncontexts. Experiments on open-source models show that SCI-Reason not only\nenhances reasoning ability but also demonstrates cross-domain generalization in\nVQA tasks. We also explore future applications of model inference capabilities\nin this domain, highlighting its potential for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) demonstrate\nimpressive problem-solving skills in many tasks and domains. However, their\nability to reason with complex images in academic domains has not been\nsystematically investigated. To bridge this gap, we present SCI-Reason, a\ndataset for complex multimodel reasoning in academic areas. SCI-Reason aims to\ntest and improve the reasoning ability of large multimodal models using real\ncomplex images in academic domains. The dataset contains 12,066 images and\n12,626 question-answer pairs extracted from PubMed, divided into training,\nvalidation and test splits. Each question-answer pair also contains an accurate\nand efficient inference chain as a guide to improving the inference properties\nof the dataset. With SCI-Reason, we performed a comprehensive evaluation of 8\nwell-known models. The best performing model, Claude-3.7-Sonnet, only achieved\nan accuracy of 55.19%. Error analysis shows that more than half of the model\nfailures are due to breakdowns in multi-step inference chains rather than\nerrors in primary visual feature extraction. This finding underscores the\ninherent limitations in reasoning capabilities exhibited by current multimodal\nmodels when processing complex image analysis tasks within authentic academic\ncontexts. Experiments on open-source models show that SCI-Reason not only\nenhances reasoning ability but also demonstrates cross-domain generalization in\nVQA tasks. We also explore future applications of model inference capabilities\nin this domain, highlighting its potential for future research."
                },
                "authors": [
                    {
                        "name": "Chenghao Ma"
                    },
                    {
                        "name": "Haihong E."
                    },
                    {
                        "name": "Junpeng Ding"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziyan Ma"
                    },
                    {
                        "name": "Huang Qing"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Meina Song"
                    }
                ],
                "author_detail": {
                    "name": "Meina Song"
                },
                "author": "Meina Song",
                "arxiv_comment": "Submitted to ICCV 2025. 11 pages (including references)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03307v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03307v4",
                "updated": "2025-04-09T07:21:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    21,
                    18,
                    2,
                    99,
                    0
                ],
                "published": "2025-02-05T16:08:05Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    8,
                    5,
                    2,
                    36,
                    0
                ],
                "title": "Intent Representation Learning with Large Language Model for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Representation Learning with Large Language Model for\n  Recommendation"
                },
                "summary": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines.Code available at\nhttps://github.com/wangyu0627/IRLLRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines.Code available at\nhttps://github.com/wangyu0627/IRLLRec."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Lei Sang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "arxiv_comment": "Accepted by SIGIR 2025 Full Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03307v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03307v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06631v1",
                "updated": "2025-04-09T07:09:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    9,
                    40,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T07:09:40Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    9,
                    40,
                    2,
                    99,
                    0
                ],
                "title": "The Method for Storing Patterns in Neural Networks-Memorization and\n  Recall of QR code Patterns-",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Method for Storing Patterns in Neural Networks-Memorization and\n  Recall of QR code Patterns-"
                },
                "summary": "In this paper, we propose a mechanism for storing complex patterns within a\nneural network and subsequently recalling them. This model is based on our work\npublished in 2018(Inazawa, 2018), which we have refined and extended in this\nwork. With the recent advancements in deep learning and large language model\n(LLM)-based AI technologies (generative AI), it can be considered that\nmethodologies for the learning are becoming increasingly well-established. In\nthe future, we expect to see further research on memory using models based on\nTransformers (Vaswani, et. al., 2017, Rae, et. al., 2020), but in this paper we\npropose a simpler and more powerful model of memory and recall in neural\nnetworks. The advantage of storing patterns in a neural network lies in its\nability to recall the original pattern even when an incomplete version is\npresented. The patterns we have produced for use in this study have been QR\ncode (DENSO WAVE, 1994), which has become widely used as an information\ntransmission tool in recent years.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a mechanism for storing complex patterns within a\nneural network and subsequently recalling them. This model is based on our work\npublished in 2018(Inazawa, 2018), which we have refined and extended in this\nwork. With the recent advancements in deep learning and large language model\n(LLM)-based AI technologies (generative AI), it can be considered that\nmethodologies for the learning are becoming increasingly well-established. In\nthe future, we expect to see further research on memory using models based on\nTransformers (Vaswani, et. al., 2017, Rae, et. al., 2020), but in this paper we\npropose a simpler and more powerful model of memory and recall in neural\nnetworks. The advantage of storing patterns in a neural network lies in its\nability to recall the original pattern even when an incomplete version is\npresented. The patterns we have produced for use in this study have been QR\ncode (DENSO WAVE, 1994), which has become widely used as an information\ntransmission tool in recent years."
                },
                "authors": [
                    {
                        "name": "Hiroshi Inazawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroshi Inazawa"
                },
                "author": "Hiroshi Inazawa",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02948v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02948v4",
                "updated": "2025-04-09T06:54:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    54,
                    20,
                    2,
                    99,
                    0
                ],
                "published": "2024-04-03T15:06:43Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    15,
                    6,
                    43,
                    2,
                    94,
                    0
                ],
                "title": "PiSSA: Principal Singular Values and Singular Vectors Adaptation of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiSSA: Principal Singular Values and Singular Vectors Adaptation of\n  Large Language Models"
                },
                "summary": "To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the\nlow-rank adaptation (LoRA) method approximates the model changes $\\Delta W \\in\n\\mathbb{R}^{m \\times n}$ through the product of two matrices $A \\in\n\\mathbb{R}^{m \\times r}$ and $B \\in \\mathbb{R}^{r \\times n}$, where $r \\ll\n\\min(m, n)$, $A$ is initialized with Gaussian noise, and $B$ with zeros. LoRA\nfreezes the original model $W$ and updates the \"Noise & Zero\" adapter, which\nmay lead to slow convergence. To overcome this limitation, we introduce\nPrincipal Singular values and Singular vectors Adaptation (PiSSA). PiSSA shares\nthe same architecture as LoRA, but initializes the adaptor matrices $A$ and $B$\nwith the principal components of the original matrix $W$, and put the remaining\ncomponents into a residual matrix $W^{res} \\in \\mathbb{R}^{m \\times n}$ which\nis frozen during fine-tuning. Compared to LoRA, PiSSA updates the principal\ncomponents while freezing the \"residual\" parts, allowing faster convergence and\nenhanced performance. Comparative experiments of PiSSA and LoRA across 12\ndifferent models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks,\nreveal that PiSSA consistently outperforms LoRA under identical experimental\nsetups. On the GSM8K benchmark, Mistral-7B fine-tuned with PiSSA achieves an\naccuracy of 72.86%, surpassing LoRA's 67.7% by 5.16%. Due to the same\narchitecture, PiSSA is also compatible with quantization to further reduce the\nmemory requirement of fine-tuning. Compared to QLoRA, QPiSSA exhibits smaller\nquantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K,\nQPiSSA attains an accuracy of 86.05%, exceeding the performances of QLoRA at\n81.73%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few\nseconds, presenting a negligible cost for transitioning from LoRA to PiSSA.\nCode is available at https://github.com/GraphPKU/PiSSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the\nlow-rank adaptation (LoRA) method approximates the model changes $\\Delta W \\in\n\\mathbb{R}^{m \\times n}$ through the product of two matrices $A \\in\n\\mathbb{R}^{m \\times r}$ and $B \\in \\mathbb{R}^{r \\times n}$, where $r \\ll\n\\min(m, n)$, $A$ is initialized with Gaussian noise, and $B$ with zeros. LoRA\nfreezes the original model $W$ and updates the \"Noise & Zero\" adapter, which\nmay lead to slow convergence. To overcome this limitation, we introduce\nPrincipal Singular values and Singular vectors Adaptation (PiSSA). PiSSA shares\nthe same architecture as LoRA, but initializes the adaptor matrices $A$ and $B$\nwith the principal components of the original matrix $W$, and put the remaining\ncomponents into a residual matrix $W^{res} \\in \\mathbb{R}^{m \\times n}$ which\nis frozen during fine-tuning. Compared to LoRA, PiSSA updates the principal\ncomponents while freezing the \"residual\" parts, allowing faster convergence and\nenhanced performance. Comparative experiments of PiSSA and LoRA across 12\ndifferent models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks,\nreveal that PiSSA consistently outperforms LoRA under identical experimental\nsetups. On the GSM8K benchmark, Mistral-7B fine-tuned with PiSSA achieves an\naccuracy of 72.86%, surpassing LoRA's 67.7% by 5.16%. Due to the same\narchitecture, PiSSA is also compatible with quantization to further reduce the\nmemory requirement of fine-tuning. Compared to QLoRA, QPiSSA exhibits smaller\nquantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K,\nQPiSSA attains an accuracy of 86.05%, exceeding the performances of QLoRA at\n81.73%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few\nseconds, presenting a negligible cost for transitioning from LoRA to PiSSA.\nCode is available at https://github.com/GraphPKU/PiSSA."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zhaohui Wang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "NeurIPS 2024 spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02948v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02948v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07800v2",
                "updated": "2025-04-09T06:37:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    37,
                    25,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-10T19:27:38Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    19,
                    27,
                    38,
                    0,
                    69,
                    0
                ],
                "title": "Using Large Language Models to Develop Requirements Elicitation Skills",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models to Develop Requirements Elicitation Skills"
                },
                "summary": "Requirements Elicitation (RE) is a crucial software engineering skill that\ninvolves interviewing a client and then devising a software design based on the\ninterview results. Teaching this inherently experiential skill effectively has\nhigh cost, such as acquiring an industry partner to interview, or training\ncourse staff or other students to play the role of a client. As a result, a\ntypical instructional approach is to provide students with transcripts of real\nor fictitious interviews to analyze, which exercises the skill of extracting\ntechnical requirements but fails to develop the equally important interview\nskill itself. As an alternative, we propose conditioning a large language model\nto play the role of the client during a chat-based interview. We perform a\nbetween-subjects study (n=120) in which students construct a high-level\napplication design from either an interactive LLM-backed interview session or\nan existing interview transcript describing the same business processes. We\nevaluate our approach using both a qualitative survey and quantitative\nobservations about participants' work. We find that both approaches provide\nsufficient information for participants to construct technically sound\nsolutions and require comparable time on task, but the LLM-based approach is\npreferred by most participants. Importantly, we observe that LLM-backed\ninterview is seen as both more realistic and more engaging, despite the LLM\noccasionally providing imprecise or contradictory information. These results,\ncombined with the wide accessibility of LLMs, suggest a new way to practice\ncritical RE skills in a scalable and realistic manner without the overhead of\narranging live interviews.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requirements Elicitation (RE) is a crucial software engineering skill that\ninvolves interviewing a client and then devising a software design based on the\ninterview results. Teaching this inherently experiential skill effectively has\nhigh cost, such as acquiring an industry partner to interview, or training\ncourse staff or other students to play the role of a client. As a result, a\ntypical instructional approach is to provide students with transcripts of real\nor fictitious interviews to analyze, which exercises the skill of extracting\ntechnical requirements but fails to develop the equally important interview\nskill itself. As an alternative, we propose conditioning a large language model\nto play the role of the client during a chat-based interview. We perform a\nbetween-subjects study (n=120) in which students construct a high-level\napplication design from either an interactive LLM-backed interview session or\nan existing interview transcript describing the same business processes. We\nevaluate our approach using both a qualitative survey and quantitative\nobservations about participants' work. We find that both approaches provide\nsufficient information for participants to construct technically sound\nsolutions and require comparable time on task, but the LLM-based approach is\npreferred by most participants. Importantly, we observe that LLM-backed\ninterview is seen as both more realistic and more engaging, despite the LLM\noccasionally providing imprecise or contradictory information. These results,\ncombined with the wide accessibility of LLMs, suggest a new way to practice\ncritical RE skills in a scalable and realistic manner without the overhead of\narranging live interviews."
                },
                "authors": [
                    {
                        "name": "Nelson Lojo"
                    },
                    {
                        "name": "Rafael González"
                    },
                    {
                        "name": "Rohan Philip"
                    },
                    {
                        "name": "José Antonio Parejo"
                    },
                    {
                        "name": "Amador Durán Toro"
                    },
                    {
                        "name": "Armando Fox"
                    },
                    {
                        "name": "Pablo Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Fernández"
                },
                "arxiv_affiliation": "SCORE Lab, Univ. of Sevilla, Sevilla, Spain",
                "author": "Pablo Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03051v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03051v4",
                "updated": "2025-04-09T06:24:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    24,
                    14,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-04T00:13:54Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    0,
                    13,
                    54,
                    4,
                    278,
                    0
                ],
                "title": "AuroraCap: Efficient, Performant Video Detailed Captioning and a New\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AuroraCap: Efficient, Performant Video Detailed Captioning and a New\n  Benchmark"
                },
                "summary": "Video detailed captioning is a key task which aims to generate comprehensive\nand coherent textual descriptions of video content, benefiting both video\nunderstanding and generation. In this paper, we propose AuroraCap, a video\ncaptioner based on a large multimodal model. We follow the simplest\narchitecture design without additional parameters for temporal modeling. To\naddress the overhead caused by lengthy video sequences, we implement the token\nmerging strategy, reducing the number of input visual tokens. Surprisingly, we\nfound that this strategy results in little performance loss. AuroraCap shows\nsuperior performance on various video and image captioning benchmarks, for\nexample, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and\nGemini-1.5 Pro (82.2). However, existing video caption benchmarks only include\nsimple descriptions, consisting of a few dozen words, which limits research in\nthis field. Therefore, we develop VDC, a video detailed captioning benchmark\nwith over one thousand carefully annotated structured captions. In addition, we\npropose a new LLM-assisted metric VDCscore for bettering evaluation, which\nadopts a divide-and-conquer strategy to transform long caption evaluation into\nmultiple short question-answer pairs. With the help of human Elo ranking, our\nexperiments show that this benchmark better correlates with human judgments of\nvideo detailed captioning quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video detailed captioning is a key task which aims to generate comprehensive\nand coherent textual descriptions of video content, benefiting both video\nunderstanding and generation. In this paper, we propose AuroraCap, a video\ncaptioner based on a large multimodal model. We follow the simplest\narchitecture design without additional parameters for temporal modeling. To\naddress the overhead caused by lengthy video sequences, we implement the token\nmerging strategy, reducing the number of input visual tokens. Surprisingly, we\nfound that this strategy results in little performance loss. AuroraCap shows\nsuperior performance on various video and image captioning benchmarks, for\nexample, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and\nGemini-1.5 Pro (82.2). However, existing video caption benchmarks only include\nsimple descriptions, consisting of a few dozen words, which limits research in\nthis field. Therefore, we develop VDC, a video detailed captioning benchmark\nwith over one thousand carefully annotated structured captions. In addition, we\npropose a new LLM-assisted metric VDCscore for bettering evaluation, which\nadopts a divide-and-conquer strategy to transform long caption evaluation into\nmultiple short question-answer pairs. With the help of human Elo ranking, our\nexperiments show that this benchmark better correlates with human judgments of\nvideo detailed captioning quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Enxin Song"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Chenlin Meng"
                    },
                    {
                        "name": "Vashisht Madhavan"
                    },
                    {
                        "name": "Omer Bar-Tal"
                    },
                    {
                        "name": "Jenq-Neng Hwang"
                    },
                    {
                        "name": "Saining Xie"
                    },
                    {
                        "name": "Christopher D. Manning"
                    }
                ],
                "author_detail": {
                    "name": "Christopher D. Manning"
                },
                "author": "Christopher D. Manning",
                "arxiv_comment": "Accepted to ICLR 2025. Code, docs, weight, benchmark and training\n  data are all avaliable at https://rese1f.github.io/aurora-web/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03051v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03051v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06614v1",
                "updated": "2025-04-09T06:18:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    18,
                    24,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T06:18:24Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    18,
                    24,
                    2,
                    99,
                    0
                ],
                "title": "AgentFM: Role-Aware Failure Management for Distributed Databases with\n  LLM-Driven Multi-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentFM: Role-Aware Failure Management for Distributed Databases with\n  LLM-Driven Multi-Agents"
                },
                "summary": "Distributed databases are critical infrastructures for today's large-scale\nsoftware systems, making effective failure management essential to ensure\nsoftware availability. However, existing approaches often overlook the role\ndistinctions within distributed databases and rely on small-scale models with\nlimited generalization capabilities. In this paper, we conduct a preliminary\nempirical study to emphasize the unique significance of different roles.\nBuilding on this insight, we propose AgentFM, a role-aware failure management\nframework for distributed databases powered by LLM-driven multi-agents. AgentFM\naddresses failure management by considering system roles, data roles, and task\nroles, with a meta-agent orchestrating these components. Preliminary\nevaluations using Apache IoTDB demonstrate the effectiveness of AgentFM and\nopen new directions for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed databases are critical infrastructures for today's large-scale\nsoftware systems, making effective failure management essential to ensure\nsoftware availability. However, existing approaches often overlook the role\ndistinctions within distributed databases and rely on small-scale models with\nlimited generalization capabilities. In this paper, we conduct a preliminary\nempirical study to emphasize the unique significance of different roles.\nBuilding on this insight, we propose AgentFM, a role-aware failure management\nframework for distributed databases powered by LLM-driven multi-agents. AgentFM\naddresses failure management by considering system roles, data roles, and task\nroles, with a meta-agent orchestrating these components. Preliminary\nevaluations using Apache IoTDB demonstrate the effectiveness of AgentFM and\nopen new directions for further research."
                },
                "authors": [
                    {
                        "name": "Lingzhe Zhang"
                    },
                    {
                        "name": "Yunpeng Zhai"
                    },
                    {
                        "name": "Tong Jia"
                    },
                    {
                        "name": "Xiaosong Huang"
                    },
                    {
                        "name": "Chiming Duan"
                    },
                    {
                        "name": "Ying Li"
                    }
                ],
                "author_detail": {
                    "name": "Ying Li"
                },
                "author": "Ying Li",
                "arxiv_doi": "10.1145/3696630.3728492",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696630.3728492",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted by FSE-IVR'25",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06611v2",
                "updated": "2025-04-10T07:46:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    46,
                    0,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-09T06:15:24Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    15,
                    24,
                    2,
                    99,
                    0
                ],
                "title": "Wanting to be Understood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wanting to be Understood"
                },
                "summary": "This paper explores an intrinsic motivation for mutual awareness,\nhypothesizing that humans possess a fundamental drive to understand and to be\nunderstood even in the absence of extrinsic rewards. Through simulations of the\nperceptual crossing paradigm, we explore the effect of various internal reward\nfunctions in reinforcement learning agents. The drive to understand is\nimplemented as an active inference type artificial curiosity reward, whereas\nthe drive to be understood is implemented through intrinsic rewards for\nimitation, influence/impressionability, and sub-reaction time anticipation of\nthe other. Results indicate that while artificial curiosity alone does not lead\nto a preference for social interaction, rewards emphasizing reciprocal\nunderstanding successfully drive agents to prioritize interaction. We\ndemonstrate that this intrinsic motivation can facilitate cooperation in tasks\nwhere only one agent receives extrinsic reward for the behaviour of the other.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores an intrinsic motivation for mutual awareness,\nhypothesizing that humans possess a fundamental drive to understand and to be\nunderstood even in the absence of extrinsic rewards. Through simulations of the\nperceptual crossing paradigm, we explore the effect of various internal reward\nfunctions in reinforcement learning agents. The drive to understand is\nimplemented as an active inference type artificial curiosity reward, whereas\nthe drive to be understood is implemented through intrinsic rewards for\nimitation, influence/impressionability, and sub-reaction time anticipation of\nthe other. Results indicate that while artificial curiosity alone does not lead\nto a preference for social interaction, rewards emphasizing reciprocal\nunderstanding successfully drive agents to prioritize interaction. We\ndemonstrate that this intrinsic motivation can facilitate cooperation in tasks\nwhere only one agent receives extrinsic reward for the behaviour of the other."
                },
                "authors": [
                    {
                        "name": "Chrisantha Fernando"
                    },
                    {
                        "name": "Dylan Banarse"
                    },
                    {
                        "name": "Simon Osindero"
                    }
                ],
                "author_detail": {
                    "name": "Simon Osindero"
                },
                "author": "Simon Osindero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.07097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07097v1",
                "updated": "2025-04-09T17:59:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    59,
                    42,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:59:42Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    59,
                    42,
                    2,
                    99,
                    0
                ],
                "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual\n  Learning"
                },
                "summary": "Continual learning in large language models (LLMs) is prone to catastrophic\nforgetting, where adapting to new tasks significantly degrades performance on\npreviously learned ones. Existing methods typically rely on low-rank,\nparameter-efficient updates that limit the model's expressivity and introduce\nadditional parameters per task, leading to scalability issues. To address these\nlimitations, we propose a novel continual full fine-tuning approach leveraging\nadaptive singular value decomposition (SVD). Our method dynamically identifies\ntask-specific low-rank parameter subspaces and constrains updates to be\northogonal to critical directions associated with prior tasks, thus effectively\nminimizing interference without additional parameter overhead or storing\nprevious task gradients. We evaluate our approach extensively on standard\ncontinual learning benchmarks using both encoder-decoder (T5-Large) and\ndecoder-only (LLaMA-2 7B) models, spanning diverse tasks including\nclassification, generation, and reasoning. Empirically, our method achieves\nstate-of-the-art results, up to 7% higher average accuracy than recent\nbaselines like O-LoRA, and notably maintains the model's general linguistic\ncapabilities, instruction-following accuracy, and safety throughout the\ncontinual learning process by reducing forgetting to near-negligible levels.\nOur adaptive SVD framework effectively balances model plasticity and knowledge\nretention, providing a practical, theoretically grounded, and computationally\nscalable solution for continual learning scenarios in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning in large language models (LLMs) is prone to catastrophic\nforgetting, where adapting to new tasks significantly degrades performance on\npreviously learned ones. Existing methods typically rely on low-rank,\nparameter-efficient updates that limit the model's expressivity and introduce\nadditional parameters per task, leading to scalability issues. To address these\nlimitations, we propose a novel continual full fine-tuning approach leveraging\nadaptive singular value decomposition (SVD). Our method dynamically identifies\ntask-specific low-rank parameter subspaces and constrains updates to be\northogonal to critical directions associated with prior tasks, thus effectively\nminimizing interference without additional parameter overhead or storing\nprevious task gradients. We evaluate our approach extensively on standard\ncontinual learning benchmarks using both encoder-decoder (T5-Large) and\ndecoder-only (LLaMA-2 7B) models, spanning diverse tasks including\nclassification, generation, and reasoning. Empirically, our method achieves\nstate-of-the-art results, up to 7% higher average accuracy than recent\nbaselines like O-LoRA, and notably maintains the model's general linguistic\ncapabilities, instruction-following accuracy, and safety throughout the\ncontinual learning process by reducing forgetting to near-negligible levels.\nOur adaptive SVD framework effectively balances model plasticity and knowledge\nretention, providing a practical, theoretically grounded, and computationally\nscalable solution for continual learning scenarios in large language models."
                },
                "authors": [
                    {
                        "name": "Nikhil Shivakumar Nayak"
                    },
                    {
                        "name": "Krishnateja Killamsetty"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Abhishek Bhandwaldar"
                    },
                    {
                        "name": "Prateek Chanda"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Aldo Pareja"
                    },
                    {
                        "name": "Oleg Silkin"
                    },
                    {
                        "name": "Mustafa Eyceoz"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "arxiv_comment": "25 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07089v1",
                "updated": "2025-04-09T17:58:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    58,
                    58,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:58:58Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    58,
                    58,
                    2,
                    99,
                    0
                ],
                "title": "OmniCaptioner: One Captioner to Rule Them All",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCaptioner: One Captioner to Rule Them All"
                },
                "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities."
                },
                "authors": [
                    {
                        "name": "Yiting Lu"
                    },
                    {
                        "name": "Jiakang Yuan"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Shitian Zhao"
                    },
                    {
                        "name": "Qi Qin"
                    },
                    {
                        "name": "Xinyue Li"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Dongyang Liu"
                    },
                    {
                        "name": "Yuewen Cao"
                    },
                    {
                        "name": "Xiangchao Yan"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Zhibo Chen"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Peng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Peng Gao"
                },
                "author": "Peng Gao",
                "arxiv_comment": "More visualizations on Homepage:\n  https://alpha-innovator.github.io/OmniCaptioner-project-page and Official\n  code: https://github.com/Alpha-Innovator/OmniCaptioner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07087v1",
                "updated": "2025-04-09T17:58:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    58,
                    47,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:58:47Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    58,
                    47,
                    2,
                    99,
                    0
                ],
                "title": "KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on\n  Textualized Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on\n  Textualized Knowledge Graphs"
                },
                "summary": "Knowledge graphs have emerged as a popular method for injecting up-to-date,\nfactual knowledge into large language models (LLMs). This is typically achieved\nby converting the knowledge graph into text that the LLM can process in\ncontext. While multiple methods of encoding knowledge graphs have been\nproposed, the impact of this textualization process on LLM performance remains\nunder-explored. We introduce KG-LLM-Bench, a comprehensive and extensible\nbenchmark spanning five knowledge graph understanding tasks, and evaluate how\ndifferent encoding strategies affect performance across various base models.\nOur extensive experiments with seven language models and five textualization\nstrategies provide insights for optimizing LLM performance on KG reasoning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs have emerged as a popular method for injecting up-to-date,\nfactual knowledge into large language models (LLMs). This is typically achieved\nby converting the knowledge graph into text that the LLM can process in\ncontext. While multiple methods of encoding knowledge graphs have been\nproposed, the impact of this textualization process on LLM performance remains\nunder-explored. We introduce KG-LLM-Bench, a comprehensive and extensible\nbenchmark spanning five knowledge graph understanding tasks, and evaluate how\ndifferent encoding strategies affect performance across various base models.\nOur extensive experiments with seven language models and five textualization\nstrategies provide insights for optimizing LLM performance on KG reasoning\ntasks."
                },
                "authors": [
                    {
                        "name": "Elan Markowitz"
                    },
                    {
                        "name": "Krupa Galiya"
                    },
                    {
                        "name": "Greg Ver Steeg"
                    },
                    {
                        "name": "Aram Galstyan"
                    }
                ],
                "author_detail": {
                    "name": "Aram Galstyan"
                },
                "author": "Aram Galstyan",
                "arxiv_comment": "To be presented at NAACL-HLT, KnowledgeNLP Workshop (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07080v1",
                "updated": "2025-04-09T17:53:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    53,
                    55,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:53:55Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    53,
                    55,
                    2,
                    99,
                    0
                ],
                "title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning"
                },
                "summary": "Despite great performance on Olympiad-level reasoning problems, frontier\nlarge language models can still struggle on high school math when presented\nwith novel problems outside standard benchmarks. Going beyond final accuracy,\nwe propose a deductive consistency metric to analyze chain-of-thought output\nfrom language models (LMs).Formally, deductive reasoning involves two subtasks:\nunderstanding a set of input premises and inferring the conclusions that follow\nfrom them. The proposed metric studies LMs' performance on these subtasks, with\nthe goal of explaining LMs' reasoning errors on novel problems: how well do LMs\nunderstand input premises with increasing context lengths, and how well can\nthey infer conclusions over multiple reasoning hops? Since existing benchmarks\nmay be memorized, we develop a pipeline to evaluate LMs' deductive consistency\non novel, perturbed versions of benchmark problems. On novel grade school math\nproblems (GSM-8k), we find that LMs are fairly robust to increasing number of\ninput premises, but suffer significant accuracy decay as the number of\nreasoning hops is increased. Interestingly, these errors are masked in the\noriginal benchmark as all models achieve near 100% accuracy. As we increase the\nnumber of solution steps using a synthetic dataset, prediction over multiple\nhops still remains the major source of error compared to understanding input\npremises. Other factors, such as shifts in language style or natural\npropagation of early errors do not explain the trends. Our analysis provides a\nnew view to characterize LM reasoning -- as computations over a window of input\npremises and reasoning hops -- that can provide unified evaluation across\nproblem domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite great performance on Olympiad-level reasoning problems, frontier\nlarge language models can still struggle on high school math when presented\nwith novel problems outside standard benchmarks. Going beyond final accuracy,\nwe propose a deductive consistency metric to analyze chain-of-thought output\nfrom language models (LMs).Formally, deductive reasoning involves two subtasks:\nunderstanding a set of input premises and inferring the conclusions that follow\nfrom them. The proposed metric studies LMs' performance on these subtasks, with\nthe goal of explaining LMs' reasoning errors on novel problems: how well do LMs\nunderstand input premises with increasing context lengths, and how well can\nthey infer conclusions over multiple reasoning hops? Since existing benchmarks\nmay be memorized, we develop a pipeline to evaluate LMs' deductive consistency\non novel, perturbed versions of benchmark problems. On novel grade school math\nproblems (GSM-8k), we find that LMs are fairly robust to increasing number of\ninput premises, but suffer significant accuracy decay as the number of\nreasoning hops is increased. Interestingly, these errors are masked in the\noriginal benchmark as all models achieve near 100% accuracy. As we increase the\nnumber of solution steps using a synthetic dataset, prediction over multiple\nhops still remains the major source of error compared to understanding input\npremises. Other factors, such as shifts in language style or natural\npropagation of early errors do not explain the trends. Our analysis provides a\nnew view to characterize LM reasoning -- as computations over a window of input\npremises and reasoning hops -- that can provide unified evaluation across\nproblem domains."
                },
                "authors": [
                    {
                        "name": "Atharva Pandey"
                    },
                    {
                        "name": "Kshitij Dubey"
                    },
                    {
                        "name": "Rahul Sharma"
                    },
                    {
                        "name": "Amit Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Amit Sharma"
                },
                "author": "Amit Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07070v1",
                "updated": "2025-04-09T17:39:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    39,
                    58,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:39:58Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    39,
                    58,
                    2,
                    99,
                    0
                ],
                "title": "A Survey on Personalized and Pluralistic Preference Alignment in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Personalized and Pluralistic Preference Alignment in Large\n  Language Models"
                },
                "summary": "Personalized preference alignment for large language models (LLMs), the\nprocess of tailoring LLMs to individual users' preferences, is an emerging\nresearch direction spanning the area of NLP and personalization. In this\nsurvey, we present an analysis of works on personalized alignment and modeling\nfor LLMs. We introduce a taxonomy of preference alignment techniques, including\ntraining time, inference time, and additionally, user-modeling based methods.\nWe provide analysis and discussion on the strengths and limitations of each\ngroup of techniques and then cover evaluation, benchmarks, as well as open\nproblems in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized preference alignment for large language models (LLMs), the\nprocess of tailoring LLMs to individual users' preferences, is an emerging\nresearch direction spanning the area of NLP and personalization. In this\nsurvey, we present an analysis of works on personalized alignment and modeling\nfor LLMs. We introduce a taxonomy of preference alignment techniques, including\ntraining time, inference time, and additionally, user-modeling based methods.\nWe provide analysis and discussion on the strengths and limitations of each\ngroup of techniques and then cover evaluation, benchmarks, as well as open\nproblems in the field."
                },
                "authors": [
                    {
                        "name": "Zhouhang Xie"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yiran Shen"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Xintong Li"
                    },
                    {
                        "name": "Aaron Chang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Sachin Kumar"
                    },
                    {
                        "name": "Bodhisattwa Prasad Majumder"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Prithviraj Ammanabrolu"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07069v1",
                "updated": "2025-04-09T17:39:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    39,
                    41,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:39:41Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    39,
                    41,
                    2,
                    99,
                    0
                ],
                "title": "HalluciNot: Hallucination Detection Through Context and Common Knowledge\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HalluciNot: Hallucination Detection Through Context and Common Knowledge\n  Verification"
                },
                "summary": "This paper introduces a comprehensive system for detecting hallucinations in\nlarge language model (LLM) outputs in enterprise settings. We present a novel\ntaxonomy of LLM responses specific to hallucination in enterprise applications,\ncategorizing them into context-based, common knowledge, enterprise-specific,\nand innocuous statements. Our hallucination detection model HDM-2 validates LLM\nresponses with respect to both context and generally known facts (common\nknowledge). It provides both hallucination scores and word-level annotations,\nenabling precise identification of problematic content. To evaluate it on\ncontext-based and common-knowledge hallucinations, we introduce a new dataset\nHDMBench. Experimental results demonstrate that HDM-2 out-performs existing\napproaches across RagTruth, TruthfulQA, and HDMBench datasets. This work\naddresses the specific challenges of enterprise deployment, including\ncomputational efficiency, domain specialization, and fine-grained error\nidentification. Our evaluation dataset, model weights, and inference code are\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a comprehensive system for detecting hallucinations in\nlarge language model (LLM) outputs in enterprise settings. We present a novel\ntaxonomy of LLM responses specific to hallucination in enterprise applications,\ncategorizing them into context-based, common knowledge, enterprise-specific,\nand innocuous statements. Our hallucination detection model HDM-2 validates LLM\nresponses with respect to both context and generally known facts (common\nknowledge). It provides both hallucination scores and word-level annotations,\nenabling precise identification of problematic content. To evaluate it on\ncontext-based and common-knowledge hallucinations, we introduce a new dataset\nHDMBench. Experimental results demonstrate that HDM-2 out-performs existing\napproaches across RagTruth, TruthfulQA, and HDMBench datasets. This work\naddresses the specific challenges of enterprise deployment, including\ncomputational efficiency, domain specialization, and fine-grained error\nidentification. Our evaluation dataset, model weights, and inference code are\npublicly available."
                },
                "authors": [
                    {
                        "name": "Bibek Paudel"
                    },
                    {
                        "name": "Alexander Lyzhov"
                    },
                    {
                        "name": "Preetam Joshi"
                    },
                    {
                        "name": "Puneet Anand"
                    }
                ],
                "author_detail": {
                    "name": "Puneet Anand"
                },
                "author": "Puneet Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02511v2",
                "updated": "2025-04-09T17:34:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    34,
                    52,
                    2,
                    99,
                    0
                ],
                "published": "2024-06-20T01:24:30Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    1,
                    24,
                    30,
                    3,
                    172,
                    0
                ],
                "title": "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on\n  Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on\n  Path Planning"
                },
                "summary": "Path planning is a fundamental scientific problem in robotics and autonomous\nnavigation, requiring the derivation of efficient routes from starting to\ndestination points while avoiding obstacles. Traditional algorithms like A* and\nits variants are capable of ensuring path validity but suffer from significant\ncomputational and memory inefficiencies as the state space grows. Conversely,\nlarge language models (LLMs) excel in broader environmental analysis through\ncontextual understanding, providing global insights into environments. However,\nthey fall short in detailed spatial and temporal reasoning, often leading to\ninvalid or inefficient routes. In this work, we propose LLM-A*, an new LLM\nbased route planning method that synergistically combines the precise\npathfinding capabilities of A* with the global reasoning capability of LLMs.\nThis hybrid approach aims to enhance pathfinding efficiency in terms of time\nand space complexity while maintaining the integrity of path validity,\nespecially in large-scale scenarios. By integrating the strengths of both\nmethodologies, LLM-A* addresses the computational and memory limitations of\nconventional algorithms without compromising on the validity required for\neffective pathfinding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path planning is a fundamental scientific problem in robotics and autonomous\nnavigation, requiring the derivation of efficient routes from starting to\ndestination points while avoiding obstacles. Traditional algorithms like A* and\nits variants are capable of ensuring path validity but suffer from significant\ncomputational and memory inefficiencies as the state space grows. Conversely,\nlarge language models (LLMs) excel in broader environmental analysis through\ncontextual understanding, providing global insights into environments. However,\nthey fall short in detailed spatial and temporal reasoning, often leading to\ninvalid or inefficient routes. In this work, we propose LLM-A*, an new LLM\nbased route planning method that synergistically combines the precise\npathfinding capabilities of A* with the global reasoning capability of LLMs.\nThis hybrid approach aims to enhance pathfinding efficiency in terms of time\nand space complexity while maintaining the integrity of path validity,\nespecially in large-scale scenarios. By integrating the strengths of both\nmethodologies, LLM-A* addresses the computational and memory limitations of\nconventional algorithms without compromising on the validity required for\neffective pathfinding."
                },
                "authors": [
                    {
                        "name": "Silin Meng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Cheng-Fu Yang"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "arxiv_comment": "Findings of the Association for Computational Linguistics: EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02361v3",
                "updated": "2025-04-09T17:26:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    26,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2024-02-04T06:11:12Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    6,
                    11,
                    12,
                    6,
                    35,
                    0
                ],
                "title": "Pruner: A Draft-then-Verify Exploration Mechanism to Accelerate Tensor\n  Program Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruner: A Draft-then-Verify Exploration Mechanism to Accelerate Tensor\n  Program Tuning"
                },
                "summary": "Tensor program tuning is essential for the efficient deployment of deep\nneural networks. Search-based approaches have demonstrated scalability and\neffectiveness in automatically finding high-performance programs for specific\nhardware. However, the search process is often inefficient, taking hours or\neven days to discover optimal programs due to the exploration mechanisms guided\nby an accurate but slow-learned cost model. Meanwhile, the learned cost model\ntrained on one platform cannot seamlessly adapt online to another, which we\ncall cross-platform online unawareness.\n  In this work, we propose Pruner and MoA-Pruner. Pruner is a\n\"Draft-then-Verify\" exploration mechanism that accelerates the schedule search\nprocess. Instead of applying the complex learned cost model to all explored\ncandidates, Pruner drafts small-scale potential candidates by introducing a\nnaive Symbol-based Analyzer (draft model), then identifies the best candidates\nby the learned cost model. MoA-Pruner introduces a Momentum online Adaptation\nstrategy to address the cross-platform online unawareness.\n  We incorporate Pruner into the TVM and conduct extensive experiments on three\nGPU-based platforms. Results show considerable speedup in schedule search time.\nIn online tuning scenarios, Pruner and MoA-Pruner achieve an average speedup of\n$2.6 \\times$ and $4.82 \\times$ compared to Ansor. In offline tuning scenarios,\nPruner achieves an average speedup of $4.75 \\times$ and $4.05\\times$ compared\nto TenSet and TLP, respectively. Furthermore, Pruner achieves an average\nspeedup of $4.08 \\times$ compared to MetaSchedule on TensorCore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor program tuning is essential for the efficient deployment of deep\nneural networks. Search-based approaches have demonstrated scalability and\neffectiveness in automatically finding high-performance programs for specific\nhardware. However, the search process is often inefficient, taking hours or\neven days to discover optimal programs due to the exploration mechanisms guided\nby an accurate but slow-learned cost model. Meanwhile, the learned cost model\ntrained on one platform cannot seamlessly adapt online to another, which we\ncall cross-platform online unawareness.\n  In this work, we propose Pruner and MoA-Pruner. Pruner is a\n\"Draft-then-Verify\" exploration mechanism that accelerates the schedule search\nprocess. Instead of applying the complex learned cost model to all explored\ncandidates, Pruner drafts small-scale potential candidates by introducing a\nnaive Symbol-based Analyzer (draft model), then identifies the best candidates\nby the learned cost model. MoA-Pruner introduces a Momentum online Adaptation\nstrategy to address the cross-platform online unawareness.\n  We incorporate Pruner into the TVM and conduct extensive experiments on three\nGPU-based platforms. Results show considerable speedup in schedule search time.\nIn online tuning scenarios, Pruner and MoA-Pruner achieve an average speedup of\n$2.6 \\times$ and $4.82 \\times$ compared to Ansor. In offline tuning scenarios,\nPruner achieves an average speedup of $4.75 \\times$ and $4.05\\times$ compared\nto TenSet and TLP, respectively. Furthermore, Pruner achieves an average\nspeedup of $4.08 \\times$ compared to MetaSchedule on TensorCore."
                },
                "authors": [
                    {
                        "name": "Liang Qiao"
                    },
                    {
                        "name": "Jun Shi"
                    },
                    {
                        "name": "Xiaoyu Hao"
                    },
                    {
                        "name": "Xi Fang"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Minfan Zhao"
                    },
                    {
                        "name": "Ziqi Zhu"
                    },
                    {
                        "name": "Junshi Chen"
                    },
                    {
                        "name": "Hong An"
                    },
                    {
                        "name": "Xulong Tang"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Honghui Yuan"
                    },
                    {
                        "name": "Xinyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinyang Wang"
                },
                "author": "Xinyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04277v2",
                "updated": "2025-04-09T17:15:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    15,
                    47,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-05T20:35:54Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    20,
                    35,
                    54,
                    5,
                    95,
                    0
                ],
                "title": "Beyond the Hype: Embeddings vs. Prompting for Multiclass Classification\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Hype: Embeddings vs. Prompting for Multiclass Classification\n  Tasks"
                },
                "summary": "Are traditional classification approaches irrelevant in this era of AI hype?\nWe show that there are multiclass classification problems where predictive\nmodels holistically outperform LLM prompt-based frameworks. Given text and\nimages from home-service project descriptions provided by Thumbtack customers,\nwe build embeddings-based softmax models that predict the professional category\n(e.g., handyman, bathroom remodeling) associated with each problem description.\nWe then compare against prompts that ask state-of-the-art LLM models to solve\nthe same problem. We find that the embeddings approach outperforms the best LLM\nprompts in terms of accuracy, calibration, latency, and financial cost. In\nparticular, the embeddings approach has 49.5% higher accuracy than the\nprompting approach, and its superiority is consistent across text-only,\nimage-only, and text-image problem descriptions. Furthermore, it yields\nwell-calibrated probabilities, which we later use as confidence signals to\nprovide contextualized user experience during deployment. On the contrary,\nprompting scores are overly uninformative. Finally, the embeddings approach is\n14 and 81 times faster than prompting in processing images and text\nrespectively, while under realistic deployment assumptions, it can be up to 10\ntimes cheaper. Based on these results, we deployed a variation of the\nembeddings approach, and through A/B testing we observed performance consistent\nwith our offline analysis. Our study shows that for multiclass classification\nproblems that can leverage proprietary datasets, an embeddings-based approach\nmay yield unequivocally better results. Hence, scientists, practitioners,\nengineers, and business leaders can use our study to go beyond the hype and\nconsider appropriate predictive models for their classification use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are traditional classification approaches irrelevant in this era of AI hype?\nWe show that there are multiclass classification problems where predictive\nmodels holistically outperform LLM prompt-based frameworks. Given text and\nimages from home-service project descriptions provided by Thumbtack customers,\nwe build embeddings-based softmax models that predict the professional category\n(e.g., handyman, bathroom remodeling) associated with each problem description.\nWe then compare against prompts that ask state-of-the-art LLM models to solve\nthe same problem. We find that the embeddings approach outperforms the best LLM\nprompts in terms of accuracy, calibration, latency, and financial cost. In\nparticular, the embeddings approach has 49.5% higher accuracy than the\nprompting approach, and its superiority is consistent across text-only,\nimage-only, and text-image problem descriptions. Furthermore, it yields\nwell-calibrated probabilities, which we later use as confidence signals to\nprovide contextualized user experience during deployment. On the contrary,\nprompting scores are overly uninformative. Finally, the embeddings approach is\n14 and 81 times faster than prompting in processing images and text\nrespectively, while under realistic deployment assumptions, it can be up to 10\ntimes cheaper. Based on these results, we deployed a variation of the\nembeddings approach, and through A/B testing we observed performance consistent\nwith our offline analysis. Our study shows that for multiclass classification\nproblems that can leverage proprietary datasets, an embeddings-based approach\nmay yield unequivocally better results. Hence, scientists, practitioners,\nengineers, and business leaders can use our study to go beyond the hype and\nconsider appropriate predictive models for their classification use cases."
                },
                "authors": [
                    {
                        "name": "Marios Kokkodis"
                    },
                    {
                        "name": "Richard Demsyn-Jones"
                    },
                    {
                        "name": "Vijay Raghavan"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Raghavan"
                },
                "author": "Vijay Raghavan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07053v1",
                "updated": "2025-04-09T17:14:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    14,
                    33,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:14:33Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    14,
                    33,
                    2,
                    99,
                    0
                ],
                "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken\n  Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken\n  Language Modeling"
                },
                "summary": "Large Language Models (LLMs) excel in text-based natural language processing\ntasks but remain constrained by their reliance on textual inputs and outputs.\nTo enable more natural human-LLM interaction, recent progress have focused on\nderiving a spoken language model (SLM) that can not only listen but also\ngenerate speech. To achieve this, a promising direction is to conduct\nspeech-text joint modeling. However, recent SLM still lag behind text LLM due\nto the modality mismatch. One significant mismatch can be the sequence lengths\nbetween speech and text tokens. To address this, we introduce Text-Aligned\nSpeech Tokenization and Embedding (TASTE), a method that directly addresses the\nmodality gap by aligning speech token with the corresponding text transcription\nduring the tokenization stage. We propose a method that can achieve this\nthrough the special aggregation mechanism and with speech reconstruction as the\ntraining objective. We conduct extensive experiments and show that TASTE can\npreserve essential paralinguistic information while dramatically reducing the\ntoken sequence length. Furthermore, by leveraging TASTE, we can adapt\ntext-based LLMs into effective SLMs with parameter-efficient fine-tuning\ntechniques such as Low-Rank Adaptation (LoRA). Experimental results on\nbenchmark tasks, including SALMON and StoryCloze, demonstrate that TASTE-based\nSLMs perform similarly to previous full-finetuning methods. To our knowledge,\nTASTE is the first end-to-end approach that utilizes a reconstruction objective\nto automatically learn a text-aligned speech tokenization and embedding\nsuitable for spoken language modeling. Our demo, code, and models are publicly\navailable at https://github.com/mtkresearch/TASTE-SpokenLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in text-based natural language processing\ntasks but remain constrained by their reliance on textual inputs and outputs.\nTo enable more natural human-LLM interaction, recent progress have focused on\nderiving a spoken language model (SLM) that can not only listen but also\ngenerate speech. To achieve this, a promising direction is to conduct\nspeech-text joint modeling. However, recent SLM still lag behind text LLM due\nto the modality mismatch. One significant mismatch can be the sequence lengths\nbetween speech and text tokens. To address this, we introduce Text-Aligned\nSpeech Tokenization and Embedding (TASTE), a method that directly addresses the\nmodality gap by aligning speech token with the corresponding text transcription\nduring the tokenization stage. We propose a method that can achieve this\nthrough the special aggregation mechanism and with speech reconstruction as the\ntraining objective. We conduct extensive experiments and show that TASTE can\npreserve essential paralinguistic information while dramatically reducing the\ntoken sequence length. Furthermore, by leveraging TASTE, we can adapt\ntext-based LLMs into effective SLMs with parameter-efficient fine-tuning\ntechniques such as Low-Rank Adaptation (LoRA). Experimental results on\nbenchmark tasks, including SALMON and StoryCloze, demonstrate that TASTE-based\nSLMs perform similarly to previous full-finetuning methods. To our knowledge,\nTASTE is the first end-to-end approach that utilizes a reconstruction objective\nto automatically learn a text-aligned speech tokenization and embedding\nsuitable for spoken language modeling. Our demo, code, and models are publicly\navailable at https://github.com/mtkresearch/TASTE-SpokenLM."
                },
                "authors": [
                    {
                        "name": "Liang-Hsuan Tseng"
                    },
                    {
                        "name": "Yi-Chang Chen"
                    },
                    {
                        "name": "Kuan-Yi Lee"
                    },
                    {
                        "name": "Da-Shan Shiu"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Preprint. Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.20331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.20331v2",
                "updated": "2025-04-09T17:13:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    13,
                    27,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-29T17:59:53Z",
                "published_parsed": [
                    2024,
                    3,
                    29,
                    17,
                    59,
                    53,
                    4,
                    89,
                    0
                ],
                "title": "Unsolvable Problem Detection: Robust Understanding Evaluation for Large\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsolvable Problem Detection: Robust Understanding Evaluation for Large\n  Multimodal Models"
                },
                "summary": "This paper introduces a novel task to evaluate the robust understanding\ncapability of Large Multimodal Models (LMMs), termed $\\textbf{Unsolvable\nProblem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely\nused to assess the understanding capability of LMMs, but it does not guarantee\nthat LMMs truly comprehend the answer. UPD assesses the LMM's ability to\nwithhold answers when encountering unsolvable problems of MCQA, verifying\nwhether the model truly understands the answer. UPD encompasses three problems:\nAbsent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and\nIncompatible Visual Question Detection (IVQD), covering unsolvable cases like\nanswer-lacking or incompatible choices and image-question mismatches. For the\nevaluation, we introduce the MM-UPD Bench, a benchmark for assessing\nperformance across various ability dimensions. Our experiments reveal that even\nmost LMMs, which demonstrate adequate performance on existing benchmarks,\nstruggle significantly with MM-UPD, underscoring a novel aspect of\ntrustworthiness that current benchmarks have overlooked. A detailed analysis\nshows that LMMs have different bottlenecks and chain-of-thought and\nself-reflection improved performance for LMMs with the bottleneck in their LLM\ncapability. We hope our insights will enhance the broader understanding and\ndevelopment of more reliable LMMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel task to evaluate the robust understanding\ncapability of Large Multimodal Models (LMMs), termed $\\textbf{Unsolvable\nProblem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely\nused to assess the understanding capability of LMMs, but it does not guarantee\nthat LMMs truly comprehend the answer. UPD assesses the LMM's ability to\nwithhold answers when encountering unsolvable problems of MCQA, verifying\nwhether the model truly understands the answer. UPD encompasses three problems:\nAbsent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and\nIncompatible Visual Question Detection (IVQD), covering unsolvable cases like\nanswer-lacking or incompatible choices and image-question mismatches. For the\nevaluation, we introduce the MM-UPD Bench, a benchmark for assessing\nperformance across various ability dimensions. Our experiments reveal that even\nmost LMMs, which demonstrate adequate performance on existing benchmarks,\nstruggle significantly with MM-UPD, underscoring a novel aspect of\ntrustworthiness that current benchmarks have overlooked. A detailed analysis\nshows that LMMs have different bottlenecks and chain-of-thought and\nself-reflection improved performance for LMMs with the bottleneck in their LLM\ncapability. We hope our insights will enhance the broader understanding and\ndevelopment of more reliable LMMs."
                },
                "authors": [
                    {
                        "name": "Atsuyuki Miyai"
                    },
                    {
                        "name": "Jingkang Yang"
                    },
                    {
                        "name": "Jingyang Zhang"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Qing Yu"
                    },
                    {
                        "name": "Go Irie"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Hai Li"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kiyoharu Aizawa"
                    }
                ],
                "author_detail": {
                    "name": "Kiyoharu Aizawa"
                },
                "author": "Kiyoharu Aizawa",
                "arxiv_comment": "Code: https://github.com/AtsuMiyai/UPD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.20331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.20331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07052v1",
                "updated": "2025-04-09T17:12:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    12,
                    49,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T17:12:49Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    12,
                    49,
                    2,
                    99,
                    0
                ],
                "title": "To Backtrack or Not to Backtrack: When Sequential Search Limits Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Backtrack or Not to Backtrack: When Sequential Search Limits Model\n  Reasoning"
                },
                "summary": "Recent advancements in large language models have significantly improved\ntheir reasoning abilities, particularly through techniques involving search and\nbacktracking. Backtracking naturally scales test-time compute by enabling\nsequential, linearized exploration via long chain-of-thought (CoT) generation.\nHowever, this is not the only strategy for scaling test-time compute: parallel\nsampling with best-of-n selection provides an alternative that generates\ndiverse solutions simultaneously. Despite the growing adoption of sequential\nsearch, its advantages over parallel sampling--especially under a fixed compute\nbudget remain poorly understood. In this paper, we systematically compare these\ntwo approaches on two challenging reasoning tasks: CountDown and Sudoku.\nSurprisingly, we find that sequential search underperforms parallel sampling on\nCountDown but outperforms it on Sudoku, suggesting that backtracking is not\nuniversally beneficial. We identify two factors that can cause backtracking to\ndegrade performance: (1) training on fixed search traces can lock models into\nsuboptimal strategies, and (2) explicit CoT supervision can discourage\n\"implicit\" (non-verbalized) reasoning. Extending our analysis to reinforcement\nlearning (RL), we show that models with backtracking capabilities benefit\nsignificantly from RL fine-tuning, while models without backtracking see\nlimited, mixed gains. Together, these findings challenge the assumption that\nbacktracking universally enhances LLM reasoning, instead revealing a complex\ninteraction between task structure, training data, model scale, and learning\nparadigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models have significantly improved\ntheir reasoning abilities, particularly through techniques involving search and\nbacktracking. Backtracking naturally scales test-time compute by enabling\nsequential, linearized exploration via long chain-of-thought (CoT) generation.\nHowever, this is not the only strategy for scaling test-time compute: parallel\nsampling with best-of-n selection provides an alternative that generates\ndiverse solutions simultaneously. Despite the growing adoption of sequential\nsearch, its advantages over parallel sampling--especially under a fixed compute\nbudget remain poorly understood. In this paper, we systematically compare these\ntwo approaches on two challenging reasoning tasks: CountDown and Sudoku.\nSurprisingly, we find that sequential search underperforms parallel sampling on\nCountDown but outperforms it on Sudoku, suggesting that backtracking is not\nuniversally beneficial. We identify two factors that can cause backtracking to\ndegrade performance: (1) training on fixed search traces can lock models into\nsuboptimal strategies, and (2) explicit CoT supervision can discourage\n\"implicit\" (non-verbalized) reasoning. Extending our analysis to reinforcement\nlearning (RL), we show that models with backtracking capabilities benefit\nsignificantly from RL fine-tuning, while models without backtracking see\nlimited, mixed gains. Together, these findings challenge the assumption that\nbacktracking universally enhances LLM reasoning, instead revealing a complex\ninteraction between task structure, training data, model scale, and learning\nparadigm."
                },
                "authors": [
                    {
                        "name": "Tian Qin"
                    },
                    {
                        "name": "David Alvarez-Melis"
                    },
                    {
                        "name": "Samy Jelassi"
                    },
                    {
                        "name": "Eran Malach"
                    }
                ],
                "author_detail": {
                    "name": "Eran Malach"
                },
                "author": "Eran Malach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07029v1",
                "updated": "2025-04-09T16:44:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    44,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T16:44:19Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    44,
                    19,
                    2,
                    99,
                    0
                ],
                "title": "Distilling Textual Priors from LLM to Efficient Image Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Textual Priors from LLM to Efficient Image Fusion"
                },
                "summary": "Multi-modality image fusion aims to synthesize a single, comprehensive image\nfrom multiple source inputs. Traditional approaches, such as CNNs and GANs,\noffer efficiency but struggle to handle low-quality or complex inputs. Recent\nadvances in text-guided methods leverage large model priors to overcome these\nlimitations, but at the cost of significant computational overhead, both in\nmemory and inference time. To address this challenge, we propose a novel\nframework for distilling large model priors, eliminating the need for text\nguidance during inference while dramatically reducing model size. Our framework\nutilizes a teacher-student architecture, where the teacher network incorporates\nlarge model priors and transfers this knowledge to a smaller student network\nvia a tailored distillation process. Additionally, we introduce spatial-channel\ncross-fusion module to enhance the model's ability to leverage textual priors\nacross both spatial and channel dimensions. Our method achieves a favorable\ntrade-off between computational efficiency and fusion quality. The distilled\nnetwork, requiring only 10\\% of the parameters and inference time of the\nteacher network, retains 90\\% of its performance and outperforms existing SOTA\nmethods. Extensive experiments demonstrate the effectiveness of our approach.\nThe implementation will be made publicly available as an open-source resource.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modality image fusion aims to synthesize a single, comprehensive image\nfrom multiple source inputs. Traditional approaches, such as CNNs and GANs,\noffer efficiency but struggle to handle low-quality or complex inputs. Recent\nadvances in text-guided methods leverage large model priors to overcome these\nlimitations, but at the cost of significant computational overhead, both in\nmemory and inference time. To address this challenge, we propose a novel\nframework for distilling large model priors, eliminating the need for text\nguidance during inference while dramatically reducing model size. Our framework\nutilizes a teacher-student architecture, where the teacher network incorporates\nlarge model priors and transfers this knowledge to a smaller student network\nvia a tailored distillation process. Additionally, we introduce spatial-channel\ncross-fusion module to enhance the model's ability to leverage textual priors\nacross both spatial and channel dimensions. Our method achieves a favorable\ntrade-off between computational efficiency and fusion quality. The distilled\nnetwork, requiring only 10\\% of the parameters and inference time of the\nteacher network, retains 90\\% of its performance and outperforms existing SOTA\nmethods. Extensive experiments demonstrate the effectiveness of our approach.\nThe implementation will be made publicly available as an open-source resource."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Xuanhua He"
                    },
                    {
                        "name": "Ke Cao"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Man Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18389v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18389v2",
                "updated": "2025-04-09T16:40:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    40,
                    21,
                    2,
                    99,
                    0
                ],
                "published": "2025-02-25T17:33:20Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    33,
                    20,
                    1,
                    56,
                    0
                ],
                "title": "Monte Carlo Temperature: a robust sampling strategy for LLM's\n  uncertainty quantification methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Temperature: a robust sampling strategy for LLM's\n  uncertainty quantification methods"
                },
                "summary": "Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential\nfor their safe and reliable deployment, particularly in critical applications\nwhere incorrect outputs can have serious consequences. Current UQ methods\ntypically rely on querying the model multiple times using non-zero temperature\nsampling to generate diverse outputs for uncertainty estimation. However, the\nimpact of selecting a given temperature parameter is understudied, and our\nanalysis reveals that temperature plays a fundamental role in the quality of\nuncertainty estimates. The conventional approach of identifying optimal\ntemperature values requires expensive hyperparameter optimization (HPO) that\nmust be repeated for each new model-dataset combination. We propose Monte Carlo\nTemperature (MCT), a robust sampling strategy that eliminates the need for\ntemperature calibration. Our analysis reveals that: 1) MCT provides more robust\nuncertainty estimates across a wide range of temperatures, 2) MCT improves the\nperformance of UQ methods by replacing fixed-temperature strategies that do not\nrely on HPO, and 3) MCT achieves statistical parity with oracle temperatures,\nwhich represent the ideal outcome of a well-tuned but computationally expensive\nHPO process. These findings demonstrate that effective UQ can be achieved\nwithout the computational burden of temperature parameter calibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential\nfor their safe and reliable deployment, particularly in critical applications\nwhere incorrect outputs can have serious consequences. Current UQ methods\ntypically rely on querying the model multiple times using non-zero temperature\nsampling to generate diverse outputs for uncertainty estimation. However, the\nimpact of selecting a given temperature parameter is understudied, and our\nanalysis reveals that temperature plays a fundamental role in the quality of\nuncertainty estimates. The conventional approach of identifying optimal\ntemperature values requires expensive hyperparameter optimization (HPO) that\nmust be repeated for each new model-dataset combination. We propose Monte Carlo\nTemperature (MCT), a robust sampling strategy that eliminates the need for\ntemperature calibration. Our analysis reveals that: 1) MCT provides more robust\nuncertainty estimates across a wide range of temperatures, 2) MCT improves the\nperformance of UQ methods by replacing fixed-temperature strategies that do not\nrely on HPO, and 3) MCT achieves statistical parity with oracle temperatures,\nwhich represent the ideal outcome of a well-tuned but computationally expensive\nHPO process. These findings demonstrate that effective UQ can be achieved\nwithout the computational burden of temperature parameter calibration."
                },
                "authors": [
                    {
                        "name": "Nicola Cecere"
                    },
                    {
                        "name": "Andrea Bacciu"
                    },
                    {
                        "name": "Ignacio Fernández Tobías"
                    },
                    {
                        "name": "Amin Mantrach"
                    }
                ],
                "author_detail": {
                    "name": "Amin Mantrach"
                },
                "author": "Amin Mantrach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18389v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18389v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07022v1",
                "updated": "2025-04-09T16:37:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    37,
                    3,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T16:37:03Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    37,
                    3,
                    2,
                    99,
                    0
                ],
                "title": "Evaluating Retrieval Augmented Generative Models for Document Queries in\n  Transportation Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Retrieval Augmented Generative Models for Document Queries in\n  Transportation Safety"
                },
                "summary": "Applications of generative Large Language Models LLMs are rapidly expanding\nacross various domains, promising significant improvements in workflow\nefficiency and information retrieval. However, their implementation in\nspecialized, high-stakes domains such as hazardous materials transportation is\nchallenging due to accuracy and reliability concerns. This study evaluates the\nperformance of three fine-tuned generative models, ChatGPT, Google's Vertex AI,\nand ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in\nretrieving regulatory information essential for hazardous material\ntransportation compliance in the United States. Utilizing approximately 40\npublicly available federal and state regulatory documents, we developed 100\nrealistic queries relevant to route planning and permitting requirements.\nResponses were qualitatively rated based on accuracy, detail, and relevance,\ncomplemented by quantitative assessments of semantic similarity between model\noutputs. Results demonstrated that the RAG-augmented LLaMA models significantly\noutperformed Vertex AI and ChatGPT, providing more detailed and generally\naccurate information, despite occasional inconsistencies. This research\nintroduces the first known application of RAG in transportation safety,\nemphasizing the need for domain-specific fine-tuning and rigorous evaluation\nmethodologies to ensure reliability and minimize the risk of inaccuracies in\nhigh-stakes environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications of generative Large Language Models LLMs are rapidly expanding\nacross various domains, promising significant improvements in workflow\nefficiency and information retrieval. However, their implementation in\nspecialized, high-stakes domains such as hazardous materials transportation is\nchallenging due to accuracy and reliability concerns. This study evaluates the\nperformance of three fine-tuned generative models, ChatGPT, Google's Vertex AI,\nand ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in\nretrieving regulatory information essential for hazardous material\ntransportation compliance in the United States. Utilizing approximately 40\npublicly available federal and state regulatory documents, we developed 100\nrealistic queries relevant to route planning and permitting requirements.\nResponses were qualitatively rated based on accuracy, detail, and relevance,\ncomplemented by quantitative assessments of semantic similarity between model\noutputs. Results demonstrated that the RAG-augmented LLaMA models significantly\noutperformed Vertex AI and ChatGPT, providing more detailed and generally\naccurate information, despite occasional inconsistencies. This research\nintroduces the first known application of RAG in transportation safety,\nemphasizing the need for domain-specific fine-tuning and rigorous evaluation\nmethodologies to ensure reliability and minimize the risk of inaccuracies in\nhigh-stakes environments."
                },
                "authors": [
                    {
                        "name": "Chad Melton"
                    },
                    {
                        "name": "Alex Sorokine"
                    },
                    {
                        "name": "Steve Peterson"
                    }
                ],
                "author_detail": {
                    "name": "Steve Peterson"
                },
                "author": "Steve Peterson",
                "arxiv_comment": "14 pages, 3 Figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19887v2",
                "updated": "2025-04-09T16:36:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    36,
                    30,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-25T17:51:50Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    51,
                    50,
                    1,
                    84,
                    0
                ],
                "title": "Countering threats to national security posed by AI systems through an\n  incident regime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Countering threats to national security posed by AI systems through an\n  incident regime"
                },
                "summary": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a proposal for a legally mandated post-deployment AI incident regime\nthat aims to counter potential national security threats from AI systems. We\nstart the paper by introducing the concept of 'security-critical' to describe\ndoctors that pose extreme risks to national security, before arguing that\n'security-critical' describes civilian nuclear power, aviation, life science\ndual-use research of concern, and frontier AI development. We then present in\ndetail our AI incident regime proposal, justifying each component of the\nproposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a proposal for a legally mandated post-deployment AI incident regime\nthat aims to counter potential national security threats from AI systems. We\nstart the paper by introducing the concept of 'security-critical' to describe\ndoctors that pose extreme risks to national security, before arguing that\n'security-critical' describes civilian nuclear power, aviation, life science\ndual-use research of concern, and frontier AI development. We then present in\ndetail our AI incident regime proposal, justifying each component of the\nproposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security."
                },
                "authors": [
                    {
                        "name": "Alejandro Ortega"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Ortega"
                },
                "author": "Alejandro Ortega",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07015v1",
                "updated": "2025-04-09T16:32:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    32,
                    13,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T16:32:13Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    32,
                    13,
                    2,
                    99,
                    0
                ],
                "title": "LLM-IFT: LLM-Powered Information Flow Tracking for Secure Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-IFT: LLM-Powered Information Flow Tracking for Secure Hardware"
                },
                "summary": "As modern hardware designs grow in complexity and size, ensuring security\nacross the confidentiality, integrity, and availability (CIA) triad becomes\nincreasingly challenging. Information flow tracking (IFT) is a widely-used\napproach to tracing data propagation, identifying unauthorized activities that\nmay compromise confidentiality or/and integrity in hardware. However,\ntraditional IFT methods struggle with scalability and adaptability,\nparticularly in high-density and interconnected architectures, leading to\ntracing bottlenecks that limit applicability in large-scale hardware. To\naddress these limitations and show the potential of transformer-based models in\nintegrated circuit (IC) design, this paper introduces LLM-IFT that integrates\nlarge language models (LLM) for the realization of the IFT process in hardware.\nLLM-IFT exploits LLM-driven structured reasoning to perform hierarchical\ndependency analysis, systematically breaking down even the most complex\ndesigns. Through a multi-step LLM invocation, the framework analyzes both\nintra-module and inter-module dependencies, enabling comprehensive IFT\nassessment. By focusing on a set of Trust-Hub vulnerability test cases at both\nthe IP level and the SoC level, our experiments demonstrate a 100\\% success\nrate in accurate IFT analysis for confidentiality and integrity checks in\nhardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As modern hardware designs grow in complexity and size, ensuring security\nacross the confidentiality, integrity, and availability (CIA) triad becomes\nincreasingly challenging. Information flow tracking (IFT) is a widely-used\napproach to tracing data propagation, identifying unauthorized activities that\nmay compromise confidentiality or/and integrity in hardware. However,\ntraditional IFT methods struggle with scalability and adaptability,\nparticularly in high-density and interconnected architectures, leading to\ntracing bottlenecks that limit applicability in large-scale hardware. To\naddress these limitations and show the potential of transformer-based models in\nintegrated circuit (IC) design, this paper introduces LLM-IFT that integrates\nlarge language models (LLM) for the realization of the IFT process in hardware.\nLLM-IFT exploits LLM-driven structured reasoning to perform hierarchical\ndependency analysis, systematically breaking down even the most complex\ndesigns. Through a multi-step LLM invocation, the framework analyzes both\nintra-module and inter-module dependencies, enabling comprehensive IFT\nassessment. By focusing on a set of Trust-Hub vulnerability test cases at both\nthe IP level and the SoC level, our experiments demonstrate a 100\\% success\nrate in accurate IFT analysis for confidentiality and integrity checks in\nhardware."
                },
                "authors": [
                    {
                        "name": "Nowfel Mashnoor"
                    },
                    {
                        "name": "Mohammad Akyash"
                    },
                    {
                        "name": "Hadi Kamali"
                    },
                    {
                        "name": "Kimia Azar"
                    }
                ],
                "author_detail": {
                    "name": "Kimia Azar"
                },
                "author": "Kimia Azar",
                "arxiv_comment": "This paper is presented at IEEE VLSI Test Symposium (VTS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03916v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03916v3",
                "updated": "2025-04-09T16:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    27,
                    2,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-07T16:31:10Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    16,
                    31,
                    10,
                    1,
                    7,
                    0
                ],
                "title": "Dolphin: Moving Towards Closed-loop Auto-research through Thinking,\n  Practice, and Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dolphin: Moving Towards Closed-loop Auto-research through Thinking,\n  Practice, and Feedback"
                },
                "summary": "The scientific research paradigm is undergoing a profound transformation\nowing to the development of Artificial Intelligence (AI). Recent works\ndemonstrate that various AI-assisted research methods can largely improve\nresearch efficiency by improving data analysis, accelerating computation, and\nfostering novel idea generation. To further move towards the ultimate goal\n(i.e., automatic scientific research), in this paper, we introduce Dolphin, a\nclosed-loop LLM-driven framework to enhance the automation level of scientific\nresearch. Dolphin first generates novel ideas based on feedback from previous\nexperiments and relevant papers ranked by the topic and task attributes. Then,\nthe generated ideas can be implemented using a code template refined and\ndebugged with the designed exception-traceback-guided local code structure.\nFinally, Dolphin automatically analyzes the results of each idea and feeds the\nresults back to the next round of idea generation. Experiments are conducted on\nthe benchmark datasets of different topics and a subset of MLE-bench. Results\nshow that Dolphin can continuously improve the performance of the input topic\nin a loop. We highlight that Dolphin can automatically propose methods that are\ncomparable to the state-of-the-art in some tasks such as 3D point\nclassification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scientific research paradigm is undergoing a profound transformation\nowing to the development of Artificial Intelligence (AI). Recent works\ndemonstrate that various AI-assisted research methods can largely improve\nresearch efficiency by improving data analysis, accelerating computation, and\nfostering novel idea generation. To further move towards the ultimate goal\n(i.e., automatic scientific research), in this paper, we introduce Dolphin, a\nclosed-loop LLM-driven framework to enhance the automation level of scientific\nresearch. Dolphin first generates novel ideas based on feedback from previous\nexperiments and relevant papers ranked by the topic and task attributes. Then,\nthe generated ideas can be implemented using a code template refined and\ndebugged with the designed exception-traceback-guided local code structure.\nFinally, Dolphin automatically analyzes the results of each idea and feeds the\nresults back to the next round of idea generation. Experiments are conducted on\nthe benchmark datasets of different topics and a subset of MLE-bench. Results\nshow that Dolphin can continuously improve the performance of the input topic\nin a loop. We highlight that Dolphin can automatically propose methods that are\ncomparable to the state-of-the-art in some tasks such as 3D point\nclassification."
                },
                "authors": [
                    {
                        "name": "Jiakang Yuan"
                    },
                    {
                        "name": "Xiangchao Yan"
                    },
                    {
                        "name": "Shiyang Feng"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "21 pages, 12 figures, and our homepage:\n  https://alpha-innovator.github.io/Dolphin-project-page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03916v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03916v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11283v2",
                "updated": "2025-04-09T16:09:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    16,
                    9,
                    27,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-15T05:05:56Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    5,
                    56,
                    1,
                    289,
                    0
                ],
                "title": "AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor\n  Generator Against LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor\n  Generator Against LLM Alignment"
                },
                "summary": "With the growing adoption of reinforcement learning with human feedback\n(RLHF) for aligning large language models (LLMs), the risk of backdoor\ninstallation during alignment has increased, leading to unintended and harmful\nbehaviors. Existing backdoor triggers are typically limited to fixed word\npatterns, making them detectable during data cleaning and easily removable\npost-poisoning. In this work, we explore the use of prompt-specific paraphrases\nas backdoor triggers, enhancing their stealth and resistance to removal during\nLLM alignment. We propose AdvBDGen, an adversarially fortified generative\nfine-tuning framework that automatically generates prompt-specific backdoors\nthat are effective, stealthy, and transferable across models. AdvBDGen employs\na generator-discriminator pair, fortified by an adversary, to ensure the\ninstallability and stealthiness of backdoors. It enables the crafting and\nsuccessful installation of complex triggers using as little as 3% of the\nfine-tuning data. Once installed, these backdoors can jailbreak LLMs during\ninference, demonstrate improved stability against perturbations compared to\ntraditional constant triggers, and are more challenging to remove. These\nfindings underscore an urgent need for the research community to develop more\nrobust defenses against adversarial backdoor threats in LLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing adoption of reinforcement learning with human feedback\n(RLHF) for aligning large language models (LLMs), the risk of backdoor\ninstallation during alignment has increased, leading to unintended and harmful\nbehaviors. Existing backdoor triggers are typically limited to fixed word\npatterns, making them detectable during data cleaning and easily removable\npost-poisoning. In this work, we explore the use of prompt-specific paraphrases\nas backdoor triggers, enhancing their stealth and resistance to removal during\nLLM alignment. We propose AdvBDGen, an adversarially fortified generative\nfine-tuning framework that automatically generates prompt-specific backdoors\nthat are effective, stealthy, and transferable across models. AdvBDGen employs\na generator-discriminator pair, fortified by an adversary, to ensure the\ninstallability and stealthiness of backdoors. It enables the crafting and\nsuccessful installation of complex triggers using as little as 3% of the\nfine-tuning data. Once installed, these backdoors can jailbreak LLMs during\ninference, demonstrate improved stability against perturbations compared to\ntraditional constant triggers, and are more challenging to remove. These\nfindings underscore an urgent need for the research community to develop more\nrobust defenses against adversarial backdoor threats in LLM alignment."
                },
                "authors": [
                    {
                        "name": "Pankayaraj Pathmanathan"
                    },
                    {
                        "name": "Udari Madhushani Sehwag"
                    },
                    {
                        "name": "Michael-Andrei Panaitescu-Liess"
                    },
                    {
                        "name": "Furong Huang"
                    }
                ],
                "author_detail": {
                    "name": "Furong Huang"
                },
                "author": "Furong Huang",
                "arxiv_comment": "Published at the Neurips Safe Generative AI Workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01999v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01999v4",
                "updated": "2025-04-09T15:52:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    52,
                    59,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-02T20:04:02Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    20,
                    4,
                    2,
                    2,
                    276,
                    0
                ],
                "title": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding &\n  Reasoning Capabilities of CodeLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding &\n  Reasoning Capabilities of CodeLLMs"
                },
                "summary": "Recent advances in Code Large Language Models (CodeLLMs) have primarily\nfocused on open-ended code generation, often overlooking the crucial aspect of\ncode understanding and reasoning. To bridge this gap, we introduce CodeMMLU, a\ncomprehensive multiple-choice benchmark designed to evaluate the depth of\nsoftware and code comprehension in LLMs. CodeMMLU includes nearly 20,000\nquestions spanning diverse domains, including code analysis, defect detection,\nand software engineering principles across multiple programming languages.\nUnlike traditional benchmarks that emphasize code generation, CodeMMLU assesses\na model's ability to reason about programs across a wide-range of tasks such as\ncode repair, execution reasoning, and fill-in-the-blank challenges. Our\nextensive evaluation reveals that even state-of-the-art models struggle with\nCodeMMLU, highlighting significant gaps in comprehension beyond generation. By\nemphasizing the essential connection between code understanding and effective\nAI-assisted development, CodeMMLU provides a critical resource for advancing\nmore reliable and capable coding assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Code Large Language Models (CodeLLMs) have primarily\nfocused on open-ended code generation, often overlooking the crucial aspect of\ncode understanding and reasoning. To bridge this gap, we introduce CodeMMLU, a\ncomprehensive multiple-choice benchmark designed to evaluate the depth of\nsoftware and code comprehension in LLMs. CodeMMLU includes nearly 20,000\nquestions spanning diverse domains, including code analysis, defect detection,\nand software engineering principles across multiple programming languages.\nUnlike traditional benchmarks that emphasize code generation, CodeMMLU assesses\na model's ability to reason about programs across a wide-range of tasks such as\ncode repair, execution reasoning, and fill-in-the-blank challenges. Our\nextensive evaluation reveals that even state-of-the-art models struggle with\nCodeMMLU, highlighting significant gaps in comprehension beyond generation. By\nemphasizing the essential connection between code understanding and effective\nAI-assisted development, CodeMMLU provides a critical resource for advancing\nmore reliable and capable coding assistants."
                },
                "authors": [
                    {
                        "name": "Dung Nguyen Manh"
                    },
                    {
                        "name": "Thang Phan Chau"
                    },
                    {
                        "name": "Nam Le Hai"
                    },
                    {
                        "name": "Thong T. Doan"
                    },
                    {
                        "name": "Nam V. Nguyen"
                    },
                    {
                        "name": "Quang Pham"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01999v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01999v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06969v1",
                "updated": "2025-04-09T15:26:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    26,
                    0,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T15:26:00Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    26,
                    0,
                    2,
                    99,
                    0
                ],
                "title": "Towards LLMs Robustness to Changes in Prompt Format Styles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LLMs Robustness to Changes in Prompt Format Styles"
                },
                "summary": "Large language models (LLMs) have gained popularity in recent years for their\nutility in various applications. However, they are sensitive to non-semantic\nchanges in prompt formats, where small changes in the prompt format can lead to\nsignificant performance fluctuations. In the literature, this problem is\ncommonly referred to as prompt brittleness. Previous research on prompt\nengineering has focused mainly on developing techniques for identifying the\noptimal prompt for specific tasks. Some studies have also explored the issue of\nprompt brittleness and proposed methods to quantify performance variations;\nhowever, no simple solution has been found to address this challenge. We\npropose Mixture of Formats (MOF), a simple and efficient technique for\naddressing prompt brittleness in LLMs by diversifying the styles used in the\nprompt few-shot examples. MOF was inspired by computer vision techniques that\nutilize diverse style datasets to prevent models from associating specific\nstyles with the target variable. Empirical results show that our proposed\ntechnique reduces style-induced prompt brittleness in various LLMs while also\nenhancing overall performance across prompt variations and different datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained popularity in recent years for their\nutility in various applications. However, they are sensitive to non-semantic\nchanges in prompt formats, where small changes in the prompt format can lead to\nsignificant performance fluctuations. In the literature, this problem is\ncommonly referred to as prompt brittleness. Previous research on prompt\nengineering has focused mainly on developing techniques for identifying the\noptimal prompt for specific tasks. Some studies have also explored the issue of\nprompt brittleness and proposed methods to quantify performance variations;\nhowever, no simple solution has been found to address this challenge. We\npropose Mixture of Formats (MOF), a simple and efficient technique for\naddressing prompt brittleness in LLMs by diversifying the styles used in the\nprompt few-shot examples. MOF was inspired by computer vision techniques that\nutilize diverse style datasets to prevent models from associating specific\nstyles with the target variable. Empirical results show that our proposed\ntechnique reduces style-induced prompt brittleness in various LLMs while also\nenhancing overall performance across prompt variations and different datasets."
                },
                "authors": [
                    {
                        "name": "Lilian Ngweta"
                    },
                    {
                        "name": "Kiran Kate"
                    },
                    {
                        "name": "Jason Tsay"
                    },
                    {
                        "name": "Yara Rizk"
                    }
                ],
                "author_detail": {
                    "name": "Yara Rizk"
                },
                "author": "Yara Rizk",
                "arxiv_comment": "NAACL Student Research Workshop (SRW) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02916v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02916v3",
                "updated": "2025-04-09T15:20:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    20,
                    33,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-03T19:07:53Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    19,
                    7,
                    53,
                    3,
                    277,
                    0
                ],
                "title": "LLM Safeguard is a Double-Edged Sword: Exploiting False Positives for\n  Denial-of-Service Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Safeguard is a Double-Edged Sword: Exploiting False Positives for\n  Denial-of-Service Attacks"
                },
                "summary": "Safety is a paramount concern for large language models (LLMs) in open\ndeployment, motivating the development of safeguard methods that enforce\nethical and responsible use through safety alignment or guardrail mechanisms.\nJailbreak attacks that exploit the \\emph{false negatives} of safeguard methods\nhave emerged as a prominent research focus in the field of LLM security.\nHowever, we found that the malicious attackers could also exploit false\npositives of safeguards, i.e., fooling the safeguard model to block safe\ncontent mistakenly, leading to a denial-of-service (DoS) affecting LLM users.\nTo bridge the knowledge gap of this overlooked threat, we explore multiple\nattack methods that include inserting a short adversarial prompt into user\nprompt templates and corrupting the LLM on the server by poisoned fine-tuning.\nIn both ways, the attack triggers safeguard rejections of user requests from\nthe client. Our evaluation demonstrates the severity of this threat across\nmultiple scenarios. For instance, in the scenario of white-box adversarial\nprompt injection, the attacker can use our optimization process to\nautomatically generate seemingly safe adversarial prompts, approximately only\n30 characters long, that universally block over 97% of user requests on Llama\nGuard 3. These findings reveal a new dimension in LLM safeguard evaluation --\nadversarial robustness to false positives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety is a paramount concern for large language models (LLMs) in open\ndeployment, motivating the development of safeguard methods that enforce\nethical and responsible use through safety alignment or guardrail mechanisms.\nJailbreak attacks that exploit the \\emph{false negatives} of safeguard methods\nhave emerged as a prominent research focus in the field of LLM security.\nHowever, we found that the malicious attackers could also exploit false\npositives of safeguards, i.e., fooling the safeguard model to block safe\ncontent mistakenly, leading to a denial-of-service (DoS) affecting LLM users.\nTo bridge the knowledge gap of this overlooked threat, we explore multiple\nattack methods that include inserting a short adversarial prompt into user\nprompt templates and corrupting the LLM on the server by poisoned fine-tuning.\nIn both ways, the attack triggers safeguard rejections of user requests from\nthe client. Our evaluation demonstrates the severity of this threat across\nmultiple scenarios. For instance, in the scenario of white-box adversarial\nprompt injection, the attacker can use our optimization process to\nautomatically generate seemingly safe adversarial prompts, approximately only\n30 characters long, that universally block over 97% of user requests on Llama\nGuard 3. These findings reveal a new dimension in LLM safeguard evaluation --\nadversarial robustness to false positives."
                },
                "authors": [
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Ziyang Xiong"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02916v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02916v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07991v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07991v5",
                "updated": "2025-04-09T15:05:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    15,
                    5,
                    27,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-10T14:48:57Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    48,
                    57,
                    3,
                    284,
                    0
                ],
                "title": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic\n  Analysis of Annotators and Targets"
                },
                "summary": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems."
                },
                "authors": [
                    {
                        "name": "Tommaso Giorgi"
                    },
                    {
                        "name": "Lorenzo Cima"
                    },
                    {
                        "name": "Tiziano Fagni"
                    },
                    {
                        "name": "Marco Avvenuti"
                    },
                    {
                        "name": "Stefano Cresci"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Cresci"
                },
                "author": "Stefano Cresci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07991v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07991v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11176v2",
                "updated": "2025-04-09T14:54:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    54,
                    12,
                    2,
                    99,
                    0
                ],
                "published": "2025-02-16T15:54:53Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    15,
                    54,
                    53,
                    6,
                    47,
                    0
                ],
                "title": "LogiDynamics: Unraveling the Dynamics of Logical Inference in Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogiDynamics: Unraveling the Dynamics of Logical Inference in Large\n  Language Model Reasoning"
                },
                "summary": "Modern large language models (LLMs) employ various forms of logical\ninference, both implicitly and explicitly, when addressing reasoning tasks.\nUnderstanding how to optimally leverage these inference paradigms is critical\nfor advancing LLMs' reasoning capabilities. This paper adopts an exploratory\napproach by introducing a controlled evaluation environment for analogical\nreasoning -- a fundamental cognitive task -- that is systematically\nparameterized across three dimensions: modality (textual, visual, symbolic),\ndifficulty (easy, medium, hard), and task format (multiple-choice or free-text\ngeneration). We analyze the comparative dynamics of inductive, abductive, and\ndeductive inference pipelines across these dimensions, and demonstrate that our\nfindings generalize to broader in-context learning tasks. Additionally, we\ninvestigate advanced paradigms such as hypothesis selection, verification, and\nrefinement, revealing their potential to scale up logical inference in LLM\nreasoning. This exploratory study provides a foundation for future research in\nenhancing LLM reasoning through systematic logical inference strategies.\nResources are available at https://github.com/HKUST-KnowComp/LogiDynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) employ various forms of logical\ninference, both implicitly and explicitly, when addressing reasoning tasks.\nUnderstanding how to optimally leverage these inference paradigms is critical\nfor advancing LLMs' reasoning capabilities. This paper adopts an exploratory\napproach by introducing a controlled evaluation environment for analogical\nreasoning -- a fundamental cognitive task -- that is systematically\nparameterized across three dimensions: modality (textual, visual, symbolic),\ndifficulty (easy, medium, hard), and task format (multiple-choice or free-text\ngeneration). We analyze the comparative dynamics of inductive, abductive, and\ndeductive inference pipelines across these dimensions, and demonstrate that our\nfindings generalize to broader in-context learning tasks. Additionally, we\ninvestigate advanced paradigms such as hypothesis selection, verification, and\nrefinement, revealing their potential to scale up logical inference in LLM\nreasoning. This exploratory study provides a foundation for future research in\nenhancing LLM reasoning through systematic logical inference strategies.\nResources are available at https://github.com/HKUST-KnowComp/LogiDynamics."
                },
                "authors": [
                    {
                        "name": "Tianshi Zheng"
                    },
                    {
                        "name": "Jiayang Cheng"
                    },
                    {
                        "name": "Chunyang Li"
                    },
                    {
                        "name": "Haochen Shi"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Ginny Y. Wong"
                    },
                    {
                        "name": "Simon See"
                    }
                ],
                "author_detail": {
                    "name": "Simon See"
                },
                "author": "Simon See",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22522v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22522v2",
                "updated": "2025-04-09T14:51:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    51,
                    14,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-28T15:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    28,
                    4,
                    4,
                    87,
                    0
                ],
                "title": "A Centralized Planning and Distributed Execution Method for Shape\n  Filling with Homogeneous Mobile Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Centralized Planning and Distributed Execution Method for Shape\n  Filling with Homogeneous Mobile Robots"
                },
                "summary": "Nature has inspired humans in different ways. The formation behavior of\nanimals can perform tasks that exceed individual capability. For example, army\nants could transverse gaps by forming bridges, and fishes could group up to\nprotect themselves from predators. The pattern formation task is essential in a\nmultiagent robotic system because it usually serves as the initial\nconfiguration of downstream tasks, such as collective manipulation and\nadaptation to various environments. The formation of complex shapes, especially\nhollow shapes, remains an open question. Traditional approaches either require\nglobal coordinates for each robot or are prone to failure when attempting to\nclose the hole due to accumulated localization errors. Inspired by the ribbon\nidea introduced in the additive self-assembly algorithm by the Kilobot team, we\ndevelop a two-stage algorithm that does not require global coordinates\ninformation and effectively forms shapes with holes. In this paper, we\ninvestigate the partitioning of the shape using ribbons in a hexagonal lattice\nsetting and propose the add-subtract algorithm based on the movement sequence\ninduced by the ribbon structure. This advancement opens the door to tasks\nrequiring complex pattern formations, such as the assembly of nanobots for\nmedical applications involving intricate structures and the deployment of\nrobots along the boundaries of areas of interest. We also provide simulation\nresults on complex shapes, an analysis of the robustness as well as a proof of\ncorrectness of the proposed algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nature has inspired humans in different ways. The formation behavior of\nanimals can perform tasks that exceed individual capability. For example, army\nants could transverse gaps by forming bridges, and fishes could group up to\nprotect themselves from predators. The pattern formation task is essential in a\nmultiagent robotic system because it usually serves as the initial\nconfiguration of downstream tasks, such as collective manipulation and\nadaptation to various environments. The formation of complex shapes, especially\nhollow shapes, remains an open question. Traditional approaches either require\nglobal coordinates for each robot or are prone to failure when attempting to\nclose the hole due to accumulated localization errors. Inspired by the ribbon\nidea introduced in the additive self-assembly algorithm by the Kilobot team, we\ndevelop a two-stage algorithm that does not require global coordinates\ninformation and effectively forms shapes with holes. In this paper, we\ninvestigate the partitioning of the shape using ribbons in a hexagonal lattice\nsetting and propose the add-subtract algorithm based on the movement sequence\ninduced by the ribbon structure. This advancement opens the door to tasks\nrequiring complex pattern formations, such as the assembly of nanobots for\nmedical applications involving intricate structures and the deployment of\nrobots along the boundaries of areas of interest. We also provide simulation\nresults on complex shapes, an analysis of the robustness as well as a proof of\ncorrectness of the proposed algorithm."
                },
                "authors": [
                    {
                        "name": "Shuqing Liu"
                    },
                    {
                        "name": "Rong Su"
                    },
                    {
                        "name": "Karl H. Johansson"
                    }
                ],
                "author_detail": {
                    "name": "Karl H. Johansson"
                },
                "author": "Karl H. Johansson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22522v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22522v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06943v1",
                "updated": "2025-04-09T14:51:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    51,
                    2,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T14:51:02Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    51,
                    2,
                    2,
                    99,
                    0
                ],
                "title": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations,\n  Architectural Components, and Cognitive Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations,\n  Architectural Components, and Cognitive Integration"
                },
                "summary": "Agents powered by Large Language Models (LLMs) have recently demonstrated\nimpressive capabilities in various tasks. Still, they face limitations in tasks\nrequiring specific, structured knowledge, flexibility, or accountable\ndecision-making. While agents are capable of perceiving their environments,\nforming inferences, planning, and executing actions towards goals, they often\nface issues such as hallucinations and lack of contextual memory across\ninteractions. This paper explores how Case-Based Reasoning (CBR), a strategy\nthat solves new problems by referencing past experiences, can be integrated\ninto LLM agent frameworks. This integration allows LLMs to leverage explicit\nknowledge, enhancing their effectiveness. We systematically review the\ntheoretical foundations of these enhanced agents, identify critical framework\ncomponents, and formulate a mathematical model for the CBR processes of case\nretrieval, adaptation, and learning. We also evaluate CBR-enhanced agents\nagainst other methods like Chain-of-Thought reasoning and standard\nRetrieval-Augmented Generation, analyzing their relative strengths. Moreover,\nwe explore how leveraging CBR's cognitive dimensions (including\nself-reflection, introspection, and curiosity) via goal-driven autonomy\nmechanisms can further enhance the LLM agent capabilities. Contributing to the\nongoing research on neuro-symbolic hybrid systems, this work posits CBR as a\nviable technique for enhancing the reasoning skills and cognitive aspects of\nautonomous LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents powered by Large Language Models (LLMs) have recently demonstrated\nimpressive capabilities in various tasks. Still, they face limitations in tasks\nrequiring specific, structured knowledge, flexibility, or accountable\ndecision-making. While agents are capable of perceiving their environments,\nforming inferences, planning, and executing actions towards goals, they often\nface issues such as hallucinations and lack of contextual memory across\ninteractions. This paper explores how Case-Based Reasoning (CBR), a strategy\nthat solves new problems by referencing past experiences, can be integrated\ninto LLM agent frameworks. This integration allows LLMs to leverage explicit\nknowledge, enhancing their effectiveness. We systematically review the\ntheoretical foundations of these enhanced agents, identify critical framework\ncomponents, and formulate a mathematical model for the CBR processes of case\nretrieval, adaptation, and learning. We also evaluate CBR-enhanced agents\nagainst other methods like Chain-of-Thought reasoning and standard\nRetrieval-Augmented Generation, analyzing their relative strengths. Moreover,\nwe explore how leveraging CBR's cognitive dimensions (including\nself-reflection, introspection, and curiosity) via goal-driven autonomy\nmechanisms can further enhance the LLM agent capabilities. Contributing to the\nongoing research on neuro-symbolic hybrid systems, this work posits CBR as a\nviable technique for enhancing the reasoning skills and cognitive aspects of\nautonomous LLM agents."
                },
                "authors": [
                    {
                        "name": "Kostas Hatalis"
                    },
                    {
                        "name": "Despina Christou"
                    },
                    {
                        "name": "Vyshnavi Kondapalli"
                    }
                ],
                "author_detail": {
                    "name": "Vyshnavi Kondapalli"
                },
                "author": "Vyshnavi Kondapalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06939v1",
                "updated": "2025-04-09T14:43:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    43,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T14:43:08Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    43,
                    8,
                    2,
                    99,
                    0
                ],
                "title": "FeedbackEval: A Benchmark for Evaluating Large Language Models in\n  Feedback-Driven Code Repair Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FeedbackEval: A Benchmark for Evaluating Large Language Models in\n  Feedback-Driven Code Repair Tasks"
                },
                "summary": "Code repair is a fundamental task in software development, facilitating\nefficient bug resolution and software maintenance. Although large language\nmodels (LLMs) have demonstrated considerable potential in automated code\nrepair, their ability to comprehend and effectively leverage diverse types of\nfeedback remains insufficiently understood. To bridge this gap, we introduce\nFeedbackEval, a systematic benchmark for evaluating LLMs' feedback\ncomprehension and performance in code repair tasks. We conduct a comprehensive\nempirical study on five state-of-the-art LLMs, including GPT-4o, Claude-3.5,\nGemini-1.5, GLM-4, and Qwen2.5, to evaluate their behavior under both\nsingle-iteration and iterative code repair settings. Our results show that\nstructured feedback, particularly in the form of test feedback, leads to the\nhighest repair success rates, while unstructured feedback proves significantly\nless effective. Iterative feedback further enhances repair performance, though\nthe marginal benefit diminishes after two or three rounds. Moreover, prompt\nstructure is shown to be critical: incorporating docstrings, contextual\ninformation, and explicit guidelines substantially improves outcomes, whereas\npersona-based, chain-of-thought, and few-shot prompting strategies offer\nlimited benefits in single-iteration scenarios. This work introduces a robust\nbenchmark and delivers practical insights to advance the understanding and\ndevelopment of feedback-driven code repair using LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code repair is a fundamental task in software development, facilitating\nefficient bug resolution and software maintenance. Although large language\nmodels (LLMs) have demonstrated considerable potential in automated code\nrepair, their ability to comprehend and effectively leverage diverse types of\nfeedback remains insufficiently understood. To bridge this gap, we introduce\nFeedbackEval, a systematic benchmark for evaluating LLMs' feedback\ncomprehension and performance in code repair tasks. We conduct a comprehensive\nempirical study on five state-of-the-art LLMs, including GPT-4o, Claude-3.5,\nGemini-1.5, GLM-4, and Qwen2.5, to evaluate their behavior under both\nsingle-iteration and iterative code repair settings. Our results show that\nstructured feedback, particularly in the form of test feedback, leads to the\nhighest repair success rates, while unstructured feedback proves significantly\nless effective. Iterative feedback further enhances repair performance, though\nthe marginal benefit diminishes after two or three rounds. Moreover, prompt\nstructure is shown to be critical: incorporating docstrings, contextual\ninformation, and explicit guidelines substantially improves outcomes, whereas\npersona-based, chain-of-thought, and few-shot prompting strategies offer\nlimited benefits in single-iteration scenarios. This work introduces a robust\nbenchmark and delivers practical insights to advance the understanding and\ndevelopment of feedback-driven code repair using LLMs."
                },
                "authors": [
                    {
                        "name": "Dekun Dai"
                    },
                    {
                        "name": "MingWei Liu"
                    },
                    {
                        "name": "Anji Li"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15050v2",
                "updated": "2025-04-09T14:18:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    18,
                    47,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-19T09:39:32Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    39,
                    32,
                    2,
                    78,
                    0
                ],
                "title": "Studying and Understanding the Effectiveness and Failures of\n  Conversational LLM-Based Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studying and Understanding the Effectiveness and Failures of\n  Conversational LLM-Based Repair"
                },
                "summary": "Automated program repair (APR) is designed to automate the process of\nbug-fixing. In recent years, thanks to the rapid development of large language\nmodels (LLMs), automated repair has achieved remarkable progress. Advanced APR\ntechniques powered by conversational LLMs, most notably ChatGPT, have exhibited\nimpressive repair abilities and gained increasing popularity due to the\ncapabilities of the underlying LLMs in providing repair feedback and performing\niterative patch improvement. Despite the superiority, conversational APR\ntechniques still fail to repair a large number of bugs. For example, a\nstate-of-the-art conversational technique ChatRepair does not correctly repair\nover half of the single-function bugs in the Defects4J dataset. To understand\nthe effectiveness and failures of conversational LLM-based repair and provide\npossible directions for improvement, we studied the exemplary ChatRepair with a\nfocus on comparing the effectiveness of its cloze-style and full function\nrepair strategies, assessing its key iterative component for patch improvement,\nand analyzing the repair failures. Our study has led to a series of findings,\nwhich we believe provide key implications for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated program repair (APR) is designed to automate the process of\nbug-fixing. In recent years, thanks to the rapid development of large language\nmodels (LLMs), automated repair has achieved remarkable progress. Advanced APR\ntechniques powered by conversational LLMs, most notably ChatGPT, have exhibited\nimpressive repair abilities and gained increasing popularity due to the\ncapabilities of the underlying LLMs in providing repair feedback and performing\niterative patch improvement. Despite the superiority, conversational APR\ntechniques still fail to repair a large number of bugs. For example, a\nstate-of-the-art conversational technique ChatRepair does not correctly repair\nover half of the single-function bugs in the Defects4J dataset. To understand\nthe effectiveness and failures of conversational LLM-based repair and provide\npossible directions for improvement, we studied the exemplary ChatRepair with a\nfocus on comparing the effectiveness of its cloze-style and full function\nrepair strategies, assessing its key iterative component for patch improvement,\nand analyzing the repair failures. Our study has led to a series of findings,\nwhich we believe provide key implications for future research."
                },
                "authors": [
                    {
                        "name": "Aolin Chen"
                    },
                    {
                        "name": "Haojun Wu"
                    },
                    {
                        "name": "Qi Xin"
                    },
                    {
                        "name": "Steven P. Reiss"
                    },
                    {
                        "name": "Jifeng Xuan"
                    }
                ],
                "author_detail": {
                    "name": "Jifeng Xuan"
                },
                "author": "Jifeng Xuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06910v1",
                "updated": "2025-04-09T14:14:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    14,
                    42,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T14:14:42Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    14,
                    42,
                    2,
                    99,
                    0
                ],
                "title": "Identifying Aspects in Peer Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Aspects in Peer Reviews"
                },
                "summary": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspect sets from review forms and guidelines of major NLP venues, yet\ndata-driven methods for aspect identification are largely underexplored. To\naddress this gap, our work takes a bottom-up approach: we propose an\noperational definition of aspect and develop a data-driven schema for deriving\nfine-grained aspects from a corpus of peer reviews. We introduce a dataset of\npeer reviews augmented with aspects and show how it can be used for\ncommunity-level review analysis. We further show how the choice of aspects can\nimpact downstream applications, such as LLM-generated review detection. Our\nresults lay a foundation for a principled and data-driven investigation of\nreview aspects, and pave the path for new applications of NLP to support peer\nreview.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspect sets from review forms and guidelines of major NLP venues, yet\ndata-driven methods for aspect identification are largely underexplored. To\naddress this gap, our work takes a bottom-up approach: we propose an\noperational definition of aspect and develop a data-driven schema for deriving\nfine-grained aspects from a corpus of peer reviews. We introduce a dataset of\npeer reviews augmented with aspects and show how it can be used for\ncommunity-level review analysis. We further show how the choice of aspects can\nimpact downstream applications, such as LLM-generated review detection. Our\nresults lay a foundation for a principled and data-driven investigation of\nreview aspects, and pave the path for new applications of NLP to support peer\nreview."
                },
                "authors": [
                    {
                        "name": "Sheng Lu"
                    },
                    {
                        "name": "Ilia Kuznetsov"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16615v2",
                "updated": "2025-04-09T14:08:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    8,
                    58,
                    2,
                    99,
                    0
                ],
                "published": "2024-12-21T13:19:15Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    19,
                    15,
                    5,
                    356,
                    0
                ],
                "title": "Large Language Model Can Be a Foundation for Hidden Rationale-Based\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Can Be a Foundation for Hidden Rationale-Based\n  Retrieval"
                },
                "summary": "Despite the recent advancement in Retrieval-Augmented Generation (RAG)\nsystems, most retrieval methodologies are often developed for factual\nretrieval, which assumes query and positive documents are semantically similar.\nIn this paper, we instead propose and study a more challenging type of\nretrieval task, called hidden rationale retrieval, in which query and document\nare not similar but can be inferred by reasoning chains, logic relationships,\nor empirical experiences. To address such problems, an instruction-tuned Large\nlanguage model (LLM) with a cross-encoder architecture could be a reasonable\nchoice. To further strengthen pioneering LLM-based retrievers, we design a\nspecial instruction that transforms the retrieval task into a generative task\nby prompting LLM to answer a binary-choice question. The model can be\nfine-tuned with direct preference optimization (DPO). The framework is also\noptimized for computational efficiency with no performance degradation. We name\nthis retrieval framework by RaHoRe and verify its zero-shot and fine-tuned\nperformance superiority on Emotional Support Conversation (ESC), compared with\nprevious retrieval works. Our study suggests the potential to employ LLM as a\nfoundation for a wider scope of retrieval tasks. Our codes, models, and\ndatasets are available on https://github.com/flyfree5/LaHoRe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent advancement in Retrieval-Augmented Generation (RAG)\nsystems, most retrieval methodologies are often developed for factual\nretrieval, which assumes query and positive documents are semantically similar.\nIn this paper, we instead propose and study a more challenging type of\nretrieval task, called hidden rationale retrieval, in which query and document\nare not similar but can be inferred by reasoning chains, logic relationships,\nor empirical experiences. To address such problems, an instruction-tuned Large\nlanguage model (LLM) with a cross-encoder architecture could be a reasonable\nchoice. To further strengthen pioneering LLM-based retrievers, we design a\nspecial instruction that transforms the retrieval task into a generative task\nby prompting LLM to answer a binary-choice question. The model can be\nfine-tuned with direct preference optimization (DPO). The framework is also\noptimized for computational efficiency with no performance degradation. We name\nthis retrieval framework by RaHoRe and verify its zero-shot and fine-tuned\nperformance superiority on Emotional Support Conversation (ESC), compared with\nprevious retrieval works. Our study suggests the potential to employ LLM as a\nfoundation for a wider scope of retrieval tasks. Our codes, models, and\ndatasets are available on https://github.com/flyfree5/LaHoRe."
                },
                "authors": [
                    {
                        "name": "Luo Ji"
                    },
                    {
                        "name": "Feixiang Guo"
                    },
                    {
                        "name": "Teng Chen"
                    },
                    {
                        "name": "Qingqing Gu"
                    },
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Ningyuan Xi"
                    },
                    {
                        "name": "Yihong Wang"
                    },
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Hongyang Lei"
                    },
                    {
                        "name": "Zhonglin Jiang"
                    },
                    {
                        "name": "Yong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yong Chen"
                },
                "author": "Yong Chen",
                "arxiv_comment": "10 pages, 3 figures, ECIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06204v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06204v3",
                "updated": "2025-04-09T13:54:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    54,
                    59,
                    2,
                    99,
                    0
                ],
                "published": "2024-06-26T16:34:33Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    16,
                    34,
                    33,
                    2,
                    178,
                    0
                ],
                "title": "A Survey on Mixture of Experts in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Mixture of Experts in Large Language Models"
                },
                "summary": "Large language models (LLMs) have garnered unprecedented advancements across\ndiverse fields, ranging from natural language processing to computer vision and\nbeyond. The prowess of LLMs is underpinned by their substantial model size,\nextensive and diverse datasets, and the vast computational power harnessed\nduring training, all of which contribute to the emergent abilities of LLMs\n(e.g., in-context learning) that are not present in small models. Within this\ncontext, the mixture of experts (MoE) has emerged as an effective method for\nsubstantially scaling up model capacity with minimal computation overhead,\ngaining significant attention from academia and industry. Despite its growing\nprevalence, there lacks a systematic and comprehensive review of the literature\non MoE. This survey seeks to bridge that gap, serving as an essential resource\nfor researchers delving into the intricacies of MoE. We first briefly introduce\nthe structure of the MoE layer, followed by proposing a new taxonomy of MoE.\nNext, we overview the core designs for various MoE models including both\nalgorithmic and systemic aspects, alongside collections of available\nopen-source implementations, hyperparameter configurations and empirical\nevaluations. Furthermore, we delineate the multifaceted applications of MoE in\npractice, and outline some potential directions for future research. To\nfacilitate ongoing updates and the sharing of cutting-edge advances in MoE\nresearch, we have established a resource repository at\nhttps://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have garnered unprecedented advancements across\ndiverse fields, ranging from natural language processing to computer vision and\nbeyond. The prowess of LLMs is underpinned by their substantial model size,\nextensive and diverse datasets, and the vast computational power harnessed\nduring training, all of which contribute to the emergent abilities of LLMs\n(e.g., in-context learning) that are not present in small models. Within this\ncontext, the mixture of experts (MoE) has emerged as an effective method for\nsubstantially scaling up model capacity with minimal computation overhead,\ngaining significant attention from academia and industry. Despite its growing\nprevalence, there lacks a systematic and comprehensive review of the literature\non MoE. This survey seeks to bridge that gap, serving as an essential resource\nfor researchers delving into the intricacies of MoE. We first briefly introduce\nthe structure of the MoE layer, followed by proposing a new taxonomy of MoE.\nNext, we overview the core designs for various MoE models including both\nalgorithmic and systemic aspects, alongside collections of available\nopen-source implementations, hyperparameter configurations and empirical\nevaluations. Furthermore, we delineate the multifaceted applications of MoE in\npractice, and outline some potential directions for future research. To\nfacilitate ongoing updates and the sharing of cutting-edge advances in MoE\nresearch, we have established a resource repository at\nhttps://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs."
                },
                "authors": [
                    {
                        "name": "Weilin Cai"
                    },
                    {
                        "name": "Juyong Jiang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Jing Tang"
                    },
                    {
                        "name": "Sunghun Kim"
                    },
                    {
                        "name": "Jiayi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Huang"
                },
                "author": "Jiayi Huang",
                "arxiv_doi": "10.1109/TKDE.2025.3554028",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TKDE.2025.3554028",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.06204v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06204v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The first three authors contributed equally to this work; Accepted by\n  TKDE",
                "arxiv_journal_ref": "IEEE Transactions on Knowledge and Data Engineering (TKDE) 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06017v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06017v2",
                "updated": "2025-04-09T13:54:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    54,
                    18,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T13:22:09Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    13,
                    22,
                    9,
                    1,
                    98,
                    0
                ],
                "title": "CAI: An Open, Bug Bounty-Ready Cybersecurity AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAI: An Open, Bug Bounty-Ready Cybersecurity AI"
                },
                "summary": "By 2028 most cybersecurity actions will be autonomous, with humans\nteleoperating. We present the first classification of autonomy levels in\ncybersecurity and introduce Cybersecurity AI (CAI), an open-source framework\nthat democratizes advanced security testing through specialized AI agents.\nThrough rigorous empirical evaluation, we demonstrate that CAI consistently\noutperforms state-of-the-art results in CTF benchmarks, solving challenges\nacross diverse categories with significantly greater efficiency -up to 3,600x\nfaster than humans in specific tasks and averaging 11x faster overall. CAI\nachieved first place among AI teams and secured a top-20 position worldwide in\nthe \"AI vs Human\" CTF live Challenge, earning a monetary reward of $750. Based\non our results, we argue against LLM-vendor claims about limited security\ncapabilities. Beyond cybersecurity competitions, CAI demonstrates real-world\neffectiveness, reaching top-30 in Spain and top-500 worldwide on Hack The Box\nwithin a week, while dramatically reducing security testing costs by an average\nof 156x. Our framework transcends theoretical benchmarks by enabling\nnon-professionals to discover significant security bugs (CVSS 4.3-7.5) at rates\ncomparable to experts during bug bounty exercises. By combining modular agent\ndesign with seamless tool integration and human oversight (HITL), CAI addresses\ncritical market gaps, offering organizations of all sizes access to AI-powered\nbug bounty security testing previously available only to well-resourced firms\n-thereby challenging the oligopolistic ecosystem currently dominated by major\nbug bounty platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By 2028 most cybersecurity actions will be autonomous, with humans\nteleoperating. We present the first classification of autonomy levels in\ncybersecurity and introduce Cybersecurity AI (CAI), an open-source framework\nthat democratizes advanced security testing through specialized AI agents.\nThrough rigorous empirical evaluation, we demonstrate that CAI consistently\noutperforms state-of-the-art results in CTF benchmarks, solving challenges\nacross diverse categories with significantly greater efficiency -up to 3,600x\nfaster than humans in specific tasks and averaging 11x faster overall. CAI\nachieved first place among AI teams and secured a top-20 position worldwide in\nthe \"AI vs Human\" CTF live Challenge, earning a monetary reward of $750. Based\non our results, we argue against LLM-vendor claims about limited security\ncapabilities. Beyond cybersecurity competitions, CAI demonstrates real-world\neffectiveness, reaching top-30 in Spain and top-500 worldwide on Hack The Box\nwithin a week, while dramatically reducing security testing costs by an average\nof 156x. Our framework transcends theoretical benchmarks by enabling\nnon-professionals to discover significant security bugs (CVSS 4.3-7.5) at rates\ncomparable to experts during bug bounty exercises. By combining modular agent\ndesign with seamless tool integration and human oversight (HITL), CAI addresses\ncritical market gaps, offering organizations of all sizes access to AI-powered\nbug bounty security testing previously available only to well-resourced firms\n-thereby challenging the oligopolistic ecosystem currently dominated by major\nbug bounty platforms."
                },
                "authors": [
                    {
                        "name": "Víctor Mayoral-Vilches"
                    },
                    {
                        "name": "Luis Javier Navarrete-Lozano"
                    },
                    {
                        "name": "María Sanz-Gómez"
                    },
                    {
                        "name": "Lidia Salas Espejo"
                    },
                    {
                        "name": "Martiño Crespo-Álvarez"
                    },
                    {
                        "name": "Francisco Oca-Gonzalez"
                    },
                    {
                        "name": "Francesco Balassone"
                    },
                    {
                        "name": "Alfonso Glera-Picón"
                    },
                    {
                        "name": "Unai Ayucar-Carbajo"
                    },
                    {
                        "name": "Jon Ander Ruiz-Alcalde"
                    },
                    {
                        "name": "Stefan Rass"
                    },
                    {
                        "name": "Martin Pinzger"
                    },
                    {
                        "name": "Endika Gil-Uriarte"
                    }
                ],
                "author_detail": {
                    "name": "Endika Gil-Uriarte"
                },
                "author": "Endika Gil-Uriarte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06017v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06017v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15209v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15209v2",
                "updated": "2025-04-09T13:46:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    46,
                    27,
                    2,
                    99,
                    0
                ],
                "published": "2024-11-20T10:32:22Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    32,
                    22,
                    2,
                    325,
                    0
                ],
                "title": "Quantized symbolic time series approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized symbolic time series approximation"
                },
                "summary": "Time series are ubiquitous in numerous science and engineering domains, e.g.,\nsignal processing, bioinformatics, and astronomy. Previous work has verified\nthe efficacy of symbolic time series representation in a variety of engineering\napplications due to its storage efficiency and numerosity reduction. The most\nrecent symbolic aggregate approximation technique, ABBA, has been shown to\npreserve essential shape information of time series and improve downstream\napplications, e.g., neural network inference regarding prediction and anomaly\ndetection in time series.\n  Motivated by the emergence of high-performance hardware which enables\nefficient computation for low bit-width representations, we present a new\nquantization-based ABBA symbolic approximation technique, QABBA, which exhibits\nimproved storage efficiency while retaining the original speed and accuracy of\nsymbolic reconstruction. We prove an upper bound for the error arising from\nquantization and discuss how the number of bits should be chosen to balance\nthis with other errors.\n  An application of QABBA with large language models (LLMs) for time series\nregression is also presented, and its utility is investigated. By representing\nthe symbolic chain of patterns on time series, QABBA not only avoids the\ntraining of embedding from scratch, but also achieves a new state-of-the-art on\nMonash regression dataset. The symbolic approximation to the time series offers\na more efficient way to fine-tune LLMs on the time series regression task which\ncontains various application domains. We further present a set of extensive\nexperiments performed across various well-established datasets to demonstrate\nthe advantages of the QABBA method for symbolic approximation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series are ubiquitous in numerous science and engineering domains, e.g.,\nsignal processing, bioinformatics, and astronomy. Previous work has verified\nthe efficacy of symbolic time series representation in a variety of engineering\napplications due to its storage efficiency and numerosity reduction. The most\nrecent symbolic aggregate approximation technique, ABBA, has been shown to\npreserve essential shape information of time series and improve downstream\napplications, e.g., neural network inference regarding prediction and anomaly\ndetection in time series.\n  Motivated by the emergence of high-performance hardware which enables\nefficient computation for low bit-width representations, we present a new\nquantization-based ABBA symbolic approximation technique, QABBA, which exhibits\nimproved storage efficiency while retaining the original speed and accuracy of\nsymbolic reconstruction. We prove an upper bound for the error arising from\nquantization and discuss how the number of bits should be chosen to balance\nthis with other errors.\n  An application of QABBA with large language models (LLMs) for time series\nregression is also presented, and its utility is investigated. By representing\nthe symbolic chain of patterns on time series, QABBA not only avoids the\ntraining of embedding from scratch, but also achieves a new state-of-the-art on\nMonash regression dataset. The symbolic approximation to the time series offers\na more efficient way to fine-tune LLMs on the time series regression task which\ncontains various application domains. We further present a set of extensive\nexperiments performed across various well-established datasets to demonstrate\nthe advantages of the QABBA method for symbolic approximation."
                },
                "authors": [
                    {
                        "name": "Erin Carson"
                    },
                    {
                        "name": "Xinye Chen"
                    },
                    {
                        "name": "Cheng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Kang"
                },
                "author": "Cheng Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15209v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15209v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06867v1",
                "updated": "2025-04-09T13:16:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    16,
                    48,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T13:16:48Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    16,
                    48,
                    2,
                    99,
                    0
                ],
                "title": "xApp Conflict Mitigation with Scheduler",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xApp Conflict Mitigation with Scheduler"
                },
                "summary": "Open RAN (O-RAN) fosters multi-vendor interoperability and data-driven\ncontrol but simultaneously introduces the challenge of managing pre-trained\nxApps that can produce conflicting actions. Although O-RAN specifications\nmandate offline training and validation to prevent untrained models,\noperational conflicts remain likely under dynamic, context-dependent\nconditions. This work proposes a scheduler-based conflict mitigation framework\nto address these challenges without requiring training xApps together or\nfurther xApp re-training. By examining an indirect conflict involving power and\nresource block allocation xApps and employing an Advantage Actor-Critic (A2C)\napproach to train both xApps and the scheduler, we illustrate that a\nstraightforward A2C-based scheduler improves performance relative to\nindependently deployed xApps and conflicting cases. Notably, augmenting the\nsystem with baseline xApps and allowing the scheduler to select from a broader\npool yields the best results, underscoring the importance of adaptive\nscheduling mechanisms. These findings highlight the context-dependent nature of\nconflicts in automated network management, as two xApps may conflict under\ncertain conditions but coexist under others. Consequently, the ability to\ndynamically update and adapt the scheduler to accommodate diverse operational\nintents is vital for future network deployments. By offering dynamic scheduling\nwithout re-training xApps, this framework advances practical conflict\nresolution solutions while supporting real-world scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open RAN (O-RAN) fosters multi-vendor interoperability and data-driven\ncontrol but simultaneously introduces the challenge of managing pre-trained\nxApps that can produce conflicting actions. Although O-RAN specifications\nmandate offline training and validation to prevent untrained models,\noperational conflicts remain likely under dynamic, context-dependent\nconditions. This work proposes a scheduler-based conflict mitigation framework\nto address these challenges without requiring training xApps together or\nfurther xApp re-training. By examining an indirect conflict involving power and\nresource block allocation xApps and employing an Advantage Actor-Critic (A2C)\napproach to train both xApps and the scheduler, we illustrate that a\nstraightforward A2C-based scheduler improves performance relative to\nindependently deployed xApps and conflicting cases. Notably, augmenting the\nsystem with baseline xApps and allowing the scheduler to select from a broader\npool yields the best results, underscoring the importance of adaptive\nscheduling mechanisms. These findings highlight the context-dependent nature of\nconflicts in automated network management, as two xApps may conflict under\ncertain conditions but coexist under others. Consequently, the ability to\ndynamically update and adapt the scheduler to accommodate diverse operational\nintents is vital for future network deployments. By offering dynamic scheduling\nwithout re-training xApps, this framework advances practical conflict\nresolution solutions while supporting real-world scalability."
                },
                "authors": [
                    {
                        "name": "Idris Cinemre"
                    },
                    {
                        "name": "Toktam Mahmoodi"
                    }
                ],
                "author_detail": {
                    "name": "Toktam Mahmoodi"
                },
                "author": "Toktam Mahmoodi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06861v1",
                "updated": "2025-04-09T13:11:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    11,
                    9,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T13:11:09Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    11,
                    9,
                    2,
                    99,
                    0
                ],
                "title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for\n  Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for\n  Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation"
                },
                "summary": "Zero-shot, training-free, image-based text-to-video generation is an emerging\narea that aims to generate videos using existing image-based diffusion models.\nCurrent methods in this space require specific architectural changes to image\ngeneration models, which limit their adaptability and scalability. In contrast\nto such methods, we provide a model-agnostic approach. We use intersections in\ndiffusion trajectories, working only with the latent values. We could not\nobtain localized frame-wise coherence and diversity using only the intersection\nof trajectories. Thus, we instead use a grid-based approach. An in-context\ntrained LLM is used to generate coherent frame-wise prompts; another is used to\nidentify differences between frames. Based on these, we obtain a CLIP-based\nattention mask that controls the timing of switching the prompts for each grid\ncell. Earlier switching results in higher variance, while later switching\nresults in more coherence. Therefore, our approach can ensure appropriate\ncontrol between coherence and variance for the frames. Our approach results in\nstate-of-the-art performance while being more flexible when working with\ndiverse image-generation models. The empirical analysis using quantitative\nmetrics and user studies confirms our model's superior temporal consistency,\nvisual fidelity and user satisfaction, thus providing a novel way to obtain\ntraining-free, image-based text-to-video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot, training-free, image-based text-to-video generation is an emerging\narea that aims to generate videos using existing image-based diffusion models.\nCurrent methods in this space require specific architectural changes to image\ngeneration models, which limit their adaptability and scalability. In contrast\nto such methods, we provide a model-agnostic approach. We use intersections in\ndiffusion trajectories, working only with the latent values. We could not\nobtain localized frame-wise coherence and diversity using only the intersection\nof trajectories. Thus, we instead use a grid-based approach. An in-context\ntrained LLM is used to generate coherent frame-wise prompts; another is used to\nidentify differences between frames. Based on these, we obtain a CLIP-based\nattention mask that controls the timing of switching the prompts for each grid\ncell. Earlier switching results in higher variance, while later switching\nresults in more coherence. Therefore, our approach can ensure appropriate\ncontrol between coherence and variance for the frames. Our approach results in\nstate-of-the-art performance while being more flexible when working with\ndiverse image-generation models. The empirical analysis using quantitative\nmetrics and user studies confirms our model's superior temporal consistency,\nvisual fidelity and user satisfaction, thus providing a novel way to obtain\ntraining-free, image-based text-to-video generation."
                },
                "authors": [
                    {
                        "name": "Diljeet Jagpal"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Vinay P. Namboodiri"
                    }
                ],
                "author_detail": {
                    "name": "Vinay P. Namboodiri"
                },
                "author": "Vinay P. Namboodiri",
                "arxiv_comment": "Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05523v2",
                "updated": "2025-04-09T13:09:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    13,
                    9,
                    6,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-07T21:51:32Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    21,
                    51,
                    32,
                    0,
                    97,
                    0
                ],
                "title": "Pretraining Language Models for Diachronic Linguistic Change Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining Language Models for Diachronic Linguistic Change Discovery"
                },
                "summary": "Large language models (LLMs) have shown potential as tools for scientific\ndiscovery. This has engendered growing interest in their use in humanistic\ndisciplines, such as historical linguistics and literary studies. These fields\noften construct arguments on the basis of delineations like genre, or more\ninflexibly, time period. Although efforts have been made to restrict inference\nto specific domains via fine-tuning or model editing, we posit that the only\ntrue guarantee is domain-restricted pretraining -- typically, a data- and\ncompute-expensive proposition.\n  We show that efficient pretraining techniques can produce useful models over\ncorpora too large for easy manual inspection but too small for \"typical\" LLM\napproaches. We employ a novel date-attribution pipeline in order to obtain a\ntemporally-segmented dataset of five 10-million-word slices. We train two\ncorresponding five-model batteries over these corpus segments, efficient\npretraining and Llama3-8B parameter efficiently finetuned.\n  We find that the pretrained models are faster to train than the finetuned\nbaselines and that they better respect the historical divisions of our corpus.\nEmphasizing speed and precision over a-historical comprehensiveness enables a\nnumber of novel approaches to hypothesis discovery and testing in our target\nfields. Taking up diachronic linguistics as a testbed, we show that our method\nenables the detection of a diverse set of phenomena, including en masse lexical\nchange, non-lexical (grammatical and morphological) change, and word sense\nintroduction/obsolescence. We provide a ready-to-use pipeline that allows\nextension of our approach to other target fields with only minimal adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown potential as tools for scientific\ndiscovery. This has engendered growing interest in their use in humanistic\ndisciplines, such as historical linguistics and literary studies. These fields\noften construct arguments on the basis of delineations like genre, or more\ninflexibly, time period. Although efforts have been made to restrict inference\nto specific domains via fine-tuning or model editing, we posit that the only\ntrue guarantee is domain-restricted pretraining -- typically, a data- and\ncompute-expensive proposition.\n  We show that efficient pretraining techniques can produce useful models over\ncorpora too large for easy manual inspection but too small for \"typical\" LLM\napproaches. We employ a novel date-attribution pipeline in order to obtain a\ntemporally-segmented dataset of five 10-million-word slices. We train two\ncorresponding five-model batteries over these corpus segments, efficient\npretraining and Llama3-8B parameter efficiently finetuned.\n  We find that the pretrained models are faster to train than the finetuned\nbaselines and that they better respect the historical divisions of our corpus.\nEmphasizing speed and precision over a-historical comprehensiveness enables a\nnumber of novel approaches to hypothesis discovery and testing in our target\nfields. Taking up diachronic linguistics as a testbed, we show that our method\nenables the detection of a diverse set of phenomena, including en masse lexical\nchange, non-lexical (grammatical and morphological) change, and word sense\nintroduction/obsolescence. We provide a ready-to-use pipeline that allows\nextension of our approach to other target fields with only minimal adaptation."
                },
                "authors": [
                    {
                        "name": "Elisabeth Fittschen"
                    },
                    {
                        "name": "Sabrina Li"
                    },
                    {
                        "name": "Tom Lippincott"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Craig Messner"
                    }
                ],
                "author_detail": {
                    "name": "Craig Messner"
                },
                "author": "Craig Messner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06835v1",
                "updated": "2025-04-09T12:51:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    51,
                    10,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:51:10Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    51,
                    10,
                    2,
                    99,
                    0
                ],
                "title": "LVC: A Lightweight Compression Framework for Enhancing VLMs in Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVC: A Lightweight Compression Framework for Enhancing VLMs in Long\n  Video Understanding"
                },
                "summary": "Long video understanding is a complex task that requires both spatial detail\nand temporal awareness. While Vision-Language Models (VLMs) obtain frame-level\nunderstanding capabilities through multi-frame input, they suffer from\ninformation loss due to the sparse sampling strategy. In contrast, Video Large\nLanguage Models (Video-LLMs) capture temporal relationships within visual\nfeatures but are limited by the scarcity of high-quality video-text datasets.\nTo transfer long video understanding capabilities to VLMs with minimal data and\ncomputational cost, we propose Lightweight Video Compression (LVC), a novel\nmethod featuring the Query-Attention Video Compression mechanism, which\neffectively tackles the sparse sampling problem in VLMs. By training only the\nalignment layer with 10k short video-text pairs, LVC significantly enhances the\ntemporal reasoning abilities of VLMs. Extensive experiments show that LVC\nprovides consistent performance improvements across various models, including\nthe InternVL2 series and Phi-3.5-Vision. Notably, the InternVL2-40B-LVC\nachieves scores of 68.2 and 65.9 on the long video understanding benchmarks\nMLVU and Video-MME, respectively, with relative improvements of 14.6% and 7.7%.\nThe enhanced models and code will be publicly available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding is a complex task that requires both spatial detail\nand temporal awareness. While Vision-Language Models (VLMs) obtain frame-level\nunderstanding capabilities through multi-frame input, they suffer from\ninformation loss due to the sparse sampling strategy. In contrast, Video Large\nLanguage Models (Video-LLMs) capture temporal relationships within visual\nfeatures but are limited by the scarcity of high-quality video-text datasets.\nTo transfer long video understanding capabilities to VLMs with minimal data and\ncomputational cost, we propose Lightweight Video Compression (LVC), a novel\nmethod featuring the Query-Attention Video Compression mechanism, which\neffectively tackles the sparse sampling problem in VLMs. By training only the\nalignment layer with 10k short video-text pairs, LVC significantly enhances the\ntemporal reasoning abilities of VLMs. Extensive experiments show that LVC\nprovides consistent performance improvements across various models, including\nthe InternVL2 series and Phi-3.5-Vision. Notably, the InternVL2-40B-LVC\nachieves scores of 68.2 and 65.9 on the long video understanding benchmarks\nMLVU and Video-MME, respectively, with relative improvements of 14.6% and 7.7%.\nThe enhanced models and code will be publicly available soon."
                },
                "authors": [
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Haoran Wu"
                    },
                    {
                        "name": "Yiming Rong"
                    },
                    {
                        "name": "Deyang Jiang"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yunlong Zhao"
                    },
                    {
                        "name": "Shuang Xu"
                    },
                    {
                        "name": "Bo XU"
                    }
                ],
                "author_detail": {
                    "name": "Bo XU"
                },
                "author": "Bo XU",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06830v1",
                "updated": "2025-04-09T12:42:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    42,
                    58,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:42:58Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    42,
                    58,
                    2,
                    99,
                    0
                ],
                "title": "Integrated Sensing and Communications Over the Years: An Evolution\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Sensing and Communications Over the Years: An Evolution\n  Perspective"
                },
                "summary": "Integrated Sensing and Communications (ISAC) enables efficient spectrum\nutilization and reduces hardware costs for beyond 5G (B5G) and 6G networks,\nfacilitating intelligent applications that require both high-performance\ncommunication and precise sensing capabilities. This survey provides a\ncomprehensive review of the evolution of ISAC over the years. We examine the\nexpansion of the spectrum across RF and optical ISAC, highlighting the role of\nadvanced technologies, along with key challenges and synergies. We further\ndiscuss the advancements in network architecture from single-cell to multi-cell\nsystems, emphasizing the integration of collaborative sensing and interference\nmitigation strategies. Moreover, we analyze the progress from single-modal to\nmulti-modal sensing, with a focus on the integration of edge intelligence to\nenable real-time data processing, reduce latency, and enhance decision-making.\nFinally, we extensively review standardization efforts by 3GPP, IEEE, and ITU,\nexamining the transition of ISAC-related technologies and their implications\nfor the deployment of 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Sensing and Communications (ISAC) enables efficient spectrum\nutilization and reduces hardware costs for beyond 5G (B5G) and 6G networks,\nfacilitating intelligent applications that require both high-performance\ncommunication and precise sensing capabilities. This survey provides a\ncomprehensive review of the evolution of ISAC over the years. We examine the\nexpansion of the spectrum across RF and optical ISAC, highlighting the role of\nadvanced technologies, along with key challenges and synergies. We further\ndiscuss the advancements in network architecture from single-cell to multi-cell\nsystems, emphasizing the integration of collaborative sensing and interference\nmitigation strategies. Moreover, we analyze the progress from single-modal to\nmulti-modal sensing, with a focus on the integration of edge intelligence to\nenable real-time data processing, reduce latency, and enhance decision-making.\nFinally, we extensively review standardization efforts by 3GPP, IEEE, and ITU,\nexamining the transition of ISAC-related technologies and their implications\nfor the deployment of 6G networks."
                },
                "authors": [
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Yuanhao Cui"
                    },
                    {
                        "name": "Xiaowen Cao"
                    },
                    {
                        "name": "Nanchi Su"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Xiaojun Jing"
                    },
                    {
                        "name": "J. Andrew Zhang"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Christos Masouros"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01359v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01359v2",
                "updated": "2025-04-09T12:32:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    32,
                    21,
                    2,
                    99,
                    0
                ],
                "published": "2024-02-02T12:27:32Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    12,
                    27,
                    32,
                    4,
                    33,
                    0
                ],
                "title": "TESSERACT: Eliminating Experimental Bias in Malware Classification\n  across Space and Time (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TESSERACT: Eliminating Experimental Bias in Malware Classification\n  across Space and Time (Extended Version)"
                },
                "summary": "Machine learning (ML) plays a pivotal role in detecting malicious software.\nDespite the high F1-scores reported in numerous studies reaching upwards of\n0.99, the issue is not completely solved. Malware detectors often experience\nperformance decay due to constantly evolving operating systems and attack\nmethods, which can render previously learned knowledge insufficient for\naccurate decision-making on new inputs. This paper argues that commonly\nreported results are inflated due to two pervasive sources of experimental bias\nin the detection task: spatial bias caused by data distributions that are not\nrepresentative of a real-world deployment; and temporal bias caused by\nincorrect time splits of data, leading to unrealistic configurations. To\naddress these biases, we introduce a set of constraints for fair experiment\ndesign, and propose a new metric, AUT, for classifier robustness in real-world\nsettings. We additionally propose an algorithm designed to tune training data\nto enhance classifier performance. Finally, we present TESSERACT, an\nopen-source framework for realistic classifier comparison. Our evaluation\nencompasses both traditional ML and deep learning methods, examining published\nworks on an extensive Android dataset with 259,230 samples over a five-year\nspan. Additionally, we conduct case studies in the Windows PE and PDF domains.\nOur findings identify the existence of biases in previous studies and reveal\nthat significant performance enhancements are possible through appropriate,\nperiodic tuning. We explore how mitigation strategies may support in achieving\na more stable and better performance over time by employing multiple strategies\nto delay performance decay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) plays a pivotal role in detecting malicious software.\nDespite the high F1-scores reported in numerous studies reaching upwards of\n0.99, the issue is not completely solved. Malware detectors often experience\nperformance decay due to constantly evolving operating systems and attack\nmethods, which can render previously learned knowledge insufficient for\naccurate decision-making on new inputs. This paper argues that commonly\nreported results are inflated due to two pervasive sources of experimental bias\nin the detection task: spatial bias caused by data distributions that are not\nrepresentative of a real-world deployment; and temporal bias caused by\nincorrect time splits of data, leading to unrealistic configurations. To\naddress these biases, we introduce a set of constraints for fair experiment\ndesign, and propose a new metric, AUT, for classifier robustness in real-world\nsettings. We additionally propose an algorithm designed to tune training data\nto enhance classifier performance. Finally, we present TESSERACT, an\nopen-source framework for realistic classifier comparison. Our evaluation\nencompasses both traditional ML and deep learning methods, examining published\nworks on an extensive Android dataset with 259,230 samples over a five-year\nspan. Additionally, we conduct case studies in the Windows PE and PDF domains.\nOur findings identify the existence of biases in previous studies and reveal\nthat significant performance enhancements are possible through appropriate,\nperiodic tuning. We explore how mitigation strategies may support in achieving\na more stable and better performance over time by employing multiple strategies\nto delay performance decay."
                },
                "authors": [
                    {
                        "name": "Zeliang Kan"
                    },
                    {
                        "name": "Shae McFadden"
                    },
                    {
                        "name": "Daniel Arp"
                    },
                    {
                        "name": "Feargus Pendlebury"
                    },
                    {
                        "name": "Roberto Jordaney"
                    },
                    {
                        "name": "Johannes Kinder"
                    },
                    {
                        "name": "Fabio Pierazzi"
                    },
                    {
                        "name": "Lorenzo Cavallaro"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Cavallaro"
                },
                "author": "Lorenzo Cavallaro",
                "arxiv_comment": "30 pages. arXiv admin note: text overlap with arXiv:1807.07838",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01359v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01359v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06823v1",
                "updated": "2025-04-09T12:31:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    31,
                    25,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:31:25Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    31,
                    25,
                    2,
                    99,
                    0
                ],
                "title": "Open Problems and a Hypothetical Path Forward in LLM Knowledge Paradigms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Problems and a Hypothetical Path Forward in LLM Knowledge Paradigms"
                },
                "summary": "Knowledge is fundamental to the overall capabilities of Large Language Models\n(LLMs). The knowledge paradigm of a model, which dictates how it encodes and\nutilizes knowledge, significantly affects its performance. Despite the\ncontinuous development of LLMs under existing knowledge paradigms, issues\nwithin these frameworks continue to constrain model potential.\n  This blog post highlight three critical open problems limiting model\ncapabilities: (1) challenges in knowledge updating for LLMs, (2) the failure of\nreverse knowledge generalization (the reversal curse), and (3) conflicts in\ninternal knowledge. We review recent progress made in addressing these issues\nand discuss potential general solutions. Based on observations in these areas,\nwe propose a hypothetical paradigm based on Contextual Knowledge Scaling, and\nfurther outline implementation pathways that remain feasible within\ncontemporary techniques. Evidence suggests this approach holds potential to\naddress current shortcomings, serving as our vision for future model paradigms.\n  This blog post aims to provide researchers with a brief overview of progress\nin LLM knowledge systems, while provide inspiration for the development of\nnext-generation model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge is fundamental to the overall capabilities of Large Language Models\n(LLMs). The knowledge paradigm of a model, which dictates how it encodes and\nutilizes knowledge, significantly affects its performance. Despite the\ncontinuous development of LLMs under existing knowledge paradigms, issues\nwithin these frameworks continue to constrain model potential.\n  This blog post highlight three critical open problems limiting model\ncapabilities: (1) challenges in knowledge updating for LLMs, (2) the failure of\nreverse knowledge generalization (the reversal curse), and (3) conflicts in\ninternal knowledge. We review recent progress made in addressing these issues\nand discuss potential general solutions. Based on observations in these areas,\nwe propose a hypothetical paradigm based on Contextual Knowledge Scaling, and\nfurther outline implementation pathways that remain feasible within\ncontemporary techniques. Evidence suggests this approach holds potential to\naddress current shortcomings, serving as our vision for future model paradigms.\n  This blog post aims to provide researchers with a brief overview of progress\nin LLM knowledge systems, while provide inspiration for the development of\nnext-generation model architectures."
                },
                "authors": [
                    {
                        "name": "Xiaotian Ye"
                    },
                    {
                        "name": "Mengqi Zhang"
                    },
                    {
                        "name": "Shu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shu Wu"
                },
                "author": "Shu Wu",
                "arxiv_comment": "Blog post preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06799v1",
                "updated": "2025-04-09T11:45:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    45,
                    10,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T11:45:10Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    45,
                    10,
                    2,
                    99,
                    0
                ],
                "title": "Compatibility of Missing Data Handling Methods across the Stages of\n  Producing Clinical Prediction Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compatibility of Missing Data Handling Methods across the Stages of\n  Producing Clinical Prediction Models"
                },
                "summary": "Missing data is a challenge when developing, validating and deploying\nclinical prediction models (CPMs). Traditionally, decisions concerning missing\ndata handling during CPM development and validation havent accounted for\nwhether missingness is allowed at deployment. We hypothesised that the missing\ndata approach used during model development should optimise model performance\nupon deployment, whilst the approach used during model validation should yield\nunbiased predictive performance estimates upon deployment; we term this\ncompatibility. We aimed to determine which combinations of missing data\nhandling methods across the CPM life cycle are compatible. We considered\nscenarios where CPMs are intended to be deployed with missing data allowed or\nnot, and we evaluated the impact of that choice on earlier modelling decisions.\nThrough a simulation study and an empirical analysis of thoracic surgery data,\nwe compared CPMs developed and validated using combinations of complete case\nanalysis, mean imputation, single regression imputation, multiple imputation,\nand pattern sub-modelling. If planning to deploy a CPM without allowing missing\ndata, then development and validation should use multiple imputation when\nrequired. Where missingness is allowed at deployment, the same imputation\nmethod must be used during development and validation. Commonly used\ncombinations of missing data handling methods result in biased predictive\nperformance estimates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing data is a challenge when developing, validating and deploying\nclinical prediction models (CPMs). Traditionally, decisions concerning missing\ndata handling during CPM development and validation havent accounted for\nwhether missingness is allowed at deployment. We hypothesised that the missing\ndata approach used during model development should optimise model performance\nupon deployment, whilst the approach used during model validation should yield\nunbiased predictive performance estimates upon deployment; we term this\ncompatibility. We aimed to determine which combinations of missing data\nhandling methods across the CPM life cycle are compatible. We considered\nscenarios where CPMs are intended to be deployed with missing data allowed or\nnot, and we evaluated the impact of that choice on earlier modelling decisions.\nThrough a simulation study and an empirical analysis of thoracic surgery data,\nwe compared CPMs developed and validated using combinations of complete case\nanalysis, mean imputation, single regression imputation, multiple imputation,\nand pattern sub-modelling. If planning to deploy a CPM without allowing missing\ndata, then development and validation should use multiple imputation when\nrequired. Where missingness is allowed at deployment, the same imputation\nmethod must be used during development and validation. Commonly used\ncombinations of missing data handling methods result in biased predictive\nperformance estimates."
                },
                "authors": [
                    {
                        "name": "Antonia Tsvetanova"
                    },
                    {
                        "name": "Matthew Sperrin"
                    },
                    {
                        "name": "David A. Jenkins"
                    },
                    {
                        "name": "Niels Peek"
                    },
                    {
                        "name": "Iain Buchan"
                    },
                    {
                        "name": "Stephanie Hyland"
                    },
                    {
                        "name": "Marcus Taylor"
                    },
                    {
                        "name": "Angela Wood"
                    },
                    {
                        "name": "Richard D. Riley"
                    },
                    {
                        "name": "Glen P. Martin"
                    }
                ],
                "author_detail": {
                    "name": "Glen P. Martin"
                },
                "author": "Glen P. Martin",
                "arxiv_comment": "40 pages, 8 figures (10 supplementary figures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06785v1",
                "updated": "2025-04-09T11:19:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    19,
                    17,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T11:19:17Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    19,
                    17,
                    2,
                    99,
                    0
                ],
                "title": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement\n  Monitoring"
                },
                "summary": "Effective and rapid evaluation of pavement surface condition is critical for\nprioritizing maintenance, ensuring transportation safety, and minimizing\nvehicle wear and tear. While conventional manual inspections suffer from\nsubjectivity, existing machine learning-based methods are constrained by their\nreliance on large and high-quality labeled datasets, which require significant\nresources and limit adaptability across varied road conditions. The\nrevolutionary advancements in Large Language Models (LLMs) present significant\npotential for overcoming these challenges. In this study, we propose an\ninnovative automated zero-shot learning approach that leverages the image\nrecognition and natural language understanding capabilities of LLMs to assess\nroad conditions effectively. Multiple LLM-based assessment models were\ndeveloped, employing prompt engineering strategies aligned with the Pavement\nSurface Condition Index (PSCI) standards. These models' accuracy and\nreliability were evaluated against official PSCI results, with an optimized\nmodel ultimately selected. Extensive tests benchmarked the optimized model\nagainst evaluations from various levels experts using Google Street View road\nimages. The results reveal that the LLM-based approach can effectively assess\nroad conditions, with the optimized model -employing comprehensive and\nstructured prompt engineering strategies -outperforming simpler configurations\nby achieving high accuracy and consistency, even surpassing expert evaluations.\nMoreover, successfully applying the optimized model to Google Street View\nimages demonstrates its potential for future city-scale deployments. These\nfindings highlight the transformative potential of LLMs in automating road\ndamage evaluations and underscore the pivotal role of detailed prompt\nengineering in achieving reliable assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and rapid evaluation of pavement surface condition is critical for\nprioritizing maintenance, ensuring transportation safety, and minimizing\nvehicle wear and tear. While conventional manual inspections suffer from\nsubjectivity, existing machine learning-based methods are constrained by their\nreliance on large and high-quality labeled datasets, which require significant\nresources and limit adaptability across varied road conditions. The\nrevolutionary advancements in Large Language Models (LLMs) present significant\npotential for overcoming these challenges. In this study, we propose an\ninnovative automated zero-shot learning approach that leverages the image\nrecognition and natural language understanding capabilities of LLMs to assess\nroad conditions effectively. Multiple LLM-based assessment models were\ndeveloped, employing prompt engineering strategies aligned with the Pavement\nSurface Condition Index (PSCI) standards. These models' accuracy and\nreliability were evaluated against official PSCI results, with an optimized\nmodel ultimately selected. Extensive tests benchmarked the optimized model\nagainst evaluations from various levels experts using Google Street View road\nimages. The results reveal that the LLM-based approach can effectively assess\nroad conditions, with the optimized model -employing comprehensive and\nstructured prompt engineering strategies -outperforming simpler configurations\nby achieving high accuracy and consistency, even surpassing expert evaluations.\nMoreover, successfully applying the optimized model to Google Street View\nimages demonstrates its potential for future city-scale deployments. These\nfindings highlight the transformative potential of LLMs in automating road\ndamage evaluations and underscore the pivotal role of detailed prompt\nengineering in achieving reliable assessments."
                },
                "authors": [
                    {
                        "name": "Shuoshuo Xu"
                    },
                    {
                        "name": "Kai Zhao"
                    },
                    {
                        "name": "James Loney"
                    },
                    {
                        "name": "Zili Li"
                    },
                    {
                        "name": "Andrea Visentin"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Visentin"
                },
                "author": "Andrea Visentin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19217v2",
                "updated": "2025-04-09T11:06:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    6,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-02-26T15:19:52Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    19,
                    52,
                    2,
                    57,
                    0
                ],
                "title": "A Lightweight and Extensible Cell Segmentation and Classification Model\n  for Whole Slide Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight and Extensible Cell Segmentation and Classification Model\n  for Whole Slide Images"
                },
                "summary": "Developing clinically useful cell-level analysis tools in digital pathology\nremains challenging due to limitations in dataset granularity, inconsistent\nannotations, high computational demands, and difficulties integrating new\ntechnologies into workflows. To address these issues, we propose a solution\nthat enhances data quality, model performance, and usability by creating a\nlightweight, extensible cell segmentation and classification model. First, we\nupdate data labels through cross-relabeling to refine annotations of PanNuke\nand MoNuSAC, producing a unified dataset with seven distinct cell types.\nSecond, we leverage the H-Optimus foundation model as a fixed encoder to\nimprove feature representation for simultaneous segmentation and classification\ntasks. Third, to address foundation models' computational demands, we distill\nknowledge to reduce model size and complexity while maintaining comparable\nperformance. Finally, we integrate the distilled model into QuPath, a widely\nused open-source digital pathology platform. Results demonstrate improved\nsegmentation and classification performance using the H-Optimus-based model\ncompared to a CNN-based model. Specifically, average $R^2$ improved from 0.575\nto 0.871, and average $PQ$ score improved from 0.450 to 0.492, indicating\nbetter alignment with actual cell counts and enhanced segmentation quality. The\ndistilled model maintains comparable performance while reducing parameter count\nby a factor of 48. By reducing computational complexity and integrating into\nworkflows, this approach may significantly impact diagnostics, reduce\npathologist workload, and improve outcomes. Although the method shows promise,\nextensive validation is necessary prior to clinical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing clinically useful cell-level analysis tools in digital pathology\nremains challenging due to limitations in dataset granularity, inconsistent\nannotations, high computational demands, and difficulties integrating new\ntechnologies into workflows. To address these issues, we propose a solution\nthat enhances data quality, model performance, and usability by creating a\nlightweight, extensible cell segmentation and classification model. First, we\nupdate data labels through cross-relabeling to refine annotations of PanNuke\nand MoNuSAC, producing a unified dataset with seven distinct cell types.\nSecond, we leverage the H-Optimus foundation model as a fixed encoder to\nimprove feature representation for simultaneous segmentation and classification\ntasks. Third, to address foundation models' computational demands, we distill\nknowledge to reduce model size and complexity while maintaining comparable\nperformance. Finally, we integrate the distilled model into QuPath, a widely\nused open-source digital pathology platform. Results demonstrate improved\nsegmentation and classification performance using the H-Optimus-based model\ncompared to a CNN-based model. Specifically, average $R^2$ improved from 0.575\nto 0.871, and average $PQ$ score improved from 0.450 to 0.492, indicating\nbetter alignment with actual cell counts and enhanced segmentation quality. The\ndistilled model maintains comparable performance while reducing parameter count\nby a factor of 48. By reducing computational complexity and integrating into\nworkflows, this approach may significantly impact diagnostics, reduce\npathologist workload, and improve outcomes. Although the method shows promise,\nextensive validation is necessary prior to clinical deployment."
                },
                "authors": [
                    {
                        "name": "Nikita Shvetsov"
                    },
                    {
                        "name": "Thomas K. Kilvaer"
                    },
                    {
                        "name": "Masoud Tafavvoghi"
                    },
                    {
                        "name": "Anders Sildnes"
                    },
                    {
                        "name": "Kajsa Møllersen"
                    },
                    {
                        "name": "Lill-Tove Rasmussen Busund"
                    },
                    {
                        "name": "Lars Ailo Bongo"
                    }
                ],
                "author_detail": {
                    "name": "Lars Ailo Bongo"
                },
                "author": "Lars Ailo Bongo",
                "arxiv_comment": "30 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.6; I.4.9; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06772v1",
                "updated": "2025-04-09T10:53:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    53,
                    3,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T10:53:03Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    53,
                    3,
                    2,
                    99,
                    0
                ],
                "title": "Towards Efficient Roadside LiDAR Deployment: A Fast Surrogate Metric\n  Based on Entropy-Guided Visibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Roadside LiDAR Deployment: A Fast Surrogate Metric\n  Based on Entropy-Guided Visibility"
                },
                "summary": "The deployment of roadside LiDAR sensors plays a crucial role in the\ndevelopment of Cooperative Intelligent Transport Systems (C-ITS). However, the\nhigh cost of LiDAR sensors necessitates efficient placement strategies to\nmaximize detection performance. Traditional roadside LiDAR deployment methods\nrely on expert insight, making them time-consuming. Automating this process,\nhowever, demands extensive computation, as it requires not only visibility\nevaluation but also assessing detection performance across different LiDAR\nplacements. To address this challenge, we propose a fast surrogate metric, the\nEntropy-Guided Visibility Score (EGVS), based on information gain to evaluate\nobject detection performance in roadside LiDAR configurations. EGVS leverages\nTraffic Probabilistic Occupancy Grids (TPOG) to prioritize critical areas and\nemploys entropy-based calculations to quantify the information captured by\nLiDAR beams. This eliminates the need for direct detection performance\nevaluation, which typically requires extensive labeling and computational\nresources. By integrating EGVS into the optimization process, we significantly\naccelerate the search for optimal LiDAR configurations. Experimental results\nusing the AWSIM simulator demonstrate that EGVS strongly correlates with\nAverage Precision (AP) scores and effectively predicts object detection\nperformance. This approach offers a computationally efficient solution for\nroadside LiDAR deployment, facilitating scalable smart infrastructure\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of roadside LiDAR sensors plays a crucial role in the\ndevelopment of Cooperative Intelligent Transport Systems (C-ITS). However, the\nhigh cost of LiDAR sensors necessitates efficient placement strategies to\nmaximize detection performance. Traditional roadside LiDAR deployment methods\nrely on expert insight, making them time-consuming. Automating this process,\nhowever, demands extensive computation, as it requires not only visibility\nevaluation but also assessing detection performance across different LiDAR\nplacements. To address this challenge, we propose a fast surrogate metric, the\nEntropy-Guided Visibility Score (EGVS), based on information gain to evaluate\nobject detection performance in roadside LiDAR configurations. EGVS leverages\nTraffic Probabilistic Occupancy Grids (TPOG) to prioritize critical areas and\nemploys entropy-based calculations to quantify the information captured by\nLiDAR beams. This eliminates the need for direct detection performance\nevaluation, which typically requires extensive labeling and computational\nresources. By integrating EGVS into the optimization process, we significantly\naccelerate the search for optimal LiDAR configurations. Experimental results\nusing the AWSIM simulator demonstrate that EGVS strongly correlates with\nAverage Precision (AP) scores and effectively predicts object detection\nperformance. This approach offers a computationally efficient solution for\nroadside LiDAR deployment, facilitating scalable smart infrastructure\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Yuze Jiang"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Hiroshi Esaki"
                    }
                ],
                "author_detail": {
                    "name": "Hiroshi Esaki"
                },
                "author": "Hiroshi Esaki",
                "arxiv_comment": "Accepted by IEEE Intelligent Vehicles Symposium (IV 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06766v1",
                "updated": "2025-04-09T10:42:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    42,
                    36,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T10:42:36Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    42,
                    36,
                    2,
                    99,
                    0
                ],
                "title": "FamilyTool: A Multi-hop Personalized Tool Use Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FamilyTool: A Multi-hop Personalized Tool Use Benchmark"
                },
                "summary": "The integration of tool learning with Large Language Models (LLMs) has\nexpanded their capabilities in handling complex tasks by leveraging external\ntools. However, existing benchmarks for tool learning inadequately address\ncritical real-world personalized scenarios, particularly those requiring\nmulti-hop reasoning and inductive knowledge adaptation in dynamic environments.\nTo bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a\nfamily-based knowledge graph (KG) that simulates personalized, multi-hop tool\nuse scenarios. FamilyTool challenges LLMs with queries spanning 1 to 3\nrelational hops (e.g., inferring familial connections and preferences) and\nincorporates an inductive KG setting where models must adapt to unseen user\npreferences and relationships without re-training, a common limitation in prior\napproaches that compromises generalization. We further propose KGETool: a\nsimple KG-augmented evaluation pipeline to systematically assess LLMs' tool use\nability in these settings. Experiments reveal significant performance gaps in\nstate-of-the-art LLMs, with accuracy dropping sharply as hop complexity\nincreases and inductive scenarios exposing severe generalization deficits.\nThese findings underscore the limitations of current LLMs in handling\npersonalized, evolving real-world contexts and highlight the urgent need for\nadvancements in tool-learning frameworks. FamilyTool serves as a critical\nresource for evaluating and advancing LLM agents' reasoning, adaptability, and\nscalability in complex, dynamic environments. Code and dataset are available at\nGithub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of tool learning with Large Language Models (LLMs) has\nexpanded their capabilities in handling complex tasks by leveraging external\ntools. However, existing benchmarks for tool learning inadequately address\ncritical real-world personalized scenarios, particularly those requiring\nmulti-hop reasoning and inductive knowledge adaptation in dynamic environments.\nTo bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a\nfamily-based knowledge graph (KG) that simulates personalized, multi-hop tool\nuse scenarios. FamilyTool challenges LLMs with queries spanning 1 to 3\nrelational hops (e.g., inferring familial connections and preferences) and\nincorporates an inductive KG setting where models must adapt to unseen user\npreferences and relationships without re-training, a common limitation in prior\napproaches that compromises generalization. We further propose KGETool: a\nsimple KG-augmented evaluation pipeline to systematically assess LLMs' tool use\nability in these settings. Experiments reveal significant performance gaps in\nstate-of-the-art LLMs, with accuracy dropping sharply as hop complexity\nincreases and inductive scenarios exposing severe generalization deficits.\nThese findings underscore the limitations of current LLMs in handling\npersonalized, evolving real-world contexts and highlight the urgent need for\nadvancements in tool-learning frameworks. FamilyTool serves as a critical\nresource for evaluating and advancing LLM agents' reasoning, adaptability, and\nscalability in complex, dynamic environments. Code and dataset are available at\nGithub."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Yiran Guo"
                    },
                    {
                        "name": "Yining Zheng"
                    },
                    {
                        "name": "Zhangyue Yin"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jiajun Chen"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17975v2",
                "updated": "2025-04-09T10:21:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    21,
                    7,
                    2,
                    99,
                    0
                ],
                "published": "2024-04-27T18:28:10Z",
                "published_parsed": [
                    2024,
                    4,
                    27,
                    18,
                    28,
                    10,
                    5,
                    118,
                    0
                ],
                "title": "Automating Customer Needs Analysis: A Comparative Study of Large\n  Language Models in the Travel Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Customer Needs Analysis: A Comparative Study of Large\n  Language Models in the Travel Industry"
                },
                "summary": "In the rapidly evolving landscape of Natural Language Processing (NLP), Large\nLanguage Models (LLMs) have emerged as powerful tools for many tasks, such as\nextracting valuable insights from vast amounts of textual data. In this study,\nwe conduct a comparative analysis of LLMs for the extraction of travel customer\nneeds from TripAdvisor and Reddit posts. Leveraging a diverse range of models,\nincluding both open-source and proprietary ones such as GPT-4 and Gemini, we\naim to elucidate their strengths and weaknesses in this specialized domain.\nThrough an evaluation process involving metrics such as BERTScore, ROUGE, and\nBLEU, we assess the performance of each model in accurately identifying and\nsummarizing customer needs. Our findings highlight the efficacy of opensource\nLLMs, particularly Mistral 7B, in achieving comparable performance to larger\nclosed models while offering affordability and customization benefits.\nAdditionally, we underscore the importance of considering factors such as model\nsize, resource requirements, and performance metrics when selecting the most\nsuitable LLM for customer needs analysis tasks. Overall, this study contributes\nvaluable insights for businesses seeking to leverage advanced NLP techniques to\nenhance customer experience and drive operational efficiency in the travel\nindustry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving landscape of Natural Language Processing (NLP), Large\nLanguage Models (LLMs) have emerged as powerful tools for many tasks, such as\nextracting valuable insights from vast amounts of textual data. In this study,\nwe conduct a comparative analysis of LLMs for the extraction of travel customer\nneeds from TripAdvisor and Reddit posts. Leveraging a diverse range of models,\nincluding both open-source and proprietary ones such as GPT-4 and Gemini, we\naim to elucidate their strengths and weaknesses in this specialized domain.\nThrough an evaluation process involving metrics such as BERTScore, ROUGE, and\nBLEU, we assess the performance of each model in accurately identifying and\nsummarizing customer needs. Our findings highlight the efficacy of opensource\nLLMs, particularly Mistral 7B, in achieving comparable performance to larger\nclosed models while offering affordability and customization benefits.\nAdditionally, we underscore the importance of considering factors such as model\nsize, resource requirements, and performance metrics when selecting the most\nsuitable LLM for customer needs analysis tasks. Overall, this study contributes\nvaluable insights for businesses seeking to leverage advanced NLP techniques to\nenhance customer experience and drive operational efficiency in the travel\nindustry."
                },
                "authors": [
                    {
                        "name": "Simone Barandoni"
                    },
                    {
                        "name": "Filippo Chiarello"
                    },
                    {
                        "name": "Lorenzo Cascone"
                    },
                    {
                        "name": "Emiliano Marrale"
                    },
                    {
                        "name": "Salvatore Puccio"
                    }
                ],
                "author_detail": {
                    "name": "Salvatore Puccio"
                },
                "author": "Salvatore Puccio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06748v1",
                "updated": "2025-04-09T10:09:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    9,
                    29,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T10:09:29Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    9,
                    29,
                    2,
                    99,
                    0
                ],
                "title": "Efficient Deployment of Spiking Neural Networks on SpiNNaker2 for DVS\n  Gesture Recognition Using Neuromorphic Intermediate Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Deployment of Spiking Neural Networks on SpiNNaker2 for DVS\n  Gesture Recognition Using Neuromorphic Intermediate Representation"
                },
                "summary": "Spiking Neural Networks (SNNs) are highly energy-efficient during inference,\nmaking them particularly suitable for deployment on neuromorphic hardware.\nTheir ability to process event-driven inputs, such as data from dynamic vision\nsensors (DVS), further enhances their applicability to edge computing tasks.\nHowever, the resource constraints of edge hardware necessitate techniques like\nweight quantization, which reduce the memory footprint of SNNs while preserving\naccuracy. Despite its importance, existing quantization methods typically focus\non synaptic weights quantization without taking account of other critical\nparameters, such as scaling neuron firing thresholds.\n  To address this limitation, we present the first benchmark for the DVS\ngesture recognition task using SNNs optimized for the many-core neuromorphic\nchip SpiNNaker2. Our study evaluates two quantization pipelines for fixed-point\ncomputations. The first approach employs post training quantization (PTQ) with\npercentile-based threshold scaling, while the second uses quantization aware\ntraining (QAT) with adaptive threshold scaling. Both methods achieve accurate\n8-bit on-chip inference, closely approximating 32-bit floating-point\nperformance. Additionally, our baseline SNNs perform competitively against\npreviously reported results without specialized techniques. These models are\ndeployed on SpiNNaker2 using the neuromorphic intermediate representation\n(NIR). Ultimately, we achieve 94.13% classification accuracy on-chip,\ndemonstrating the SpiNNaker2's potential for efficient, low-energy neuromorphic\ncomputing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) are highly energy-efficient during inference,\nmaking them particularly suitable for deployment on neuromorphic hardware.\nTheir ability to process event-driven inputs, such as data from dynamic vision\nsensors (DVS), further enhances their applicability to edge computing tasks.\nHowever, the resource constraints of edge hardware necessitate techniques like\nweight quantization, which reduce the memory footprint of SNNs while preserving\naccuracy. Despite its importance, existing quantization methods typically focus\non synaptic weights quantization without taking account of other critical\nparameters, such as scaling neuron firing thresholds.\n  To address this limitation, we present the first benchmark for the DVS\ngesture recognition task using SNNs optimized for the many-core neuromorphic\nchip SpiNNaker2. Our study evaluates two quantization pipelines for fixed-point\ncomputations. The first approach employs post training quantization (PTQ) with\npercentile-based threshold scaling, while the second uses quantization aware\ntraining (QAT) with adaptive threshold scaling. Both methods achieve accurate\n8-bit on-chip inference, closely approximating 32-bit floating-point\nperformance. Additionally, our baseline SNNs perform competitively against\npreviously reported results without specialized techniques. These models are\ndeployed on SpiNNaker2 using the neuromorphic intermediate representation\n(NIR). Ultimately, we achieve 94.13% classification accuracy on-chip,\ndemonstrating the SpiNNaker2's potential for efficient, low-energy neuromorphic\ncomputing."
                },
                "authors": [
                    {
                        "name": "Sirine Arfa"
                    },
                    {
                        "name": "Bernhard Vogginger"
                    },
                    {
                        "name": "Chen Liu"
                    },
                    {
                        "name": "Johannes Partzsch"
                    },
                    {
                        "name": "Mark Schone"
                    },
                    {
                        "name": "Christian Mayr"
                    }
                ],
                "author_detail": {
                    "name": "Christian Mayr"
                },
                "author": "Christian Mayr",
                "arxiv_comment": "8 pages, 3 figures, 8 tables, Conference-2025 Neuro Inspired\n  Computational Elements (NICE)",
                "arxiv_journal_ref": "2025 Neuro Inspired Computational Elements (NICE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06742v2",
                "updated": "2025-04-10T07:04:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    7,
                    4,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-09T09:53:39Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    53,
                    39,
                    2,
                    99,
                    0
                ],
                "title": "nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection"
                },
                "summary": "Landmark detection plays a crucial role in medical imaging tasks that rely on\nprecise spatial localization, including specific applications in diagnosis,\ntreatment planning, image registration, and surgical navigation. However,\nmanual annotation is labor-intensive and requires expert knowledge. While deep\nlearning shows promise in automating this task, progress is hindered by limited\npublic datasets, inconsistent benchmarks, and non-standardized baselines,\nrestricting reproducibility, fair comparisons, and model generalizability. This\nwork introduces nnLandmark, a self-configuring deep learning framework for 3D\nmedical landmark detection, adapting nnU-Net to perform heatmap-based\nregression. By leveraging nnU-Net's automated configuration, nnLandmark\neliminates the need for manual parameter tuning, offering out-of-the-box\nusability. It achieves state-of-the-art accuracy across two public datasets,\nwith a mean radial error (MRE) of 1.5 mm on the Mandibular Molar Landmark (MML)\ndental CT dataset and 1.2 mm for anatomical fiducials on a brain MRI dataset\n(AFIDs), where nnLandmark aligns with the inter-rater variability of 1.5 mm.\nWith its strong generalization, reproducibility, and ease of deployment,\nnnLandmark establishes a reliable baseline for 3D landmark detection,\nsupporting research in anatomical localization and clinical workflows that\ndepend on precise landmark identification. The code will be available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Landmark detection plays a crucial role in medical imaging tasks that rely on\nprecise spatial localization, including specific applications in diagnosis,\ntreatment planning, image registration, and surgical navigation. However,\nmanual annotation is labor-intensive and requires expert knowledge. While deep\nlearning shows promise in automating this task, progress is hindered by limited\npublic datasets, inconsistent benchmarks, and non-standardized baselines,\nrestricting reproducibility, fair comparisons, and model generalizability. This\nwork introduces nnLandmark, a self-configuring deep learning framework for 3D\nmedical landmark detection, adapting nnU-Net to perform heatmap-based\nregression. By leveraging nnU-Net's automated configuration, nnLandmark\neliminates the need for manual parameter tuning, offering out-of-the-box\nusability. It achieves state-of-the-art accuracy across two public datasets,\nwith a mean radial error (MRE) of 1.5 mm on the Mandibular Molar Landmark (MML)\ndental CT dataset and 1.2 mm for anatomical fiducials on a brain MRI dataset\n(AFIDs), where nnLandmark aligns with the inter-rater variability of 1.5 mm.\nWith its strong generalization, reproducibility, and ease of deployment,\nnnLandmark establishes a reliable baseline for 3D landmark detection,\nsupporting research in anatomical localization and clinical workflows that\ndepend on precise landmark identification. The code will be available soon."
                },
                "authors": [
                    {
                        "name": "Alexandra Ertl"
                    },
                    {
                        "name": "Shuhan Xiao"
                    },
                    {
                        "name": "Stefan Denner"
                    },
                    {
                        "name": "Robin Peretzke"
                    },
                    {
                        "name": "David Zimmerer"
                    },
                    {
                        "name": "Peter Neher"
                    },
                    {
                        "name": "Fabian Isensee"
                    },
                    {
                        "name": "Klaus Maier-Hein"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Maier-Hein"
                },
                "author": "Klaus Maier-Hein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24206v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24206v2",
                "updated": "2025-04-09T09:45:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    45,
                    55,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-31T15:24:05Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    15,
                    24,
                    5,
                    0,
                    90,
                    0
                ],
                "title": "Synthetic News Generation for Fake News Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic News Generation for Fake News Classification"
                },
                "summary": "This study explores the generation and evaluation of synthetic fake news\nthrough fact based manipulations using large language models (LLMs). We\nintroduce a novel methodology that extracts key facts from real articles,\nmodifies them, and regenerates content to simulate fake news while maintaining\ncoherence. To assess the quality of the generated content, we propose a set of\nevaluation metrics coherence, dissimilarity, and correctness. The research also\ninvestigates the application of synthetic data in fake news classification,\ncomparing traditional machine learning models with transformer based models\nsuch as BERT. Our experiments demonstrate that transformer models, especially\nBERT, effectively leverage synthetic data for fake news detection, showing\nimprovements with smaller proportions of synthetic data. Additionally, we find\nthat fact verification features, which focus on identifying factual\ninconsistencies, provide the most promising results in distinguishing synthetic\nfake news. The study highlights the potential of synthetic data to enhance fake\nnews detection systems, offering valuable insights for future research and\nsuggesting that targeted improvements in synthetic data generation can further\nstrengthen detection models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the generation and evaluation of synthetic fake news\nthrough fact based manipulations using large language models (LLMs). We\nintroduce a novel methodology that extracts key facts from real articles,\nmodifies them, and regenerates content to simulate fake news while maintaining\ncoherence. To assess the quality of the generated content, we propose a set of\nevaluation metrics coherence, dissimilarity, and correctness. The research also\ninvestigates the application of synthetic data in fake news classification,\ncomparing traditional machine learning models with transformer based models\nsuch as BERT. Our experiments demonstrate that transformer models, especially\nBERT, effectively leverage synthetic data for fake news detection, showing\nimprovements with smaller proportions of synthetic data. Additionally, we find\nthat fact verification features, which focus on identifying factual\ninconsistencies, provide the most promising results in distinguishing synthetic\nfake news. The study highlights the potential of synthetic data to enhance fake\nnews detection systems, offering valuable insights for future research and\nsuggesting that targeted improvements in synthetic data generation can further\nstrengthen detection models."
                },
                "authors": [
                    {
                        "name": "Abdul Sittar"
                    },
                    {
                        "name": "Luka Golob"
                    },
                    {
                        "name": "Mateja Smiljanic"
                    }
                ],
                "author_detail": {
                    "name": "Mateja Smiljanic"
                },
                "author": "Mateja Smiljanic",
                "arxiv_comment": "One of the authors objected to submit the paper because he was not\n  aware of that and he likes to modify the paper before submitting to arXiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24206v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24206v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06727v1",
                "updated": "2025-04-09T09:32:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    32,
                    52,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T09:32:52Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    32,
                    52,
                    2,
                    99,
                    0
                ],
                "title": "A Survey of New Mid-Band/FR3 for 6G: Channel Measurement,\n  Characterization and Modeling in Outdoor Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of New Mid-Band/FR3 for 6G: Channel Measurement,\n  Characterization and Modeling in Outdoor Environment"
                },
                "summary": "The new mid-band (6-24 GHz) has attracted significant attention from both\nacademia and industry, which is the spectrum with continuous bandwidth that\ncombines the coverage benefits of low frequency with the capacity advantages of\nhigh frequency. Since outdoor environments represent the primary application\nscenario for mobile communications, this paper presents the first comprehensive\nreview and summary of multi-scenario and multi-frequency channel\ncharacteristics based on extensive outdoor new mid-band channel measurement\ndata, including UMa, UMi, and O2I. Specifically, a survey of the progress of\nthe channel characteristics is presented, such as path loss, delay spread,\nangular spread, channel sparsity, capacity and near-field spatial\nnon-stationary characteristics. Then, considering that satellite communication\nwill be an important component of future communication systems, we examine the\nimpact of clutter loss in air-ground communications. Our analysis of the\nfrequency dependence of mid-band clutter loss suggests that its impact is not\nsignificant. Additionally, given that penetration loss is frequency-dependent,\nwe summarize its variation within the FR3 band. Based on experimental results,\ncomparisons with the standard model reveal that while the 3GPP TR 38.901 model\nremains a useful reference for penetration loss in wood and glass, it shows\nsignificant deviations for concrete and glass, indicating the need for further\nrefinement. In summary, the findings of this survey provide both empirical data\nand theoretical support for the deployment of mid-band in future communication\nsystems, as well as guidance for optimizing mid-band base station deployment in\nthe outdoor environment. This survey offers the reference for improving\nstandard models and advancing channel modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new mid-band (6-24 GHz) has attracted significant attention from both\nacademia and industry, which is the spectrum with continuous bandwidth that\ncombines the coverage benefits of low frequency with the capacity advantages of\nhigh frequency. Since outdoor environments represent the primary application\nscenario for mobile communications, this paper presents the first comprehensive\nreview and summary of multi-scenario and multi-frequency channel\ncharacteristics based on extensive outdoor new mid-band channel measurement\ndata, including UMa, UMi, and O2I. Specifically, a survey of the progress of\nthe channel characteristics is presented, such as path loss, delay spread,\nangular spread, channel sparsity, capacity and near-field spatial\nnon-stationary characteristics. Then, considering that satellite communication\nwill be an important component of future communication systems, we examine the\nimpact of clutter loss in air-ground communications. Our analysis of the\nfrequency dependence of mid-band clutter loss suggests that its impact is not\nsignificant. Additionally, given that penetration loss is frequency-dependent,\nwe summarize its variation within the FR3 band. Based on experimental results,\ncomparisons with the standard model reveal that while the 3GPP TR 38.901 model\nremains a useful reference for penetration loss in wood and glass, it shows\nsignificant deviations for concrete and glass, indicating the need for further\nrefinement. In summary, the findings of this survey provide both empirical data\nand theoretical support for the deployment of mid-band in future communication\nsystems, as well as guidance for optimizing mid-band base station deployment in\nthe outdoor environment. This survey offers the reference for improving\nstandard models and advancing channel modeling."
                },
                "authors": [
                    {
                        "name": "Haiyang Miao"
                    },
                    {
                        "name": "Jianhua Zhang"
                    },
                    {
                        "name": "Pan Tang"
                    },
                    {
                        "name": "Jie Meng"
                    },
                    {
                        "name": "Qi Zhen"
                    },
                    {
                        "name": "Ximan Liu"
                    },
                    {
                        "name": "Enrui Liu"
                    },
                    {
                        "name": "Peijie Liu"
                    },
                    {
                        "name": "Lei Tian"
                    },
                    {
                        "name": "Guangyi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Guangyi Liu"
                },
                "author": "Guangyi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00850v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00850v3",
                "updated": "2025-04-09T09:09:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    11,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-30T11:16:04Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    11,
                    16,
                    4,
                    2,
                    304,
                    0
                ],
                "title": "GWQ: Gradient-Aware Weight Quantization for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GWQ: Gradient-Aware Weight Quantization for Large Language Models"
                },
                "summary": "Large language models (LLMs) show impressive performance in solving complex\nlanguage tasks. However, its large number of parameters presents significant\nchallenges for the deployment. So, compressing LLMs to low bits can enable to\ndeploy on resource-constrained devices. To address this problem, we propose\ngradient-aware weight quantization (GWQ), the first quantization approach for\nlow-bit weight quantization that leverages gradients to localize outliers,\nrequiring only a minimal amount of calibration data for outlier detection. GWQ\nretains the top 1\\% outliers preferentially at FP16 precision, while the\nremaining non-outlier weights are stored in a low-bit. We widely evaluate GWQ\non different task include language modeling, grounding detection, massive\nmultitask language understanding and vision-language question and answering.\nResults show that models quantified by GWQ performs better than other\nquantization method. During quantization process, GWQ only need one calibration\nset to realize effective quant. Also, GWQ achieves 1.2x inference speedup in\ncomparison to the original model and effectively reduces the inference memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show impressive performance in solving complex\nlanguage tasks. However, its large number of parameters presents significant\nchallenges for the deployment. So, compressing LLMs to low bits can enable to\ndeploy on resource-constrained devices. To address this problem, we propose\ngradient-aware weight quantization (GWQ), the first quantization approach for\nlow-bit weight quantization that leverages gradients to localize outliers,\nrequiring only a minimal amount of calibration data for outlier detection. GWQ\nretains the top 1\\% outliers preferentially at FP16 precision, while the\nremaining non-outlier weights are stored in a low-bit. We widely evaluate GWQ\non different task include language modeling, grounding detection, massive\nmultitask language understanding and vision-language question and answering.\nResults show that models quantified by GWQ performs better than other\nquantization method. During quantization process, GWQ only need one calibration\nset to realize effective quant. Also, GWQ achieves 1.2x inference speedup in\ncomparison to the original model and effectively reduces the inference memory."
                },
                "authors": [
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Haiyang Liu"
                    },
                    {
                        "name": "Zijian Ling"
                    },
                    {
                        "name": "Minxi Yan"
                    },
                    {
                        "name": "Ziyang Yan"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Jingcai Guo"
                    },
                    {
                        "name": "Ling Shao"
                    },
                    {
                        "name": "Hao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Tang"
                },
                "author": "Hao Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00850v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12946v2",
                "updated": "2025-04-09T08:59:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    59,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2024-11-20T00:31:23Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    31,
                    23,
                    2,
                    325,
                    0
                ],
                "title": "A Flexible Large Language Models Guardrail Development Methodology\n  Applied to Off-Topic Prompt Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Flexible Large Language Models Guardrail Development Methodology\n  Applied to Off-Topic Prompt Detection"
                },
                "summary": "Large Language Models (LLMs) are prone to off-topic misuse, where users may\nprompt these models to perform tasks beyond their intended scope. Current\nguardrails, which often rely on curated examples or custom classifiers, suffer\nfrom high false-positive rates, limited adaptability, and the impracticality of\nrequiring real-world data that is not available in pre-production. In this\npaper, we introduce a flexible, data-free guardrail development methodology\nthat addresses these challenges. By thoroughly defining the problem space\nqualitatively and passing this to an LLM to generate diverse prompts, we\nconstruct a synthetic dataset to benchmark and train off-topic guardrails that\noutperform heuristic approaches. Additionally, by framing the task as\nclassifying whether the user prompt is relevant with respect to the system\nprompt, our guardrails effectively generalize to other misuse categories,\nincluding jailbreak and harmful prompts. Lastly, we further contribute to the\nfield by open-sourcing both the synthetic dataset and the off-topic guardrail\nmodels, providing valuable resources for developing guardrails in\npre-production environments and supporting future research and development in\nLLM safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to off-topic misuse, where users may\nprompt these models to perform tasks beyond their intended scope. Current\nguardrails, which often rely on curated examples or custom classifiers, suffer\nfrom high false-positive rates, limited adaptability, and the impracticality of\nrequiring real-world data that is not available in pre-production. In this\npaper, we introduce a flexible, data-free guardrail development methodology\nthat addresses these challenges. By thoroughly defining the problem space\nqualitatively and passing this to an LLM to generate diverse prompts, we\nconstruct a synthetic dataset to benchmark and train off-topic guardrails that\noutperform heuristic approaches. Additionally, by framing the task as\nclassifying whether the user prompt is relevant with respect to the system\nprompt, our guardrails effectively generalize to other misuse categories,\nincluding jailbreak and harmful prompts. Lastly, we further contribute to the\nfield by open-sourcing both the synthetic dataset and the off-topic guardrail\nmodels, providing valuable resources for developing guardrails in\npre-production environments and supporting future research and development in\nLLM safety."
                },
                "authors": [
                    {
                        "name": "Gabriel Chua"
                    },
                    {
                        "name": "Shing Yee Chan"
                    },
                    {
                        "name": "Shaun Khoo"
                    }
                ],
                "author_detail": {
                    "name": "Shaun Khoo"
                },
                "author": "Shaun Khoo",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12848v2",
                "updated": "2025-04-09T08:38:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    38,
                    44,
                    2,
                    99,
                    0
                ],
                "published": "2024-12-17T12:22:44Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    12,
                    22,
                    44,
                    1,
                    352,
                    0
                ],
                "title": "ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical\n  Insights from Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical\n  Insights from Large Language Models"
                },
                "summary": "With the rise and widespread use of Large Language Models (LLMs), ensuring\ntheir safety is crucial to prevent harm to humans and promote ethical\nbehaviors. However, directly assessing value valence (i.e., support or oppose)\nby leveraging large-scale data training is untrustworthy and inexplainable. We\nassume that emulating humans to rely on social norms to make moral decisions\ncan help LLMs understand and predict moral judgment. However, capturing human\nvalues remains a challenge, as multiple related norms might conflict in\nspecific contexts. Consider norms that are upheld by the majority and promote\nthe well-being of society are more likely to be accepted and widely adopted\n(e.g., \"don't cheat,\"). Therefore, it is essential for LLM to identify the\nappropriate norms for a given scenario before making moral decisions. To this\nend, we introduce a novel moral judgment approach called \\textit{ClarityEthic}\nthat leverages LLMs' reasoning ability and contrastive learning to uncover\nrelevant social norms for human actions from different perspectives and select\nthe most reliable one to enhance judgment accuracy. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art approaches in moral\njudgment tasks. Moreover, human evaluations confirm that the generated social\nnorms provide plausible explanations that support the judgments. This suggests\nthat modeling human moral judgment with the emulating humans moral strategy is\npromising for improving the ethical behaviors of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise and widespread use of Large Language Models (LLMs), ensuring\ntheir safety is crucial to prevent harm to humans and promote ethical\nbehaviors. However, directly assessing value valence (i.e., support or oppose)\nby leveraging large-scale data training is untrustworthy and inexplainable. We\nassume that emulating humans to rely on social norms to make moral decisions\ncan help LLMs understand and predict moral judgment. However, capturing human\nvalues remains a challenge, as multiple related norms might conflict in\nspecific contexts. Consider norms that are upheld by the majority and promote\nthe well-being of society are more likely to be accepted and widely adopted\n(e.g., \"don't cheat,\"). Therefore, it is essential for LLM to identify the\nappropriate norms for a given scenario before making moral decisions. To this\nend, we introduce a novel moral judgment approach called \\textit{ClarityEthic}\nthat leverages LLMs' reasoning ability and contrastive learning to uncover\nrelevant social norms for human actions from different perspectives and select\nthe most reliable one to enhance judgment accuracy. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art approaches in moral\njudgment tasks. Moreover, human evaluations confirm that the generated social\nnorms provide plausible explanations that support the judgments. This suggests\nthat modeling human moral judgment with the emulating humans moral strategy is\npromising for improving the ethical behaviors of LLMs."
                },
                "authors": [
                    {
                        "name": "Yuxi Sun"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenxuan Zhang"
                },
                "author": "Wenxuan Zhang",
                "arxiv_comment": "We have noticed that this version of our experiment and method\n  description isn't quite complete or accurate. To make sure we present our\n  best work, we think it would be a good idea to withdraw the manuscript for\n  now and take some time to revise and reformat it",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01705v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01705v2",
                "updated": "2025-04-09T08:36:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    36,
                    10,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-03T09:04:45Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    4,
                    45,
                    4,
                    3,
                    0
                ],
                "title": "The Essence of Contextual Understanding in Theory of Mind: A Study on\n  Question Answering with Story Characters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Essence of Contextual Understanding in Theory of Mind: A Study on\n  Question Answering with Story Characters"
                },
                "summary": "Theory-of-Mind (ToM) is a fundamental psychological capability that allows\nhumans to understand and interpret the mental states of others. Humans infer\nothers' thoughts by integrating causal cues and indirect clues from broad\ncontextual information, often derived from past interactions. In other words,\nhuman ToM heavily relies on the understanding about the backgrounds and life\nstories of others. Unfortunately, this aspect is largely overlooked in existing\nbenchmarks for evaluating machines' ToM capabilities, due to their usage of\nshort narratives without global context, especially personal background of\ncharacters. In this paper, we verify the importance of comprehensive contextual\nunderstanding about personal backgrounds in ToM and assess the performance of\nLLMs in such complex scenarios. To achieve this, we introduce CharToM\nbenchmark, comprising 1,035 ToM questions based on characters from classic\nnovels. Our human study reveals a significant disparity in performance: the\nsame group of educated participants performs dramatically better when they have\nread the novels compared to when they have not. In parallel, our experiments on\nstate-of-the-art LLMs, including the very recent o1 and DeepSeek-R1 models,\nshow that LLMs still perform notably worse than humans, despite that they have\nseen these stories during pre-training. This highlights the limitations of\ncurrent LLMs in capturing the nuanced contextual information required for ToM\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory-of-Mind (ToM) is a fundamental psychological capability that allows\nhumans to understand and interpret the mental states of others. Humans infer\nothers' thoughts by integrating causal cues and indirect clues from broad\ncontextual information, often derived from past interactions. In other words,\nhuman ToM heavily relies on the understanding about the backgrounds and life\nstories of others. Unfortunately, this aspect is largely overlooked in existing\nbenchmarks for evaluating machines' ToM capabilities, due to their usage of\nshort narratives without global context, especially personal background of\ncharacters. In this paper, we verify the importance of comprehensive contextual\nunderstanding about personal backgrounds in ToM and assess the performance of\nLLMs in such complex scenarios. To achieve this, we introduce CharToM\nbenchmark, comprising 1,035 ToM questions based on characters from classic\nnovels. Our human study reveals a significant disparity in performance: the\nsame group of educated participants performs dramatically better when they have\nread the novels compared to when they have not. In parallel, our experiments on\nstate-of-the-art LLMs, including the very recent o1 and DeepSeek-R1 models,\nshow that LLMs still perform notably worse than humans, despite that they have\nseen these stories during pre-training. This highlights the limitations of\ncurrent LLMs in capturing the nuanced contextual information required for ToM\nreasoning."
                },
                "authors": [
                    {
                        "name": "Chulun Zhou"
                    },
                    {
                        "name": "Qiujing Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Xiaoqian Yue"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Jiangnan Li"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Shunchi Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01705v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01705v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02514v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02514v3",
                "updated": "2025-04-09T08:33:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    33,
                    54,
                    2,
                    99,
                    0
                ],
                "published": "2025-02-04T17:33:08Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    33,
                    8,
                    1,
                    35,
                    0
                ],
                "title": "Privacy Attacks on Image AutoRegressive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Attacks on Image AutoRegressive Models"
                },
                "summary": "Image autoregressive generation has emerged as a powerful new paradigm, with\nimage autoregressive models (IARs) matching state-of-the-art diffusion models\n(DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for higher\ngeneration speed. However, the privacy risks associated with IARs remain\nunexplored, raising concerns about their responsible deployment. To address\nthis gap, we conduct a comprehensive privacy analysis of IARs, comparing their\nprivacy risks to those of DMs as a reference point. Specifically, we develop a\nnovel membership inference attack (MIA) that achieves a remarkably high success\nrate in detecting training images, with a True Positive Rate at False Positive\nRate = 1% (TPR@FPR=1%) of 86.38%, compared to just 6.38% for DMs using\ncomparable attacks. We leverage our novel MIA to perform dataset inference (DI)\nfor IARs and show that it requires as few as 6 samples to detect dataset\nmembership, compared to 200 samples for DI in DMs. This confirms a higher level\nof information leakage in IARs. Finally, we are able to extract hundreds of\ntraining data points from an IAR (e.g., 698 from VAR-d30). Our results suggest\na fundamental privacy-utility trade-off: while IARs excel in image generation\nquality and speed, they are empirically significantly more vulnerable to\nprivacy attacks compared to DMs that achieve similar performance. This trend\nsuggests that incorporating techniques from DMs into IARs, such as modeling the\nper-token probability distribution using a diffusion procedure, could help\nmitigate IARs' vulnerability to privacy attacks. We make our code available at:\nhttps://github.com/sprintml/privacy_attacks_against_iars",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image autoregressive generation has emerged as a powerful new paradigm, with\nimage autoregressive models (IARs) matching state-of-the-art diffusion models\n(DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for higher\ngeneration speed. However, the privacy risks associated with IARs remain\nunexplored, raising concerns about their responsible deployment. To address\nthis gap, we conduct a comprehensive privacy analysis of IARs, comparing their\nprivacy risks to those of DMs as a reference point. Specifically, we develop a\nnovel membership inference attack (MIA) that achieves a remarkably high success\nrate in detecting training images, with a True Positive Rate at False Positive\nRate = 1% (TPR@FPR=1%) of 86.38%, compared to just 6.38% for DMs using\ncomparable attacks. We leverage our novel MIA to perform dataset inference (DI)\nfor IARs and show that it requires as few as 6 samples to detect dataset\nmembership, compared to 200 samples for DI in DMs. This confirms a higher level\nof information leakage in IARs. Finally, we are able to extract hundreds of\ntraining data points from an IAR (e.g., 698 from VAR-d30). Our results suggest\na fundamental privacy-utility trade-off: while IARs excel in image generation\nquality and speed, they are empirically significantly more vulnerable to\nprivacy attacks compared to DMs that achieve similar performance. This trend\nsuggests that incorporating techniques from DMs into IARs, such as modeling the\nper-token probability distribution using a diffusion procedure, could help\nmitigate IARs' vulnerability to privacy attacks. We make our code available at:\nhttps://github.com/sprintml/privacy_attacks_against_iars"
                },
                "authors": [
                    {
                        "name": "Antoni Kowalczuk"
                    },
                    {
                        "name": "Jan Dubiński"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Adam Dziedzic"
                    }
                ],
                "author_detail": {
                    "name": "Adam Dziedzic"
                },
                "author": "Adam Dziedzic",
                "arxiv_comment": "Code: https://github.com/sprintml/privacy_attacks_against_iars",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02514v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02514v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14026v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14026v5",
                "updated": "2025-04-09T08:23:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    23,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2024-06-20T06:46:23Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    6,
                    46,
                    23,
                    3,
                    172,
                    0
                ],
                "title": "Demystifying Language Model Forgetting with Low-rank Example\n  Associations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Language Model Forgetting with Low-rank Example\n  Associations"
                },
                "summary": "Large Language models (LLMs) suffer from forgetting of upstream data when\nfine-tuned. Despite efforts on mitigating forgetting, few have investigated\nwhether, and how forgotten upstream examples are dependent on newly learned\ntasks. Insights on such dependencies enable efficient and targeted mitigation\nof forgetting. In this paper, we empirically analyze forgetting that occurs in\n$N$ upstream examples of language modeling or instruction-tuning after\nfine-tuning LLMs on one of $M$ new tasks, visualized in $M\\times N$ matrices.\nWe show that the matrices are often well-approximated with low-rank matrices,\nindicating the dominance of simple associations between the learned tasks and\nforgotten upstream examples. Leveraging the analysis, we predict forgetting of\nupstream examples when fine-tuning on unseen tasks with matrix completion over\nthe empirical associations. This enables fast identification of most forgotten\nexamples without expensive inference on the entire upstream data. The approach,\ndespite simplicity, outperforms prior approaches that learn semantic\nrelationships of learned tasks and upstream examples with LMs for predicting\nforgetting. We demonstrate the practical utility of our analysis by showing\nstatistically significantly reduced forgetting as we upweight predicted\nexamples for replay at fine-tuning. Project page:\nhttps://inklab.usc.edu/lm-forgetting-prediction/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) suffer from forgetting of upstream data when\nfine-tuned. Despite efforts on mitigating forgetting, few have investigated\nwhether, and how forgotten upstream examples are dependent on newly learned\ntasks. Insights on such dependencies enable efficient and targeted mitigation\nof forgetting. In this paper, we empirically analyze forgetting that occurs in\n$N$ upstream examples of language modeling or instruction-tuning after\nfine-tuning LLMs on one of $M$ new tasks, visualized in $M\\times N$ matrices.\nWe show that the matrices are often well-approximated with low-rank matrices,\nindicating the dominance of simple associations between the learned tasks and\nforgotten upstream examples. Leveraging the analysis, we predict forgetting of\nupstream examples when fine-tuning on unseen tasks with matrix completion over\nthe empirical associations. This enables fast identification of most forgotten\nexamples without expensive inference on the entire upstream data. The approach,\ndespite simplicity, outperforms prior approaches that learn semantic\nrelationships of learned tasks and upstream examples with LMs for predicting\nforgetting. We demonstrate the practical utility of our analysis by showing\nstatistically significantly reduced forgetting as we upweight predicted\nexamples for replay at fine-tuning. Project page:\nhttps://inklab.usc.edu/lm-forgetting-prediction/"
                },
                "authors": [
                    {
                        "name": "Xisen Jin"
                    },
                    {
                        "name": "Xiang Ren"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Ren"
                },
                "author": "Xiang Ren",
                "arxiv_comment": "8 pages; preprint, fixed Table 5 in Appendix D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14026v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14026v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04713v2",
                "updated": "2025-04-09T08:15:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    15,
                    21,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-07T03:50:12Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    50,
                    12,
                    0,
                    97,
                    0
                ],
                "title": "Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting\n  Sequential Needles from Long Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting\n  Sequential Needles from Long Contexts"
                },
                "summary": "Evaluating the ability of large language models (LLMs) to handle extended\ncontexts is critical, particularly for retrieving information relevant to\nspecific queries embedded within lengthy inputs. We introduce Sequential-NIAH,\na benchmark specifically designed to evaluate the capability of LLMs to extract\nsequential information items (known as needles) from long contexts. The\nbenchmark comprises three types of needle generation pipelines: synthetic,\nreal, and open-domain QA. It includes contexts ranging from 8K to 128K tokens\nin length, with a dataset of 14,000 samples (2,000 reserved for testing). To\nfacilitate evaluation on this benchmark, we trained a synthetic data-driven\nevaluation model capable of evaluating answer correctness based on\nchronological or logical order, achieving an accuracy of 99.49% on synthetic\ntest data. We conducted experiments on six well-known LLMs, revealing that even\nthe best-performing model achieved a maximum accuracy of only 63.15%. Further\nanalysis highlights the growing challenges posed by increasing context lengths\nand the number of needles, underscoring substantial room for improvement.\nAdditionally, noise robustness experiments validate the reliability of the\nbenchmark, making Sequential-NIAH an important reference for advancing research\non long text extraction capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the ability of large language models (LLMs) to handle extended\ncontexts is critical, particularly for retrieving information relevant to\nspecific queries embedded within lengthy inputs. We introduce Sequential-NIAH,\na benchmark specifically designed to evaluate the capability of LLMs to extract\nsequential information items (known as needles) from long contexts. The\nbenchmark comprises three types of needle generation pipelines: synthetic,\nreal, and open-domain QA. It includes contexts ranging from 8K to 128K tokens\nin length, with a dataset of 14,000 samples (2,000 reserved for testing). To\nfacilitate evaluation on this benchmark, we trained a synthetic data-driven\nevaluation model capable of evaluating answer correctness based on\nchronological or logical order, achieving an accuracy of 99.49% on synthetic\ntest data. We conducted experiments on six well-known LLMs, revealing that even\nthe best-performing model achieved a maximum accuracy of only 63.15%. Further\nanalysis highlights the growing challenges posed by increasing context lengths\nand the number of needles, underscoring substantial room for improvement.\nAdditionally, noise robustness experiments validate the reliability of the\nbenchmark, making Sequential-NIAH an important reference for advancing research\non long text extraction capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Yifei Yu"
                    },
                    {
                        "name": "Qian-Wen Zhang"
                    },
                    {
                        "name": "Lingfeng Qiao"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Fang Li"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Zengxi Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Xiaolong Liang"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06667v1",
                "updated": "2025-04-09T08:08:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    8,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T08:08:16Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    8,
                    16,
                    2,
                    99,
                    0
                ],
                "title": "Toward Holistic Evaluation of Recommender Systems Powered by Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Holistic Evaluation of Recommender Systems Powered by Generative\n  Models"
                },
                "summary": "Recommender systems powered by generative models (Gen-RecSys) extend beyond\nclassical item ranking by producing open-ended content, which simultaneously\nunlocks richer user experiences and introduces new risks. On one hand, these\nsystems can enhance personalization and appeal through dynamic explanations and\nmulti-turn dialogues. On the other hand, they might venture into unknown\nterritory-hallucinating nonexistent items, amplifying bias, or leaking private\ninformation. Traditional accuracy metrics cannot fully capture these\nchallenges, as they fail to measure factual correctness, content safety, or\nalignment with user intent.\n  This paper makes two main contributions. First, we categorize the evaluation\nchallenges of Gen-RecSys into two groups: (i) existing concerns that are\nexacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new\nrisks (e.g., item hallucinations, contradictory explanations). Second, we\npropose a holistic evaluation approach that includes scenario-based assessments\nand multi-metric checks-incorporating relevance, factual grounding, bias\ndetection, and policy compliance. Our goal is to provide a guiding framework so\nresearchers and practitioners can thoroughly assess Gen-RecSys, ensuring\neffective personalization and responsible deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems powered by generative models (Gen-RecSys) extend beyond\nclassical item ranking by producing open-ended content, which simultaneously\nunlocks richer user experiences and introduces new risks. On one hand, these\nsystems can enhance personalization and appeal through dynamic explanations and\nmulti-turn dialogues. On the other hand, they might venture into unknown\nterritory-hallucinating nonexistent items, amplifying bias, or leaking private\ninformation. Traditional accuracy metrics cannot fully capture these\nchallenges, as they fail to measure factual correctness, content safety, or\nalignment with user intent.\n  This paper makes two main contributions. First, we categorize the evaluation\nchallenges of Gen-RecSys into two groups: (i) existing concerns that are\nexacerbated by generative outputs (e.g., bias, privacy) and (ii) entirely new\nrisks (e.g., item hallucinations, contradictory explanations). Second, we\npropose a holistic evaluation approach that includes scenario-based assessments\nand multi-metric checks-incorporating relevance, factual grounding, bias\ndetection, and policy compliance. Our goal is to provide a guiding framework so\nresearchers and practitioners can thoroughly assess Gen-RecSys, ensuring\neffective personalization and responsible deployment."
                },
                "authors": [
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Nikhil Mehta"
                    },
                    {
                        "name": "Maheswaran Sathiamoorthy"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Pablo Castells"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08397v2",
                "updated": "2025-04-09T08:01:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    8,
                    1,
                    55,
                    2,
                    99,
                    0
                ],
                "published": "2024-11-13T07:32:58Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    7,
                    32,
                    58,
                    2,
                    318,
                    0
                ],
                "title": "CLaSP: Learning Concepts for Time-Series Signals from Natural Language\n  Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLaSP: Learning Concepts for Time-Series Signals from Natural Language\n  Supervision"
                },
                "summary": "This paper presents CLaSP, a novel model for retrieving time-series signals\nusing natural language queries that describe signal characteristics. The\nability to search time-series signals based on descriptive queries is essential\nin domains such as industrial diagnostics, where data scientists often need to\nfind signals with specific characteristics. However, existing methods rely on\nsketch-based inputs, predefined synonym dictionaries, or domain-specific manual\ndesigns, limiting their scalability and adaptability. CLaSP addresses these\nchallenges by employing contrastive learning to map time-series signals to\nnatural language descriptions. Unlike prior approaches, it eliminates the need\nfor predefined synonym dictionaries and leverages the rich contextual knowledge\nof large language models (LLMs). Using the TRUCE and SUSHI datasets, which pair\ntime-series signals with natural language descriptions, we demonstrate that\nCLaSP achieves high accuracy in retrieving a variety of time series patterns\nbased on natural language queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents CLaSP, a novel model for retrieving time-series signals\nusing natural language queries that describe signal characteristics. The\nability to search time-series signals based on descriptive queries is essential\nin domains such as industrial diagnostics, where data scientists often need to\nfind signals with specific characteristics. However, existing methods rely on\nsketch-based inputs, predefined synonym dictionaries, or domain-specific manual\ndesigns, limiting their scalability and adaptability. CLaSP addresses these\nchallenges by employing contrastive learning to map time-series signals to\nnatural language descriptions. Unlike prior approaches, it eliminates the need\nfor predefined synonym dictionaries and leverages the rich contextual knowledge\nof large language models (LLMs). Using the TRUCE and SUSHI datasets, which pair\ntime-series signals with natural language descriptions, we demonstrate that\nCLaSP achieves high accuracy in retrieving a variety of time series patterns\nbased on natural language queries."
                },
                "authors": [
                    {
                        "name": "Aoi Ito"
                    },
                    {
                        "name": "Kota Dohi"
                    },
                    {
                        "name": "Yohei Kawaguchi"
                    }
                ],
                "author_detail": {
                    "name": "Yohei Kawaguchi"
                },
                "author": "Yohei Kawaguchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06664v1",
                "updated": "2025-04-09T07:56:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    56,
                    56,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T07:56:56Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    56,
                    56,
                    2,
                    99,
                    0
                ],
                "title": "SEE: Continual Fine-tuning with Sequential Ensemble of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEE: Continual Fine-tuning with Sequential Ensemble of Experts"
                },
                "summary": "Continual fine-tuning of large language models (LLMs) suffers from\ncatastrophic forgetting. Rehearsal-based methods mitigate this problem by\nretaining a small set of old data. Nevertheless, they still suffer inevitable\nperformance loss. Although training separate experts for each task can help\nprevent forgetting, effectively assembling them remains a challenge. Some\napproaches use routers to assign tasks to experts, but in continual learning,\nthey often require retraining for optimal performance. To address these\nchallenges, we introduce the Sequential Ensemble of Experts (SEE) framework.\nSEE removes the need for an additional router, allowing each expert to\nindependently decide whether a query should be handled. The framework employs\ndistributed routing, and during continual fine-tuning, SEE only requires the\ntraining of new experts for incoming tasks rather than retraining the entire\nsystem. Experiments reveal that the SEE outperforms prior approaches, including\nmulti-task learning, in continual fine-tuning. It also demonstrates remarkable\ngeneralization ability, as the expert can effectively identify\nout-of-distribution queries, which can then be directed to a more generalized\nmodel for resolution. This work highlights the promising potential of\nintegrating routing and response mechanisms within each expert, paving the way\nfor the future of distributed model ensembling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual fine-tuning of large language models (LLMs) suffers from\ncatastrophic forgetting. Rehearsal-based methods mitigate this problem by\nretaining a small set of old data. Nevertheless, they still suffer inevitable\nperformance loss. Although training separate experts for each task can help\nprevent forgetting, effectively assembling them remains a challenge. Some\napproaches use routers to assign tasks to experts, but in continual learning,\nthey often require retraining for optimal performance. To address these\nchallenges, we introduce the Sequential Ensemble of Experts (SEE) framework.\nSEE removes the need for an additional router, allowing each expert to\nindependently decide whether a query should be handled. The framework employs\ndistributed routing, and during continual fine-tuning, SEE only requires the\ntraining of new experts for incoming tasks rather than retraining the entire\nsystem. Experiments reveal that the SEE outperforms prior approaches, including\nmulti-task learning, in continual fine-tuning. It also demonstrates remarkable\ngeneralization ability, as the expert can effectively identify\nout-of-distribution queries, which can then be directed to a more generalized\nmodel for resolution. This work highlights the promising potential of\nintegrating routing and response mechanisms within each expert, paving the way\nfor the future of distributed model ensembling."
                },
                "authors": [
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "9pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19271v2",
                "updated": "2025-04-09T07:51:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    51,
                    49,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-25T02:05:46Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    2,
                    5,
                    46,
                    1,
                    84,
                    0
                ],
                "title": "MARS: Memory-Enhanced Agents with Reflective Self-improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARS: Memory-Enhanced Agents with Reflective Self-improvement"
                },
                "summary": "Large language models (LLMs) have made significant advances in the field of\nnatural language processing, but they still face challenges such as continuous\ndecision-making, lack of long-term memory, and limited context windows in\ndynamic environments. To address these issues, this paper proposes an\ninnovative framework Memory-Enhanced Agents with Reflective Self-improvement.\nThe MARS framework comprises three agents: the User, the Assistant, and the\nChecker. By integrating iterative feedback, reflective mechanisms, and a memory\noptimization mechanism based on the Ebbinghaus forgetting curve, it\nsignificantly enhances the agents capabilities in handling multi-tasking and\nlong-span information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made significant advances in the field of\nnatural language processing, but they still face challenges such as continuous\ndecision-making, lack of long-term memory, and limited context windows in\ndynamic environments. To address these issues, this paper proposes an\ninnovative framework Memory-Enhanced Agents with Reflective Self-improvement.\nThe MARS framework comprises three agents: the User, the Assistant, and the\nChecker. By integrating iterative feedback, reflective mechanisms, and a memory\noptimization mechanism based on the Ebbinghaus forgetting curve, it\nsignificantly enhances the agents capabilities in handling multi-tasking and\nlong-span information."
                },
                "authors": [
                    {
                        "name": "Xuechen Liang"
                    },
                    {
                        "name": "Meiling Tao"
                    },
                    {
                        "name": "Yinghui Xia"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Yijin Wang"
                    },
                    {
                        "name": "Jingsong Yang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Yuantao Wang"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Xueqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqian Wang"
                },
                "author": "Xueqian Wang",
                "arxiv_comment": "We are withdrawing this version because it duplicates our previous\n  submission (arXiv:2409.00872)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06659v1",
                "updated": "2025-04-09T07:49:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    49,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T07:49:08Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    49,
                    8,
                    2,
                    99,
                    0
                ],
                "title": "Bridging the Gap Between Preference Alignment and Machine Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Gap Between Preference Alignment and Machine Unlearning"
                },
                "summary": "Despite advances in Preference Alignment (PA) for Large Language Models\n(LLMs), mainstream methods like Reinforcement Learning with Human Feedback\n(RLHF) face notable challenges. These approaches require high-quality datasets\nof positive preference examples, which are costly to obtain and computationally\nintensive due to training instability, limiting their use in low-resource\nscenarios. LLM unlearning technique presents a promising alternative, by\ndirectly removing the influence of negative examples. However, current research\nhas primarily focused on empirical validation, lacking systematic quantitative\nanalysis. To bridge this gap, we propose a framework to explore the\nrelationship between PA and LLM unlearning. Specifically, we introduce a\nbi-level optimization-based method to quantify the impact of unlearning\nspecific negative examples on PA performance. Our analysis reveals that not all\nnegative examples contribute equally to alignment improvement when unlearned,\nand the effect varies significantly across examples. Building on this insight,\nwe pose a crucial question: how can we optimally select and weight negative\nexamples for unlearning to maximize PA performance? To answer this, we propose\na framework called Unlearning to Align (U2A), which leverages bi-level\noptimization to efficiently select and unlearn examples for optimal PA\nperformance. We validate the proposed method through extensive experiments,\nwith results confirming its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in Preference Alignment (PA) for Large Language Models\n(LLMs), mainstream methods like Reinforcement Learning with Human Feedback\n(RLHF) face notable challenges. These approaches require high-quality datasets\nof positive preference examples, which are costly to obtain and computationally\nintensive due to training instability, limiting their use in low-resource\nscenarios. LLM unlearning technique presents a promising alternative, by\ndirectly removing the influence of negative examples. However, current research\nhas primarily focused on empirical validation, lacking systematic quantitative\nanalysis. To bridge this gap, we propose a framework to explore the\nrelationship between PA and LLM unlearning. Specifically, we introduce a\nbi-level optimization-based method to quantify the impact of unlearning\nspecific negative examples on PA performance. Our analysis reveals that not all\nnegative examples contribute equally to alignment improvement when unlearned,\nand the effect varies significantly across examples. Building on this insight,\nwe pose a crucial question: how can we optimally select and weight negative\nexamples for unlearning to maximize PA performance? To answer this, we propose\na framework called Unlearning to Align (U2A), which leverages bi-level\noptimization to efficiently select and unlearn examples for optimal PA\nperformance. We validate the proposed method through extensive experiments,\nwith results confirming its effectiveness."
                },
                "authors": [
                    {
                        "name": "Xiaohua Feng"
                    },
                    {
                        "name": "Yuyuan Li"
                    },
                    {
                        "name": "Huwei Ji"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Chaochao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chaochao Chen"
                },
                "author": "Chaochao Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06658v1",
                "updated": "2025-04-09T07:48:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    48,
                    10,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T07:48:10Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    48,
                    10,
                    2,
                    99,
                    0
                ],
                "title": "A Neuro-inspired Interpretation of Unlearning in Large Language Models\n  through Sample-level Unlearning Difficulty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Neuro-inspired Interpretation of Unlearning in Large Language Models\n  through Sample-level Unlearning Difficulty"
                },
                "summary": "Driven by privacy protection laws and regulations, unlearning in Large\nLanguage Models (LLMs) is gaining increasing attention. However, current\nresearch often neglects the interpretability of the unlearning process,\nparticularly concerning sample-level unlearning difficulty. Existing studies\ntypically assume a uniform unlearning difficulty across samples. This\nsimplification risks attributing the performance of unlearning algorithms to\nsample selection rather than the algorithm's design, potentially steering the\ndevelopment of LLM unlearning in the wrong direction. Thus, we investigate the\nrelationship between LLM unlearning and sample characteristics, with a focus on\nunlearning difficulty. Drawing inspiration from neuroscience, we propose a\nMemory Removal Difficulty ($\\mathrm{MRD}$) metric to quantify sample-level\nunlearning difficulty. Using $\\mathrm{MRD}$, we analyze the characteristics of\nhard-to-unlearn versus easy-to-unlearn samples. Furthermore, we propose an\n$\\mathrm{MRD}$-based weighted sampling method to optimize existing unlearning\nalgorithms, which prioritizes easily forgettable samples, thereby improving\nunlearning efficiency and effectiveness. We validate the proposed metric and\nmethod using public benchmarks and datasets, with results confirming its\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by privacy protection laws and regulations, unlearning in Large\nLanguage Models (LLMs) is gaining increasing attention. However, current\nresearch often neglects the interpretability of the unlearning process,\nparticularly concerning sample-level unlearning difficulty. Existing studies\ntypically assume a uniform unlearning difficulty across samples. This\nsimplification risks attributing the performance of unlearning algorithms to\nsample selection rather than the algorithm's design, potentially steering the\ndevelopment of LLM unlearning in the wrong direction. Thus, we investigate the\nrelationship between LLM unlearning and sample characteristics, with a focus on\nunlearning difficulty. Drawing inspiration from neuroscience, we propose a\nMemory Removal Difficulty ($\\mathrm{MRD}$) metric to quantify sample-level\nunlearning difficulty. Using $\\mathrm{MRD}$, we analyze the characteristics of\nhard-to-unlearn versus easy-to-unlearn samples. Furthermore, we propose an\n$\\mathrm{MRD}$-based weighted sampling method to optimize existing unlearning\nalgorithms, which prioritizes easily forgettable samples, thereby improving\nunlearning efficiency and effectiveness. We validate the proposed metric and\nmethod using public benchmarks and datasets, with results confirming its\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Xiaohua Feng"
                    },
                    {
                        "name": "Yuyuan Li"
                    },
                    {
                        "name": "Chengye Wang"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Chaochao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chaochao Chen"
                },
                "author": "Chaochao Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06650v1",
                "updated": "2025-04-09T07:37:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    37,
                    27,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T07:37:27Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    37,
                    27,
                    2,
                    99,
                    0
                ],
                "title": "ThoughtProbe: Classifier-Guided Thought Space Exploration Leveraging LLM\n  Intrinsic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThoughtProbe: Classifier-Guided Thought Space Exploration Leveraging LLM\n  Intrinsic Reasoning"
                },
                "summary": "Pre-trained large language models (LLMs) have been demonstrated to possess\nintrinsic reasoning capabilities that can emerge naturally when expanding the\nresponse space. However, the neural representation mechanisms underlying these\nintrinsic capabilities and approaches for their optimal utilization remain\ninadequately understood. In this work, we make the key discovery that a simple\nlinear classifier can effectively detect intrinsic reasoning capabilities in\nLLMs' activation space, particularly within specific representation types and\nnetwork layers. Based on this finding, we propose a classifier-guided search\nframework that strategically explore a tree-structured response space. In each\nnode expansion, the classifier serves as a scoring and ranking mechanism that\nefficiently allocates computational resources by identifying and prioritizing\nmore thoughtful reasoning directions for continuation. After completing the\ntree expansion, we collect answers from all branches to form a candidate answer\npool. We propose a branch-aggregation selection method that marginalizes over\nall supporting branches by aggregating their thoughtfulness scores, thereby\nidentifying the optimal answer from the pool. Experimental results show that\nour framework's comprehensive exploration not only covers valid reasoning\nchains but also effectively identifies them, achieving significant improvements\nacross multiple arithmetic reasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained large language models (LLMs) have been demonstrated to possess\nintrinsic reasoning capabilities that can emerge naturally when expanding the\nresponse space. However, the neural representation mechanisms underlying these\nintrinsic capabilities and approaches for their optimal utilization remain\ninadequately understood. In this work, we make the key discovery that a simple\nlinear classifier can effectively detect intrinsic reasoning capabilities in\nLLMs' activation space, particularly within specific representation types and\nnetwork layers. Based on this finding, we propose a classifier-guided search\nframework that strategically explore a tree-structured response space. In each\nnode expansion, the classifier serves as a scoring and ranking mechanism that\nefficiently allocates computational resources by identifying and prioritizing\nmore thoughtful reasoning directions for continuation. After completing the\ntree expansion, we collect answers from all branches to form a candidate answer\npool. We propose a branch-aggregation selection method that marginalizes over\nall supporting branches by aggregating their thoughtfulness scores, thereby\nidentifying the optimal answer from the pool. Experimental results show that\nour framework's comprehensive exploration not only covers valid reasoning\nchains but also effectively identifies them, achieving significant improvements\nacross multiple arithmetic reasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Zijian Wang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06637v1",
                "updated": "2025-04-09T07:26:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    26,
                    24,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T07:26:24Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    26,
                    24,
                    2,
                    99,
                    0
                ],
                "title": "SCI-Reason: A Dataset with Chain-of-Thought Rationales for Complex\n  Multimodal Reasoning in Academic Areas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCI-Reason: A Dataset with Chain-of-Thought Rationales for Complex\n  Multimodal Reasoning in Academic Areas"
                },
                "summary": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) demonstrate\nimpressive problem-solving skills in many tasks and domains. However, their\nability to reason with complex images in academic domains has not been\nsystematically investigated. To bridge this gap, we present SCI-Reason, a\ndataset for complex multimodel reasoning in academic areas. SCI-Reason aims to\ntest and improve the reasoning ability of large multimodal models using real\ncomplex images in academic domains. The dataset contains 12,066 images and\n12,626 question-answer pairs extracted from PubMed, divided into training,\nvalidation and test splits. Each question-answer pair also contains an accurate\nand efficient inference chain as a guide to improving the inference properties\nof the dataset. With SCI-Reason, we performed a comprehensive evaluation of 8\nwell-known models. The best performing model, Claude-3.7-Sonnet, only achieved\nan accuracy of 55.19%. Error analysis shows that more than half of the model\nfailures are due to breakdowns in multi-step inference chains rather than\nerrors in primary visual feature extraction. This finding underscores the\ninherent limitations in reasoning capabilities exhibited by current multimodal\nmodels when processing complex image analysis tasks within authentic academic\ncontexts. Experiments on open-source models show that SCI-Reason not only\nenhances reasoning ability but also demonstrates cross-domain generalization in\nVQA tasks. We also explore future applications of model inference capabilities\nin this domain, highlighting its potential for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) demonstrate\nimpressive problem-solving skills in many tasks and domains. However, their\nability to reason with complex images in academic domains has not been\nsystematically investigated. To bridge this gap, we present SCI-Reason, a\ndataset for complex multimodel reasoning in academic areas. SCI-Reason aims to\ntest and improve the reasoning ability of large multimodal models using real\ncomplex images in academic domains. The dataset contains 12,066 images and\n12,626 question-answer pairs extracted from PubMed, divided into training,\nvalidation and test splits. Each question-answer pair also contains an accurate\nand efficient inference chain as a guide to improving the inference properties\nof the dataset. With SCI-Reason, we performed a comprehensive evaluation of 8\nwell-known models. The best performing model, Claude-3.7-Sonnet, only achieved\nan accuracy of 55.19%. Error analysis shows that more than half of the model\nfailures are due to breakdowns in multi-step inference chains rather than\nerrors in primary visual feature extraction. This finding underscores the\ninherent limitations in reasoning capabilities exhibited by current multimodal\nmodels when processing complex image analysis tasks within authentic academic\ncontexts. Experiments on open-source models show that SCI-Reason not only\nenhances reasoning ability but also demonstrates cross-domain generalization in\nVQA tasks. We also explore future applications of model inference capabilities\nin this domain, highlighting its potential for future research."
                },
                "authors": [
                    {
                        "name": "Chenghao Ma"
                    },
                    {
                        "name": "Haihong E."
                    },
                    {
                        "name": "Junpeng Ding"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziyan Ma"
                    },
                    {
                        "name": "Huang Qing"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Meina Song"
                    }
                ],
                "author_detail": {
                    "name": "Meina Song"
                },
                "author": "Meina Song",
                "arxiv_comment": "Submitted to ICCV 2025. 11 pages (including references)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03307v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03307v4",
                "updated": "2025-04-09T07:21:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    21,
                    18,
                    2,
                    99,
                    0
                ],
                "published": "2025-02-05T16:08:05Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    8,
                    5,
                    2,
                    36,
                    0
                ],
                "title": "Intent Representation Learning with Large Language Model for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Representation Learning with Large Language Model for\n  Recommendation"
                },
                "summary": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines.Code available at\nhttps://github.com/wangyu0627/IRLLRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines.Code available at\nhttps://github.com/wangyu0627/IRLLRec."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Lei Sang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "arxiv_comment": "Accepted by SIGIR 2025 Full Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03307v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03307v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06631v1",
                "updated": "2025-04-09T07:09:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    9,
                    40,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T07:09:40Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    9,
                    40,
                    2,
                    99,
                    0
                ],
                "title": "The Method for Storing Patterns in Neural Networks-Memorization and\n  Recall of QR code Patterns-",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Method for Storing Patterns in Neural Networks-Memorization and\n  Recall of QR code Patterns-"
                },
                "summary": "In this paper, we propose a mechanism for storing complex patterns within a\nneural network and subsequently recalling them. This model is based on our work\npublished in 2018(Inazawa, 2018), which we have refined and extended in this\nwork. With the recent advancements in deep learning and large language model\n(LLM)-based AI technologies (generative AI), it can be considered that\nmethodologies for the learning are becoming increasingly well-established. In\nthe future, we expect to see further research on memory using models based on\nTransformers (Vaswani, et. al., 2017, Rae, et. al., 2020), but in this paper we\npropose a simpler and more powerful model of memory and recall in neural\nnetworks. The advantage of storing patterns in a neural network lies in its\nability to recall the original pattern even when an incomplete version is\npresented. The patterns we have produced for use in this study have been QR\ncode (DENSO WAVE, 1994), which has become widely used as an information\ntransmission tool in recent years.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a mechanism for storing complex patterns within a\nneural network and subsequently recalling them. This model is based on our work\npublished in 2018(Inazawa, 2018), which we have refined and extended in this\nwork. With the recent advancements in deep learning and large language model\n(LLM)-based AI technologies (generative AI), it can be considered that\nmethodologies for the learning are becoming increasingly well-established. In\nthe future, we expect to see further research on memory using models based on\nTransformers (Vaswani, et. al., 2017, Rae, et. al., 2020), but in this paper we\npropose a simpler and more powerful model of memory and recall in neural\nnetworks. The advantage of storing patterns in a neural network lies in its\nability to recall the original pattern even when an incomplete version is\npresented. The patterns we have produced for use in this study have been QR\ncode (DENSO WAVE, 1994), which has become widely used as an information\ntransmission tool in recent years."
                },
                "authors": [
                    {
                        "name": "Hiroshi Inazawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroshi Inazawa"
                },
                "author": "Hiroshi Inazawa",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02948v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02948v4",
                "updated": "2025-04-09T06:54:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    54,
                    20,
                    2,
                    99,
                    0
                ],
                "published": "2024-04-03T15:06:43Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    15,
                    6,
                    43,
                    2,
                    94,
                    0
                ],
                "title": "PiSSA: Principal Singular Values and Singular Vectors Adaptation of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiSSA: Principal Singular Values and Singular Vectors Adaptation of\n  Large Language Models"
                },
                "summary": "To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the\nlow-rank adaptation (LoRA) method approximates the model changes $\\Delta W \\in\n\\mathbb{R}^{m \\times n}$ through the product of two matrices $A \\in\n\\mathbb{R}^{m \\times r}$ and $B \\in \\mathbb{R}^{r \\times n}$, where $r \\ll\n\\min(m, n)$, $A$ is initialized with Gaussian noise, and $B$ with zeros. LoRA\nfreezes the original model $W$ and updates the \"Noise & Zero\" adapter, which\nmay lead to slow convergence. To overcome this limitation, we introduce\nPrincipal Singular values and Singular vectors Adaptation (PiSSA). PiSSA shares\nthe same architecture as LoRA, but initializes the adaptor matrices $A$ and $B$\nwith the principal components of the original matrix $W$, and put the remaining\ncomponents into a residual matrix $W^{res} \\in \\mathbb{R}^{m \\times n}$ which\nis frozen during fine-tuning. Compared to LoRA, PiSSA updates the principal\ncomponents while freezing the \"residual\" parts, allowing faster convergence and\nenhanced performance. Comparative experiments of PiSSA and LoRA across 12\ndifferent models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks,\nreveal that PiSSA consistently outperforms LoRA under identical experimental\nsetups. On the GSM8K benchmark, Mistral-7B fine-tuned with PiSSA achieves an\naccuracy of 72.86%, surpassing LoRA's 67.7% by 5.16%. Due to the same\narchitecture, PiSSA is also compatible with quantization to further reduce the\nmemory requirement of fine-tuning. Compared to QLoRA, QPiSSA exhibits smaller\nquantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K,\nQPiSSA attains an accuracy of 86.05%, exceeding the performances of QLoRA at\n81.73%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few\nseconds, presenting a negligible cost for transitioning from LoRA to PiSSA.\nCode is available at https://github.com/GraphPKU/PiSSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the\nlow-rank adaptation (LoRA) method approximates the model changes $\\Delta W \\in\n\\mathbb{R}^{m \\times n}$ through the product of two matrices $A \\in\n\\mathbb{R}^{m \\times r}$ and $B \\in \\mathbb{R}^{r \\times n}$, where $r \\ll\n\\min(m, n)$, $A$ is initialized with Gaussian noise, and $B$ with zeros. LoRA\nfreezes the original model $W$ and updates the \"Noise & Zero\" adapter, which\nmay lead to slow convergence. To overcome this limitation, we introduce\nPrincipal Singular values and Singular vectors Adaptation (PiSSA). PiSSA shares\nthe same architecture as LoRA, but initializes the adaptor matrices $A$ and $B$\nwith the principal components of the original matrix $W$, and put the remaining\ncomponents into a residual matrix $W^{res} \\in \\mathbb{R}^{m \\times n}$ which\nis frozen during fine-tuning. Compared to LoRA, PiSSA updates the principal\ncomponents while freezing the \"residual\" parts, allowing faster convergence and\nenhanced performance. Comparative experiments of PiSSA and LoRA across 12\ndifferent models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks,\nreveal that PiSSA consistently outperforms LoRA under identical experimental\nsetups. On the GSM8K benchmark, Mistral-7B fine-tuned with PiSSA achieves an\naccuracy of 72.86%, surpassing LoRA's 67.7% by 5.16%. Due to the same\narchitecture, PiSSA is also compatible with quantization to further reduce the\nmemory requirement of fine-tuning. Compared to QLoRA, QPiSSA exhibits smaller\nquantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K,\nQPiSSA attains an accuracy of 86.05%, exceeding the performances of QLoRA at\n81.73%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few\nseconds, presenting a negligible cost for transitioning from LoRA to PiSSA.\nCode is available at https://github.com/GraphPKU/PiSSA."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zhaohui Wang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "NeurIPS 2024 spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02948v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02948v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07800v2",
                "updated": "2025-04-09T06:37:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    37,
                    25,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-10T19:27:38Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    19,
                    27,
                    38,
                    0,
                    69,
                    0
                ],
                "title": "Using Large Language Models to Develop Requirements Elicitation Skills",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models to Develop Requirements Elicitation Skills"
                },
                "summary": "Requirements Elicitation (RE) is a crucial software engineering skill that\ninvolves interviewing a client and then devising a software design based on the\ninterview results. Teaching this inherently experiential skill effectively has\nhigh cost, such as acquiring an industry partner to interview, or training\ncourse staff or other students to play the role of a client. As a result, a\ntypical instructional approach is to provide students with transcripts of real\nor fictitious interviews to analyze, which exercises the skill of extracting\ntechnical requirements but fails to develop the equally important interview\nskill itself. As an alternative, we propose conditioning a large language model\nto play the role of the client during a chat-based interview. We perform a\nbetween-subjects study (n=120) in which students construct a high-level\napplication design from either an interactive LLM-backed interview session or\nan existing interview transcript describing the same business processes. We\nevaluate our approach using both a qualitative survey and quantitative\nobservations about participants' work. We find that both approaches provide\nsufficient information for participants to construct technically sound\nsolutions and require comparable time on task, but the LLM-based approach is\npreferred by most participants. Importantly, we observe that LLM-backed\ninterview is seen as both more realistic and more engaging, despite the LLM\noccasionally providing imprecise or contradictory information. These results,\ncombined with the wide accessibility of LLMs, suggest a new way to practice\ncritical RE skills in a scalable and realistic manner without the overhead of\narranging live interviews.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requirements Elicitation (RE) is a crucial software engineering skill that\ninvolves interviewing a client and then devising a software design based on the\ninterview results. Teaching this inherently experiential skill effectively has\nhigh cost, such as acquiring an industry partner to interview, or training\ncourse staff or other students to play the role of a client. As a result, a\ntypical instructional approach is to provide students with transcripts of real\nor fictitious interviews to analyze, which exercises the skill of extracting\ntechnical requirements but fails to develop the equally important interview\nskill itself. As an alternative, we propose conditioning a large language model\nto play the role of the client during a chat-based interview. We perform a\nbetween-subjects study (n=120) in which students construct a high-level\napplication design from either an interactive LLM-backed interview session or\nan existing interview transcript describing the same business processes. We\nevaluate our approach using both a qualitative survey and quantitative\nobservations about participants' work. We find that both approaches provide\nsufficient information for participants to construct technically sound\nsolutions and require comparable time on task, but the LLM-based approach is\npreferred by most participants. Importantly, we observe that LLM-backed\ninterview is seen as both more realistic and more engaging, despite the LLM\noccasionally providing imprecise or contradictory information. These results,\ncombined with the wide accessibility of LLMs, suggest a new way to practice\ncritical RE skills in a scalable and realistic manner without the overhead of\narranging live interviews."
                },
                "authors": [
                    {
                        "name": "Nelson Lojo"
                    },
                    {
                        "name": "Rafael González"
                    },
                    {
                        "name": "Rohan Philip"
                    },
                    {
                        "name": "José Antonio Parejo"
                    },
                    {
                        "name": "Amador Durán Toro"
                    },
                    {
                        "name": "Armando Fox"
                    },
                    {
                        "name": "Pablo Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Fernández"
                },
                "arxiv_affiliation": "SCORE Lab, Univ. of Sevilla, Sevilla, Spain",
                "author": "Pablo Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03051v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03051v4",
                "updated": "2025-04-09T06:24:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    24,
                    14,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-04T00:13:54Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    0,
                    13,
                    54,
                    4,
                    278,
                    0
                ],
                "title": "AuroraCap: Efficient, Performant Video Detailed Captioning and a New\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AuroraCap: Efficient, Performant Video Detailed Captioning and a New\n  Benchmark"
                },
                "summary": "Video detailed captioning is a key task which aims to generate comprehensive\nand coherent textual descriptions of video content, benefiting both video\nunderstanding and generation. In this paper, we propose AuroraCap, a video\ncaptioner based on a large multimodal model. We follow the simplest\narchitecture design without additional parameters for temporal modeling. To\naddress the overhead caused by lengthy video sequences, we implement the token\nmerging strategy, reducing the number of input visual tokens. Surprisingly, we\nfound that this strategy results in little performance loss. AuroraCap shows\nsuperior performance on various video and image captioning benchmarks, for\nexample, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and\nGemini-1.5 Pro (82.2). However, existing video caption benchmarks only include\nsimple descriptions, consisting of a few dozen words, which limits research in\nthis field. Therefore, we develop VDC, a video detailed captioning benchmark\nwith over one thousand carefully annotated structured captions. In addition, we\npropose a new LLM-assisted metric VDCscore for bettering evaluation, which\nadopts a divide-and-conquer strategy to transform long caption evaluation into\nmultiple short question-answer pairs. With the help of human Elo ranking, our\nexperiments show that this benchmark better correlates with human judgments of\nvideo detailed captioning quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video detailed captioning is a key task which aims to generate comprehensive\nand coherent textual descriptions of video content, benefiting both video\nunderstanding and generation. In this paper, we propose AuroraCap, a video\ncaptioner based on a large multimodal model. We follow the simplest\narchitecture design without additional parameters for temporal modeling. To\naddress the overhead caused by lengthy video sequences, we implement the token\nmerging strategy, reducing the number of input visual tokens. Surprisingly, we\nfound that this strategy results in little performance loss. AuroraCap shows\nsuperior performance on various video and image captioning benchmarks, for\nexample, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and\nGemini-1.5 Pro (82.2). However, existing video caption benchmarks only include\nsimple descriptions, consisting of a few dozen words, which limits research in\nthis field. Therefore, we develop VDC, a video detailed captioning benchmark\nwith over one thousand carefully annotated structured captions. In addition, we\npropose a new LLM-assisted metric VDCscore for bettering evaluation, which\nadopts a divide-and-conquer strategy to transform long caption evaluation into\nmultiple short question-answer pairs. With the help of human Elo ranking, our\nexperiments show that this benchmark better correlates with human judgments of\nvideo detailed captioning quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Enxin Song"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Chenlin Meng"
                    },
                    {
                        "name": "Vashisht Madhavan"
                    },
                    {
                        "name": "Omer Bar-Tal"
                    },
                    {
                        "name": "Jenq-Neng Hwang"
                    },
                    {
                        "name": "Saining Xie"
                    },
                    {
                        "name": "Christopher D. Manning"
                    }
                ],
                "author_detail": {
                    "name": "Christopher D. Manning"
                },
                "author": "Christopher D. Manning",
                "arxiv_comment": "Accepted to ICLR 2025. Code, docs, weight, benchmark and training\n  data are all avaliable at https://rese1f.github.io/aurora-web/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03051v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03051v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06614v1",
                "updated": "2025-04-09T06:18:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    18,
                    24,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T06:18:24Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    18,
                    24,
                    2,
                    99,
                    0
                ],
                "title": "AgentFM: Role-Aware Failure Management for Distributed Databases with\n  LLM-Driven Multi-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentFM: Role-Aware Failure Management for Distributed Databases with\n  LLM-Driven Multi-Agents"
                },
                "summary": "Distributed databases are critical infrastructures for today's large-scale\nsoftware systems, making effective failure management essential to ensure\nsoftware availability. However, existing approaches often overlook the role\ndistinctions within distributed databases and rely on small-scale models with\nlimited generalization capabilities. In this paper, we conduct a preliminary\nempirical study to emphasize the unique significance of different roles.\nBuilding on this insight, we propose AgentFM, a role-aware failure management\nframework for distributed databases powered by LLM-driven multi-agents. AgentFM\naddresses failure management by considering system roles, data roles, and task\nroles, with a meta-agent orchestrating these components. Preliminary\nevaluations using Apache IoTDB demonstrate the effectiveness of AgentFM and\nopen new directions for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed databases are critical infrastructures for today's large-scale\nsoftware systems, making effective failure management essential to ensure\nsoftware availability. However, existing approaches often overlook the role\ndistinctions within distributed databases and rely on small-scale models with\nlimited generalization capabilities. In this paper, we conduct a preliminary\nempirical study to emphasize the unique significance of different roles.\nBuilding on this insight, we propose AgentFM, a role-aware failure management\nframework for distributed databases powered by LLM-driven multi-agents. AgentFM\naddresses failure management by considering system roles, data roles, and task\nroles, with a meta-agent orchestrating these components. Preliminary\nevaluations using Apache IoTDB demonstrate the effectiveness of AgentFM and\nopen new directions for further research."
                },
                "authors": [
                    {
                        "name": "Lingzhe Zhang"
                    },
                    {
                        "name": "Yunpeng Zhai"
                    },
                    {
                        "name": "Tong Jia"
                    },
                    {
                        "name": "Xiaosong Huang"
                    },
                    {
                        "name": "Chiming Duan"
                    },
                    {
                        "name": "Ying Li"
                    }
                ],
                "author_detail": {
                    "name": "Ying Li"
                },
                "author": "Ying Li",
                "arxiv_doi": "10.1145/3696630.3728492",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696630.3728492",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted by FSE-IVR'25",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06606v1",
                "updated": "2025-04-09T06:09:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    9,
                    40,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T06:09:40Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    6,
                    9,
                    40,
                    2,
                    99,
                    0
                ],
                "title": "Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program"
                },
                "summary": "Recent advancements in reward signal usage for Large Language Models (LLMs)\nare remarkable. However, significant challenges exist when transitioning reward\nsignal to the multimodal domain, including labor-intensive annotations,\nover-reliance on one-step rewards, and inadequate evaluation. To address these\nissues, we propose SVIP, a novel approach to train a step-level\nmulti-dimensional Chain-of-Thought~(CoT) reward model automatically. It\ngenerates code for solving visual tasks and transforms the analysis of code\nblocks into the evaluation of CoT step as training samples. Then, we train\nSVIP-Reward model using a multi-head attention mechanism called TriAtt-CoT. The\nadvantages of SVIP-Reward are evident throughout the entire process of MLLM. We\nalso introduce a benchmark for CoT reward model training and testing.\nExperimental results demonstrate that SVIP-Reward improves MLLM performance\nacross training and inference-time scaling, yielding better results on\nbenchmarks while reducing hallucinations and enhancing reasoning ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reward signal usage for Large Language Models (LLMs)\nare remarkable. However, significant challenges exist when transitioning reward\nsignal to the multimodal domain, including labor-intensive annotations,\nover-reliance on one-step rewards, and inadequate evaluation. To address these\nissues, we propose SVIP, a novel approach to train a step-level\nmulti-dimensional Chain-of-Thought~(CoT) reward model automatically. It\ngenerates code for solving visual tasks and transforms the analysis of code\nblocks into the evaluation of CoT step as training samples. Then, we train\nSVIP-Reward model using a multi-head attention mechanism called TriAtt-CoT. The\nadvantages of SVIP-Reward are evident throughout the entire process of MLLM. We\nalso introduce a benchmark for CoT reward model training and testing.\nExperimental results demonstrate that SVIP-Reward improves MLLM performance\nacross training and inference-time scaling, yielding better results on\nbenchmarks while reducing hallucinations and enhancing reasoning ability."
                },
                "authors": [
                    {
                        "name": "Minghe Gao"
                    },
                    {
                        "name": "Xuqi Liu"
                    },
                    {
                        "name": "Zhongqi Yue"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06600v1",
                "updated": "2025-04-09T05:52:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    5,
                    52,
                    50,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T05:52:50Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    5,
                    52,
                    50,
                    2,
                    99,
                    0
                ],
                "title": "Automated Business Process Analysis: An LLM-Based Approach to Value\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Business Process Analysis: An LLM-Based Approach to Value\n  Assessment"
                },
                "summary": "Business processes are fundamental to organizational operations, yet their\noptimization remains challenging due to the timeconsuming nature of manual\nprocess analysis. Our paper harnesses Large Language Models (LLMs) to automate\nvalue-added analysis, a qualitative process analysis technique that aims to\nidentify steps in the process that do not deliver value. To date, this\ntechnique is predominantly manual, time-consuming, and subjective. Our method\noffers a more principled approach which operates in two phases: first,\ndecomposing high-level activities into detailed steps to enable granular\nanalysis, and second, performing a value-added analysis to classify each step\naccording to Lean principles. This approach enables systematic identification\nof waste while maintaining the semantic understanding necessary for qualitative\nanalysis. We develop our approach using 50 business process models, for which\nwe collect and publish manual ground-truth labels. Our evaluation, comparing\nzero-shot baselines with more structured prompts reveals (a) a consistent\nbenefit of structured prompting and (b) promising performance for both tasks.\nWe discuss the potential for LLMs to augment human expertise in qualitative\nprocess analysis while reducing the time and subjectivity inherent in manual\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Business processes are fundamental to organizational operations, yet their\noptimization remains challenging due to the timeconsuming nature of manual\nprocess analysis. Our paper harnesses Large Language Models (LLMs) to automate\nvalue-added analysis, a qualitative process analysis technique that aims to\nidentify steps in the process that do not deliver value. To date, this\ntechnique is predominantly manual, time-consuming, and subjective. Our method\noffers a more principled approach which operates in two phases: first,\ndecomposing high-level activities into detailed steps to enable granular\nanalysis, and second, performing a value-added analysis to classify each step\naccording to Lean principles. This approach enables systematic identification\nof waste while maintaining the semantic understanding necessary for qualitative\nanalysis. We develop our approach using 50 business process models, for which\nwe collect and publish manual ground-truth labels. Our evaluation, comparing\nzero-shot baselines with more structured prompts reveals (a) a consistent\nbenefit of structured prompting and (b) promising performance for both tasks.\nWe discuss the potential for LLMs to augment human expertise in qualitative\nprocess analysis while reducing the time and subjectivity inherent in manual\napproaches."
                },
                "authors": [
                    {
                        "name": "William De Michele"
                    },
                    {
                        "name": "Abel Armas Cervantes"
                    },
                    {
                        "name": "Lea Frermann"
                    }
                ],
                "author_detail": {
                    "name": "Lea Frermann"
                },
                "author": "Lea Frermann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06593v1",
                "updated": "2025-04-09T05:42:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    5,
                    42,
                    33,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T05:42:33Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    5,
                    42,
                    33,
                    2,
                    99,
                    0
                ],
                "title": "A Multi-Modal Interaction Framework for Efficient Human-Robot\n  Collaborative Shelf Picking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Modal Interaction Framework for Efficient Human-Robot\n  Collaborative Shelf Picking"
                },
                "summary": "The growing presence of service robots in human-centric environments, such as\nwarehouses, demands seamless and intuitive human-robot collaboration. In this\npaper, we propose a collaborative shelf-picking framework that combines\nmultimodal interaction, physics-based reasoning, and task division for enhanced\nhuman-robot teamwork.\n  The framework enables the robot to recognize human pointing gestures,\ninterpret verbal cues and voice commands, and communicate through visual and\nauditory feedback. Moreover, it is powered by a Large Language Model (LLM)\nwhich utilizes Chain of Thought (CoT) and a physics-based simulation engine for\nsafely retrieving cluttered stacks of boxes on shelves, relationship graph for\nsub-task generation, extraction sequence planning and decision making.\nFurthermore, we validate the framework through real-world shelf picking\nexperiments such as 1) Gesture-Guided Box Extraction, 2) Collaborative Shelf\nClearing and 3) Collaborative Stability Assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing presence of service robots in human-centric environments, such as\nwarehouses, demands seamless and intuitive human-robot collaboration. In this\npaper, we propose a collaborative shelf-picking framework that combines\nmultimodal interaction, physics-based reasoning, and task division for enhanced\nhuman-robot teamwork.\n  The framework enables the robot to recognize human pointing gestures,\ninterpret verbal cues and voice commands, and communicate through visual and\nauditory feedback. Moreover, it is powered by a Large Language Model (LLM)\nwhich utilizes Chain of Thought (CoT) and a physics-based simulation engine for\nsafely retrieving cluttered stacks of boxes on shelves, relationship graph for\nsub-task generation, extraction sequence planning and decision making.\nFurthermore, we validate the framework through real-world shelf picking\nexperiments such as 1) Gesture-Guided Box Extraction, 2) Collaborative Shelf\nClearing and 3) Collaborative Stability Assistance."
                },
                "authors": [
                    {
                        "name": "Abhinav Pathak"
                    },
                    {
                        "name": "Kalaichelvi Venkatesan"
                    },
                    {
                        "name": "Tarek Taha"
                    },
                    {
                        "name": "Rajkumar Muthusamy"
                    }
                ],
                "author_detail": {
                    "name": "Rajkumar Muthusamy"
                },
                "author": "Rajkumar Muthusamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06581v1",
                "updated": "2025-04-09T05:04:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    5,
                    4,
                    1,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T05:04:01Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    5,
                    4,
                    1,
                    2,
                    99,
                    0
                ],
                "title": "Right Prediction, Wrong Reasoning: Uncovering LLM Misalignment in RA\n  Disease Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Right Prediction, Wrong Reasoning: Uncovering LLM Misalignment in RA\n  Disease Diagnosis"
                },
                "summary": "Large language models (LLMs) offer a promising pre-screening tool, improving\nearly disease detection and providing enhanced healthcare access for\nunderprivileged communities. The early diagnosis of various diseases continues\nto be a significant challenge in healthcare, primarily due to the nonspecific\nnature of early symptoms, the shortage of expert medical practitioners, and the\nneed for prolonged clinical evaluations, all of which can delay treatment and\nadversely affect patient outcomes. With impressive accuracy in prediction\nacross a range of diseases, LLMs have the potential to revolutionize clinical\npre-screening and decision-making for various medical conditions. In this work,\nwe study the diagnostic capability of LLMs for Rheumatoid Arthritis (RA) with\nreal world patients data. Patient data was collected alongside diagnoses from\nmedical experts, and the performance of LLMs was evaluated in comparison to\nexpert diagnoses for RA disease prediction. We notice an interesting pattern in\ndisease diagnosis and find an unexpected \\textit{misalignment between\nprediction and explanation}. We conduct a series of multi-round analyses using\ndifferent LLM agents. The best-performing model accurately predicts rheumatoid\narthritis (RA) diseases approximately 95\\% of the time. However, when medical\nexperts evaluated the reasoning generated by the model, they found that nearly\n68\\% of the reasoning was incorrect. This study highlights a clear misalignment\nbetween LLMs high prediction accuracy and its flawed reasoning, raising\nimportant questions about relying on LLM explanations in clinical settings.\n\\textbf{LLMs provide incorrect reasoning to arrive at the correct answer for RA\ndisease diagnosis.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer a promising pre-screening tool, improving\nearly disease detection and providing enhanced healthcare access for\nunderprivileged communities. The early diagnosis of various diseases continues\nto be a significant challenge in healthcare, primarily due to the nonspecific\nnature of early symptoms, the shortage of expert medical practitioners, and the\nneed for prolonged clinical evaluations, all of which can delay treatment and\nadversely affect patient outcomes. With impressive accuracy in prediction\nacross a range of diseases, LLMs have the potential to revolutionize clinical\npre-screening and decision-making for various medical conditions. In this work,\nwe study the diagnostic capability of LLMs for Rheumatoid Arthritis (RA) with\nreal world patients data. Patient data was collected alongside diagnoses from\nmedical experts, and the performance of LLMs was evaluated in comparison to\nexpert diagnoses for RA disease prediction. We notice an interesting pattern in\ndisease diagnosis and find an unexpected \\textit{misalignment between\nprediction and explanation}. We conduct a series of multi-round analyses using\ndifferent LLM agents. The best-performing model accurately predicts rheumatoid\narthritis (RA) diseases approximately 95\\% of the time. However, when medical\nexperts evaluated the reasoning generated by the model, they found that nearly\n68\\% of the reasoning was incorrect. This study highlights a clear misalignment\nbetween LLMs high prediction accuracy and its flawed reasoning, raising\nimportant questions about relying on LLM explanations in clinical settings.\n\\textbf{LLMs provide incorrect reasoning to arrive at the correct answer for RA\ndisease diagnosis.}"
                },
                "authors": [
                    {
                        "name": "Umakanta Maharana"
                    },
                    {
                        "name": "Sarthak Verma"
                    },
                    {
                        "name": "Avarna Agarwal"
                    },
                    {
                        "name": "Prakashini Mruthyunjaya"
                    },
                    {
                        "name": "Dwarikanath Mahapatra"
                    },
                    {
                        "name": "Sakir Ahmed"
                    },
                    {
                        "name": "Murari Mandal"
                    }
                ],
                "author_detail": {
                    "name": "Murari Mandal"
                },
                "author": "Murari Mandal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06577v1",
                "updated": "2025-04-09T04:58:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    4,
                    58,
                    14,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T04:58:14Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    4,
                    58,
                    14,
                    2,
                    99,
                    0
                ],
                "title": "Bypassing Safety Guardrails in LLMs Using Humor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bypassing Safety Guardrails in LLMs Using Humor"
                },
                "summary": "In this paper, we show it is possible to bypass the safety guardrails of\nlarge language models (LLMs) through a humorous prompt including the unsafe\nrequest. In particular, our method does not edit the unsafe request and follows\na fixed template -- it is simple to implement and does not need additional LLMs\nto craft prompts. Extensive experiments show the effectiveness of our method\nacross different LLMs. We also show that both removing and adding more humor to\nour method can reduce its effectiveness -- excessive humor possibly distracts\nthe LLM from fulfilling its unsafe request. Thus, we argue that LLM\njailbreaking occurs when there is a proper balance between focus on the unsafe\nrequest and presence of humor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we show it is possible to bypass the safety guardrails of\nlarge language models (LLMs) through a humorous prompt including the unsafe\nrequest. In particular, our method does not edit the unsafe request and follows\na fixed template -- it is simple to implement and does not need additional LLMs\nto craft prompts. Extensive experiments show the effectiveness of our method\nacross different LLMs. We also show that both removing and adding more humor to\nour method can reduce its effectiveness -- excessive humor possibly distracts\nthe LLM from fulfilling its unsafe request. Thus, we argue that LLM\njailbreaking occurs when there is a proper balance between focus on the unsafe\nrequest and presence of humor."
                },
                "authors": [
                    {
                        "name": "Pedro Cisneros-Velarde"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Cisneros-Velarde"
                },
                "author": "Pedro Cisneros-Velarde",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16899v2",
                "updated": "2025-04-09T04:44:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    4,
                    44,
                    48,
                    2,
                    99,
                    0
                ],
                "published": "2024-05-29T09:06:18Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    6,
                    18,
                    2,
                    150,
                    0
                ],
                "title": "Prompting or Fine-tuning? Exploring Large Language Models for Causal\n  Graph Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting or Fine-tuning? Exploring Large Language Models for Causal\n  Graph Validation"
                },
                "summary": "This study explores the capability of Large Language Models (LLMs) to\nevaluate causality in causal graphs generated by conventional statistical\ncausal discovery methods-a task traditionally reliant on manual assessment by\nhuman subject matter experts. To bridge this gap in causality assessment, LLMs\nare employed to evaluate the causal relationships by determining whether a\ncausal connection between variable pairs can be inferred from textual context.\nOur study compares two approaches: (1) prompting-based method for zero-shot and\nfew-shot causal inference and, (2) fine-tuning language models for the causal\nrelation prediction task. While prompt-based LLMs have demonstrated versatility\nacross various NLP tasks, our experiments on biomedical and general-domain\ndatasets show that fine-tuned models consistently outperform them, achieving up\nto a 20.5-point improvement in F1 score-even when using smaller-parameter\nlanguage models. These findings provide valuable insights into the strengths\nand limitations of both approaches for causal graph evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the capability of Large Language Models (LLMs) to\nevaluate causality in causal graphs generated by conventional statistical\ncausal discovery methods-a task traditionally reliant on manual assessment by\nhuman subject matter experts. To bridge this gap in causality assessment, LLMs\nare employed to evaluate the causal relationships by determining whether a\ncausal connection between variable pairs can be inferred from textual context.\nOur study compares two approaches: (1) prompting-based method for zero-shot and\nfew-shot causal inference and, (2) fine-tuning language models for the causal\nrelation prediction task. While prompt-based LLMs have demonstrated versatility\nacross various NLP tasks, our experiments on biomedical and general-domain\ndatasets show that fine-tuned models consistently outperform them, achieving up\nto a 20.5-point improvement in F1 score-even when using smaller-parameter\nlanguage models. These findings provide valuable insights into the strengths\nand limitations of both approaches for causal graph evaluation."
                },
                "authors": [
                    {
                        "name": "Yuni Susanti"
                    },
                    {
                        "name": "Nina Holsmoelle"
                    }
                ],
                "author_detail": {
                    "name": "Nina Holsmoelle"
                },
                "author": "Nina Holsmoelle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06575v2",
                "updated": "2025-04-10T03:23:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    23,
                    40,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-09T04:38:17Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    4,
                    38,
                    17,
                    2,
                    99,
                    0
                ],
                "title": "Defending LLM Watermarking Against Spoofing Attacks with Contrastive\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending LLM Watermarking Against Spoofing Attacks with Contrastive\n  Representation Learning"
                },
                "summary": "Watermarking has emerged as a promising technique for detecting texts\ngenerated by LLMs. Current research has primarily focused on three design\ncriteria: high quality of the watermarked text, high detectability, and\nrobustness against removal attack. However, the security against spoofing\nattacks remains relatively understudied. For example, a piggyback attack can\nmaliciously alter the meaning of watermarked text-transforming it into hate\nspeech-while preserving the original watermark, thereby damaging the reputation\nof the LLM provider. We identify two core challenges that make defending\nagainst spoofing difficult: (1) the need for watermarks to be both sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving edits, and\n(2) the contradiction between the need to detect global semantic shifts and the\nlocal, auto-regressive nature of most watermarking schemes. To address these\nchallenges, we propose a semantic-aware watermarking algorithm that post-hoc\nembeds watermarks into a given target text while preserving its original\nmeaning. Our method introduces a semantic mapping model, which guides the\ngeneration of a green-red token list, contrastively trained to be sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving changes.\nExperiments on two standard benchmarks demonstrate strong robustness against\nremoval attacks and security against spoofing attacks, including sentiment\nreversal and toxic content insertion, while maintaining high watermark\ndetectability. Our approach offers a significant step toward more secure and\nsemantically aware watermarking for LLMs. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/contrastive-watermark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has emerged as a promising technique for detecting texts\ngenerated by LLMs. Current research has primarily focused on three design\ncriteria: high quality of the watermarked text, high detectability, and\nrobustness against removal attack. However, the security against spoofing\nattacks remains relatively understudied. For example, a piggyback attack can\nmaliciously alter the meaning of watermarked text-transforming it into hate\nspeech-while preserving the original watermark, thereby damaging the reputation\nof the LLM provider. We identify two core challenges that make defending\nagainst spoofing difficult: (1) the need for watermarks to be both sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving edits, and\n(2) the contradiction between the need to detect global semantic shifts and the\nlocal, auto-regressive nature of most watermarking schemes. To address these\nchallenges, we propose a semantic-aware watermarking algorithm that post-hoc\nembeds watermarks into a given target text while preserving its original\nmeaning. Our method introduces a semantic mapping model, which guides the\ngeneration of a green-red token list, contrastively trained to be sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving changes.\nExperiments on two standard benchmarks demonstrate strong robustness against\nremoval attacks and security against spoofing attacks, including sentiment\nreversal and toxic content insertion, while maintaining high watermark\ndetectability. Our approach offers a significant step toward more secure and\nsemantically aware watermarking for LLMs. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/contrastive-watermark."
                },
                "authors": [
                    {
                        "name": "Li An"
                    },
                    {
                        "name": "Yujian Liu"
                    },
                    {
                        "name": "Yepeng Liu"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Yuheng Bu"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06160v2",
                "updated": "2025-04-09T04:24:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    4,
                    24,
                    38,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T15:56:57Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    56,
                    57,
                    1,
                    98,
                    0
                ],
                "title": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack\n  Narratives Targeting Mental Health Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack\n  Narratives Targeting Mental Health Groups"
                },
                "summary": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to demonstrate imbalanced biases\nagainst certain groups. However, the study of unprovoked targeted attacks by\nLLMs towards at-risk populations remains underexplored. Our paper presents\nthree novel contributions: (1) the explicit evaluation of LLM-generated attacks\non highly vulnerable mental health groups; (2) a network-based framework to\nstudy the propagation of relative biases; and (3) an assessment of the relative\ndegree of stigmatization that emerges from these attacks. Our analysis of a\nrecently released large-scale bias audit dataset reveals that mental health\nentities occupy central positions within attack narrative networks, as revealed\nby a significantly higher mean centrality of closeness (p-value = 4.06e-10) and\ndense clustering (Gini coefficient = 0.7). Drawing from sociological\nfoundations of stigmatization theory, our stigmatization analysis indicates\nincreased labeling components for mental health disorder-related targets\nrelative to initial targets in generation chains. Taken together, these\ninsights shed light on the structural predilections of large language models to\nheighten harmful discourse and highlight the need for suitable approaches for\nmitigation."
                },
                "authors": [
                    {
                        "name": "Rijul Magu"
                    },
                    {
                        "name": "Arka Dutta"
                    },
                    {
                        "name": "Sean Kim"
                    },
                    {
                        "name": "Ashiqur R. KhudaBukhsh"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Munmun De Choudhury"
                },
                "author": "Munmun De Choudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; K.4.1; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06122v2",
                "updated": "2025-04-09T04:03:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    4,
                    3,
                    0,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T15:15:26Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    15,
                    15,
                    26,
                    1,
                    98,
                    0
                ],
                "title": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leanabell-Prover: Posttraining Scaling in Formal Reasoning"
                },
                "summary": "Recent advances in automated theorem proving (ATP) through LLMs have\nhighlighted the potential of formal reasoning with Lean 4 codes. However, ATP\nhas not yet be revolutionized by the recent posttraining scaling as\ndemonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the\nentire posttraining of ATP, aiming to align it with breakthroughs in reasoning\nmodels in natural languages. To begin, we continual train current ATP models\nwith a hybrid dataset, which consists of numerous statement-proof pairs, and\nadditional data aimed at incorporating cognitive behaviors that emulate human\nreasoning and hypothesis refinement. Next, we explore reinforcement learning\nwith the use of outcome reward returned by Lean 4 compiler. Through our\ndesigned continual training and reinforcement learning processes, we have\nsuccessfully improved existing formal provers, including both\nDeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance\nin the field of whole-proof generation. For example, we achieve a 59.8% pass\nrate (pass@32) on MiniF2F. This is an on-going project and we will\nprogressively update our findings, release our data and training details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in automated theorem proving (ATP) through LLMs have\nhighlighted the potential of formal reasoning with Lean 4 codes. However, ATP\nhas not yet be revolutionized by the recent posttraining scaling as\ndemonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the\nentire posttraining of ATP, aiming to align it with breakthroughs in reasoning\nmodels in natural languages. To begin, we continual train current ATP models\nwith a hybrid dataset, which consists of numerous statement-proof pairs, and\nadditional data aimed at incorporating cognitive behaviors that emulate human\nreasoning and hypothesis refinement. Next, we explore reinforcement learning\nwith the use of outcome reward returned by Lean 4 compiler. Through our\ndesigned continual training and reinforcement learning processes, we have\nsuccessfully improved existing formal provers, including both\nDeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance\nin the field of whole-proof generation. For example, we achieve a 59.8% pass\nrate (pass@32) on MiniF2F. This is an on-going project and we will\nprogressively update our findings, release our data and training details."
                },
                "authors": [
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Xingguang Ji"
                    },
                    {
                        "name": "Yahui Liu"
                    },
                    {
                        "name": "Yang Yue"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06564v1",
                "updated": "2025-04-09T03:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    58,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T03:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    58,
                    19,
                    2,
                    99,
                    0
                ],
                "title": "Do Reasoning Models Show Better Verbalized Calibration?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Reasoning Models Show Better Verbalized Calibration?"
                },
                "summary": "Large reasoning models (LRMs) have recently shown impressive capabilities in\ncomplex reasoning by leveraging increased test-time computation and exhibiting\nbehaviors akin to human-like deliberation. Despite these advances, it remains\nan open question whether LRMs are better calibrated - particularly in their\nverbalized confidence - compared to instruction-tuned counterparts. In this\npaper, we investigate the calibration properties of LRMs trained via supervised\nfine-tuning distillation on long reasoning traces (henceforth SFT reasoning\nmodels) and outcome-based reinforcement learning for reasoning (henceforth RL\nreasoning models) across diverse domains. Our findings reveal that LRMs\nsignificantly outperform instruction-tuned models on complex reasoning tasks in\nboth accuracy and confidence calibration. In contrast, we find surprising\ntrends in the domain of factuality in particular. On factuality tasks, while\nDeepseek-R1 shows strong calibration behavior, smaller QwQ-32B shows no\nimprovement over instruct models; moreover, SFT reasoning models display worse\ncalibration (greater overconfidence) compared to instruct models. Our results\nprovide evidence for a potentially critical role of reasoning-oriented RL\ntraining in improving LLMs' capacity for generating trustworthy, self-aware\noutputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) have recently shown impressive capabilities in\ncomplex reasoning by leveraging increased test-time computation and exhibiting\nbehaviors akin to human-like deliberation. Despite these advances, it remains\nan open question whether LRMs are better calibrated - particularly in their\nverbalized confidence - compared to instruction-tuned counterparts. In this\npaper, we investigate the calibration properties of LRMs trained via supervised\nfine-tuning distillation on long reasoning traces (henceforth SFT reasoning\nmodels) and outcome-based reinforcement learning for reasoning (henceforth RL\nreasoning models) across diverse domains. Our findings reveal that LRMs\nsignificantly outperform instruction-tuned models on complex reasoning tasks in\nboth accuracy and confidence calibration. In contrast, we find surprising\ntrends in the domain of factuality in particular. On factuality tasks, while\nDeepseek-R1 shows strong calibration behavior, smaller QwQ-32B shows no\nimprovement over instruct models; moreover, SFT reasoning models display worse\ncalibration (greater overconfidence) compared to instruct models. Our results\nprovide evidence for a potentially critical role of reasoning-oriented RL\ntraining in improving LLMs' capacity for generating trustworthy, self-aware\noutputs."
                },
                "authors": [
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Leyang Cui"
                    },
                    {
                        "name": "Rob Voigt"
                    }
                ],
                "author_detail": {
                    "name": "Rob Voigt"
                },
                "author": "Rob Voigt",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06562v1",
                "updated": "2025-04-09T03:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    51,
                    53,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T03:51:53Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    51,
                    53,
                    2,
                    99,
                    0
                ],
                "title": "FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion"
                },
                "summary": "Heterogeneous model fusion enhances the performance of LLMs by integrating\nthe knowledge and capabilities of multiple structurally diverse models.\nHowever, existing approaches often rely solely on selecting the best output for\neach prompt from source models, which underutilizes their full potential due to\nlimited source knowledge and results in sparse optimization signals. To address\nthis limitation, we propose FuseRL, a novel two-stage framework comprising\nFuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT\nestablishes a robust initialization by integrating the strengths of\nheterogeneous source models through weighted supervised fine-tuning (SFT) on\ndiverse outputs for each prompt. FusePO optimizes weighted preferences based on\nthe outputs of multiple source models to enable superior alignment performance.\nExtensive experiments demonstrate the effectiveness of our framework across\nvarious preference alignment methods, including RLOO, DPO, and SimPO. Using\nLlama-3.1-8B-Instruct as the target model, our approach achieves\nstate-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard\nbenchmarks. Further analysis suggests that FuseSFT regularizes the training\nprocess to reduce overfitting, while FusePO introduces dense and diverse\nsignals for preference optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous model fusion enhances the performance of LLMs by integrating\nthe knowledge and capabilities of multiple structurally diverse models.\nHowever, existing approaches often rely solely on selecting the best output for\neach prompt from source models, which underutilizes their full potential due to\nlimited source knowledge and results in sparse optimization signals. To address\nthis limitation, we propose FuseRL, a novel two-stage framework comprising\nFuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT\nestablishes a robust initialization by integrating the strengths of\nheterogeneous source models through weighted supervised fine-tuning (SFT) on\ndiverse outputs for each prompt. FusePO optimizes weighted preferences based on\nthe outputs of multiple source models to enable superior alignment performance.\nExtensive experiments demonstrate the effectiveness of our framework across\nvarious preference alignment methods, including RLOO, DPO, and SimPO. Using\nLlama-3.1-8B-Instruct as the target model, our approach achieves\nstate-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard\nbenchmarks. Further analysis suggests that FuseSFT regularizes the training\nprocess to reduce overfitting, while FusePO introduces dense and diverse\nsignals for preference optimization."
                },
                "authors": [
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Ziyi Yang"
                    },
                    {
                        "name": "Guosheng Liang"
                    },
                    {
                        "name": "Tianyuan Shi"
                    },
                    {
                        "name": "Xiaojun Quan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Quan"
                },
                "author": "Xiaojun Quan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06560v1",
                "updated": "2025-04-09T03:46:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    46,
                    56,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T03:46:56Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    46,
                    56,
                    2,
                    99,
                    0
                ],
                "title": "NeedleInATable: Exploring Long-Context Capability of Large Language\n  Models towards Long-Structured Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeedleInATable: Exploring Long-Context Capability of Large Language\n  Models towards Long-Structured Tables"
                },
                "summary": "Processing structured tabular data, particularly lengthy tables, constitutes\na fundamental yet challenging task for large language models (LLMs). However,\nexisting long-context benchmarks primarily focus on unstructured text,\nneglecting the challenges of long and complex structured tables. To address\nthis gap, we introduce NeedleInATable (NIAT), a novel task that treats each\ntable cell as a \"needle\" and requires the model to extract the target cell\nunder different queries. Evaluation results of mainstream LLMs on this\nbenchmark show they lack robust long-table comprehension, often relying on\nsuperficial correlations or shortcuts for complex table understanding tasks,\nrevealing significant limitations in processing intricate tabular data. To this\nend, we propose a data synthesis method to enhance models' long-table\ncomprehension capabilities. Experimental results show that our synthesized\ntraining data significantly enhances LLMs' performance on the NIAT task,\noutperforming both long-context LLMs and long-table agent methods. This work\nadvances the evaluation of LLMs' genuine long-structured table comprehension\ncapabilities and paves the way for progress in long-context and table\nunderstanding applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing structured tabular data, particularly lengthy tables, constitutes\na fundamental yet challenging task for large language models (LLMs). However,\nexisting long-context benchmarks primarily focus on unstructured text,\nneglecting the challenges of long and complex structured tables. To address\nthis gap, we introduce NeedleInATable (NIAT), a novel task that treats each\ntable cell as a \"needle\" and requires the model to extract the target cell\nunder different queries. Evaluation results of mainstream LLMs on this\nbenchmark show they lack robust long-table comprehension, often relying on\nsuperficial correlations or shortcuts for complex table understanding tasks,\nrevealing significant limitations in processing intricate tabular data. To this\nend, we propose a data synthesis method to enhance models' long-table\ncomprehension capabilities. Experimental results show that our synthesized\ntraining data significantly enhances LLMs' performance on the NIAT task,\noutperforming both long-context LLMs and long-table agent methods. This work\nadvances the evaluation of LLMs' genuine long-structured table comprehension\ncapabilities and paves the way for progress in long-context and table\nunderstanding applications."
                },
                "authors": [
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Mingyu Zheng"
                    },
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Yanan Cao"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03784v2",
                "updated": "2025-04-09T03:41:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    41,
                    9,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-03T16:16:35Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    16,
                    35,
                    3,
                    93,
                    0
                ],
                "title": "Robust Reinforcement Learning from Human Feedback for Large Language\n  Models Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Reinforcement Learning from Human Feedback for Large Language\n  Models Fine-Tuning"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has emerged as a key\ntechnique for aligning the output of large language models (LLMs) with human\npreferences. To learn the reward function, most existing RLHF algorithms use\nthe Bradley-Terry model, which relies on assumptions about human preferences\nthat may not reflect the complexity and variability of real-world judgments. In\nthis paper, we propose a robust algorithm to enhance the performance of\nexisting approaches under such reward model misspecifications. Theoretically,\nour algorithm reduces the variance of reward and policy estimators, leading to\nimproved regret bounds. Empirical evaluations on LLM benchmark datasets\ndemonstrate that the proposed algorithm consistently outperforms existing\nmethods, with 77-81% of responses being favored over baselines on the Anthropic\nHelpful and Harmless dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has emerged as a key\ntechnique for aligning the output of large language models (LLMs) with human\npreferences. To learn the reward function, most existing RLHF algorithms use\nthe Bradley-Terry model, which relies on assumptions about human preferences\nthat may not reflect the complexity and variability of real-world judgments. In\nthis paper, we propose a robust algorithm to enhance the performance of\nexisting approaches under such reward model misspecifications. Theoretically,\nour algorithm reduces the variance of reward and policy estimators, leading to\nimproved regret bounds. Empirical evaluations on LLM benchmark datasets\ndemonstrate that the proposed algorithm consistently outperforms existing\nmethods, with 77-81% of responses being favored over baselines on the Anthropic\nHelpful and Harmless dataset."
                },
                "authors": [
                    {
                        "name": "Kai Ye"
                    },
                    {
                        "name": "Hongyi Zhou"
                    },
                    {
                        "name": "Jin Zhu"
                    },
                    {
                        "name": "Francesco Quinzan"
                    },
                    {
                        "name": "Chengchung Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chengchung Shi"
                },
                "author": "Chengchung Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06553v2",
                "updated": "2025-04-10T01:34:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    1,
                    34,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-09T03:22:52Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    22,
                    52,
                    2,
                    99,
                    0
                ],
                "title": "ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis"
                },
                "summary": "While recent work in scene reconstruction and understanding has made strides\nin grounding natural language to physical 3D environments, it is still\nchallenging to ground abstract, high-level instructions to a 3D scene.\nHigh-level instructions might not explicitly invoke semantic elements in the\nscene, and even the process of breaking a high-level task into a set of more\nconcrete subtasks, a process called hierarchical task analysis, is\nenvironment-dependent. In this work, we propose ASHiTA, the first framework\nthat generates a task hierarchy grounded to a 3D scene graph by breaking down\nhigh-level tasks into grounded subtasks. ASHiTA alternates LLM-assisted\nhierarchical task analysis, to generate the task breakdown, with task-driven 3D\nscene graph construction to generate a suitable representation of the\nenvironment. Our experiments show that ASHiTA performs significantly better\nthan LLM baselines in breaking down high-level tasks into environment-dependent\nsubtasks and is additionally able to achieve grounding performance comparable\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent work in scene reconstruction and understanding has made strides\nin grounding natural language to physical 3D environments, it is still\nchallenging to ground abstract, high-level instructions to a 3D scene.\nHigh-level instructions might not explicitly invoke semantic elements in the\nscene, and even the process of breaking a high-level task into a set of more\nconcrete subtasks, a process called hierarchical task analysis, is\nenvironment-dependent. In this work, we propose ASHiTA, the first framework\nthat generates a task hierarchy grounded to a 3D scene graph by breaking down\nhigh-level tasks into grounded subtasks. ASHiTA alternates LLM-assisted\nhierarchical task analysis, to generate the task breakdown, with task-driven 3D\nscene graph construction to generate a suitable representation of the\nenvironment. Our experiments show that ASHiTA performs significantly better\nthan LLM baselines in breaking down high-level tasks into environment-dependent\nsubtasks and is additionally able to achieve grounding performance comparable\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yun Chang"
                    },
                    {
                        "name": "Leonor Fermoselle"
                    },
                    {
                        "name": "Duy Ta"
                    },
                    {
                        "name": "Bernadette Bucher"
                    },
                    {
                        "name": "Luca Carlone"
                    },
                    {
                        "name": "Jiuguang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiuguang Wang"
                },
                "author": "Jiuguang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05632v2",
                "updated": "2025-04-09T03:05:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    5,
                    13,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T03:21:51Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    21,
                    51,
                    1,
                    98,
                    0
                ],
                "title": "Reasoning Towards Fairness: Mitigating Bias in Language Models through\n  Reasoning-Guided Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Towards Fairness: Mitigating Bias in Language Models through\n  Reasoning-Guided Fine-Tuning"
                },
                "summary": "Recent advances in large-scale generative language models have shown that\nreasoning capabilities can significantly improve model performance across a\nvariety of tasks. However, the impact of reasoning on a model's ability to\nmitigate stereotypical responses remains largely underexplored. In this work,\nwe investigate the crucial relationship between a model's reasoning ability and\nfairness, and ask whether improved reasoning capabilities can mitigate harmful\nstereotypical responses, especially those arising due to shallow or flawed\nreasoning. We conduct a comprehensive evaluation of multiple open-source LLMs,\nand find that larger models with stronger reasoning abilities exhibit\nsubstantially lower stereotypical bias on existing fairness benchmarks.\nBuilding on this insight, we introduce ReGiFT -- Reasoning Guided Fine-Tuning,\na novel approach that extracts structured reasoning traces from advanced\nreasoning models and infuses them into models that lack such capabilities. We\nuse only general-purpose reasoning and do not require any fairness-specific\nsupervision for bias mitigation. Notably, we see that models fine-tuned using\nReGiFT not only improve fairness relative to their non-reasoning counterparts\nbut also outperform advanced reasoning models on fairness benchmarks. We also\nanalyze how variations in the correctness of the reasoning traces and their\nlength influence model fairness and their overall performance. Our findings\nhighlight that enhancing reasoning capabilities is an effective,\nfairness-agnostic strategy for mitigating stereotypical bias caused by\nreasoning flaws.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale generative language models have shown that\nreasoning capabilities can significantly improve model performance across a\nvariety of tasks. However, the impact of reasoning on a model's ability to\nmitigate stereotypical responses remains largely underexplored. In this work,\nwe investigate the crucial relationship between a model's reasoning ability and\nfairness, and ask whether improved reasoning capabilities can mitigate harmful\nstereotypical responses, especially those arising due to shallow or flawed\nreasoning. We conduct a comprehensive evaluation of multiple open-source LLMs,\nand find that larger models with stronger reasoning abilities exhibit\nsubstantially lower stereotypical bias on existing fairness benchmarks.\nBuilding on this insight, we introduce ReGiFT -- Reasoning Guided Fine-Tuning,\na novel approach that extracts structured reasoning traces from advanced\nreasoning models and infuses them into models that lack such capabilities. We\nuse only general-purpose reasoning and do not require any fairness-specific\nsupervision for bias mitigation. Notably, we see that models fine-tuned using\nReGiFT not only improve fairness relative to their non-reasoning counterparts\nbut also outperform advanced reasoning models on fairness benchmarks. We also\nanalyze how variations in the correctness of the reasoning traces and their\nlength influence model fairness and their overall performance. Our findings\nhighlight that enhancing reasoning capabilities is an effective,\nfairness-agnostic strategy for mitigating stereotypical bias caused by\nreasoning flaws."
                },
                "authors": [
                    {
                        "name": "Sanchit Kabra"
                    },
                    {
                        "name": "Akshita Jha"
                    },
                    {
                        "name": "Chandan K. Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K. Reddy"
                },
                "author": "Chandan K. Reddy",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06536v1",
                "updated": "2025-04-09T02:25:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    2,
                    25,
                    53,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T02:25:53Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    2,
                    25,
                    53,
                    2,
                    99,
                    0
                ],
                "title": "Lugha-Llama: Adapting Large Language Models for African Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lugha-Llama: Adapting Large Language Models for African Languages"
                },
                "summary": "Large language models (LLMs) have achieved impressive results in a wide range\nof natural language applications. However, they often struggle to recognize\nlow-resource languages, in particular African languages, which are not well\nrepresented in large training corpora. In this paper, we consider how to adapt\nLLMs to low-resource African languages. We find that combining curated data\nfrom African languages with high-quality English educational texts results in a\ntraining mix that substantially improves the model's performance on these\nlanguages. On the challenging IrokoBench dataset, our models consistently\nachieve the best performance amongst similarly sized baselines, particularly on\nknowledge-intensive multiple-choice questions (AfriMMLU). Additionally, on the\ncross-lingual question answering benchmark AfriQA, our models outperform the\nbase model by over 10%. To better understand the role of English data during\ntraining, we translate a subset of 200M tokens into Swahili language and\nperform an analysis which reveals that the content of these data is primarily\nresponsible for the strong performance. We release our models and data to\nencourage future research on African languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive results in a wide range\nof natural language applications. However, they often struggle to recognize\nlow-resource languages, in particular African languages, which are not well\nrepresented in large training corpora. In this paper, we consider how to adapt\nLLMs to low-resource African languages. We find that combining curated data\nfrom African languages with high-quality English educational texts results in a\ntraining mix that substantially improves the model's performance on these\nlanguages. On the challenging IrokoBench dataset, our models consistently\nachieve the best performance amongst similarly sized baselines, particularly on\nknowledge-intensive multiple-choice questions (AfriMMLU). Additionally, on the\ncross-lingual question answering benchmark AfriQA, our models outperform the\nbase model by over 10%. To better understand the role of English data during\ntraining, we translate a subset of 200M tokens into Swahili language and\nperform an analysis which reveals that the content of these data is primarily\nresponsible for the strong performance. We release our models and data to\nencourage future research on African languages."
                },
                "authors": [
                    {
                        "name": "Happy Buzaaba"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "David Ifeoluwa Adelani"
                    },
                    {
                        "name": "Christiane Fellbaum"
                    }
                ],
                "author_detail": {
                    "name": "Christiane Fellbaum"
                },
                "author": "Christiane Fellbaum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06514v1",
                "updated": "2025-04-09T01:25:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    1,
                    25,
                    27,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T01:25:27Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    1,
                    25,
                    27,
                    2,
                    99,
                    0
                ],
                "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?"
                },
                "summary": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem."
                },
                "authors": [
                    {
                        "name": "Chenrui Fan"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06512v1",
                "updated": "2025-04-09T01:21:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    1,
                    21,
                    30,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T01:21:30Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    1,
                    21,
                    30,
                    2,
                    99,
                    0
                ],
                "title": "ICPS: Real-Time Resource Configuration for Cloud Serverless Functions\n  Considering Affinity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICPS: Real-Time Resource Configuration for Cloud Serverless Functions\n  Considering Affinity"
                },
                "summary": "Serverless computing, with its operational simplicity and on-demand\nscalability, has become a preferred paradigm for deploying workflow\napplications. However, resource allocation for workflows, particularly those\nwith branching structures, is complicated by cold starts and network delays\nbetween dependent functions, significantly degrading execution efficiency and\nresponse times. In this paper, we propose the Invocation Concurrency\nPrediction-Based Scaling (ICPS) algorithm to address these challenges. ICPS\nemploys Long Short-Term Memory (LSTM) networks to predict function concurrency,\ndynamically pre-warming function instances, and an affinity-based deployment\nstrategy to co-locate dependent functions on the same worker node, minimizing\nnetwork latency. The experimental results demonstrate that ICPS consistently\noutperforms existing approaches in diverse scenarios. The results confirm ICPS\nas a robust and scalable solution for optimizing serverless workflow execution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing, with its operational simplicity and on-demand\nscalability, has become a preferred paradigm for deploying workflow\napplications. However, resource allocation for workflows, particularly those\nwith branching structures, is complicated by cold starts and network delays\nbetween dependent functions, significantly degrading execution efficiency and\nresponse times. In this paper, we propose the Invocation Concurrency\nPrediction-Based Scaling (ICPS) algorithm to address these challenges. ICPS\nemploys Long Short-Term Memory (LSTM) networks to predict function concurrency,\ndynamically pre-warming function instances, and an affinity-based deployment\nstrategy to co-locate dependent functions on the same worker node, minimizing\nnetwork latency. The experimental results demonstrate that ICPS consistently\noutperforms existing approaches in diverse scenarios. The results confirm ICPS\nas a robust and scalable solution for optimizing serverless workflow execution."
                },
                "authors": [
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Xinshuai Hua"
                    },
                    {
                        "name": "Jinquan Zhang"
                    },
                    {
                        "name": "Wenshuai Li"
                    },
                    {
                        "name": "Xiaoping Li"
                    },
                    {
                        "name": "Shijie Guo"
                    }
                ],
                "author_detail": {
                    "name": "Shijie Guo"
                },
                "author": "Shijie Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17620v2",
                "updated": "2025-04-09T01:19:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    1,
                    19,
                    22,
                    2,
                    99,
                    0
                ],
                "published": "2025-03-22T02:32:09Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    2,
                    32,
                    9,
                    5,
                    81,
                    0
                ],
                "title": "A Case Study of Scalable Content Annotation Using Multi-LLM Consensus\n  and Human Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case Study of Scalable Content Annotation Using Multi-LLM Consensus\n  and Human Review"
                },
                "summary": "Content annotation at scale remains challenging, requiring substantial human\nexpertise and effort. This paper presents a case study in code documentation\nanalysis, where we explore the balance between automation efficiency and\nannotation accuracy. We present MCHR (Multi-LLM Consensus with Human Review), a\nnovel semi-automated framework that enhances annotation scalability through the\nsystematic integration of multiple LLMs and targeted human review. Our\nframework introduces a structured consensus-building mechanism among LLMs and\nan adaptive review protocol that strategically engages human expertise. Through\nour case study, we demonstrate that MCHR reduces annotation time by 32% to 100%\ncompared to manual annotation while maintaining high accuracy (85.5% to 98%)\nacross different difficulty levels, from basic binary classification to\nchallenging open-set scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content annotation at scale remains challenging, requiring substantial human\nexpertise and effort. This paper presents a case study in code documentation\nanalysis, where we explore the balance between automation efficiency and\nannotation accuracy. We present MCHR (Multi-LLM Consensus with Human Review), a\nnovel semi-automated framework that enhances annotation scalability through the\nsystematic integration of multiple LLMs and targeted human review. Our\nframework introduces a structured consensus-building mechanism among LLMs and\nan adaptive review protocol that strategically engages human expertise. Through\nour case study, we demonstrate that MCHR reduces annotation time by 32% to 100%\ncompared to manual annotation while maintaining high accuracy (85.5% to 98%)\nacross different difficulty levels, from basic binary classification to\nchallenging open-set scenarios."
                },
                "authors": [
                    {
                        "name": "Mingyue Yuan"
                    },
                    {
                        "name": "Jieshan Chen"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Gelareh Mohammadi"
                    },
                    {
                        "name": "Aaron Quigley"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Quigley"
                },
                "author": "Aaron Quigley",
                "arxiv_comment": "4 pages, GenAICHI 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06511v1",
                "updated": "2025-04-09T01:12:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    1,
                    12,
                    7,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T01:12:07Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    1,
                    12,
                    7,
                    2,
                    99,
                    0
                ],
                "title": "GTS-LUM: Reshaping User Behavior Modeling with LLMs in\n  Telecommunications Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTS-LUM: Reshaping User Behavior Modeling with LLMs in\n  Telecommunications Industry"
                },
                "summary": "As telecommunication service providers shifting their focus to analyzing user\nbehavior for package design and marketing interventions, a critical challenge\nlies in developing a unified, end-to-end framework capable of modeling\nlong-term and periodic user behavior sequences with diverse time granularities,\nmulti-modal data inputs, and heterogeneous labels. This paper introduces\nGTS-LUM, a novel user behavior model that redefines modeling paradigms in\ntelecommunication settings. GTS-LUM adopts a (multi-modal) encoder-adapter-LLM\ndecoder architecture, enhanced with several telecom-specific innovations.\nSpecifically, the model incorporates an advanced timestamp processing method to\nhandle varying time granularities. It also supports multi-modal data inputs --\nincluding structured tables and behavior co-occurrence graphs -- and aligns\nthese with semantic information extracted by a tokenizer using a Q-former\nstructure. Additionally, GTS-LUM integrates a front-placed target-aware\nmechanism to highlight historical behaviors most relevant to the target.\nExtensive experiments on industrial dataset validate the effectiveness of this\nend-to-end framework and also demonstrate that GTS-LUM outperforms LLM4Rec\napproaches which are popular in recommendation systems, offering an effective\nand generalizing solution for user behavior modeling in telecommunications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As telecommunication service providers shifting their focus to analyzing user\nbehavior for package design and marketing interventions, a critical challenge\nlies in developing a unified, end-to-end framework capable of modeling\nlong-term and periodic user behavior sequences with diverse time granularities,\nmulti-modal data inputs, and heterogeneous labels. This paper introduces\nGTS-LUM, a novel user behavior model that redefines modeling paradigms in\ntelecommunication settings. GTS-LUM adopts a (multi-modal) encoder-adapter-LLM\ndecoder architecture, enhanced with several telecom-specific innovations.\nSpecifically, the model incorporates an advanced timestamp processing method to\nhandle varying time granularities. It also supports multi-modal data inputs --\nincluding structured tables and behavior co-occurrence graphs -- and aligns\nthese with semantic information extracted by a tokenizer using a Q-former\nstructure. Additionally, GTS-LUM integrates a front-placed target-aware\nmechanism to highlight historical behaviors most relevant to the target.\nExtensive experiments on industrial dataset validate the effectiveness of this\nend-to-end framework and also demonstrate that GTS-LUM outperforms LLM4Rec\napproaches which are popular in recommendation systems, offering an effective\nand generalizing solution for user behavior modeling in telecommunications."
                },
                "authors": [
                    {
                        "name": "Liu Shi"
                    },
                    {
                        "name": "Tianwu Zhou"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Zhexin Cui"
                    },
                    {
                        "name": "Shaoyi Liang"
                    },
                    {
                        "name": "Haoxing Niu"
                    },
                    {
                        "name": "Yichong Tian"
                    },
                    {
                        "name": "Jianwei Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Guo"
                },
                "author": "Jianwei Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11871v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11871v4",
                "updated": "2025-04-09T00:21:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    0,
                    21,
                    7,
                    2,
                    99,
                    0
                ],
                "published": "2024-05-31T01:41:48Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    1,
                    41,
                    48,
                    4,
                    152,
                    0
                ],
                "title": "Generative AI Voting: Fair Collective Choice is Resilient to LLM Biases\n  and Inconsistencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI Voting: Fair Collective Choice is Resilient to LLM Biases\n  and Inconsistencies"
                },
                "summary": "Scaling up deliberative and voting participation is a longstanding endeavor\n-- a cornerstone for direct democracy and legitimate collective choice. Recent\nbreakthroughs in generative artificial intelligence (AI) and large language\nmodels (LLMs) unravel new capabilities for AI personal assistants to overcome\ncognitive bandwidth limitations of humans, providing decision support or even\ndirect representation of human voters at large scale. However, the quality of\nthis representation and what underlying biases manifest when delegating\ncollective decision-making to LLMs is an alarming and timely challenge to\ntackle. By rigorously emulating with high realism more than >50K LLM voting\npersonas in 306 real-world voting elections, we disentangle the nature of\ndifferent biases in LLMS (GPT 3, GPT 3.5, and Llama2). Complex preferential\nballot formats exhibit significant inconsistencies compared to simpler\nmajoritarian elections that show higher consistency. Strikingly though, by\ndemonstrating for the first time in real-world a proportional representation of\nvoters in direct democracy, we are also able to show that fair ballot\naggregation methods, such as equal shares, prove to be a win-win: fairer voting\noutcomes for humans with fairer AI representation, especially for voters who\nare likely to abstain. This novel underlying relationship proves paramount for\ndemocratic resilience in progressives scenarios with low voters turnout and\nvoter fatigue supported by AI representatives: abstained voters are mitigated\nby recovering highly representative voting outcomes that are fairer. These\ninterdisciplinary insights provide remarkable foundations for science,\npolicymakers, and citizens to develop safeguards and resilience for AI risks in\ndemocratic innovations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up deliberative and voting participation is a longstanding endeavor\n-- a cornerstone for direct democracy and legitimate collective choice. Recent\nbreakthroughs in generative artificial intelligence (AI) and large language\nmodels (LLMs) unravel new capabilities for AI personal assistants to overcome\ncognitive bandwidth limitations of humans, providing decision support or even\ndirect representation of human voters at large scale. However, the quality of\nthis representation and what underlying biases manifest when delegating\ncollective decision-making to LLMs is an alarming and timely challenge to\ntackle. By rigorously emulating with high realism more than >50K LLM voting\npersonas in 306 real-world voting elections, we disentangle the nature of\ndifferent biases in LLMS (GPT 3, GPT 3.5, and Llama2). Complex preferential\nballot formats exhibit significant inconsistencies compared to simpler\nmajoritarian elections that show higher consistency. Strikingly though, by\ndemonstrating for the first time in real-world a proportional representation of\nvoters in direct democracy, we are also able to show that fair ballot\naggregation methods, such as equal shares, prove to be a win-win: fairer voting\noutcomes for humans with fairer AI representation, especially for voters who\nare likely to abstain. This novel underlying relationship proves paramount for\ndemocratic resilience in progressives scenarios with low voters turnout and\nvoter fatigue supported by AI representatives: abstained voters are mitigated\nby recovering highly representative voting outcomes that are fairer. These\ninterdisciplinary insights provide remarkable foundations for science,\npolicymakers, and citizens to develop safeguards and resilience for AI risks in\ndemocratic innovations."
                },
                "authors": [
                    {
                        "name": "Srijoni Majumdar"
                    },
                    {
                        "name": "Edith Elkind"
                    },
                    {
                        "name": "Evangelos Pournaras"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Pournaras"
                },
                "author": "Evangelos Pournaras",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11871v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11871v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10999v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10999v4",
                "updated": "2025-04-08T23:59:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    23,
                    59,
                    8,
                    1,
                    98,
                    0
                ],
                "published": "2024-06-16T16:25:22Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    16,
                    25,
                    22,
                    6,
                    168,
                    0
                ],
                "title": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Rigor and Utility: Mitigating Cognitive Biases in Large\n  Language Models for Multiple-Choice Questions"
                },
                "summary": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. When properly balanced, we show that certain cognitive\nbiases can enhance decision-making efficiency through rational deviations and\nheuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the role of cognitive biases in the decision-making\nprocesses of large language models (LLMs), challenging the conventional goal of\neliminating all biases. When properly balanced, we show that certain cognitive\nbiases can enhance decision-making efficiency through rational deviations and\nheuristic shortcuts. By introducing heuristic moderation and an abstention\noption, which allows LLMs to withhold responses when uncertain, we reduce error\nrates, improve decision accuracy, and optimize decision rates. Using the\nBalance Rigor and Utility (BRU) dataset, developed through expert\ncollaboration, our findings demonstrate that targeted inspection of cognitive\nbiases aligns LLM decisions more closely with human reasoning, enhancing\nreliability and suggesting strategies for future improvements. This approach\noffers a novel way to leverage cognitive biases to improve the practical\nutility of LLMs across various applications."
                },
                "authors": [
                    {
                        "name": "Hanyang Zhong"
                    },
                    {
                        "name": "Liman Wang"
                    },
                    {
                        "name": "Wenting Cao"
                    },
                    {
                        "name": "Zeyuan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zeyuan Sun"
                },
                "author": "Zeyuan Sun",
                "arxiv_comment": "This work has been accepted as a full paper at the 2025 Annual\n  Conference of the Cognitive Science Society (CogSci 2025) and will be\n  presented in the form of a poster. The associated public dataset and project\n  website are available at: https://hanyangzhong.github.io/BRU-website/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10999v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10999v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06486v1",
                "updated": "2025-04-08T23:19:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    23,
                    19,
                    0,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T23:19:00Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    23,
                    19,
                    0,
                    1,
                    98,
                    0
                ],
                "title": "Mind the Gap: Evaluating Vision Systems in Small Data Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: Evaluating Vision Systems in Small Data Applications"
                },
                "summary": "The practical application of AI tools for specific computer vision tasks\nrelies on the \"small-data regime\" of hundreds to thousands of labeled samples.\nThis small-data regime is vital for applications requiring expensive expert\nannotations, such as ecological monitoring, medical diagnostics or industrial\nquality control. We find, however, that computer vision research has ignored\nthe small data regime as evaluations increasingly focus on zero- and few-shot\nlearning. We use the Natural World Tasks (NeWT) benchmark to compare\nmulti-modal large language models (MLLMs) and vision-only methods across\nvarying training set sizes. MLLMs exhibit early performance plateaus, while\nvision-only methods improve throughout the small-data regime, with performance\ngaps widening beyond 10 training examples. We provide the first comprehensive\ncomparison between these approaches in small-data contexts and advocate for\nexplicit small-data evaluations in AI research to better bridge theoretical\nadvances with practical deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The practical application of AI tools for specific computer vision tasks\nrelies on the \"small-data regime\" of hundreds to thousands of labeled samples.\nThis small-data regime is vital for applications requiring expensive expert\nannotations, such as ecological monitoring, medical diagnostics or industrial\nquality control. We find, however, that computer vision research has ignored\nthe small data regime as evaluations increasingly focus on zero- and few-shot\nlearning. We use the Natural World Tasks (NeWT) benchmark to compare\nmulti-modal large language models (MLLMs) and vision-only methods across\nvarying training set sizes. MLLMs exhibit early performance plateaus, while\nvision-only methods improve throughout the small-data regime, with performance\ngaps widening beyond 10 training examples. We provide the first comprehensive\ncomparison between these approaches in small-data contexts and advocate for\nexplicit small-data evaluations in AI research to better bridge theoretical\nadvances with practical deployments."
                },
                "authors": [
                    {
                        "name": "Samuel Stevens"
                    },
                    {
                        "name": "S M Rayeed"
                    },
                    {
                        "name": "Jenna Kline"
                    }
                ],
                "author_detail": {
                    "name": "Jenna Kline"
                },
                "author": "Jenna Kline",
                "arxiv_comment": "4 pages (main text), 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14567v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14567v3",
                "updated": "2025-04-08T22:24:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    22,
                    24,
                    8,
                    1,
                    98,
                    0
                ],
                "published": "2024-10-18T16:11:29Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    11,
                    29,
                    4,
                    292,
                    0
                ],
                "title": "ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions"
                },
                "summary": "Large Language Models (LLMs) are widely used in Conversational AI systems to\ngenerate responses to user inquiries. However, many natural questions lack\nwell-defined answers. While existing studies primarily focus on question types\nsuch as false premises, they often overlook out-of-scope questions, where the\nprovided document is semantically highly similar to the query but does not\ncontain the required answer. In this paper, we propose a guided\nhallucination-based method to efficiently generate a diverse set of\nout-of-scope questions from a given document corpus. We then evaluate multiple\nLLMs based on their effectiveness in confusion detection and appropriate\nresponse generation. Furthermore, we introduce an improved method for detecting\nsuch out-of-scope questions, enhancing the reliability of LLM-based\nquestion-answering systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in Conversational AI systems to\ngenerate responses to user inquiries. However, many natural questions lack\nwell-defined answers. While existing studies primarily focus on question types\nsuch as false premises, they often overlook out-of-scope questions, where the\nprovided document is semantically highly similar to the query but does not\ncontain the required answer. In this paper, we propose a guided\nhallucination-based method to efficiently generate a diverse set of\nout-of-scope questions from a given document corpus. We then evaluate multiple\nLLMs based on their effectiveness in confusion detection and appropriate\nresponse generation. Furthermore, we introduce an improved method for detecting\nsuch out-of-scope questions, enhancing the reliability of LLM-based\nquestion-answering systems."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Jinming Nian"
                    },
                    {
                        "name": "Alexandre Evfimievski"
                    },
                    {
                        "name": "Yi Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Fang"
                },
                "author": "Yi Fang",
                "arxiv_comment": "Accepted by SIGIR'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14567v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14567v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06460v1",
                "updated": "2025-04-08T22:00:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    22,
                    0,
                    32,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T22:00:32Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    22,
                    0,
                    32,
                    1,
                    98,
                    0
                ],
                "title": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for\n  Counterfactual Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for\n  Counterfactual Instruction Following"
                },
                "summary": "Large Language Models (LLMs) are now increasingly widely used to simulate\npersonas in virtual environments, leveraging their instruction-following\ncapability. However, we discovered that even state-of-the-art LLMs cannot\nsimulate personas with reversed performance (e.g., student personas with low\nproficiency in educational settings), which impairs the simulation diversity\nand limits the practical applications of the simulated environments. In this\nwork, using mathematical reasoning as a representative scenario, we propose the\nfirst benchmark dataset for evaluating LLMs on simulating personas with\nreversed performance, a capability that we dub \"counterfactual instruction\nfollowing\". We evaluate both open-weight and closed-source LLMs on this task\nand find that LLMs, including the OpenAI o1 reasoning model, all struggle to\nfollow counterfactual instructions for simulating reversedly performing\npersonas. Intersectionally simulating both the performance level and the race\npopulation of a persona worsens the effect even further. These results\nhighlight the challenges of counterfactual instruction following and the need\nfor further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are now increasingly widely used to simulate\npersonas in virtual environments, leveraging their instruction-following\ncapability. However, we discovered that even state-of-the-art LLMs cannot\nsimulate personas with reversed performance (e.g., student personas with low\nproficiency in educational settings), which impairs the simulation diversity\nand limits the practical applications of the simulated environments. In this\nwork, using mathematical reasoning as a representative scenario, we propose the\nfirst benchmark dataset for evaluating LLMs on simulating personas with\nreversed performance, a capability that we dub \"counterfactual instruction\nfollowing\". We evaluate both open-weight and closed-source LLMs on this task\nand find that LLMs, including the OpenAI o1 reasoning model, all struggle to\nfollow counterfactual instructions for simulating reversedly performing\npersonas. Intersectionally simulating both the performance level and the race\npopulation of a persona worsens the effect even further. These results\nhighlight the challenges of counterfactual instruction following and the need\nfor further research."
                },
                "authors": [
                    {
                        "name": "Sai Adith Senthil Kumar"
                    },
                    {
                        "name": "Hao Yan"
                    },
                    {
                        "name": "Saipavan Perepa"
                    },
                    {
                        "name": "Murong Yue"
                    },
                    {
                        "name": "Ziyu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Yao"
                },
                "author": "Ziyu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]