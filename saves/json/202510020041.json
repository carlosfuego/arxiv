[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.26541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v1",
                "updated": "2025-09-30T17:15:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v2",
                "updated": "2025-09-30T16:42:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    42,
                    50,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26432v1",
                "updated": "2025-09-30T15:53:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size"
                },
                "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs."
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao"
                    },
                    {
                        "name": "Chen"
                    },
                    {
                        "name": "Yuto Karashima"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "arxiv_affiliation": "Mark",
                "author": "Hongxiang Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v3",
                "updated": "2025-09-30T15:44:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    44,
                    29,
                    1,
                    273,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_doi": "10.1109/TED.2025.3617043",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TED.2025.3617043",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to IEEE Trans. Elec. Dev. Work enabled in part by NanoIC\n  pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26328v1",
                "updated": "2025-09-30T14:40:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:40:18Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM v2: Efficient Block-Diffusion LLM"
                },
                "summary": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v4",
                "updated": "2025-09-30T14:13:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    13,
                    20,
                    1,
                    273,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by NeurIPS 2025. Code at https://github.com/NVlabs/Long-RL\n  and model at https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v2",
                "updated": "2025-09-30T09:10:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    9,
                    10,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Lin Shi"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shi"
                },
                "author": "Lin Shi",
                "arxiv_comment": "Accepted by ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v2",
                "updated": "2025-09-30T02:51:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    51,
                    5,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "NeurIPS 2025 Oral. Code: https://github.com/snu-mllab/KVzip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25681v1",
                "updated": "2025-09-30T02:36:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T02:36:11Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought"
                },
                "summary": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Yi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Xu"
                },
                "author": "Yi Xu",
                "arxiv_comment": "technique report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25454v2",
                "updated": "2025-10-01T05:09:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    9,
                    42,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T20:00:29Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    20,
                    0,
                    29,
                    0,
                    272,
                    0
                ],
                "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search"
                },
                "summary": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation."
                },
                "authors": [
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Aaron Tu"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25401v1",
                "updated": "2025-09-29T18:57:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T18:57:14Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers"
                },
                "summary": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality."
                },
                "authors": [
                    {
                        "name": "Liang Qiao"
                    },
                    {
                        "name": "Yue Dai"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Hongyu Kan"
                    },
                    {
                        "name": "Jun Shi"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25188v1",
                "updated": "2025-09-29T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    59,
                    54,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:59:54Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    59,
                    54,
                    0,
                    272,
                    0
                ],
                "title": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Adaptive Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Adaptive Parallel Decoding"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache."
                },
                "authors": [
                    {
                        "name": "Wenrui Bao"
                    },
                    {
                        "name": "Zhiben Chen"
                    },
                    {
                        "name": "Dan Xu"
                    },
                    {
                        "name": "Yuzhang Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhang Shang"
                },
                "author": "Yuzhang Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25155v1",
                "updated": "2025-09-29T17:55:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:55:43Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "title": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units"
                },
                "summary": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts."
                },
                "authors": [
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rakshith Jayanth"
                    },
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IEEE HiPC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v2",
                "updated": "2025-09-29T15:20:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    20,
                    29,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "EMNLP 2025; 15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v2",
                "updated": "2025-09-29T15:15:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    49,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24832v1",
                "updated": "2025-09-29T14:16:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:16:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching"
                },
                "summary": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinye Zhao"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "arxiv_comment": "11 figures, 14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24791v1",
                "updated": "2025-09-29T13:45:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:45:35Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "title": "Vision Function Layer in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Function Layer in Multimodal LLMs"
                },
                "summary": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models."
                },
                "authors": [
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Sibei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sibei Yang"
                },
                "author": "Sibei Yang",
                "arxiv_comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v3",
                "updated": "2025-09-29T12:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    34,
                    50,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely used technique for accelerating inference in\nlarge language models (LLMs), but its performance degrades as input length\ngrows, with significant drops even at moderate lengths. Yet, this early\ndegradation has remained largely underexplored. We introduce SpecExtend, a\ndrop-in enhancement that improves speculative decoding on long sequences\nwithout additional training. SpecExtend integrates efficient attention\nmechanisms such as FlashAttention and Hybrid Tree Attention to accelerate\nprefill and verification steps. To improve both draft accuracy and speed on\nlong inputs without retraining, we propose Cross-model Retrieval, a novel KV\ncache eviction strategy that leverages the target model's attention scores to\ndynamically select relevant context for the smaller draft model. Extensive\nevaluations show that SpecExtend accelerates speculative decoding by up to\n2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while\npreserving the short-input performance of state-of-the-art frameworks. Our code\nis available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v1",
                "updated": "2025-09-29T12:28:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24626v1",
                "updated": "2025-09-29T11:35:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T11:35:55Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    11,
                    35,
                    55,
                    0,
                    272,
                    0
                ],
                "title": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in\n  Long-Context LLM Serving"
                },
                "summary": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "14 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24407v1",
                "updated": "2025-09-29T07:54:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T07:54:44Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    7,
                    54,
                    44,
                    0,
                    272,
                    0
                ],
                "title": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-REACH: Quantum information Repetition, Error Analysis and Correction\n  using Caching Network"
                },
                "summary": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum repeaters incorporating quantum memory play a pivotal role in\nmitigating loss in transmitted quantum information (photons) due to link\nattenuation over a long-distance quantum communication network. However,\nlimited availability of available storage in such quantum repeaters and the\nimpact on the time spent within the memory unit presents a trade-off between\nquantum information fidelity (a metric that quantifies the degree of similarity\nbetween a pair of quantum states) and qubit transmission rate. Thus, effective\nmanagement of storage time for qubits becomes a key consideration in multi-hop\nquantum networks. To address these challenges, we propose Q-REACH, which\nleverages queuing theory in caching networks to tune qubit transmission rate\nwhile considering fidelity as the cost metric. Our contributions in this work\ninclude (i) utilizing a method of repetition that encodes and broadcasts\nmultiple qubits through different quantum paths, (ii) analytically estimating\nthe time spent by these emitted qubits as a function of the number of paths and\nrepeaters, as well as memory units within a repeater, and (iii) formulating\noptimization problem that leverages this analysis to correct the transmitted\nlogic qubit and select the optimum repetition rate at the transmitter."
                },
                "authors": [
                    {
                        "name": "Karl C. Linne"
                    },
                    {
                        "name": "Yuanyuan Li"
                    },
                    {
                        "name": "Debashri Roy"
                    },
                    {
                        "name": "Kaushik Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Chowdhury"
                },
                "arxiv_affiliation": "Kai Li",
                "author": "Kaushik Chowdhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v2",
                "updated": "2025-09-29T05:12:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    5,
                    12,
                    51,
                    0,
                    272,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16257v2",
                "updated": "2025-09-29T02:46:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    2,
                    46,
                    45,
                    0,
                    272,
                    0
                ],
                "published": "2025-03-20T15:52:43Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    15,
                    52,
                    43,
                    3,
                    79,
                    0
                ],
                "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language\n  Models"
                },
                "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, the key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24178v1",
                "updated": "2025-09-29T01:52:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T01:52:10Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    1,
                    52,
                    10,
                    0,
                    272,
                    0
                ],
                "title": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BladderFormer: A Streaming Transformer for Real-Time Urological State\n  Monitoring"
                },
                "summary": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bladder pressure monitoring systems are increasingly vital in diagnosing and\nmanaging urinary tract dysfunction. Existing solutions rely heavily on\nhand-crafted features and shallow classifiers, limiting their adaptability to\ncomplex signal dynamics. We propose a one-layer streaming transformer model for\nreal-time classification of bladder pressure states, operating on\nwavelet-transformed representations of raw time-series data. Our model\nincorporates temporal multi-head self-attention and state caching, enabling\nefficient online inference with high adaptability. Trained on a dataset of 91\npatients with 20,000-80,000 samples each, our method demonstrates improved\naccuracy, higher energy- and latency-efficiency. Implementation considerations\nfor edge deployment on low-power hardware, such as edge graphical processing\nunits (GPU) and micro-controllers, are also discussed."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhou"
                    },
                    {
                        "name": "Steve Majerus"
                    },
                    {
                        "name": "Gourav Datta"
                    }
                ],
                "author_detail": {
                    "name": "Gourav Datta"
                },
                "author": "Gourav Datta",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24088v1",
                "updated": "2025-09-28T21:47:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T21:47:20Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    21,
                    47,
                    20,
                    6,
                    271,
                    0
                ],
                "title": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORRECT: COndensed eRror RECognition via knowledge Transfer in\n  multi-agent systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly capable of tackling complex\nreal-world tasks, yet their reliance on inter-agent coordination, tool use, and\nlong-horizon reasoning makes error recognition particularly challenging. Minor\nerrors can propagate across agents, escalating into task failures while\nproducing long, intertwined execution trajectories that impose significant\ncosts for both human developers and automated systems to debug and analyze. Our\nkey insight is that, despite surface differences in failure trajectories (e.g.,\nlogs), MAS errors often recur with similar structural patterns. This paper\npresents CORRECT, the first lightweight, training-free framework that leverages\nan online cache of distilled error schemata to recognize and transfer knowledge\nof failure structures across new requests. This cache-based reuse allows LLMs\nto perform targeted error localization at inference time, avoiding the need for\nexpensive retraining while adapting to dynamic MAS deployments in subseconds.\nTo support rigorous study in this domain, we also introduce CORRECT-Error, a\nlarge-scale dataset of over 2,000 annotated trajectories collected through a\nnovel error-injection pipeline guided by real-world distributions, and further\nvalidated through human evaluation to ensure alignment with natural failure\npatterns. Experiments across seven diverse MAS applications show that CORRECT\nimproves step-level error localization up to 19.8% over existing advances while\nat near-zero overhead, substantially narrowing the gap between automated and\nhuman-level error recognition."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Moyan Li"
                    },
                    {
                        "name": "Shaoyuan Xu"
                    },
                    {
                        "name": "Jinmiao Fu"
                    },
                    {
                        "name": "Xinhai Hou"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Bryan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Wang"
                },
                "author": "Bryan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24007v1",
                "updated": "2025-09-28T17:59:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T17:59:15Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    17,
                    59,
                    15,
                    6,
                    271,
                    0
                ],
                "title": "Sequential Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Diffusion Language Models"
                },
                "summary": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have strong theoretical efficiency but are\nlimited by fixed-length decoding and incompatibility with key-value (KV)\ncaches. Block diffusion mitigates these issues, yet still enforces a fixed\nblock size and requires expensive training. We introduce Next Sequence\nPrediction (NSP), which unifies next-token and next-block prediction, enabling\nthe model to adaptively determine the generation length at each step. When the\nlength is fixed to 1, NSP reduces to standard next-token prediction. Building\non NSP, we propose Sequential Diffusion Language Model (SDLM), which can\nretrofit pre-trained autoregressive language models (ALMs) at minimal cost.\nSpecifically, SDLM performs diffusion inference within fixed-size mask blocks,\nbut dynamically decodes consecutive subsequences based on model confidence,\nthereby preserving KV-cache compatibility and improving robustness to varying\nuncertainty and semantics across the sequence. Experiments show that SDLM\nmatches or surpasses strong autoregressive baselines using only 3.5M training\nsamples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the\nSDLM-32B model delivers even more pronounced efficiency gains, demonstrating\nthe strong scalability potential of our modeling paradigm. Project page and\ncodes: https://github.com/OpenGVLab/SDLM"
                },
                "authors": [
                    {
                        "name": "Yangzhou Liu"
                    },
                    {
                        "name": "Yue Cao"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Xiaobo Liang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Lijun Wu"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Yanting Zhang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Tong Lu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "14 pages, 5 figures, technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23928v1",
                "updated": "2025-09-28T15:05:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T15:05:21Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    15,
                    5,
                    21,
                    6,
                    271,
                    0
                ],
                "title": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in\n  Vision-Language Models"
                },
                "summary": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective approach for accelerating inference in\nLarge Language models (LLMs), but its adaptation to Vision-Language models\n(VLMs) remains challenging for additional visual tokens in multimodal inputs.\nFirst, owing to the fact that the drafter and the target VLM may derived from\ndifferent families, the semantic representations of visual tokens in the target\nVLM are misaligned with those in the drafter, introducing bias into the\nKV-cache during the prefill stage. Second, the large number of visual tokens\nsubstantially slows down the drafter's self-attention during the decoding\nstage. We propose Hiding Visual Tokens from the Drafter for Speculative\nDecoding in Vision-Language Models (HiViS), an explicit-implicit input\ndecomposition framework that alleviates the above inefficiency. All visual\ntokens are removed from the drafter's input, retaining only textual tokens as\nexplicit inputs, while directly reusing the target VLM's corresponding\nlast-layer hidden states as implicit visual information without additional\nprocessing. To train the drafter efficiently, we introduces multi-step\nself-feedback training strategy with dynamic data selection and sequential\nembedding supervision to simulate reasoning during training. Our approach\ncompresses the prefill sequence length of the drafter to only 0.7%-1.3% of the\ntarget VLM's input, while maintaining lossless generation quality. Extensive\nexperiments across diverse models and tasks demonstrate up to 2.65x speedup,\nconfirming the effectiveness of HiViS in accelerating VLM inference."
                },
                "authors": [
                    {
                        "name": "Zhinan Xie"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Jian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jian Cheng"
                },
                "author": "Jian Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v2",
                "updated": "2025-09-28T08:32:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    8,
                    32,
                    26,
                    6,
                    271,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23601v1",
                "updated": "2025-09-28T03:12:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "published": "2025-09-28T03:12:43Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    12,
                    43,
                    6,
                    271,
                    0
                ],
                "title": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration"
                },
                "summary": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Mamba-based image restoration methods have achieved promising results\nbut remain\n  limited by fixed scanning patterns and inefficient feature utilization.\nConventional Mamba\n  architectures rely on predetermined paths that cannot adapt to diverse\ndegradations, constraining\n  both restoration performance and computational efficiency. To overcome these\nlimitations, we\n  propose VAMamba, a Visual Adaptive Mamba framework with two key innovations.\nFirst,\n  QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha\n  FIFO cache that stores historical representations. Similarity between current\nLoRA-adapted and\n  cached features guides intelligent fusion, enabling dynamic reuse while\neffectively controlling\n  memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning.\nA\n  Vision Transformer generates score maps to estimate pixel importance, and a\ngreedy strategy de termines optimal forward and backward scanning paths. These\nlearned trajectories replace rigid\n  patterns, enabling SS2D to perform targeted feature extraction. The\nintegration of QCLAM and\n  GPS-SS2D allows VAMamba to adaptively focus on degraded regions while\nmaintaining high\n  computational efficiency. Extensive experiments across diverse restoration\ntasks demonstrate\n  that VAMamba consistently outperforms existing approaches in both restoration\nquality and\n  efficiency, establishing new benchmarks for adaptive image restoration. Our\ncode is available\n  at https://github.com/WaterHQH/VAMamba."
                },
                "authors": [
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Chen Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Lyu"
                },
                "author": "Chen Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09072v2",
                "updated": "2025-09-27T20:13:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    20,
                    13,
                    25,
                    5,
                    270,
                    0
                ],
                "published": "2025-08-12T16:47:48Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    16,
                    47,
                    48,
                    1,
                    224,
                    0
                ],
                "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference"
                },
                "summary": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Language Models instantiate a factorized likelihood over token\nsequences, yet their strictly sequential decoding process imposes an intrinsic\nlower bound on inference latency. This bottleneck has emerged as a central\nobstacle to the scalable deployment of large-scale generative models. Existing\nacceleration techniques partially mitigate token-level latency by relying on\nauxiliary draft models or introducing an additional training phase, but fail to\naddress the dominant memory and communication costs. We present READER, a\nprovably lossless speculative decoding framework that bypasses the training of\nthe auxiliary draft model. READER formalizes speculative decoding as a\nstochastic tree construction problem and exploits the empirical redundancy\nstructure of natural language to generate high-probability candidate\ncontinuations. Our method revisits the problem of constructing draft trees,\nestablishing substantial statistical improvements over stochastic draft-tree\nmethods and providing a complexity-theoretic analysis that characterizes the\noptimality frontier of speculative decoding under bounded computation and\nmemory resources. Beyond the single-sequence regime traditionally considered in\nprior work, we introduce a memory-optimal key-value cache-serving strategy that\nguarantees amortized sublinear overhead in the batch dimension, allowing READER\nto scale to realistic inference workloads. Comprehensive experiments\ndemonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to\n5.92x on batched inference, consistently surpassing prior speculative decoding\nbaselines, while preserving exact output equivalence, with even more pronounced\ngains in retrieval-augmented generation pipelines. Our results close a key gap\nbetween theoretical parallelism limits and practical LLM inference, suggesting\na new standard for efficient deployment."
                },
                "authors": [
                    {
                        "name": "Maxim Divilkovskiy"
                    },
                    {
                        "name": "Vitaly Malygin"
                    },
                    {
                        "name": "Sergey Zlobin"
                    },
                    {
                        "name": "Stanislav Ilyushin"
                    },
                    {
                        "name": "Sultan Isali"
                    },
                    {
                        "name": "Vasily Kalugin"
                    },
                    {
                        "name": "Nuriza Aitassova"
                    },
                    {
                        "name": "Fei Yi"
                    },
                    {
                        "name": "Weidi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Zeng"
                },
                "author": "Weidi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23179v1",
                "updated": "2025-09-27T08:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T08:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    8,
                    15,
                    17,
                    5,
                    270,
                    0
                ],
                "title": "A Near-Cache Architectural Framework for Cryptographic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Near-Cache Architectural Framework for Cryptographic Computing"
                },
                "summary": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath."
                },
                "authors": [
                    {
                        "name": "Jingyao Zhang"
                    },
                    {
                        "name": "Elaheh Sadredini"
                    }
                ],
                "author_detail": {
                    "name": "Elaheh Sadredini"
                },
                "author": "Elaheh Sadredini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v4",
                "updated": "2025-09-27T07:41:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    7,
                    41,
                    38,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "Runtime Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runtime Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23094v1",
                "updated": "2025-09-27T04:07:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "published": "2025-09-27T04:07:23Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    4,
                    7,
                    23,
                    5,
                    270,
                    0
                ],
                "title": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching"
                },
                "summary": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs), despite their promising\nperformance, still suffer from inferior inference efficiency. This is because\ndLLMs rely on bidirectional attention and cannot directly benefit from the\nstandard key-value (KV) cache as autoregressive models (ARMs) do. To tackle\nthis issue, we introduce \\textit{Dual aDaptive Cache} (d$^2$Cache), which is a\ntraining-free approximate KV cache framework for accelerating dLLM inference.\nd$^2$Cache features a two-stage fine-grained selection strategy to identify\ntokens and adaptively update their KV states at each decoding step, while\ncaching the KV states of the remaining tokens for reuse. Furthermore,\nd$^2$Cache naturally offers a more reliable decoding alternative, which can\nenable quasi left-to-right generation and mitigate premature overconfidence in\ntokens at the end of the sequence. Extensive experimental results on two\nrepresentative dLLMs (\\ie, LLaDA and Dream) demonstrate that d$^2$Cache not\nonly achieves substantial inference speedups, but also yields consistent\nimprovements in generation quality. The code is available at\nhttps://github.com/Kamichanw/d2Cache."
                },
                "authors": [
                    {
                        "name": "Yuchu Jiang"
                    },
                    {
                        "name": "Yue Cai"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Jiale Fu"
                    },
                    {
                        "name": "Jiarui Wang"
                    },
                    {
                        "name": "Chonghan Liu"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v3",
                "updated": "2025-09-27T03:37:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    27,
                    3,
                    37,
                    40,
                    5,
                    270,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance, but\ntheir long-context reasoning remains constrained by the excessive memory\nrequired for the Key-Value (KV) cache. This makes KV cache compression a\ncritical step toward efficient long-context inference. Recent methods have\nexplored low-rank techniques to reduce the hidden size of the KV cache.\nHowever, they neglect the distinct roles and varying importance of Keys and\nValues, leading to significant performance drops under high compression. To\naddress this, we propose ReCalKV, a post-training low-rank KV cache compression\napproach with tailored strategies for Keys and Values. For Keys, we propose\nHead-wise Similarity aware Reordering (HSR), which clusters structurally\nsimilar heads into groups, enabling more accurate low-rank approximation via\ngrouped SVD. For Values, we propose Offline Value Calibration (OVC), which\nefficiently calibrates the value projection matrix using calibration data\nwithout training, ensuring an accurate representation of contextual\ninformation. Extensive experiments show that ReCalKV consistently outperforms\nexisting low-rank compression methods, achieving high compression ratios with\nminimal performance loss. The code and models will be available\nat:https://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v4",
                "updated": "2025-09-26T21:40:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    21,
                    40,
                    58,
                    4,
                    269,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "vCache: Verified Semantic Prompt Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vCache: Verified Semantic Prompt Caching"
                },
                "summary": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caches return cached responses for semantically similar prompts to\nreduce LLM inference latency and cost. They embed cached prompts and store them\nalongside their response in a vector database. Embedding similarity metrics\nassign a numerical score to quantify the similarity between a request and its\nnearest neighbor prompt from the cache. Existing systems use the same static\nsimilarity threshold across all requests to determine whether two prompts can\nshare similar responses. However, we observe that static thresholds do not give\nformal correctness guarantees, can result in unexpected error rates, and lead\nto suboptimal cache hit rates. This paper proposes vCache, the first verified\nsemantic cache with user-defined error rate guarantees. It employs an online\nlearning algorithm to estimate an optimal threshold for each cached prompt,\nenabling reliable cache responses without additional training. Our experiments\nshow that vCache consistently meets the specified error bounds while\noutperforming state-of-the-art static-threshold and fine-tuned embedding\nbaselines. We release the vCache implementation and three benchmarks to support\nfuture research."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Aditya Desai"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Kyle Chu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22875v1",
                "updated": "2025-09-26T19:40:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T19:40:33Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    19,
                    40,
                    33,
                    4,
                    269,
                    0
                ],
                "title": "On KV-Poisson Structure and related invariants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On KV-Poisson Structure and related invariants"
                },
                "summary": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an deepened analysis of KV-Poisson structures of on IR^2. We\npresent their classification their properties an their possible applications in\ndifferent domains. We prove that these structure give rise to a new\nCohomological invariant. We explicitly compute the Cohomological groups of some\nof these structures."
                },
                "authors": [
                    {
                        "name": "Prosper Rosaire Mama Assandje"
                    },
                    {
                        "name": "Herguey Mopeng"
                    },
                    {
                        "name": "Joseph Dongho"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Dongho"
                },
                "author": "Joseph Dongho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08799v2",
                "updated": "2025-09-26T17:59:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    59,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-11T17:59:36Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    59,
                    36,
                    4,
                    192,
                    0
                ],
                "title": "KV Cache Steering for Controlling Frozen LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Steering for Controlling Frozen LLMs"
                },
                "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach constructs\nsteering vectors from reasoning traces, obtained either from teacher models\n(e.g., GPT-4o) or existing human annotations, that shift model behavior toward\nmore explicit, multi-step reasoning without fine-tuning or prompt\nmodifications. Experimental evaluations on diverse reasoning benchmarks\ndemonstrate that cache steering improves both the qualitative structure of\nmodel reasoning and quantitative task performance. Additional experiments show\nthat the method also scales to larger models and yields further gains on\nchallenging datasets such as GPQA and MATH. Compared to prior activation\nsteering techniques that require continuous interventions, our one-shot cache\nsteering offers substantial advantages in terms of inference latency,\nhyperparameter stability, and ease of integration with existing inference APIs.\nBeyond mere reasoning induction, we show that cache steering enables\ncontrollable transfer of reasoning styles (e.g., stepwise, causal, analogical),\nmaking it a practical tool for behavior-level guidance of language models."
                },
                "authors": [
                    {
                        "name": "Max Belitsky"
                    },
                    {
                        "name": "Dawid J. Kopiczko"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "M. Jehanzeb Mirza"
                    },
                    {
                        "name": "James R. Glass"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v1",
                "updated": "2025-09-26T17:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22548v1",
                "updated": "2025-09-26T16:29:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:29:37Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    29,
                    37,
                    4,
                    269,
                    0
                ],
                "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory\n  for Vision-Language Navigation"
                },
                "summary": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation requires an embodied agent to navigate through\nunseen environments, guided by natural language instructions and a continuous\nvideo stream. Recent advances in VLN have been driven by the powerful semantic\nunderstanding of Multimodal Large Language Models. However, these methods\ntypically rely on explicit semantic memory, such as building textual cognitive\nmaps or storing historical visual frames. This type of method suffers from\nspatial information loss, computational redundancy, and memory bloat, which\nimpede efficient navigation. Inspired by the implicit scene representation in\nhuman navigation, analogous to the left brain's semantic understanding and the\nright brain's spatial cognition, we propose JanusVLN, a novel VLN framework\nfeaturing a dual implicit neural memory that models spatial-geometric and\nvisual-semantic memory as separate, compact, and fixed-size neural\nrepresentations. This framework first extends the MLLM to incorporate 3D prior\nknowledge from the spatial-geometric encoder, thereby enhancing the spatial\nreasoning capabilities of models based solely on RGB input. Then, the\nhistorical key-value caches from the spatial-geometric and visual-semantic\nencoders are constructed into a dual implicit memory. By retaining only the KVs\nof tokens in the initial and sliding window, redundant computation is avoided,\nenabling efficient incremental updates. Extensive experiments demonstrate that\nJanusVLN outperforms over 20 recent methods to achieve SOTA performance. For\nexample, the success rate improves by 10.5-35.5 compared to methods using\nmultiple data types as input and by 3.6-10.8 compared to methods using more RGB\ntraining data. This indicates that the proposed dual implicit neural memory, as\na novel paradigm, explores promising new directions for future VLN research.\nOurs project page: https://miv-xjtu.github.io/JanusVLN.github.io/."
                },
                "authors": [
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Dekang Qi"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Feng Xiong"
                    },
                    {
                        "name": "Shichao Xie"
                    },
                    {
                        "name": "Xiaolong Wu"
                    },
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Wei"
                },
                "author": "Xing Wei",
                "arxiv_comment": "Project page: https://miv-xjtu.github.io/JanusVLN.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22516v1",
                "updated": "2025-09-26T16:00:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T16:00:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    0,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent\n  and Explainable Digital Assessments"
                },
                "summary": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TrueGradeAI, an AI-driven digital examination framework\ndesigned to overcome the shortcomings of traditional paper-based assessments,\nincluding excessive paper usage, logistical complexity, grading delays, and\nevaluator bias. The system preserves natural handwriting by capturing stylus\ninput on secure tablets and applying transformer-based optical character\nrecognition for transcription. Evaluation is conducted through a\nretrieval-augmented pipeline that integrates faculty solutions, cache layers,\nand external references, enabling a large language model to assign scores with\nexplicit, evidence-linked reasoning. Unlike prior tablet-based exam systems\nthat primarily digitize responses, TrueGradeAI advances the field by\nincorporating explainable automation, bias mitigation, and auditable grading\ntrails. By uniting handwriting preservation with scalable and transparent\nevaluation, the framework reduces environmental costs, accelerates feedback\ncycles, and progressively builds a reusable knowledge base, while actively\nworking to mitigate grading bias and ensure fairness in assessment."
                },
                "authors": [
                    {
                        "name": "Rakesh Thakur"
                    },
                    {
                        "name": "Shivaansh Kaushik"
                    },
                    {
                        "name": "Gauri Chopra"
                    },
                    {
                        "name": "Harsh Rohilla"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Rohilla"
                },
                "author": "Harsh Rohilla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22512v1",
                "updated": "2025-09-26T15:54:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:54:50Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    54,
                    50,
                    4,
                    269,
                    0
                ],
                "title": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AxLLM: accelerator architecture for large language models with\n  computation reuse capability"
                },
                "summary": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware."
                },
                "authors": [
                    {
                        "name": "Soroush Ahadi"
                    },
                    {
                        "name": "Mehdi Modarressi"
                    },
                    {
                        "name": "Masoud Daneshtalab"
                    }
                ],
                "author_detail": {
                    "name": "Masoud Daneshtalab"
                },
                "author": "Masoud Daneshtalab",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "n/a",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22488v1",
                "updated": "2025-09-26T15:35:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T15:35:05Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    15,
                    35,
                    5,
                    4,
                    269,
                    0
                ],
                "title": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Organ dose optimization for a point-of-care forearm X-ray\n  photon-counting CT"
                },
                "summary": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Spectral shaping is a computed tomography (CT) dose optimization\ntechnique that adjusts source voltage and filtration to reduce patient\nradiation exposure without compromising image quality. Traditionally, radiation\ndose has been assessed using the computed tomography dose index (CTDI).\nHowever, emerging dosimetric approaches aim to enable patient-specific\nevaluations by estimating organ absorbed doses, providing a more accurate\nrepresentation of the biological impact. This study investigates spectral\nshaping for an extremity photon-counting detector (PCD) CT, through organ\nabsorbed dose estimation and image quality evaluation. Method: Monte Carlo\nsimulations were conducted to evaluate various combinations of source voltage\nand filtration. Tube voltage ranged from 80 to 140 kV, combined with three\ndistinct filtration material and thicknesses. Simulations included three\nstages: a standardized phantom for CTDI assessment, an adult forearm phantom\nfor organ dose measurement, and an image quality phantom for evaluation of an\nadvanced image quality metric: the detectability index. Results: In a wrist\nPCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation\ndose by up to 50%. This reduction is achieved while maintaining the same\ndetectability index value as the standard 120 kV protocol. However, the optimal\nfiltration depends on the organ targeted for dose reduction, as bone and skin\nbenefit from opposing filtration approaches. While CTDI provides a useful\ninitial estimate, it may lead to suboptimal optimization compared to\norgan-specific dose evaluation. Conclusions: Patient-specific dosimetry based\non organ absorbed dose estimation offers a more accurate framework for\noptimizing CT protocols through spectral shaping than conventional CTDI-based\napproaches."
                },
                "authors": [
                    {
                        "name": "Pierre-Antoine Rodesch"
                    },
                    {
                        "name": "Anaïs Viry"
                    },
                    {
                        "name": "Mouad Khorsi"
                    },
                    {
                        "name": "Fabio Becce"
                    },
                    {
                        "name": "Jérôme Damet"
                    },
                    {
                        "name": "Lucía Gallego Manzano"
                    }
                ],
                "author_detail": {
                    "name": "Lucía Gallego Manzano"
                },
                "author": "Lucía Gallego Manzano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v3",
                "updated": "2025-09-26T14:35:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    35,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Consolidation for\n  Generalised Reasoning"
                },
                "summary": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer LLMs have been shown to exhibit strong reasoning ability that\nscales with inference-time compute, most prominently through token-space\n\"thinking\" chains of thought. A growing line of work pushes extra computation\ninto the model's latent space, which we term Auxiliary Latent-Space Computation\n(ALSC). Existing ALSC methods largely fall into three buckets: (i)\ntoken-mediated latent rollouts, (ii) residual/activation steering, and (iii)\nmemory (KV) compression. An underexplored alternative is memory\nconsolidation/reconsolidation, two processes in the brain that are responsible\nfor stabilising newly formed memory traces, and, upon recall, transiently\nrendering established traces plastic such they can integrate new contextual\ninformation before restabilising. In Transformer LLMs, this can be seen as\nanalogous to performing in-place rewrites of new KV segments, and rewrites of\nrecalled past segments. In this work, we give a theoretical justification as to\nwhy memory (re)consolidation via KV cache rewrites is beneficial for improved\nreasoning. We do this through the lens of Information Bottleneck (IB) theory,\nwhich posits that model generalisation emerges from an optimal balance between\ninput information compression and retention of predictive information in latent\nrepresentations. We then introduce the Bottlenecked Transformer, which augments\na backbone LLM with a Cache Processor, an auxiliary Transformer that performs\nperiodic, non-causal, in-place KV rewrites at newline-delimited reasoning step\nboundaries. The Processor consolidates recently written KV entries and\nreconsolidates a small, top-k attention-selected set of prior entries. We\nevaluate our Bottlenecked Transformer architecture on math reasoning\nbenchmarks. Our model sees consistent performance gains over vanilla\nTransformers and pause-token augmented baselines, with gains of up to +6.6pp\nfor selected tasks/backbones."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22323v1",
                "updated": "2025-09-26T13:20:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T13:20:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    20,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion\n  Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered\nby slow sampling. Existing training-free accelerators - step reduction, feature\ncaching, and sparse attention - enhance inference speed but typically rely on a\nuniform heuristic or a manually designed adaptive strategy for all images,\nleaving quality on the table. Alternatively, dynamic neural networks offer\nper-image adaptive acceleration, but their high fine-tuning costs limit broader\napplicability. To address these limitations, we introduce RAPID3: Tri-Level\nReinforced Acceleration Policies for Diffusion Transformers, a framework that\ndelivers image-wise acceleration with zero updates to the base generator.\nSpecifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and\nSparse-Attention - observe the current denoising state and independently decide\ntheir corresponding speed-up at each timestep. All policy parameters are\ntrained online via Group Relative Policy Optimization (GRPO) while the\ngenerator remains frozen. Meanwhile, an adversarially learned discriminator\naugments the reward signal, discouraging reward hacking by boosting returns\nonly when generated samples stay close to the original model's distribution.\nAcross state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,\nRAPID3 achieves nearly 3x faster sampling with competitive generation quality."
                },
                "authors": [
                    {
                        "name": "Wangbo Zhao"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Zhiwei Tang"
                    },
                    {
                        "name": "Jiasheng Tang"
                    },
                    {
                        "name": "Pengfei Zhou"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v8",
                "updated": "2025-09-26T10:00:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    10,
                    0,
                    54,
                    4,
                    269,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22756v1",
                "updated": "2025-09-26T09:33:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    9,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T09:33:36Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    9,
                    33,
                    36,
                    4,
                    269,
                    0
                ],
                "title": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Autoregressive Mapping with Traffic Rules for Autonomous\n  Driving"
                },
                "summary": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe autonomous driving requires both accurate HD map construction and\npersistent awareness of traffic rules, even when their associated signs are no\nlonger visible. However, existing methods either focus solely on geometric\nelements or treat rules as temporary classifications, failing to capture their\npersistent effectiveness across extended driving sequences. In this paper, we\npresent PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel\nframework that performs autoregressive co-construction of lane vectors and\ntraffic rules from visual observations. Our approach introduces two key\nmechanisms: Map-Rule Co-Construction for processing driving scenes in temporal\nsegments, and Map-Rule Cache for maintaining rule consistency across these\nsegments. To properly evaluate continuous and consistent map generation, we\ndevelop MapDRv2, featuring improved lane geometry annotations. Extensive\nexperiments demonstrate that PAMR achieves superior performance in joint\nvector-rule mapping tasks, while maintaining persistent rule effectiveness\nthroughout extended driving sequences."
                },
                "authors": [
                    {
                        "name": "Shiyi Liang"
                    },
                    {
                        "name": "Xinyuan Chang"
                    },
                    {
                        "name": "Changjie Wu"
                    },
                    {
                        "name": "Huiyuan Yan"
                    },
                    {
                        "name": "Yifan Bai"
                    },
                    {
                        "name": "Xinran Liu"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Yujian Yuan"
                    },
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Mu Xu"
                    },
                    {
                        "name": "Xing Wei"
                    }
                ],
                "author_detail": {
                    "name": "Xing Wei"
                },
                "author": "Xing Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v2",
                "updated": "2025-09-26T07:14:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    7,
                    14,
                    44,
                    4,
                    269,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. As a result,\nthese models cannot accurately identify and prioritize the most relevant\ncontext, leading to degraded response quality. In this paper, we present\nLoopServe, an adaptive dual-phase inference acceleration framework for large\nlanguage models in multi-turn dialogues. LoopServe introduces two main\ninnovations. First, it performs online sparsification during the prefilling\nphase by dynamically selecting the most important parts of the attention matrix\nfor each new input. Second, it uses progressive key value compression during\ndecoding by adaptively maintaining a relevant and efficient cache based on the\nmost recently generated output tokens. We also propose a new benchmark with\neleven multi-turn datasets that reflect realistic query positions and\nconversational dependencies. Extensive experiments demonstrate that LoopServe\nconsistently achieves superior effectiveness compared to existing baselines and\nsignificantly accelerates LLM inference across a wide range of long-context\ndialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21917v1",
                "updated": "2025-09-26T05:57:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    57,
                    4,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T05:57:04Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    57,
                    4,
                    4,
                    269,
                    0
                ],
                "title": "Taming Flow-based I2V Models for Creative Video Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Flow-based I2V Models for Creative Video Editing"
                },
                "summary": "Although image editing techniques have advanced significantly, video editing,\nwhich aims to manipulate videos according to user intent, remains an emerging\nchallenge. Most existing image-conditioned video editing methods either require\ninversion with model-specific design or need extensive optimization, limiting\ntheir capability of leveraging up-to-date image-to-video (I2V) models to\ntransfer the editing capability of image editing models to the video domain. To\nthis end, we propose IF-V2V, an Inversion-Free method that can adapt\noff-the-shelf flow-matching-based I2V models for video editing without\nsignificant computational overhead. To circumvent inversion, we devise Vector\nField Rectification with Sample Deviation to incorporate information from the\nsource video into the denoising process by introducing a deviation term into\nthe denoising vector field. To further ensure consistency with the source video\nin a model-agnostic way, we introduce Structure-and-Motion-Preserving\nInitialization to generate motion-aware temporally correlated noise with\nstructural information embedded. We also present a Deviation Caching mechanism\nto minimize the additional computational cost for denoising vector\nrectification without significantly impacting editing quality. Evaluations\ndemonstrate that our method achieves superior editing quality and consistency\nover existing approaches, offering a lightweight plug-and-play solution to\nrealize visual creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although image editing techniques have advanced significantly, video editing,\nwhich aims to manipulate videos according to user intent, remains an emerging\nchallenge. Most existing image-conditioned video editing methods either require\ninversion with model-specific design or need extensive optimization, limiting\ntheir capability of leveraging up-to-date image-to-video (I2V) models to\ntransfer the editing capability of image editing models to the video domain. To\nthis end, we propose IF-V2V, an Inversion-Free method that can adapt\noff-the-shelf flow-matching-based I2V models for video editing without\nsignificant computational overhead. To circumvent inversion, we devise Vector\nField Rectification with Sample Deviation to incorporate information from the\nsource video into the denoising process by introducing a deviation term into\nthe denoising vector field. To further ensure consistency with the source video\nin a model-agnostic way, we introduce Structure-and-Motion-Preserving\nInitialization to generate motion-aware temporally correlated noise with\nstructural information embedded. We also present a Deviation Caching mechanism\nto minimize the additional computational cost for denoising vector\nrectification without significantly impacting editing quality. Evaluations\ndemonstrate that our method achieves superior editing quality and consistency\nover existing approaches, offering a lightweight plug-and-play solution to\nrealize visual creativity."
                },
                "authors": [
                    {
                        "name": "Xianghao Kong"
                    },
                    {
                        "name": "Hansheng Chen"
                    },
                    {
                        "name": "Yuwei Guo"
                    },
                    {
                        "name": "Lvmin Zhang"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Maneesh Agrawala"
                    },
                    {
                        "name": "Anyi Rao"
                    }
                ],
                "author_detail": {
                    "name": "Anyi Rao"
                },
                "author": "Anyi Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21857v1",
                "updated": "2025-09-26T04:32:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    32,
                    56,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T04:32:56Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    32,
                    56,
                    4,
                    269,
                    0
                ],
                "title": "2.34 kV \\b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with\n  sub-micron fin width",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.34 kV \\b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with\n  sub-micron fin width"
                },
                "summary": "In this letter, we present a kilovolt-class \\b{eta}-Ga2O3 vertical trench\nSchottky barrier diode with a field plate incorporating narrow fin width (Wfin)\nstructures of sub-micron dimensions. We used a nanolaminate dielectric\ncomprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric\nand for field plate edge termination. Both Wfin of 200 nm and 500 nm\ndemonstrate excellent on-state performance with specific on-resistance (Ron,sp)\nof 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist\nplanarization and etch-back process was employed to expose the top of the fins\nfor Schottky contact formation, eliminating critical lithographic alignment\nchallenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV\nwith very low leakage currents before catastrophic breakdown. The measured\nbreakdown voltage is limited by dielectric breakdown at the trench bottom\ncorner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD\nsimulation shows a reduced electric field at the surface of the\nmetal-semiconductor junction due to the RESURF effect, resulting in very low\nreverse leakage before breakdown. The parallel plane electric field in the\n\\b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using\naccurately extracted drift layer doping profile from high voltage CV\nmeasurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current\nspreading) was calculated. Enhanced RESURF by integration of high-k dielectrics\nwith self-aligned photoresist planarization, offers a promising pathway towards\nhigh figure of merit, low leakage high-performance vertical devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we present a kilovolt-class \\b{eta}-Ga2O3 vertical trench\nSchottky barrier diode with a field plate incorporating narrow fin width (Wfin)\nstructures of sub-micron dimensions. We used a nanolaminate dielectric\ncomprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric\nand for field plate edge termination. Both Wfin of 200 nm and 500 nm\ndemonstrate excellent on-state performance with specific on-resistance (Ron,sp)\nof 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist\nplanarization and etch-back process was employed to expose the top of the fins\nfor Schottky contact formation, eliminating critical lithographic alignment\nchallenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV\nwith very low leakage currents before catastrophic breakdown. The measured\nbreakdown voltage is limited by dielectric breakdown at the trench bottom\ncorner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD\nsimulation shows a reduced electric field at the surface of the\nmetal-semiconductor junction due to the RESURF effect, resulting in very low\nreverse leakage before breakdown. The parallel plane electric field in the\n\\b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using\naccurately extracted drift layer doping profile from high voltage CV\nmeasurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current\nspreading) was calculated. Enhanced RESURF by integration of high-k dielectrics\nwith self-aligned photoresist planarization, offers a promising pathway towards\nhigh figure of merit, low leakage high-performance vertical devices."
                },
                "authors": [
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21842v1",
                "updated": "2025-09-26T04:03:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    3,
                    52,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-26T04:03:52Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    3,
                    52,
                    4,
                    269,
                    0
                ],
                "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for\n  Autonomous Travel Planning Agents"
                },
                "summary": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks."
                },
                "authors": [
                    {
                        "name": "Yansong Ning"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Jun Fang"
                    },
                    {
                        "name": "Kan Zheng"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v3",
                "updated": "2025-09-26T03:24:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    3,
                    24,
                    20,
                    4,
                    269,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v3",
                "updated": "2025-09-26T03:17:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    3,
                    17,
                    15,
                    4,
                    269,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21623v1",
                "updated": "2025-09-25T21:42:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    42,
                    27,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T21:42:27Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    21,
                    42,
                    27,
                    3,
                    268,
                    0
                ],
                "title": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's\n  Rule",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's\n  Rule"
                },
                "summary": "The expanding long-context capabilities of large language models are\nconstrained by a significant memory bottleneck: the key-value (KV) cache\nrequired for autoregressive generation. This bottleneck is substantial; for\ninstance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of\n4 requires approximately 16GB for its KV cache, a size exceeding the model's\nweights. While KV-cache compression via low-rank projection is a promising\ndirection, existing methods rely on a static, offline-learned subspace that\nperforms poorly under data distribution shifts. To overcome these limitations,\nwe introduce OjaKV, a novel framework that integrates a strategic hybrid\nstorage policy with online subspace adaptation. First, OjaKV recognizes that\nnot all tokens are equally important for compression; it preserves the crucial\nfirst and most recent tokens in full-rank, maintaining high-fidelity anchors\nfor attention. Second, for the vast majority of intermediate tokens, it applies\nlow-rank compression by incrementally adapting the projection basis using Oja's\nalgorithm for online principal component analysis. This adaptation involves a\ncomprehensive update during prompt prefilling and lightweight periodic updates\nduring decoding, ensuring the subspace remains aligned with the evolving\ncontext. Crucially, our framework is fully compatible with modern attention\nmodules like FlashAttention. Experiments demonstrate that OjaKV maintains or\neven improves zero-shot accuracy at high compression ratios. In particular,\nOjaKV achieves its strongest gains on very long-context benchmarks that require\ncomplex reasoning, highlighting the importance of online subspace adaptation in\ndynamically tracking context shifts. These results establish our hybrid\nframework as a practical, plug-and-play solution for memory-efficient\nlong-context inference without requiring model fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding long-context capabilities of large language models are\nconstrained by a significant memory bottleneck: the key-value (KV) cache\nrequired for autoregressive generation. This bottleneck is substantial; for\ninstance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of\n4 requires approximately 16GB for its KV cache, a size exceeding the model's\nweights. While KV-cache compression via low-rank projection is a promising\ndirection, existing methods rely on a static, offline-learned subspace that\nperforms poorly under data distribution shifts. To overcome these limitations,\nwe introduce OjaKV, a novel framework that integrates a strategic hybrid\nstorage policy with online subspace adaptation. First, OjaKV recognizes that\nnot all tokens are equally important for compression; it preserves the crucial\nfirst and most recent tokens in full-rank, maintaining high-fidelity anchors\nfor attention. Second, for the vast majority of intermediate tokens, it applies\nlow-rank compression by incrementally adapting the projection basis using Oja's\nalgorithm for online principal component analysis. This adaptation involves a\ncomprehensive update during prompt prefilling and lightweight periodic updates\nduring decoding, ensuring the subspace remains aligned with the evolving\ncontext. Crucially, our framework is fully compatible with modern attention\nmodules like FlashAttention. Experiments demonstrate that OjaKV maintains or\neven improves zero-shot accuracy at high compression ratios. In particular,\nOjaKV achieves its strongest gains on very long-context benchmarks that require\ncomplex reasoning, highlighting the importance of online subspace adaptation in\ndynamically tracking context shifts. These results establish our hybrid\nframework as a practical, plug-and-play solution for memory-efficient\nlong-context inference without requiring model fine-tuning."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pin-Yu Chen"
                },
                "author": "Pin-Yu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21463v1",
                "updated": "2025-09-25T19:29:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    29,
                    25,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T19:29:25Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    19,
                    29,
                    25,
                    3,
                    268,
                    0
                ],
                "title": "Enhanced Generative Machine Listener",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Generative Machine Listener"
                },
                "summary": "We present GMLv2, a reference-based model designed for the prediction of\nsubjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta\ndistribution-based loss to model the listener ratings and incorporates\nadditional neural audio coding (NAC) subjective datasets to extend its\ngeneralization and applicability. Extensive evaluations on diverse testset\ndemonstrate that proposed GMLv2 consistently outperforms widely used metrics,\nsuch as PEAQ and ViSQOL, both in terms of correlation with subjective scores\nand in reliably predicting these scores across diverse content types and codec\nconfigurations. Consequently, GMLv2 offers a scalable and automated framework\nfor perceptual audio quality evaluation, poised to accelerate research and\ndevelopment in modern audio coding technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GMLv2, a reference-based model designed for the prediction of\nsubjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta\ndistribution-based loss to model the listener ratings and incorporates\nadditional neural audio coding (NAC) subjective datasets to extend its\ngeneralization and applicability. Extensive evaluations on diverse testset\ndemonstrate that proposed GMLv2 consistently outperforms widely used metrics,\nsuch as PEAQ and ViSQOL, both in terms of correlation with subjective scores\nand in reliably predicting these scores across diverse content types and codec\nconfigurations. Consequently, GMLv2 offers a scalable and automated framework\nfor perceptual audio quality evaluation, poised to accelerate research and\ndevelopment in modern audio coding technologies."
                },
                "authors": [
                    {
                        "name": "Vishnu Raj"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Shiv Gehlot"
                    },
                    {
                        "name": "Lars Villemoes"
                    },
                    {
                        "name": "Arijit Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Arijit Biswas"
                },
                "author": "Arijit Biswas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v2",
                "updated": "2025-09-25T13:55:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    55,
                    44,
                    3,
                    268,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21153v1",
                "updated": "2025-09-25T13:39:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:39:16Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP"
                },
                "summary": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings."
                },
                "authors": [
                    {
                        "name": "Moshe Kimhi"
                    },
                    {
                        "name": "Erez Koifman"
                    },
                    {
                        "name": "Ehud Rivlin"
                    },
                    {
                        "name": "Eli Schwartz"
                    },
                    {
                        "name": "Chaim Baskin"
                    }
                ],
                "author_detail": {
                    "name": "Chaim Baskin"
                },
                "author": "Chaim Baskin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v2",
                "updated": "2025-09-25T13:15:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    15,
                    45,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "18 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v2",
                "updated": "2025-09-25T10:24:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    24,
                    14,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20979v1",
                "updated": "2025-09-25T10:23:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T10:23:50Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "title": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference"
                },
                "summary": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Yirong Zhang"
                    },
                    {
                        "name": "Jiahong Yu"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Jianping Zou"
                    },
                    {
                        "name": "Gang Xiong"
                    },
                    {
                        "name": "Kingsum Chow"
                    },
                    {
                        "name": "Shuibing He"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v5",
                "updated": "2025-09-25T09:49:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    9,
                    49,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v2",
                "updated": "2025-09-25T03:30:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    30,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v2",
                "updated": "2025-09-25T03:00:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    0,
                    22,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20617v1",
                "updated": "2025-09-24T23:47:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T23:47:55Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "title": "DELM: a Python toolkit for Data Extraction with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELM: a Python toolkit for Data Extraction with Language Models"
                },
                "summary": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}."
                },
                "authors": [
                    {
                        "name": "Eric Fithian"
                    },
                    {
                        "name": "Kirill Skobelev"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Skobelev"
                },
                "author": "Kirill Skobelev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v2",
                "updated": "2025-09-24T16:56:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    56,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective"
                },
                "summary": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19729v1",
                "updated": "2025-09-24T03:15:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T03:15:37Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "title": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference"
                },
                "summary": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Haoyu Chen"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Kun Qian"
                    },
                    {
                        "name": "Yu Guan"
                    },
                    {
                        "name": "Jin Zhao"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "12 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v2",
                "updated": "2025-09-24T01:32:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    32,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_doi": "10.1145/3731599.3767498",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731599.3767498",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19599v1",
                "updated": "2025-09-23T21:46:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T21:46:38Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "title": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing."
                },
                "authors": [
                    {
                        "name": "Danilo Trombino"
                    },
                    {
                        "name": "Vincenzo Pecorella"
                    },
                    {
                        "name": "Alessandro de Giulii"
                    },
                    {
                        "name": "Davide Tresoldi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Tresoldi"
                },
                "author": "Davide Tresoldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v4",
                "updated": "2025-09-23T21:08:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    8,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "CVPR 2025. Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v3",
                "updated": "2025-09-23T20:25:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    20,
                    25,
                    15,
                    1,
                    266,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19459v1",
                "updated": "2025-09-23T18:14:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T18:14:21Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "title": "Automated Insertion of Flushes and Fences for Persistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Insertion of Flushes and Fences for Persistency"
                },
                "summary": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations."
                },
                "authors": [
                    {
                        "name": "Yutong Guo"
                    },
                    {
                        "name": "Weiyu Luo"
                    },
                    {
                        "name": "Brian Demsky"
                    }
                ],
                "author_detail": {
                    "name": "Brian Demsky"
                },
                "author": "Brian Demsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19228v1",
                "updated": "2025-09-23T16:49:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:49:43Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "title": "CompLLM: Compression for Long Context Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompLLM: Compression for Long Context Q&A"
                },
                "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility."
                },
                "authors": [
                    {
                        "name": "Gabriele Berton"
                    },
                    {
                        "name": "Jayakrishnan Unnikrishnan"
                    },
                    {
                        "name": "Son Tran"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19061v1",
                "updated": "2025-09-23T14:25:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:25:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics"
                },
                "summary": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators."
                },
                "authors": [
                    {
                        "name": "Marcel Ferrari"
                    },
                    {
                        "name": "Cyrill Püntener"
                    },
                    {
                        "name": "Alexander Sotoudeh"
                    },
                    {
                        "name": "Niklas Viebig"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Viebig"
                },
                "author": "Niklas Viebig",
                "arxiv_comment": "15 pages, 5 figures, appendix has 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F08, 65N55, 65N22, 76M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.8; F.2.1; D.1.3; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18909v1",
                "updated": "2025-09-23T12:32:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:32:51Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "title": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation"
                },
                "summary": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge."
                },
                "authors": [
                    {
                        "name": "Jan Wichelmann"
                    },
                    {
                        "name": "Anja Rabich"
                    },
                    {
                        "name": "Anna P\"atschke"
                    },
                    {
                        "name": "Thomas Eisenbarth"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Eisenbarth"
                },
                "author": "Thomas Eisenbarth",
                "arxiv_doi": "10.1109/SP54263.2024.00261",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP54263.2024.00261",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.18909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 IEEE Symposium on Security and Privacy (SP), San Francisco,\n  CA, USA, 2024, pp. 4182-4199",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v3",
                "updated": "2025-09-23T08:31:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    31,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v2",
                "updated": "2025-09-23T08:24:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    24,
                    7,
                    1,
                    266,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18684v1",
                "updated": "2025-09-23T06:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T06:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "title": "Static Estimation of Reuse Profiles for Arrays in Nested Loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Estimation of Reuse Profiles for Arrays in Nested Loops"
                },
                "summary": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "This paper is accepted at the MEMSYS 2025 conference, 11th\n  International Symposium on Memory Systems, Washington D.C., October 7 -\n  October 8, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18670v1",
                "updated": "2025-09-23T05:39:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T05:39:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases"
                },
                "summary": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Kyuri Park"
                    },
                    {
                        "name": "Youngjae Kim"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "arxiv_comment": "11 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18592v1",
                "updated": "2025-09-23T03:23:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T03:23:03Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation"
                },
                "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/."
                },
                "authors": [
                    {
                        "name": "Neel P. Bhatt"
                    },
                    {
                        "name": "Yunhao Yang"
                    },
                    {
                        "name": "Rohan Siva"
                    },
                    {
                        "name": "Pranay Samineni"
                    },
                    {
                        "name": "Daniel Milan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "arxiv_comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00329v2",
                "updated": "2025-09-22T19:20:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    20,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-31T00:52:17Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation"
                },
                "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}."
                },
                "authors": [
                    {
                        "name": "Muhammad Adnan"
                    },
                    {
                        "name": "Nithesh Kurella"
                    },
                    {
                        "name": "Akhil Arunkumar"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "arxiv_comment": "Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18344v1",
                "updated": "2025-09-22T19:08:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T19:08:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding"
                },
                "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."
                },
                "authors": [
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Chun-Che Yang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18307v1",
                "updated": "2025-09-22T18:32:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T18:32:32Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "title": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT"
                },
                "summary": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments."
                },
                "authors": [
                    {
                        "name": "Sudam Masanta"
                    },
                    {
                        "name": "Gurvinder Singh"
                    },
                    {
                        "name": "Shefali Pahwa"
                    },
                    {
                        "name": "Shekhar Dwivedi"
                    },
                    {
                        "name": "Devaraju Sampathirao"
                    },
                    {
                        "name": "Ramandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Ramandeep Singh"
                },
                "author": "Ramandeep Singh",
                "arxiv_comment": "8 pages; comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v1",
                "updated": "2025-09-22T17:58:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00919v2",
                "updated": "2025-09-22T16:16:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    16,
                    25,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-02T21:15:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    15,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings"
                },
                "summary": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining."
                },
                "authors": [
                    {
                        "name": "Stephen Zhang"
                    },
                    {
                        "name": "Mustafa Khan"
                    },
                    {
                        "name": "Vardan Papyan"
                    }
                ],
                "author_detail": {
                    "name": "Vardan Papyan"
                },
                "author": "Vardan Papyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v2",
                "updated": "2025-09-22T12:28:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    28,
                    41,
                    0,
                    265,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "MaoXun Huang"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "13 pages, accepted as a main conference paper at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v3",
                "updated": "2025-09-22T12:03:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    3,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17650v1",
                "updated": "2025-09-22T11:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T11:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers"
                },
                "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical."
                },
                "authors": [
                    {
                        "name": "Soroush Mahdi"
                    },
                    {
                        "name": "Fardin Ayar"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Mahdi Javanmardi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Javanmardi"
                },
                "author": "Mahdi Javanmardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v1",
                "updated": "2025-09-22T06:52:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17360v1",
                "updated": "2025-09-22T05:24:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T05:24:22Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access"
                },
                "summary": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads."
                },
                "authors": [
                    {
                        "name": "Chaoyi Ruan"
                    },
                    {
                        "name": "Chao Bi"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Ziji Shi"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17257v1",
                "updated": "2025-09-21T22:14:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T22:14:56Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "title": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices"
                },
                "summary": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper."
                },
                "authors": [
                    {
                        "name": "Sven Christophersen"
                    }
                ],
                "author_detail": {
                    "name": "Sven Christophersen"
                },
                "author": "Sven Christophersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F55, 65F08, 65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v1",
                "updated": "2025-09-21T21:05:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v2",
                "updated": "2025-09-21T11:48:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    11,
                    48,
                    15,
                    6,
                    264,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v2",
                "updated": "2025-09-21T07:03:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    7,
                    3,
                    46,
                    6,
                    264,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v2",
                "updated": "2025-09-21T03:35:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    3,
                    35,
                    36,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16857v1",
                "updated": "2025-09-21T00:59:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T00:59:45Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching"
                },
                "summary": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput."
                },
                "authors": [
                    {
                        "name": "Xingyu Xiang"
                    },
                    {
                        "name": "Raj Joshi"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Chenxingyu Zhao"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Eddie Kohler"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v2",
                "updated": "2025-09-20T13:54:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    54,
                    37,
                    5,
                    263,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "17 pages, 13 figures, the second version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16686v1",
                "updated": "2025-09-20T13:27:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T13:27:13Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "title": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs"
                },
                "summary": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs."
                },
                "authors": [
                    {
                        "name": "Zhengge Cai"
                    },
                    {
                        "name": "Haowen Hou"
                    }
                ],
                "author_detail": {
                    "name": "Haowen Hou"
                },
                "author": "Haowen Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16630v1",
                "updated": "2025-09-20T11:09:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T11:09:01Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation"
                },
                "summary": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/."
                },
                "authors": [
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Hongfa Wang"
                    },
                    {
                        "name": "Heng Pan"
                    },
                    {
                        "name": "Yingqing He"
                    },
                    {
                        "name": "Junkun Yuan"
                    },
                    {
                        "name": "Ailing Zeng"
                    },
                    {
                        "name": "Chengfei Cai"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Zhifeng Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen",
                "arxiv_comment": "accepted by IJCV2025. project\n  page:https://follow-your-emoji.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21354v1",
                "updated": "2025-09-20T02:04:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    2,
                    4,
                    24,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T02:04:24Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    2,
                    4,
                    24,
                    5,
                    263,
                    0
                ],
                "title": "KV-Efficient VLA: A Method of Speed up Vision Language Model with\n  RNN-Gated Chunked KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Efficient VLA: A Method of Speed up Vision Language Model with\n  RNN-Gated Chunked KV Cache"
                },
                "summary": "Vision-Language-Action (VLA) models promise unified robotic perception and\ncontrol, yet their scalability is constrained by the quadratic cost of\nattention and the unbounded growth of key-value (KV) memory during long-horizon\ninference. While recent methods improve generalization through scaling backbone\narchitectures, they often neglect the inference inefficiencies critical to\nreal-time deployment. In this work, we present KV-Efficient VLA, a\nmodel-agnostic memory compression framework that addresses these limitations by\nintroducing a lightweight, training-friendly mechanism to selectively retain\nhigh-utility context. Our method partitions the KV cache into fixed size chunks\nand employs a recurrent gating module to summarize and filter historical\ncontext according to learned utility scores. This design preserves recent\nfine-grained detail while aggressively pruning stale, low-relevance memory, all\nwhile maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x\ninference speedup and 36% KV memory reduction, with minimal impact on task\nsuccess. Our method integrates seamlessly into existing autoregressive and\nhybrid VLA stacks, enabling scalable inference without modifying training\npipelines or downstream control logic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models promise unified robotic perception and\ncontrol, yet their scalability is constrained by the quadratic cost of\nattention and the unbounded growth of key-value (KV) memory during long-horizon\ninference. While recent methods improve generalization through scaling backbone\narchitectures, they often neglect the inference inefficiencies critical to\nreal-time deployment. In this work, we present KV-Efficient VLA, a\nmodel-agnostic memory compression framework that addresses these limitations by\nintroducing a lightweight, training-friendly mechanism to selectively retain\nhigh-utility context. Our method partitions the KV cache into fixed size chunks\nand employs a recurrent gating module to summarize and filter historical\ncontext according to learned utility scores. This design preserves recent\nfine-grained detail while aggressively pruning stale, low-relevance memory, all\nwhile maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x\ninference speedup and 36% KV memory reduction, with minimal impact on task\nsuccess. Our method integrates seamlessly into existing autoregressive and\nhybrid VLA stacks, enabling scalable inference without modifying training\npipelines or downstream control logic."
                },
                "authors": [
                    {
                        "name": "Wanshun Xu"
                    },
                    {
                        "name": "Long Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Long Zhuang"
                },
                "author": "Long Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16495v1",
                "updated": "2025-09-20T01:56:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T01:56:25Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "title": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads"
                },
                "summary": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads."
                },
                "authors": [
                    {
                        "name": "Mert Hidayetoglu"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Michael Wyatt"
                    },
                    {
                        "name": "Jeff Rasley"
                    },
                    {
                        "name": "Yuxiong He"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    }
                ],
                "author_detail": {
                    "name": "Samyam Rajbhandari"
                },
                "author": "Samyam Rajbhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16471v1",
                "updated": "2025-09-19T23:46:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T23:46:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes"
                },
                "summary": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts"
                },
                "authors": [
                    {
                        "name": "Sima Zeinali Danalou"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Niher R. Sarker"
                    },
                    {
                        "name": "Hooman Chamani"
                    },
                    {
                        "name": "Jane Y. Howe"
                    },
                    {
                        "name": "Patrick C. Lee"
                    },
                    {
                        "name": "Jay R. Werber"
                    }
                ],
                "author_detail": {
                    "name": "Jay R. Werber"
                },
                "author": "Jay R. Werber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.26646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26646v1",
                "updated": "2025-09-30T17:59:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    59,
                    58,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:59:58Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    59,
                    58,
                    1,
                    273,
                    0
                ],
                "title": "The Connection between Dusty Star-Forming Galaxies and the First Massive\n  Quenched Galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Connection between Dusty Star-Forming Galaxies and the First Massive\n  Quenched Galaxies"
                },
                "summary": "High-redshift (z > 2) massive quiescent (MQ) galaxies provide an opportunity\nto probe the key physical processes driving the fuelling and quenching of star\nformation in the early Universe. Observational evidence suggests a possible\nevolutionary link between MQs and dusty star-forming galaxies (DSFGs; or\nsubmillimetre galaxies), another extreme high-redshift population. However,\ngalaxy formation models have historically struggled to reproduce these\npopulations - especially simultaneously - limiting our understanding of their\nformation and connection, particularly in light of recent JWST findings. In\nprevious work, we presented a re-calibrated version of the L-Galaxies\nsemi-analytic model that provides an improved match to observationally-inferred\nnumber densities of both DSFG and MQ populations. In this work, we use this new\nmodel to investigate the progenitors of MQs at z > 2 and the physical\nmechanisms that lead to their quenching. We find that most MQs at z > 2 were\nsub-millimetre-bright ($S_{870}$ > 1 mJy) at some point in their cosmic past.\nThe stellar mass of MQs is strongly correlated with the maximum submillimetre\nflux density attained over their history, and this relation appears to be\nindependent of redshift. However, only a minority of high-redshift DSFGs evolve\ninto MQs by z = 2. The key distinction between typical DSFGs and MQ progenitors\nlies in their merger histories: MQ progenitors experience an early major merger\nthat triggers a brief, intense starburst and rapid black hole growth, depleting\ntheir cold gas reservoirs. In our model, AGN feedback subsequently prevents\nfurther gas cooling, resulting in quenching. In contrast, the broader DSFG\npopulation remains sub-millimetre-bright, with star formation proceeding\nprimarily via secular processes, becoming quenched later.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-redshift (z > 2) massive quiescent (MQ) galaxies provide an opportunity\nto probe the key physical processes driving the fuelling and quenching of star\nformation in the early Universe. Observational evidence suggests a possible\nevolutionary link between MQs and dusty star-forming galaxies (DSFGs; or\nsubmillimetre galaxies), another extreme high-redshift population. However,\ngalaxy formation models have historically struggled to reproduce these\npopulations - especially simultaneously - limiting our understanding of their\nformation and connection, particularly in light of recent JWST findings. In\nprevious work, we presented a re-calibrated version of the L-Galaxies\nsemi-analytic model that provides an improved match to observationally-inferred\nnumber densities of both DSFG and MQ populations. In this work, we use this new\nmodel to investigate the progenitors of MQs at z > 2 and the physical\nmechanisms that lead to their quenching. We find that most MQs at z > 2 were\nsub-millimetre-bright ($S_{870}$ > 1 mJy) at some point in their cosmic past.\nThe stellar mass of MQs is strongly correlated with the maximum submillimetre\nflux density attained over their history, and this relation appears to be\nindependent of redshift. However, only a minority of high-redshift DSFGs evolve\ninto MQs by z = 2. The key distinction between typical DSFGs and MQ progenitors\nlies in their merger histories: MQ progenitors experience an early major merger\nthat triggers a brief, intense starburst and rapid black hole growth, depleting\ntheir cold gas reservoirs. In our model, AGN feedback subsequently prevents\nfurther gas cooling, resulting in quenching. In contrast, the broader DSFG\npopulation remains sub-millimetre-bright, with star formation proceeding\nprimarily via secular processes, becoming quenched later."
                },
                "authors": [
                    {
                        "name": "Pablo Araya-Araya"
                    },
                    {
                        "name": "Rachel K. Cochrane"
                    },
                    {
                        "name": "Laerte Sodré Jr."
                    },
                    {
                        "name": "Robert M. Yates"
                    },
                    {
                        "name": "Christopher C. Hayward"
                    },
                    {
                        "name": "Marcel P. van Daalen"
                    },
                    {
                        "name": "Marcelo C. Vicentin"
                    },
                    {
                        "name": "Bitten Gullberg"
                    },
                    {
                        "name": "Francesco Valentino"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Valentino"
                },
                "author": "Francesco Valentino",
                "arxiv_comment": "Submitted to A&A, 15 pages, 11 Figures (including Appendix). Abstract\n  shortened to meet ArXiv requirements. Comments are very welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26635v1",
                "updated": "2025-09-30T17:59:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    59,
                    12,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:59:12Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    59,
                    12,
                    1,
                    273,
                    0
                ],
                "title": "A Tractable Family of Smooth Copulas with Rotational Dependence:\n  Properties, Inference, and Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tractable Family of Smooth Copulas with Rotational Dependence:\n  Properties, Inference, and Application"
                },
                "summary": "We introduce a new family of copula densities constructed from univariate\ndistributions on $[0,1]$. Although our construction is structurally simple, the\nresulting family is versatile: it includes both smooth and irregular examples,\nand reveals clear links between properties of the underlying univariate\ndistribution and the strength, direction, and form of multivariate dependence.\nThe framework brings with it a range of explicit mathematical properties,\nincluding interpretable characterizations of dependence and transparent\ndescriptions of how rotational forms arise. We propose model selection and\ninference methods in parametric and nonparametric settings, supported by\nasymptotic theory that reduces multivariate estimation to well-studied\nunivariate problems. Simulation studies confirm the reliable recovery of\nstructural features, and an application involving neural connectivity data\nillustrates how the family can yield a better fit than existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new family of copula densities constructed from univariate\ndistributions on $[0,1]$. Although our construction is structurally simple, the\nresulting family is versatile: it includes both smooth and irregular examples,\nand reveals clear links between properties of the underlying univariate\ndistribution and the strength, direction, and form of multivariate dependence.\nThe framework brings with it a range of explicit mathematical properties,\nincluding interpretable characterizations of dependence and transparent\ndescriptions of how rotational forms arise. We propose model selection and\ninference methods in parametric and nonparametric settings, supported by\nasymptotic theory that reduces multivariate estimation to well-studied\nunivariate problems. Simulation studies confirm the reliable recovery of\nstructural features, and an application involving neural connectivity data\nillustrates how the family can yield a better fit than existing models."
                },
                "authors": [
                    {
                        "name": "Michaël Lalancette"
                    },
                    {
                        "name": "Robert Zimmerman"
                    }
                ],
                "author_detail": {
                    "name": "Robert Zimmerman"
                },
                "author": "Robert Zimmerman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H05 (Primary) 60E05 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26634v1",
                "updated": "2025-09-30T17:59:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    59,
                    9,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:59:09Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    59,
                    9,
                    1,
                    273,
                    0
                ],
                "title": "Scaling Spoken Language Models with Syllabic Speech Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Spoken Language Models with Syllabic Speech Tokenization"
                },
                "summary": "Spoken language models (SLMs) typically discretize speech into\nhigh-frame-rate tokens extracted from SSL speech models. As the most successful\nLMs are based on the Transformer architecture, processing these long token\nstreams with self-attention is expensive, as attention scales quadratically\nwith sequence length. A recent SSL work introduces acoustic tokenization of\nspeech at the syllable level, which is more interpretable and potentially more\nscalable with significant compression in token lengths (4-5 Hz). Yet, their\nvalue for spoken language modeling is not yet fully explored. We present the\nfirst systematic study of syllabic tokenization for spoken language modeling,\nevaluating models on a suite of SLU benchmarks while varying training data\nscale. Syllabic tokens can match or surpass the previous high-frame rate tokens\nwhile significantly cutting training and inference costs, achieving more than a\n2x reduction in training time and a 5x reduction in FLOPs. Our findings\nhighlight syllable-level language modeling as a promising path to efficient\nlong-context spoken language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken language models (SLMs) typically discretize speech into\nhigh-frame-rate tokens extracted from SSL speech models. As the most successful\nLMs are based on the Transformer architecture, processing these long token\nstreams with self-attention is expensive, as attention scales quadratically\nwith sequence length. A recent SSL work introduces acoustic tokenization of\nspeech at the syllable level, which is more interpretable and potentially more\nscalable with significant compression in token lengths (4-5 Hz). Yet, their\nvalue for spoken language modeling is not yet fully explored. We present the\nfirst systematic study of syllabic tokenization for spoken language modeling,\nevaluating models on a suite of SLU benchmarks while varying training data\nscale. Syllabic tokens can match or surpass the previous high-frame rate tokens\nwhile significantly cutting training and inference costs, achieving more than a\n2x reduction in training time and a 5x reduction in FLOPs. Our findings\nhighlight syllable-level language modeling as a promising path to efficient\nlong-context spoken language models."
                },
                "authors": [
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Cheol Jun Cho"
                    },
                    {
                        "name": "Alan W Black"
                    },
                    {
                        "name": "Gopala K. Anumanchipalli"
                    }
                ],
                "author_detail": {
                    "name": "Gopala K. Anumanchipalli"
                },
                "author": "Gopala K. Anumanchipalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26631v1",
                "updated": "2025-09-30T17:58:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    58,
                    55,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:58:55Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    58,
                    55,
                    1,
                    273,
                    0
                ],
                "title": "Learning Generalizable Shape Completion with SIM(3) Equivariance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Generalizable Shape Completion with SIM(3) Equivariance"
                },
                "summary": "3D shape completion methods typically assume scans are pre-aligned to a\ncanonical frame. This leaks pose and scale cues that networks may exploit to\nmemorize absolute positions rather than inferring intrinsic geometry. When such\nalignment is absent in real data, performance collapses. We argue that robust\ngeneralization demands architectural equivariance to the similarity group,\nSIM(3), so the model remains agnostic to pose and scale. Following this\nprinciple, we introduce the first SIM(3)-equivariant shape completion network,\nwhose modular layers successively canonicalize features, reason over\nsimilarity-invariant geometry, and restore the original frame. Under a\nde-biased evaluation protocol that removes the hidden cues, our model\noutperforms both equivariant and augmentation baselines on the PCN benchmark.\nIt also sets new cross-domain records on real driving and indoor scans,\nlowering minimal matching distance on KITTI by 17% and Chamfer distance $\\ell1$\non OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol\nstill outperforms competitors under their biased settings. These results\nestablish full SIM(3) equivariance as an effective route to truly generalizable\nshape completion. Project page: https://sime-completion.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape completion methods typically assume scans are pre-aligned to a\ncanonical frame. This leaks pose and scale cues that networks may exploit to\nmemorize absolute positions rather than inferring intrinsic geometry. When such\nalignment is absent in real data, performance collapses. We argue that robust\ngeneralization demands architectural equivariance to the similarity group,\nSIM(3), so the model remains agnostic to pose and scale. Following this\nprinciple, we introduce the first SIM(3)-equivariant shape completion network,\nwhose modular layers successively canonicalize features, reason over\nsimilarity-invariant geometry, and restore the original frame. Under a\nde-biased evaluation protocol that removes the hidden cues, our model\noutperforms both equivariant and augmentation baselines on the PCN benchmark.\nIt also sets new cross-domain records on real driving and indoor scans,\nlowering minimal matching distance on KITTI by 17% and Chamfer distance $\\ell1$\non OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol\nstill outperforms competitors under their biased settings. These results\nestablish full SIM(3) equivariance as an effective route to truly generalizable\nshape completion. Project page: https://sime-completion.github.io."
                },
                "authors": [
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Zhaiyu Chen"
                    },
                    {
                        "name": "Xiao Xiang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Xiang Zhu"
                },
                "author": "Xiao Xiang Zhu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26628v1",
                "updated": "2025-09-30T17:58:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    58,
                    34,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:58:34Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    58,
                    34,
                    1,
                    273,
                    0
                ],
                "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models"
                },
                "summary": "Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency."
                },
                "authors": [
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Jiakang Wang"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Zhihui Xie"
                    },
                    {
                        "name": "Chenxin An"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Lei Lin"
                    },
                    {
                        "name": "Wenping Hu"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15849v2",
                "updated": "2025-09-30T17:58:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    58,
                    13,
                    1,
                    273,
                    0
                ],
                "published": "2025-07-21T17:56:09Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    56,
                    9,
                    0,
                    202,
                    0
                ],
                "title": "The Impact of Language Mixing on Bilingual LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Language Mixing on Bilingual LLM Reasoning"
                },
                "summary": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing-alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We show that language mixing can enhance\nreasoning: enforcing monolingual decoding reduces accuracy by 5.6 percentage\npoints on MATH500. Additionally, a lightweight probe can be trained to predict\nwhether a potential language switch would benefit or harm reasoning, and when\nused to guide decoding, increases accuracy by 2.92 percentage points. Our\nfindings suggest that language mixing is not merely a byproduct of multilingual\ntraining, but is a strategic reasoning behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing-alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We show that language mixing can enhance\nreasoning: enforcing monolingual decoding reduces accuracy by 5.6 percentage\npoints on MATH500. Additionally, a lightweight probe can be trained to predict\nwhether a potential language switch would benefit or harm reasoning, and when\nused to guide decoding, increases accuracy by 2.92 percentage points. Our\nfindings suggest that language mixing is not merely a byproduct of multilingual\ntraining, but is a strategic reasoning behavior."
                },
                "authors": [
                    {
                        "name": "Yihao Li"
                    },
                    {
                        "name": "Jiayi Xin"
                    },
                    {
                        "name": "Miranda Muqing Miao"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Lyle Ungar"
                    }
                ],
                "author_detail": {
                    "name": "Lyle Ungar"
                },
                "author": "Lyle Ungar",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26626v1",
                "updated": "2025-09-30T17:58:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    58,
                    3,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:58:03Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    58,
                    3,
                    1,
                    273,
                    0
                ],
                "title": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language\n  Models"
                },
                "summary": "Test-time scaling methods improve the capabilities of large language models\n(LLMs) by increasing the amount of compute used during inference to make a\nprediction. Inference-time compute can be scaled in parallel by choosing among\nmultiple independent solutions or sequentially through self-refinement. We\npropose Recursive Self-Aggregation (RSA), a test-time scaling method inspired\nby evolutionary methods that combines the benefits of both parallel and\nsequential scaling. Each step of RSA refines a population of candidate\nreasoning chains through aggregation of subsets to yield a population of\nimproved solutions, which are then used as the candidate pool for the next\niteration. RSA exploits the rich information embedded in the reasoning chains\n-- not just the final answers -- and enables bootstrapping from partially\ncorrect intermediate steps within different chains of thought. Empirically, RSA\ndelivers substantial performance gains with increasing compute budgets across\ndiverse tasks, model families and sizes. Notably, RSA enables\nQwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning\nmodels, including DeepSeek-R1 and o3-mini (high), while outperforming purely\nparallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning\nGym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the\nmodel to combine solutions via a novel aggregation-aware reinforcement learning\napproach yields significant performance gains. Code available at\nhttps://github.com/HyperPotatoNeo/RSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling methods improve the capabilities of large language models\n(LLMs) by increasing the amount of compute used during inference to make a\nprediction. Inference-time compute can be scaled in parallel by choosing among\nmultiple independent solutions or sequentially through self-refinement. We\npropose Recursive Self-Aggregation (RSA), a test-time scaling method inspired\nby evolutionary methods that combines the benefits of both parallel and\nsequential scaling. Each step of RSA refines a population of candidate\nreasoning chains through aggregation of subsets to yield a population of\nimproved solutions, which are then used as the candidate pool for the next\niteration. RSA exploits the rich information embedded in the reasoning chains\n-- not just the final answers -- and enables bootstrapping from partially\ncorrect intermediate steps within different chains of thought. Empirically, RSA\ndelivers substantial performance gains with increasing compute budgets across\ndiverse tasks, model families and sizes. Notably, RSA enables\nQwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning\nmodels, including DeepSeek-R1 and o3-mini (high), while outperforming purely\nparallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning\nGym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the\nmodel to combine solutions via a novel aggregation-aware reinforcement learning\napproach yields significant performance gains. Code available at\nhttps://github.com/HyperPotatoNeo/RSA."
                },
                "authors": [
                    {
                        "name": "Siddarth Venkatraman"
                    },
                    {
                        "name": "Vineet Jain"
                    },
                    {
                        "name": "Sarthak Mittal"
                    },
                    {
                        "name": "Vedant Shah"
                    },
                    {
                        "name": "Johan Obando-Ceron"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Brian R. Bartoldson"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Guillaume Lajoie"
                    },
                    {
                        "name": "Glen Berseth"
                    },
                    {
                        "name": "Nikolay Malkin"
                    },
                    {
                        "name": "Moksh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Moksh Jain"
                },
                "author": "Moksh Jain",
                "arxiv_comment": "24 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26625v1",
                "updated": "2025-09-30T17:57:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    57,
                    44,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:57:44Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    57,
                    44,
                    1,
                    273,
                    0
                ],
                "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training"
                },
                "summary": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs."
                },
                "authors": [
                    {
                        "name": "Junlin Han"
                    },
                    {
                        "name": "Shengbang Tong"
                    },
                    {
                        "name": "David Fan"
                    },
                    {
                        "name": "Yufan Ren"
                    },
                    {
                        "name": "Koustuv Sinha"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Filippos Kokkinos"
                    }
                ],
                "author_detail": {
                    "name": "Filippos Kokkinos"
                },
                "author": "Filippos Kokkinos",
                "arxiv_comment": "Project page: https://junlinhan.github.io/projects/lsbs/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05526v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05526v3",
                "updated": "2025-09-30T17:57:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    57,
                    27,
                    1,
                    273,
                    0
                ],
                "published": "2025-06-05T19:19:01Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    19,
                    1,
                    3,
                    156,
                    0
                ],
                "title": "On Fitting Flow Models with Large Sinkhorn Couplings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Fitting Flow Models with Large Sinkhorn Couplings"
                },
                "summary": "Flow models transform data gradually from one modality (e.g. noise) onto\nanother (e.g. images). Such models are parameterized by a time-dependent\nvelocity field, trained to fit segments connecting pairs of source and target\npoints. When the pairing between source and target points is given, training\nflow models boils down to a supervised regression problem. When no such pairing\nexists, as is the case when generating data from noise, training flows is much\nharder. A popular approach lies in picking source and target points\nindependently. This can, however, lead to velocity fields that are slow to\ntrain, but also costly to integrate at inference time. In theory, one would\ngreatly benefit from training flow models by sampling pairs from an optimal\ntransport (OT) measure coupling source and target, since this would lead to a\nhighly efficient flow solving the Benamou and Brenier dynamical OT problem. In\npractice, recent works have proposed to sample mini-batches of $n$ source and\n$n$ target points and reorder them using an OT solver to form better pairs.\nThese works have advocated using batches of size $n\\approx 256$, and considered\nOT solvers that return couplings that are either sharp (using e.g. the\nHungarian algorithm) or blurred (using e.g. entropic regularization, a.k.a.\nSinkhorn). We follow in the footsteps of these works by exploring the benefits\nof increasing $n$ by three to four orders of magnitude, and look more carefully\non the effect of the entropic regularization $\\varepsilon$ used in the Sinkhorn\nalgorithm. Our analysis is facilitated by new scale invariant quantities to\nreport the sharpness of a coupling, while our sharded computations across\nmultiple GPU or GPU nodes allow scaling up $n$. We show that in both synthetic\nand image generation tasks, flow models greatly benefit when fitted with large\nSinkhorn couplings, with a low entropic regularization $\\varepsilon$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow models transform data gradually from one modality (e.g. noise) onto\nanother (e.g. images). Such models are parameterized by a time-dependent\nvelocity field, trained to fit segments connecting pairs of source and target\npoints. When the pairing between source and target points is given, training\nflow models boils down to a supervised regression problem. When no such pairing\nexists, as is the case when generating data from noise, training flows is much\nharder. A popular approach lies in picking source and target points\nindependently. This can, however, lead to velocity fields that are slow to\ntrain, but also costly to integrate at inference time. In theory, one would\ngreatly benefit from training flow models by sampling pairs from an optimal\ntransport (OT) measure coupling source and target, since this would lead to a\nhighly efficient flow solving the Benamou and Brenier dynamical OT problem. In\npractice, recent works have proposed to sample mini-batches of $n$ source and\n$n$ target points and reorder them using an OT solver to form better pairs.\nThese works have advocated using batches of size $n\\approx 256$, and considered\nOT solvers that return couplings that are either sharp (using e.g. the\nHungarian algorithm) or blurred (using e.g. entropic regularization, a.k.a.\nSinkhorn). We follow in the footsteps of these works by exploring the benefits\nof increasing $n$ by three to four orders of magnitude, and look more carefully\non the effect of the entropic regularization $\\varepsilon$ used in the Sinkhorn\nalgorithm. Our analysis is facilitated by new scale invariant quantities to\nreport the sharpness of a coupling, while our sharded computations across\nmultiple GPU or GPU nodes allow scaling up $n$. We show that in both synthetic\nand image generation tasks, flow models greatly benefit when fitted with large\nSinkhorn couplings, with a low entropic regularization $\\varepsilon$."
                },
                "authors": [
                    {
                        "name": "Stephen Zhang"
                    },
                    {
                        "name": "Alireza Mousavi-Hosseini"
                    },
                    {
                        "name": "Michal Klein"
                    },
                    {
                        "name": "Marco Cuturi"
                    }
                ],
                "author_detail": {
                    "name": "Marco Cuturi"
                },
                "author": "Marco Cuturi",
                "arxiv_comment": "23 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05526v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05526v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26616v1",
                "updated": "2025-09-30T17:54:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    54,
                    25,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:54:25Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    54,
                    25,
                    1,
                    273,
                    0
                ],
                "title": "Black-box Context-free Grammar Inference for Readable & Natural Grammars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Context-free Grammar Inference for Readable & Natural Grammars"
                },
                "summary": "Black-box context-free grammar inference is crucial for program analysis,\nreverse engineering, and security, yet existing tools such as Arvada, TreeVada,\nand Kedavra struggle with scalability, readability, and accuracy on large,\ncomplex languages. We present NatGI, a novel LLM-guided grammar inference\nframework that extends TreeVada's parse tree recovery with three key\ninnovations: bracket-guided bubble exploration, LLM-driven bubble generation\nand non-terminal labeling, and hierarchical delta debugging (HDD) for\nsystematic tree simplification. Bracket-guided exploration leverages syntactic\ncues such as parentheses to propose well-structured grammar fragments, while\nLLM guidance produces meaningful non-terminal names and selects more promising\nmerges. Finally, HDD incrementally reduces unnecessary rules, which makes the\ngrammars both compact and interpretable. In our experiments, we evaluate NatGI\non a comprehensive benchmark suite ranging from small languages to larger ones\nsuch as lua, c, and mysql. Our results show that NatGI consistently outperforms\nstrong baselines in terms of F1 score. On average, NatGI achieves an F1 score\nof 0.57, which is 25pp (percentage points) higher than the best-performing\nbaseline, TreeVada. In the case of interpretability, our generated grammars\nperform significantly better than those produced by existing approaches.\nLeveraging LLM-based node renaming and bubble exploration, NatGI produces rules\nwith meaningful non-terminal names and compact structures that align more\nclosely with human intuition. As a result, developers and researchers can\nachieve higher accuracy while still being able to easily inspect, verify, and\nreason about the structure and semantics of the induced grammars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box context-free grammar inference is crucial for program analysis,\nreverse engineering, and security, yet existing tools such as Arvada, TreeVada,\nand Kedavra struggle with scalability, readability, and accuracy on large,\ncomplex languages. We present NatGI, a novel LLM-guided grammar inference\nframework that extends TreeVada's parse tree recovery with three key\ninnovations: bracket-guided bubble exploration, LLM-driven bubble generation\nand non-terminal labeling, and hierarchical delta debugging (HDD) for\nsystematic tree simplification. Bracket-guided exploration leverages syntactic\ncues such as parentheses to propose well-structured grammar fragments, while\nLLM guidance produces meaningful non-terminal names and selects more promising\nmerges. Finally, HDD incrementally reduces unnecessary rules, which makes the\ngrammars both compact and interpretable. In our experiments, we evaluate NatGI\non a comprehensive benchmark suite ranging from small languages to larger ones\nsuch as lua, c, and mysql. Our results show that NatGI consistently outperforms\nstrong baselines in terms of F1 score. On average, NatGI achieves an F1 score\nof 0.57, which is 25pp (percentage points) higher than the best-performing\nbaseline, TreeVada. In the case of interpretability, our generated grammars\nperform significantly better than those produced by existing approaches.\nLeveraging LLM-based node renaming and bubble exploration, NatGI produces rules\nwith meaningful non-terminal names and compact structures that align more\nclosely with human intuition. As a result, developers and researchers can\nachieve higher accuracy while still being able to easily inspect, verify, and\nreason about the structure and semantics of the induced grammars."
                },
                "authors": [
                    {
                        "name": "Mohammad Rifat Arefin"
                    },
                    {
                        "name": "Shanto Rahman"
                    },
                    {
                        "name": "Christoph Csallner"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Csallner"
                },
                "author": "Christoph Csallner",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q42, 68Q45 (Primary), 68T50 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5; F.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15489v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15489v3",
                "updated": "2025-09-30T17:53:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    53,
                    25,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-21T13:14:32Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    14,
                    32,
                    2,
                    141,
                    0
                ],
                "title": "Seeing Through Deception: Uncovering Misleading Creator Intent in\n  Multimodal News with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing Through Deception: Uncovering Misleading Creator Intent in\n  Multimodal News with Vision-Language Models"
                },
                "summary": "The impact of misinformation arises not only from factual inaccuracies but\nalso from the misleading narratives that creators deliberately embed.\nInterpreting such creator intent is therefore essential for multimodal\nmisinformation detection (MMD) and effective information governance. To this\nend, we introduce DeceptionDecoded, a large-scale benchmark of 12,000\nimage-caption pairs grounded in trustworthy reference articles, created using\nan intent-guided simulation framework that models both the desired influence\nand the execution plan of news creators. The dataset captures both misleading\nand non-misleading cases, spanning manipulations across visual and textual\nmodalities, and supports three intent-centric tasks: (1) misleading intent\ndetection, (2) misleading source attribution, and (3) creator desire inference.\nWe evaluate 14 state-of-the-art vision-language models (VLMs) and find that\nthey struggle with intent reasoning, often relying on shallow cues such as\nsurface-level alignment, stylistic polish, or heuristic authenticity signals.\nThese results highlight the limitations of current VLMs and position\nDeceptionDecoded as a foundation for developing intent-aware models that go\nbeyond shallow cues in MMD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of misinformation arises not only from factual inaccuracies but\nalso from the misleading narratives that creators deliberately embed.\nInterpreting such creator intent is therefore essential for multimodal\nmisinformation detection (MMD) and effective information governance. To this\nend, we introduce DeceptionDecoded, a large-scale benchmark of 12,000\nimage-caption pairs grounded in trustworthy reference articles, created using\nan intent-guided simulation framework that models both the desired influence\nand the execution plan of news creators. The dataset captures both misleading\nand non-misleading cases, spanning manipulations across visual and textual\nmodalities, and supports three intent-centric tasks: (1) misleading intent\ndetection, (2) misleading source attribution, and (3) creator desire inference.\nWe evaluate 14 state-of-the-art vision-language models (VLMs) and find that\nthey struggle with intent reasoning, often relying on shallow cues such as\nsurface-level alignment, stylistic polish, or heuristic authenticity signals.\nThese results highlight the limitations of current VLMs and position\nDeceptionDecoded as a foundation for developing intent-aware models that go\nbeyond shallow cues in MMD."
                },
                "authors": [
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Fanxiao Li"
                    },
                    {
                        "name": "Zihang Fu"
                    },
                    {
                        "name": "Min-Yen Kan"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15489v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15489v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26606v1",
                "updated": "2025-09-30T17:50:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    50,
                    22,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:50:22Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    50,
                    22,
                    1,
                    273,
                    0
                ],
                "title": "Beyond Suboptimality: Resource-Rationality and Task Demands Shape the\n  Complexity of Perceptual Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Suboptimality: Resource-Rationality and Task Demands Shape the\n  Complexity of Perceptual Representations"
                },
                "summary": "Early theories of perception as probabilistic inference propose that\nuncertainty about the interpretation of sensory input is represented as a\nprobability distribution over many interpretations -- a relatively complex\nrepresentation. However, critics argue that persistent demonstrations of\nsuboptimal perceptual decision-making indicate limits in representational\ncomplexity. We contend that suboptimality arises not from genuine limits, but\nparticipants' resource-rational adaptations to task demands. For example, when\ntasks are solvable with minimal attention to stimuli, participants may neglect\ninformation needed for complex representations, relying instead on simpler ones\nthat engender suboptimality. Across three experiments, we progressively reduced\nthe efficacy of resource-rational strategies on a carefully controlled decision\ntask. Model fits favored simple representations when resource-rational\nstrategies were effective, and favored complex representations when\nineffective, suggesting that perceptual representations can be simple or\ncomplex depending on task demands. We conclude that resource-rationality is an\nepistemic constraint for experimental design and essential to a complete theory\nof perception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early theories of perception as probabilistic inference propose that\nuncertainty about the interpretation of sensory input is represented as a\nprobability distribution over many interpretations -- a relatively complex\nrepresentation. However, critics argue that persistent demonstrations of\nsuboptimal perceptual decision-making indicate limits in representational\ncomplexity. We contend that suboptimality arises not from genuine limits, but\nparticipants' resource-rational adaptations to task demands. For example, when\ntasks are solvable with minimal attention to stimuli, participants may neglect\ninformation needed for complex representations, relying instead on simpler ones\nthat engender suboptimality. Across three experiments, we progressively reduced\nthe efficacy of resource-rational strategies on a carefully controlled decision\ntask. Model fits favored simple representations when resource-rational\nstrategies were effective, and favored complex representations when\nineffective, suggesting that perceptual representations can be simple or\ncomplex depending on task demands. We conclude that resource-rationality is an\nepistemic constraint for experimental design and essential to a complete theory\nof perception."
                },
                "authors": [
                    {
                        "name": "Andrew Jun Lee"
                    },
                    {
                        "name": "Daniel Turek"
                    },
                    {
                        "name": "Omer Daglar Tanrikulu"
                    }
                ],
                "author_detail": {
                    "name": "Omer Daglar Tanrikulu"
                },
                "author": "Omer Daglar Tanrikulu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05634v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05634v4",
                "updated": "2025-09-30T17:49:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    49,
                    59,
                    1,
                    273,
                    0
                ],
                "published": "2025-07-08T03:29:16Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    3,
                    29,
                    16,
                    1,
                    189,
                    0
                ],
                "title": "A Note on Inferential Decisions, Errors and Path-Dependency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Note on Inferential Decisions, Errors and Path-Dependency"
                },
                "summary": "Consider the sequential inference of a binary outcome. The process of a\nposteriori beliefs and its objectively true conditional-probability counterpart\ngenerally differ but should lead to the same result eventually in well-defined\ntests. We show that unless the two are 'essentially identical', differing only\nby an a priori factor, time-homogeneous continuous decisions based on the\nformer must be path-dependent with respect to state-variables based on the\nlatter or any non-essentially-identical process. Inferential errors decompose\ninto path-dependent and path-independent parts, whose distinct properties are\nrelevant to error mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consider the sequential inference of a binary outcome. The process of a\nposteriori beliefs and its objectively true conditional-probability counterpart\ngenerally differ but should lead to the same result eventually in well-defined\ntests. We show that unless the two are 'essentially identical', differing only\nby an a priori factor, time-homogeneous continuous decisions based on the\nformer must be path-dependent with respect to state-variables based on the\nlatter or any non-essentially-identical process. Inferential errors decompose\ninto path-dependent and path-independent parts, whose distinct properties are\nrelevant to error mitigation."
                },
                "authors": [
                    {
                        "name": "Kangda K. Wren"
                    }
                ],
                "author_detail": {
                    "name": "Kangda K. Wren"
                },
                "author": "Kangda K. Wren",
                "arxiv_comment": "11 pages: 1 for highlight, 6 for main text, 3 for appendix and 1 for\n  references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05634v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05634v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M, 60G25, 60G35, 93E10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26601v1",
                "updated": "2025-09-30T17:48:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    48,
                    58,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:48:58Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    48,
                    58,
                    1,
                    273,
                    0
                ],
                "title": "MENLO: From Preferences to Proficiency -- Evaluating and Modeling\n  Native-like Quality Across 47 Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MENLO: From Preferences to Proficiency -- Evaluating and Modeling\n  Native-like Quality Across 47 Languages"
                },
                "summary": "Ensuring native-like quality of large language model (LLM) responses across\nmany languages is challenging. To address this, we introduce MENLO, a framework\nthat operationalizes the evaluation of native-like response quality based on\naudience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423\nhuman-annotated prompt-response preference pairs covering four quality\ndimensions with high inter-annotator agreement in 47 language varieties. Our\nevaluation reveals that zero-shot LLM judges benefit significantly from\npairwise evaluation and our structured annotation rubrics, yet they still\nunderperform human annotators on our dataset. We demonstrate substantial\nimprovements through fine-tuning with reinforcement learning, reward shaping,\nand multi-task learning approaches. Additionally, we show that RL-trained\njudges can serve as generative reward models to enhance LLMs' multilingual\nproficiency, though discrepancies with human judgment remain. Our findings\nsuggest promising directions for scalable multilingual evaluation and\npreference alignment. We release our dataset and evaluation framework to\nsupport further research in multilingual LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring native-like quality of large language model (LLM) responses across\nmany languages is challenging. To address this, we introduce MENLO, a framework\nthat operationalizes the evaluation of native-like response quality based on\naudience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423\nhuman-annotated prompt-response preference pairs covering four quality\ndimensions with high inter-annotator agreement in 47 language varieties. Our\nevaluation reveals that zero-shot LLM judges benefit significantly from\npairwise evaluation and our structured annotation rubrics, yet they still\nunderperform human annotators on our dataset. We demonstrate substantial\nimprovements through fine-tuning with reinforcement learning, reward shaping,\nand multi-task learning approaches. Additionally, we show that RL-trained\njudges can serve as generative reward models to enhance LLMs' multilingual\nproficiency, though discrepancies with human judgment remain. Our findings\nsuggest promising directions for scalable multilingual evaluation and\npreference alignment. We release our dataset and evaluation framework to\nsupport further research in multilingual LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Chenxi Whitehouse"
                    },
                    {
                        "name": "Sebastian Ruder"
                    },
                    {
                        "name": "Tony Lin"
                    },
                    {
                        "name": "Oksana Kurylo"
                    },
                    {
                        "name": "Haruka Takagi"
                    },
                    {
                        "name": "Janice Lam"
                    },
                    {
                        "name": "Nicolò Busetto"
                    },
                    {
                        "name": "Denise Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Denise Diaz"
                },
                "author": "Denise Diaz",
                "arxiv_comment": "10 pages, 23 tables, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26600v1",
                "updated": "2025-09-30T17:48:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    48,
                    35,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:48:35Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    48,
                    35,
                    1,
                    273,
                    0
                ],
                "title": "Deconstructing Self-Bias in LLM-generated Translation Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deconstructing Self-Bias in LLM-generated Translation Benchmarks"
                },
                "summary": "As large language models (LLMs) begin to saturate existing benchmarks,\nautomated benchmark creation using LLMs (LLM as a benchmark) has emerged as a\nscalable alternative to slow and costly human curation. While these generated\ntest sets have to potential to cheaply rank models, we demonstrate a critical\nflaw. LLM generated benchmarks systematically favor the model that created the\nbenchmark, they exhibit self bias on low resource languages to English\ntranslation tasks. We show three key findings on automatic benchmarking of LLMs\nfor translation: First, this bias originates from two sources: the generated\ntest data (LLM as a testset) and the evaluation method (LLM as an evaluator),\nwith their combination amplifying the effect. Second, self bias in LLM as a\nbenchmark is heavily influenced by the model's generation capabilities in the\nsource language. For instance, we observe more pronounced bias in into English\ntranslation, where the model's generation system is developed, than in out of\nEnglish translation tasks. Third, we observe that low diversity in source text\nis one attribution to self bias. Our results suggest that improving the\ndiversity of these generated source texts can mitigate some of the observed\nself bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) begin to saturate existing benchmarks,\nautomated benchmark creation using LLMs (LLM as a benchmark) has emerged as a\nscalable alternative to slow and costly human curation. While these generated\ntest sets have to potential to cheaply rank models, we demonstrate a critical\nflaw. LLM generated benchmarks systematically favor the model that created the\nbenchmark, they exhibit self bias on low resource languages to English\ntranslation tasks. We show three key findings on automatic benchmarking of LLMs\nfor translation: First, this bias originates from two sources: the generated\ntest data (LLM as a testset) and the evaluation method (LLM as an evaluator),\nwith their combination amplifying the effect. Second, self bias in LLM as a\nbenchmark is heavily influenced by the model's generation capabilities in the\nsource language. For instance, we observe more pronounced bias in into English\ntranslation, where the model's generation system is developed, than in out of\nEnglish translation tasks. Third, we observe that low diversity in source text\nis one attribution to self bias. Our results suggest that improving the\ndiversity of these generated source texts can mitigate some of the observed\nself bias."
                },
                "authors": [
                    {
                        "name": "Wenda Xu"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Markus Freitag"
                    },
                    {
                        "name": "Daniel Deutsch"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Deutsch"
                },
                "author": "Daniel Deutsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26598v1",
                "updated": "2025-09-30T17:47:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    47,
                    9,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:47:09Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    47,
                    9,
                    1,
                    273,
                    0
                ],
                "title": "Are Robust LLM Fingerprints Adversarially Robust?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Robust LLM Fingerprints Adversarially Robust?"
                },
                "summary": "Model fingerprinting has emerged as a promising paradigm for claiming model\nownership. However, robustness evaluations of these schemes have mostly focused\non benign perturbations such as incremental fine-tuning, model merging, and\nprompting. Lack of systematic investigations into {\\em adversarial robustness}\nagainst a malicious model host leaves current systems vulnerable. To bridge\nthis gap, we first define a concrete, practical threat model against model\nfingerprinting. We then take a critical look at existing model fingerprinting\nschemes to identify their fundamental vulnerabilities. Based on these, we\ndevelop adaptive adversarial attacks tailored for each vulnerability, and\ndemonstrate that these can bypass model authentication completely for ten\nrecently proposed fingerprinting schemes while maintaining high utility of the\nmodel for the end users. Our work encourages fingerprint designers to adopt\nadversarial robustness by design. We end with recommendations for future\nfingerprinting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model fingerprinting has emerged as a promising paradigm for claiming model\nownership. However, robustness evaluations of these schemes have mostly focused\non benign perturbations such as incremental fine-tuning, model merging, and\nprompting. Lack of systematic investigations into {\\em adversarial robustness}\nagainst a malicious model host leaves current systems vulnerable. To bridge\nthis gap, we first define a concrete, practical threat model against model\nfingerprinting. We then take a critical look at existing model fingerprinting\nschemes to identify their fundamental vulnerabilities. Based on these, we\ndevelop adaptive adversarial attacks tailored for each vulnerability, and\ndemonstrate that these can bypass model authentication completely for ten\nrecently proposed fingerprinting schemes while maintaining high utility of the\nmodel for the end users. Our work encourages fingerprint designers to adopt\nadversarial robustness by design. We end with recommendations for future\nfingerprinting methods."
                },
                "authors": [
                    {
                        "name": "Anshul Nasery"
                    },
                    {
                        "name": "Edoardo Contente"
                    },
                    {
                        "name": "Alkin Kaz"
                    },
                    {
                        "name": "Pramod Viswanath"
                    },
                    {
                        "name": "Sewoong Oh"
                    }
                ],
                "author_detail": {
                    "name": "Sewoong Oh"
                },
                "author": "Sewoong Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26593v1",
                "updated": "2025-09-30T17:46:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    46,
                    39,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:46:39Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    46,
                    39,
                    1,
                    273,
                    0
                ],
                "title": "Exploring Large Language Model as an Interactive Sports Coach: Lessons\n  from a Single-Subject Half Marathon Preparation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Large Language Model as an Interactive Sports Coach: Lessons\n  from a Single-Subject Half Marathon Preparation"
                },
                "summary": "Large language models (LLMs) are emerging as everyday assistants, but their\nrole as longitudinal virtual coaches is underexplored. This two-month single\nsubject case study documents LLM guided half marathon preparation\n(July-September 2025). Using text based interactions and consumer app logs, the\nLLM acted as planner, explainer, and occasional motivator. Performance improved\nfrom sustaining 2 km at 7min 54sec per km to completing 21.1 km at 6min 30sec\nper km, with gains in cadence, pace HR coupling, and efficiency index trends.\nWhile causal attribution is limited without a control, outcomes demonstrate\nsafe, measurable progress. At the same time, gaps were evident, no realtime\nsensor integration, text only feedback, motivation support that was user\ninitiated, and limited personalization or safety guardrails. We propose design\nrequirements for next generation systems, persistent athlete models with\nexplicit guardrails, multimodal on device sensing, audio, haptic, visual\nfeedback, proactive motivation scaffolds, and privacy-preserving\npersonalization. This study offers grounded evidence and a design agenda for\nevolving LLMs from retrospective advisors to closed-loop coaching companions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are emerging as everyday assistants, but their\nrole as longitudinal virtual coaches is underexplored. This two-month single\nsubject case study documents LLM guided half marathon preparation\n(July-September 2025). Using text based interactions and consumer app logs, the\nLLM acted as planner, explainer, and occasional motivator. Performance improved\nfrom sustaining 2 km at 7min 54sec per km to completing 21.1 km at 6min 30sec\nper km, with gains in cadence, pace HR coupling, and efficiency index trends.\nWhile causal attribution is limited without a control, outcomes demonstrate\nsafe, measurable progress. At the same time, gaps were evident, no realtime\nsensor integration, text only feedback, motivation support that was user\ninitiated, and limited personalization or safety guardrails. We propose design\nrequirements for next generation systems, persistent athlete models with\nexplicit guardrails, multimodal on device sensing, audio, haptic, visual\nfeedback, proactive motivation scaffolds, and privacy-preserving\npersonalization. This study offers grounded evidence and a design agenda for\nevolving LLMs from retrospective advisors to closed-loop coaching companions."
                },
                "authors": [
                    {
                        "name": "Kichang Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kichang Lee"
                },
                "author": "Kichang Lee",
                "arxiv_comment": "23 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4, K.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26592v1",
                "updated": "2025-09-30T17:46:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    46,
                    8,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:46:08Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    46,
                    8,
                    1,
                    273,
                    0
                ],
                "title": "Generating Difficult-to-Translate Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Difficult-to-Translate Texts"
                },
                "summary": "Machine translation benchmarks sourced from the real world are quickly\nobsoleted, due to most examples being easy for state-of-the-art translation\nmodels. This limits the benchmark's ability to distinguish which model is\nbetter or to reveal models' weaknesses. Current methods for creating difficult\ntest cases, such as subsampling or from-scratch synthesis, either fall short of\nidentifying difficult examples or suffer from a lack of diversity and\nnaturalness. Inspired by the iterative process of human experts probing for\nmodel failures, we propose MT-breaker, a method where a large language model\niteratively refines a source text to increase its translation difficulty. The\nLLM iteratively queries a target machine translation model to guide its\ngeneration of difficult examples. Our approach generates examples that are more\nchallenging for the target MT model while preserving the diversity of natural\ntexts. While the examples are tailored to a particular machine translation\nmodel during the generation, the difficulty also transfers to other models and\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine translation benchmarks sourced from the real world are quickly\nobsoleted, due to most examples being easy for state-of-the-art translation\nmodels. This limits the benchmark's ability to distinguish which model is\nbetter or to reveal models' weaknesses. Current methods for creating difficult\ntest cases, such as subsampling or from-scratch synthesis, either fall short of\nidentifying difficult examples or suffer from a lack of diversity and\nnaturalness. Inspired by the iterative process of human experts probing for\nmodel failures, we propose MT-breaker, a method where a large language model\niteratively refines a source text to increase its translation difficulty. The\nLLM iteratively queries a target machine translation model to guide its\ngeneration of difficult examples. Our approach generates examples that are more\nchallenging for the target MT model while preserving the diversity of natural\ntexts. While the examples are tailored to a particular machine translation\nmodel during the generation, the difficulty also transfers to other models and\nlanguages."
                },
                "authors": [
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Wenda Xu"
                    },
                    {
                        "name": "Parker Riley"
                    },
                    {
                        "name": "Juraj Juraska"
                    },
                    {
                        "name": "Mara Finkelstein"
                    },
                    {
                        "name": "Markus Freitag"
                    },
                    {
                        "name": "Dan Deutsch"
                    }
                ],
                "author_detail": {
                    "name": "Dan Deutsch"
                },
                "author": "Dan Deutsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26591v1",
                "updated": "2025-09-30T17:45:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    45,
                    29,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:45:29Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    45,
                    29,
                    1,
                    273,
                    0
                ],
                "title": "The JWST EXCELS Survey: A spectroscopic investigation of the ionizing\n  properties of star-forming galaxies at 1<z<8",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The JWST EXCELS Survey: A spectroscopic investigation of the ionizing\n  properties of star-forming galaxies at 1<z<8"
                },
                "summary": "Charting the Epoch of Reionization demands robust assessments of what drives\nthe production of ionizing photons in high-redshift star-forming galaxies\n(SFGs), and requires better predictive capabilities from current observations.\nUsing a sample of $N=159$ SFGs at $1<z<8$, observed with deep medium-resolution\nspectroscopy from the JWST/NIRSpec EXCELS survey, we perform a statistical\nanalysis of their ionizing photon production efficiencies ($\\xi_\\rm{ion}$). We\nconsider $\\xi_\\rm{ion}$, measured with Balmer line measurements, in relation to\na number of key galaxy properties including; nebular emission line strengths\n($W_\\lambda(\\rm{H\\alpha})$ and $W_\\lambda$( [OIII])), UV luminosity\n($M_\\rm{UV}$) and UV slope ($\\beta_\\rm{UV}$), as well as dust attenuation\n($E(B-V)_\\rm{neb}$) and redshift. Implementing a Bayesian linear regression\nmethodology, we fit $\\xi_\\rm{ion}$ against the principal observables while\nfully marginalising over all measurement uncertainties, mitigating against the\nimpact of outliers and determining the intrinsic scatter. Significant relations\nbetween $\\xi_\\rm{ion}$ and $ W_\\lambda(\\rm{H\\alpha})$, $W_\\lambda$([OIII]) and\n$\\beta_\\rm{UV}$ are recovered. Moreover, the weak trends with $M_\\rm{UV}$ and\nredshift can be fully explained by the remaining property dependencies.\nExpanding our analysis to multivariate regression, we determine that\n$W_\\lambda(\\rm{H\\alpha})$ or $W_\\lambda$([OIII]), along with $\\beta_\\rm{UV}$\nand $E(B-V)_\\rm{neb}$, are the most important observables for accurately\npredicting $\\xi_\\rm{ion,0}$. The latter identifies the most common outliers as\nSFGs with relatively high $E(B-V)_\\rm{neb}\\gtrsim0.5$, possibly indicative of\nobscured star-formation or strong differential attenuation. Combining these\nproperties enable $\\xi_\\rm{ion,0}$ to be inferred with an accuracy of\n$\\sim0.15\\,$dex, with a population intrinsic scatter of\n$\\sigma_\\rm{int}\\sim0.035\\,$dex.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting the Epoch of Reionization demands robust assessments of what drives\nthe production of ionizing photons in high-redshift star-forming galaxies\n(SFGs), and requires better predictive capabilities from current observations.\nUsing a sample of $N=159$ SFGs at $1<z<8$, observed with deep medium-resolution\nspectroscopy from the JWST/NIRSpec EXCELS survey, we perform a statistical\nanalysis of their ionizing photon production efficiencies ($\\xi_\\rm{ion}$). We\nconsider $\\xi_\\rm{ion}$, measured with Balmer line measurements, in relation to\na number of key galaxy properties including; nebular emission line strengths\n($W_\\lambda(\\rm{H\\alpha})$ and $W_\\lambda$( [OIII])), UV luminosity\n($M_\\rm{UV}$) and UV slope ($\\beta_\\rm{UV}$), as well as dust attenuation\n($E(B-V)_\\rm{neb}$) and redshift. Implementing a Bayesian linear regression\nmethodology, we fit $\\xi_\\rm{ion}$ against the principal observables while\nfully marginalising over all measurement uncertainties, mitigating against the\nimpact of outliers and determining the intrinsic scatter. Significant relations\nbetween $\\xi_\\rm{ion}$ and $ W_\\lambda(\\rm{H\\alpha})$, $W_\\lambda$([OIII]) and\n$\\beta_\\rm{UV}$ are recovered. Moreover, the weak trends with $M_\\rm{UV}$ and\nredshift can be fully explained by the remaining property dependencies.\nExpanding our analysis to multivariate regression, we determine that\n$W_\\lambda(\\rm{H\\alpha})$ or $W_\\lambda$([OIII]), along with $\\beta_\\rm{UV}$\nand $E(B-V)_\\rm{neb}$, are the most important observables for accurately\npredicting $\\xi_\\rm{ion,0}$. The latter identifies the most common outliers as\nSFGs with relatively high $E(B-V)_\\rm{neb}\\gtrsim0.5$, possibly indicative of\nobscured star-formation or strong differential attenuation. Combining these\nproperties enable $\\xi_\\rm{ion,0}$ to be inferred with an accuracy of\n$\\sim0.15\\,$dex, with a population intrinsic scatter of\n$\\sigma_\\rm{int}\\sim0.035\\,$dex."
                },
                "authors": [
                    {
                        "name": "R. Begley"
                    },
                    {
                        "name": "R. J. McLure"
                    },
                    {
                        "name": "F. Cullen"
                    },
                    {
                        "name": "A. C. Carnall"
                    },
                    {
                        "name": "T. M. Stanton"
                    },
                    {
                        "name": "D. Scholte"
                    },
                    {
                        "name": "D. J. McLeod"
                    },
                    {
                        "name": "J. S. Dunlop"
                    },
                    {
                        "name": "K. Z. Arellano-Córdova"
                    },
                    {
                        "name": "C. Bondestam"
                    },
                    {
                        "name": "C. T. Donnan"
                    },
                    {
                        "name": "M. L. Hamadouch"
                    },
                    {
                        "name": "A. E. Shapley"
                    },
                    {
                        "name": "S. Stevenson"
                    }
                ],
                "author_detail": {
                    "name": "S. Stevenson"
                },
                "author": "S. Stevenson",
                "arxiv_comment": "19 pages, 14 figures + 1 appendix. Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23459v2",
                "updated": "2025-09-30T17:43:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    43,
                    21,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-27T19:07:50Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    19,
                    7,
                    50,
                    5,
                    270,
                    0
                ],
                "title": "MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction"
                },
                "summary": "Large language models (LLMs) have shown promising performance on tasks that\nrequire reasoning, such as text-to-SQL, code generation, and debugging.\nHowever, regulatory frameworks with strict privacy requirements constrain their\nintegration into sensitive systems. State-of-the-art LLMs are also proprietary,\ncostly, and resource-intensive, making local deployment impractical.\nConsequently, utilizing such LLMs often requires sharing data with third-party\nproviders, raising privacy concerns and risking noncompliance with regulations.\nAlthough fine-tuned small language models (SLMs) can outperform LLMs on certain\ntasks and be deployed locally to mitigate privacy concerns, they underperform\non more complex tasks such as text-to-SQL translation. In this work, we\nintroduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a\nprivacy protection mechanism to mask sensitive information in LLM prompts.\nUnlike redaction, which removes content entirely, or generalization, which\nbroadens tokens, abstraction retains essential information while discarding\nunnecessary details, striking an effective privacy-utility balance for the\ntext-to-SQL task. Moreover, by providing mechanisms to control the\nprivacy-utility tradeoff, MaskSQL facilitates adoption across a broader range\nof use cases. Our experimental results show that MaskSQL outperforms leading\nSLM-based text-to-SQL models and achieves performance approaching\nstate-of-the-art LLM-based models, while preserving privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promising performance on tasks that\nrequire reasoning, such as text-to-SQL, code generation, and debugging.\nHowever, regulatory frameworks with strict privacy requirements constrain their\nintegration into sensitive systems. State-of-the-art LLMs are also proprietary,\ncostly, and resource-intensive, making local deployment impractical.\nConsequently, utilizing such LLMs often requires sharing data with third-party\nproviders, raising privacy concerns and risking noncompliance with regulations.\nAlthough fine-tuned small language models (SLMs) can outperform LLMs on certain\ntasks and be deployed locally to mitigate privacy concerns, they underperform\non more complex tasks such as text-to-SQL translation. In this work, we\nintroduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a\nprivacy protection mechanism to mask sensitive information in LLM prompts.\nUnlike redaction, which removes content entirely, or generalization, which\nbroadens tokens, abstraction retains essential information while discarding\nunnecessary details, striking an effective privacy-utility balance for the\ntext-to-SQL task. Moreover, by providing mechanisms to control the\nprivacy-utility tradeoff, MaskSQL facilitates adoption across a broader range\nof use cases. Our experimental results show that MaskSQL outperforms leading\nSLM-based text-to-SQL models and achieves performance approaching\nstate-of-the-art LLM-based models, while preserving privacy."
                },
                "authors": [
                    {
                        "name": "Sepideh Abedini"
                    },
                    {
                        "name": "Shubhankar Mohapatra"
                    },
                    {
                        "name": "D. B. Emerson"
                    },
                    {
                        "name": "Masoumeh Shafieinejad"
                    },
                    {
                        "name": "Jesse C. Cresswell"
                    },
                    {
                        "name": "Xi He"
                    }
                ],
                "author_detail": {
                    "name": "Xi He"
                },
                "author": "Xi He",
                "arxiv_comment": "Accepted to the 3rd Workshop on Regulatable ML at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26584v1",
                "updated": "2025-09-30T17:42:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    42,
                    35,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:42:35Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    42,
                    35,
                    1,
                    273,
                    0
                ],
                "title": "Fairness Testing in Retrieval-Augmented Generation: How Small\n  Perturbations Reveal Bias in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness Testing in Retrieval-Augmented Generation: How Small\n  Perturbations Reveal Bias in Small Language Models"
                },
                "summary": "Large Language Models (LLMs) are widely used across multiple domains but\ncontinue to raise concerns regarding security and fairness. Beyond known attack\nvectors such as data poisoning and prompt injection, LLMs are also vulnerable\nto fairness bugs. These refer to unintended behaviors influenced by sensitive\ndemographic cues (e.g., race or sexual orientation) that should not affect\noutcomes. Another key issue is hallucination, where models generate plausible\nyet false information. Retrieval-Augmented Generation (RAG) has emerged as a\nstrategy to mitigate hallucinations by combining external retrieval with text\ngeneration. However, its adoption raises new fairness concerns, as the\nretrieved content itself may surface or amplify bias. This study conducts\nfairness testing through metamorphic testing (MT), introducing controlled\ndemographic perturbations in prompts to assess fairness in sentiment analysis\nperformed by three Small Language Models (SLMs) hosted on HuggingFace\n(Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B),\neach integrated into a RAG pipeline. Results show that minor demographic\nvariations can break up to one third of metamorphic relations (MRs). A detailed\nanalysis of these failures reveals a consistent bias hierarchy, with\nperturbations involving racial cues being the predominant cause of the\nviolations. In addition to offering a comparative evaluation, this work\nreinforces that the retrieval component in RAG must be carefully curated to\nprevent bias amplification. The findings serve as a practical alert for\ndevelopers, testers and small organizations aiming to adopt accessible SLMs\nwithout compromising fairness or reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across multiple domains but\ncontinue to raise concerns regarding security and fairness. Beyond known attack\nvectors such as data poisoning and prompt injection, LLMs are also vulnerable\nto fairness bugs. These refer to unintended behaviors influenced by sensitive\ndemographic cues (e.g., race or sexual orientation) that should not affect\noutcomes. Another key issue is hallucination, where models generate plausible\nyet false information. Retrieval-Augmented Generation (RAG) has emerged as a\nstrategy to mitigate hallucinations by combining external retrieval with text\ngeneration. However, its adoption raises new fairness concerns, as the\nretrieved content itself may surface or amplify bias. This study conducts\nfairness testing through metamorphic testing (MT), introducing controlled\ndemographic perturbations in prompts to assess fairness in sentiment analysis\nperformed by three Small Language Models (SLMs) hosted on HuggingFace\n(Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B),\neach integrated into a RAG pipeline. Results show that minor demographic\nvariations can break up to one third of metamorphic relations (MRs). A detailed\nanalysis of these failures reveals a consistent bias hierarchy, with\nperturbations involving racial cues being the predominant cause of the\nviolations. In addition to offering a comparative evaluation, this work\nreinforces that the retrieval component in RAG must be carefully curated to\nprevent bias amplification. The findings serve as a practical alert for\ndevelopers, testers and small organizations aiming to adopt accessible SLMs\nwithout compromising fairness or reliability."
                },
                "authors": [
                    {
                        "name": "Matheus Vinicius da Silva de Oliveira"
                    },
                    {
                        "name": "Jonathan de Andrade Silva"
                    },
                    {
                        "name": "Awdren de Lima Fontao"
                    }
                ],
                "author_detail": {
                    "name": "Awdren de Lima Fontao"
                },
                "author": "Awdren de Lima Fontao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26578v1",
                "updated": "2025-09-30T17:38:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    38,
                    45,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:38:45Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    38,
                    45,
                    1,
                    273,
                    0
                ],
                "title": "Linking Process to Outcome: Conditional Reward Modeling for LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linking Process to Outcome: Conditional Reward Modeling for LLM\n  Reasoning"
                },
                "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning capabilities of large language models (LLMs) by guiding their\nstep-by-step reasoning toward a final answer. However, existing PRMs either\ntreat each reasoning step in isolation, failing to capture inter-step\ndependencies, or struggle to align process rewards with the final outcome.\nConsequently, the reward signal fails to respect temporal causality in\nsequential reasoning and faces ambiguous credit assignment. These limitations\nmake downstream models vulnerable to reward hacking and lead to suboptimal\nperformance. In this work, we propose Conditional Reward Modeling (CRM) that\nframes LLM reasoning as a temporal process leading to a correct answer. The\nreward of each reasoning step is not only conditioned on the preceding steps\nbut also explicitly linked to the final outcome of the reasoning trajectory. By\nenforcing conditional probability rules, our design captures the causal\nrelationships among reasoning steps, with the link to the outcome allowing\nprecise attribution of each intermediate step, thereby resolving credit\nassignment ambiguity. Further, through this consistent probabilistic modeling,\nthe rewards produced by CRM enable more reliable cross-sample comparison.\nExperiments across Best-of-N sampling, beam search and reinforcement learning\ndemonstrate that CRM consistently outperforms existing reward models, offering\na principled framework for enhancing LLM reasoning. In particular, CRM is more\nrobust to reward hacking and delivers stable downstream improvements without\nrelying on verifiable rewards derived from ground truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning capabilities of large language models (LLMs) by guiding their\nstep-by-step reasoning toward a final answer. However, existing PRMs either\ntreat each reasoning step in isolation, failing to capture inter-step\ndependencies, or struggle to align process rewards with the final outcome.\nConsequently, the reward signal fails to respect temporal causality in\nsequential reasoning and faces ambiguous credit assignment. These limitations\nmake downstream models vulnerable to reward hacking and lead to suboptimal\nperformance. In this work, we propose Conditional Reward Modeling (CRM) that\nframes LLM reasoning as a temporal process leading to a correct answer. The\nreward of each reasoning step is not only conditioned on the preceding steps\nbut also explicitly linked to the final outcome of the reasoning trajectory. By\nenforcing conditional probability rules, our design captures the causal\nrelationships among reasoning steps, with the link to the outcome allowing\nprecise attribution of each intermediate step, thereby resolving credit\nassignment ambiguity. Further, through this consistent probabilistic modeling,\nthe rewards produced by CRM enable more reliable cross-sample comparison.\nExperiments across Best-of-N sampling, beam search and reinforcement learning\ndemonstrate that CRM consistently outperforms existing reward models, offering\na principled framework for enhancing LLM reasoning. In particular, CRM is more\nrobust to reward hacking and delivers stable downstream improvements without\nrelying on verifiable rewards derived from ground truth."
                },
                "authors": [
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Ziwei Shan"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Yexin Li"
                    },
                    {
                        "name": "Kan Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kan Ren"
                },
                "author": "Kan Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26577v1",
                "updated": "2025-09-30T17:38:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    38,
                    20,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:38:20Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    38,
                    20,
                    1,
                    273,
                    0
                ],
                "title": "Stochasticity and Practical Identifiability in Epidemic Models: A Monte\n  Carlo Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochasticity and Practical Identifiability in Epidemic Models: A Monte\n  Carlo Perspective"
                },
                "summary": "Assessing the practical identifiability of epidemic models is essential for\ndetermining whether parameters can be meaningfully estimated from observed\ndata. Monte Carlo (MC) methods provide an accessible and intuitive framework;\nhowever, their standard implementation - perturbing deterministic trajectories\nwith independent Gaussian noise - rests on assumptions poorly suited to\nepidemic processes, which are inherently stochastic, temporally correlated, and\nhighly variable, especially in small populations or under slow transmission. In\nthis study, we investigate the structure of stochastic variability in the\nclassic Susceptible-Infected-Recovered (SIR) model across a range of\nepidemiological regimes, and assess whether it can be represented within the\nindependent Gaussian noise framework. We show that continuous-time Markov chain\n(CTMC) trajectories consistently exhibit super-Poissonian variability and\nstrong temporal dependence. Through coverage analysis, we further demonstrate\nthat independent Gaussian noise systematically underestimates the variability\nof the underlying stochastic process, leading to overly optimistic conclusions\nabout parameter identifiability. In addition, we propose a hybrid simulation\napproach that introduces time- and amplitude-dependent variability into\ndeterministic ODE trajectories, preserving computational efficiency while\ncapturing key features of epidemic stochasticity. Our findings highlight the\nlimitations of the standard MC algorithm and provide a pathway for\nincorporating more realistic noise structures into epidemic inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the practical identifiability of epidemic models is essential for\ndetermining whether parameters can be meaningfully estimated from observed\ndata. Monte Carlo (MC) methods provide an accessible and intuitive framework;\nhowever, their standard implementation - perturbing deterministic trajectories\nwith independent Gaussian noise - rests on assumptions poorly suited to\nepidemic processes, which are inherently stochastic, temporally correlated, and\nhighly variable, especially in small populations or under slow transmission. In\nthis study, we investigate the structure of stochastic variability in the\nclassic Susceptible-Infected-Recovered (SIR) model across a range of\nepidemiological regimes, and assess whether it can be represented within the\nindependent Gaussian noise framework. We show that continuous-time Markov chain\n(CTMC) trajectories consistently exhibit super-Poissonian variability and\nstrong temporal dependence. Through coverage analysis, we further demonstrate\nthat independent Gaussian noise systematically underestimates the variability\nof the underlying stochastic process, leading to overly optimistic conclusions\nabout parameter identifiability. In addition, we propose a hybrid simulation\napproach that introduces time- and amplitude-dependent variability into\ndeterministic ODE trajectories, preserving computational efficiency while\ncapturing key features of epidemic stochasticity. Our findings highlight the\nlimitations of the standard MC algorithm and provide a pathway for\nincorporating more realistic noise structures into epidemic inference."
                },
                "authors": [
                    {
                        "name": "Chiara Mattamira"
                    },
                    {
                        "name": "Olivia Prosper Feldman"
                    }
                ],
                "author_detail": {
                    "name": "Olivia Prosper Feldman"
                },
                "author": "Olivia Prosper Feldman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26574v2",
                "updated": "2025-10-01T02:12:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    12,
                    55,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T17:34:03Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    34,
                    3,
                    1,
                    273,
                    0
                ],
                "title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics\n  Research Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics\n  Research Benchmark"
                },
                "summary": "While large language models (LLMs) with reasoning capabilities are\nprogressing rapidly on high-school math competitions and coding, can they\nreason effectively through complex, open-ended challenges found in frontier\nphysics research? And crucially, what kinds of reasoning tasks do physicists\nwant LLMs to assist with? To address these questions, we present the CritPt\n(Complex Research using Integrated Thinking - Physics Test, pronounced\n\"critical point\"), the first benchmark designed to test LLMs on unpublished,\nresearch-level reasoning tasks that broadly covers modern physics research\nareas, including condensed matter, quantum physics, atomic, molecular & optical\nphysics, astrophysics, high energy physics, mathematical physics, statistical\nphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.\nCritPt consists of 71 composite research challenges designed to simulate\nfull-scale research projects at the entry level, which are also decomposed to\n190 simpler checkpoint tasks for more fine-grained insights. All problems are\nnewly created by 50+ active physics researchers based on their own research.\nEvery problem is hand-curated to admit a guess-resistant and machine-verifiable\nanswer and is evaluated by an automated grading pipeline heavily customized for\nadvanced physics-specific output formats. We find that while current\nstate-of-the-art LLMs show early promise on isolated checkpoints, they remain\nfar from being able to reliably solve full research-scale challenges: the best\naverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),\nmoderately rising to around 10% when equipped with coding tools. Through the\nrealistic yet standardized evaluation offered by CritPt, we highlight a large\ndisconnect between current model capabilities and realistic physics research\ndemands, offering a foundation to guide the development of scientifically\ngrounded AI tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) with reasoning capabilities are\nprogressing rapidly on high-school math competitions and coding, can they\nreason effectively through complex, open-ended challenges found in frontier\nphysics research? And crucially, what kinds of reasoning tasks do physicists\nwant LLMs to assist with? To address these questions, we present the CritPt\n(Complex Research using Integrated Thinking - Physics Test, pronounced\n\"critical point\"), the first benchmark designed to test LLMs on unpublished,\nresearch-level reasoning tasks that broadly covers modern physics research\nareas, including condensed matter, quantum physics, atomic, molecular & optical\nphysics, astrophysics, high energy physics, mathematical physics, statistical\nphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.\nCritPt consists of 71 composite research challenges designed to simulate\nfull-scale research projects at the entry level, which are also decomposed to\n190 simpler checkpoint tasks for more fine-grained insights. All problems are\nnewly created by 50+ active physics researchers based on their own research.\nEvery problem is hand-curated to admit a guess-resistant and machine-verifiable\nanswer and is evaluated by an automated grading pipeline heavily customized for\nadvanced physics-specific output formats. We find that while current\nstate-of-the-art LLMs show early promise on isolated checkpoints, they remain\nfar from being able to reliably solve full research-scale challenges: the best\naverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),\nmoderately rising to around 10% when equipped with coding tools. Through the\nrealistic yet standardized evaluation offered by CritPt, we highlight a large\ndisconnect between current model capabilities and realistic physics research\ndemands, offering a foundation to guide the development of scientifically\ngrounded AI tools."
                },
                "authors": [
                    {
                        "name": "Minhui Zhu"
                    },
                    {
                        "name": "Minyang Tian"
                    },
                    {
                        "name": "Xiaocheng Yang"
                    },
                    {
                        "name": "Tianci Zhou"
                    },
                    {
                        "name": "Penghao Zhu"
                    },
                    {
                        "name": "Eli Chertkov"
                    },
                    {
                        "name": "Shengyan Liu"
                    },
                    {
                        "name": "Yufeng Du"
                    },
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Ziming Ji"
                    },
                    {
                        "name": "Indranil Das"
                    },
                    {
                        "name": "Junyi Cao"
                    },
                    {
                        "name": "Yufeng Du"
                    },
                    {
                        "name": "Jinchen He"
                    },
                    {
                        "name": "Yifan Su"
                    },
                    {
                        "name": "Jiabin Yu"
                    },
                    {
                        "name": "Yikun Jiang"
                    },
                    {
                        "name": "Yujie Zhang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ze-Min Huang"
                    },
                    {
                        "name": "Weizhen Jia"
                    },
                    {
                        "name": "Xinan Chen"
                    },
                    {
                        "name": "Peixue Wu"
                    },
                    {
                        "name": "Yunkai Wang"
                    },
                    {
                        "name": "Juntai Zhou"
                    },
                    {
                        "name": "Yong Zhao"
                    },
                    {
                        "name": "Farshid Jafarpour"
                    },
                    {
                        "name": "Jessie Shelton"
                    },
                    {
                        "name": "Aaron Young"
                    },
                    {
                        "name": "John Bartolotta"
                    },
                    {
                        "name": "Wenchao Xu"
                    },
                    {
                        "name": "Yue Sun"
                    },
                    {
                        "name": "Anjun Chu"
                    },
                    {
                        "name": "Victor Colussi"
                    },
                    {
                        "name": "Chris Akers"
                    },
                    {
                        "name": "Nathan Brooks"
                    },
                    {
                        "name": "Wenbo Fu"
                    },
                    {
                        "name": "Christopher Wilson"
                    },
                    {
                        "name": "Jinchao Zhao"
                    },
                    {
                        "name": "Marvin Qi"
                    },
                    {
                        "name": "Anqi Mu"
                    },
                    {
                        "name": "Yubo Yang"
                    },
                    {
                        "name": "Allen Zang"
                    },
                    {
                        "name": "Yang Lyu"
                    },
                    {
                        "name": "Peizhi Mai"
                    },
                    {
                        "name": "Xuefei Guo"
                    },
                    {
                        "name": "Luyu Gao"
                    },
                    {
                        "name": "Ze Yang"
                    },
                    {
                        "name": "Chi Xue"
                    },
                    {
                        "name": "Dmytro Bandak"
                    },
                    {
                        "name": "Yaïr Hein"
                    },
                    {
                        "name": "Yonatan Kahn"
                    },
                    {
                        "name": "Kevin Zhou"
                    },
                    {
                        "name": "John Drew Wilson"
                    },
                    {
                        "name": "Jarrod T. Reilly"
                    },
                    {
                        "name": "Di Luo"
                    },
                    {
                        "name": "Daniel Inafuku"
                    },
                    {
                        "name": "Hao Tong"
                    },
                    {
                        "name": "Liang Yang"
                    },
                    {
                        "name": "Ruixing Zhang"
                    },
                    {
                        "name": "Xueying Wang"
                    },
                    {
                        "name": "Ofir Press"
                    },
                    {
                        "name": "Nicolas Chia"
                    },
                    {
                        "name": "Eliu Huerta"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "arxiv_comment": "39 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26573v1",
                "updated": "2025-09-30T17:33:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    33,
                    58,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:33:58Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    33,
                    58,
                    1,
                    273,
                    0
                ],
                "title": "Statistical Inference Framework for Extended Target Detection in mmWave\n  Automotive Radar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference Framework for Extended Target Detection in mmWave\n  Automotive Radar"
                },
                "summary": "Millimeter wave (mmWave) radar systems, owing to their large bandwidth,\nprovide fine range resolution that enables the observation of multiple\nscatterers originating from a single automotive target commonly referred to as\nan extended target. Conventional CFAR-based detection algorithms typically\ntreat these scatterers as independent detections, thereby discarding the\nspatial scattering structure intrinsic to the target. To preserve this\nscattering spread, this paper proposes a Range-Doppler (RD) segment framework\ndesigned to encapsulate the typical scattering profile of an automobile. The\nstatistical characterization of the segment is performed using Maximum\nLikelihood Estimation (MLE) and posterior density modeling facilitated through\nGibbs Markov Chain Monte Carlo (MCMC) sampling. A skewness-based test\nstatistic, derived from the estimated statistical model, is introduced for\nbinary hypothesis classification of extended targets. Additionally, the paper\npresents a detection pipeline that incorporates Intersection over Union (IoU)\nand segment centering based on peak response, optimized to work within a single\ndwell. Extensive evaluations using both simulated and real-world datasets\ndemonstrate the effectiveness of the proposed approach, underscoring its\nsuitability for automotive radar applications through improved detection\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millimeter wave (mmWave) radar systems, owing to their large bandwidth,\nprovide fine range resolution that enables the observation of multiple\nscatterers originating from a single automotive target commonly referred to as\nan extended target. Conventional CFAR-based detection algorithms typically\ntreat these scatterers as independent detections, thereby discarding the\nspatial scattering structure intrinsic to the target. To preserve this\nscattering spread, this paper proposes a Range-Doppler (RD) segment framework\ndesigned to encapsulate the typical scattering profile of an automobile. The\nstatistical characterization of the segment is performed using Maximum\nLikelihood Estimation (MLE) and posterior density modeling facilitated through\nGibbs Markov Chain Monte Carlo (MCMC) sampling. A skewness-based test\nstatistic, derived from the estimated statistical model, is introduced for\nbinary hypothesis classification of extended targets. Additionally, the paper\npresents a detection pipeline that incorporates Intersection over Union (IoU)\nand segment centering based on peak response, optimized to work within a single\ndwell. Extensive evaluations using both simulated and real-world datasets\ndemonstrate the effectiveness of the proposed approach, underscoring its\nsuitability for automotive radar applications through improved detection\naccuracy."
                },
                "authors": [
                    {
                        "name": "Vinay Kulkarni"
                    },
                    {
                        "name": "V. V. Reddy"
                    },
                    {
                        "name": "Neha Maheshwari"
                    }
                ],
                "author_detail": {
                    "name": "Neha Maheshwari"
                },
                "author": "Neha Maheshwari",
                "arxiv_comment": "12 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26562v1",
                "updated": "2025-09-30T17:29:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    29,
                    2,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:29:02Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    29,
                    2,
                    1,
                    273,
                    0
                ],
                "title": "DeepProv: Behavioral Characterization and Repair of Neural Networks via\n  Inference Provenance Graph Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepProv: Behavioral Characterization and Repair of Neural Networks via\n  Inference Provenance Graph Analysis"
                },
                "summary": "Deep neural networks (DNNs) are increasingly being deployed in high-stakes\napplications, from self-driving cars to biometric authentication. However,\ntheir unpredictable and unreliable behaviors in real-world settings require new\napproaches to characterize and ensure their reliability.\n  This paper introduces DeepProv, a novel and customizable system designed to\ncapture and characterize the runtime behavior of DNNs during inference by using\ntheir underlying graph structure. Inspired by system audit provenance graphs,\nDeepProv models the computational information flow of a DNN's inference process\nthrough Inference Provenance Graphs (IPGs). These graphs provide a detailed\nstructural representation of the behavior of DNN, allowing both empirical and\nstructural analysis. DeepProv uses these insights to systematically repair DNNs\nfor specific objectives, such as improving robustness, privacy, or fairness.\n  We instantiate DeepProv with adversarial robustness as the goal of model\nrepair and conduct extensive case studies to evaluate its effectiveness. Our\nresults demonstrate its effectiveness and scalability across diverse\nclassification tasks, attack scenarios, and model complexities. DeepProv\nautomatically identifies repair actions at the node and edge-level within IPGs,\nsignificantly enhancing the robustness of the model. In particular, applying\nDeepProv repair strategies to just a single layer of a DNN yields an average\n55% improvement in adversarial accuracy. Moreover, DeepProv complements\nexisting defenses, achieving substantial gains in adversarial robustness.\nBeyond robustness, we demonstrate the broader potential of DeepProv as an\nadaptable system to characterize DNN behavior in other critical areas, such as\nprivacy auditing and fairness analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) are increasingly being deployed in high-stakes\napplications, from self-driving cars to biometric authentication. However,\ntheir unpredictable and unreliable behaviors in real-world settings require new\napproaches to characterize and ensure their reliability.\n  This paper introduces DeepProv, a novel and customizable system designed to\ncapture and characterize the runtime behavior of DNNs during inference by using\ntheir underlying graph structure. Inspired by system audit provenance graphs,\nDeepProv models the computational information flow of a DNN's inference process\nthrough Inference Provenance Graphs (IPGs). These graphs provide a detailed\nstructural representation of the behavior of DNN, allowing both empirical and\nstructural analysis. DeepProv uses these insights to systematically repair DNNs\nfor specific objectives, such as improving robustness, privacy, or fairness.\n  We instantiate DeepProv with adversarial robustness as the goal of model\nrepair and conduct extensive case studies to evaluate its effectiveness. Our\nresults demonstrate its effectiveness and scalability across diverse\nclassification tasks, attack scenarios, and model complexities. DeepProv\nautomatically identifies repair actions at the node and edge-level within IPGs,\nsignificantly enhancing the robustness of the model. In particular, applying\nDeepProv repair strategies to just a single layer of a DNN yields an average\n55% improvement in adversarial accuracy. Moreover, DeepProv complements\nexisting defenses, achieving substantial gains in adversarial robustness.\nBeyond robustness, we demonstrate the broader potential of DeepProv as an\nadaptable system to characterize DNN behavior in other critical areas, such as\nprivacy auditing and fairness analysis."
                },
                "authors": [
                    {
                        "name": "Firas Ben Hmida"
                    },
                    {
                        "name": "Abderrahmen Amich"
                    },
                    {
                        "name": "Ata Kaboudi"
                    },
                    {
                        "name": "Birhanu Eshete"
                    }
                ],
                "author_detail": {
                    "name": "Birhanu Eshete"
                },
                "author": "Birhanu Eshete",
                "arxiv_comment": "18 pages, 9 figures, 6 tables, To appear in the 41st Annual Computer\n  Security Applications Conference (ACSAC), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03015v2",
                "updated": "2025-09-30T17:27:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    27,
                    42,
                    1,
                    273,
                    0
                ],
                "published": "2025-04-03T20:20:00Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    20,
                    20,
                    0,
                    3,
                    93,
                    0
                ],
                "title": "AuDeRe: Automated Strategy Decision and Realization in Robot Planning\n  and Control via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AuDeRe: Automated Strategy Decision and Realization in Robot Planning\n  and Control via LLMs"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown significant\npromise in various domains, especially robotics. However, most prior LLM-based\nwork in robotic applications either directly predicts waypoints or applies LLMs\nwithin fixed tool integration frameworks, offering limited flexibility in\nexploring and configuring solutions best suited to different tasks. In this\nwork, we propose a framework that leverages LLMs to select appropriate planning\nand control strategies based on task descriptions, environmental constraints,\nand system dynamics. These strategies are then executed by calling the\navailable comprehensive planning and control APIs. Our approach employs\niterative LLM-based reasoning with performance feedback to refine the algorithm\nselection. We validate our approach through extensive experiments across tasks\nof varying complexity, from simple tracking to complex planning scenarios\ninvolving spatiotemporal constraints. The results demonstrate that using LLMs\nto determine planning and control strategies from natural language descriptions\nsignificantly enhances robotic autonomy while reducing the need for extensive\nmanual tuning and expert knowledge. Furthermore, our framework maintains\ngeneralizability across different tasks and notably outperforms baseline\nmethods that rely on LLMs for direct trajectory, control sequence, or code\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown significant\npromise in various domains, especially robotics. However, most prior LLM-based\nwork in robotic applications either directly predicts waypoints or applies LLMs\nwithin fixed tool integration frameworks, offering limited flexibility in\nexploring and configuring solutions best suited to different tasks. In this\nwork, we propose a framework that leverages LLMs to select appropriate planning\nand control strategies based on task descriptions, environmental constraints,\nand system dynamics. These strategies are then executed by calling the\navailable comprehensive planning and control APIs. Our approach employs\niterative LLM-based reasoning with performance feedback to refine the algorithm\nselection. We validate our approach through extensive experiments across tasks\nof varying complexity, from simple tracking to complex planning scenarios\ninvolving spatiotemporal constraints. The results demonstrate that using LLMs\nto determine planning and control strategies from natural language descriptions\nsignificantly enhances robotic autonomy while reducing the need for extensive\nmanual tuning and expert knowledge. Furthermore, our framework maintains\ngeneralizability across different tasks and notably outperforms baseline\nmethods that rely on LLMs for direct trajectory, control sequence, or code\ngeneration."
                },
                "authors": [
                    {
                        "name": "Yue Meng"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Yongchao Chen"
                    },
                    {
                        "name": "Chuchu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Chuchu Fan"
                },
                "author": "Chuchu Fan",
                "arxiv_comment": "8 pages, 14 figures, submitted to the 2026 American Control\n  Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26557v1",
                "updated": "2025-09-30T17:23:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    23,
                    43,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:23:43Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    23,
                    43,
                    1,
                    273,
                    0
                ],
                "title": "The Invisible Mentor: Inferring User Actions from Screen Recordings to\n  Recommend Better Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Invisible Mentor: Inferring User Actions from Screen Recordings to\n  Recommend Better Workflows"
                },
                "summary": "Many users struggle to notice when a more efficient workflow exists in\nfeature-rich tools like Excel. Existing AI assistants offer help only after\nusers describe their goals or problems, which can be effortful and imprecise.\nWe present InvisibleMentor, a system that turns screen recordings of task\ncompletion into vision-grounded reflections on tasks. It detects issues such as\nrepetitive edits and recommends more efficient alternatives based on observed\nbehavior. Unlike prior systems that rely on logs, APIs, or user prompts,\nInvisibleMentor operates directly on screen recordings. It uses a two-stage\npipeline: a vision-language model reconstructs actions and context, and a\nlanguage model generates structured, high-fidelity suggestions. In evaluation,\nInvisibleMentor accurately identified inefficient workflows, and participants\nfound its suggestions more actionable, tailored, and more helpful for learning\nand improvement compared to a prompt-based spreadsheet assistant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many users struggle to notice when a more efficient workflow exists in\nfeature-rich tools like Excel. Existing AI assistants offer help only after\nusers describe their goals or problems, which can be effortful and imprecise.\nWe present InvisibleMentor, a system that turns screen recordings of task\ncompletion into vision-grounded reflections on tasks. It detects issues such as\nrepetitive edits and recommends more efficient alternatives based on observed\nbehavior. Unlike prior systems that rely on logs, APIs, or user prompts,\nInvisibleMentor operates directly on screen recordings. It uses a two-stage\npipeline: a vision-language model reconstructs actions and context, and a\nlanguage model generates structured, high-fidelity suggestions. In evaluation,\nInvisibleMentor accurately identified inefficient workflows, and participants\nfound its suggestions more actionable, tailored, and more helpful for learning\nand improvement compared to a prompt-based spreadsheet assistant."
                },
                "authors": [
                    {
                        "name": "Litao Yan"
                    },
                    {
                        "name": "Andrew Head"
                    },
                    {
                        "name": "Ken Milne"
                    },
                    {
                        "name": "Vu Le"
                    },
                    {
                        "name": "Sumit Gulwani"
                    },
                    {
                        "name": "Chris Parnin"
                    },
                    {
                        "name": "Emerson Murphy-Hill"
                    }
                ],
                "author_detail": {
                    "name": "Emerson Murphy-Hill"
                },
                "author": "Emerson Murphy-Hill",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26556v1",
                "updated": "2025-09-30T17:22:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    22,
                    56,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:22:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    22,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "CHEMOUT: CHEMical complexity in star-forming regions of the OUTer\n  Galaxy. V. Chemical composition gradients as a function of the Galactocentric\n  radius",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHEMOUT: CHEMical complexity in star-forming regions of the OUTer\n  Galaxy. V. Chemical composition gradients as a function of the Galactocentric\n  radius"
                },
                "summary": "The outer Galaxy is characterized by a lower metallicity than regions near\nthe Sun, suggesting differences in the formation and survival of molecules in\nstar-forming regions. To understand chemical evolution across the Milky Way,\nderiving molecular abundances in star-forming regions in the outer Galaxy is\nessential for refining models of sub-Solar metallicity environments. We\nanalyzed IRAM 30m observations at 3 and 2 mm toward 35 sources at\nGalactocentric distances of 9$-$24 kpc, within the \"CHEMical complexity in\nstar-forming regions of the outer Galaxy\" (CHEMOUT) project. We focused on\nspecies with the highest detection rates (i.e., HCN, HCO$^+$, c-C$_3$H$_2$,\nH$^{13}$CO$^+$, HCO, SO) and searched for trends in column densities,\nabundances, and line widths with Galactocentric distance. Abundances for\nH$_2$CO and CH$_3$OH were updated using H$_2$ column densities from new NIKA2\ndust maps. Fractional abundances relative to H$_2$ of most species (HCN,\nHCO$^+$, c-C$_3$H$_2$, HCO, H$_2$CO, CH$_3$OH) scale at most with the elemental\ncarbon abundance ([C/H]) up to $\\sim$24 kpc. SO shows a steeper gradient than\nsulfur abundance ([S/H]), while H$^{13}$CO$^+$ shows a shallower gradient than\n[$^{13}$C/H]. Gas turbulence, inferred from line widths, decreases with\nGalactocentric distance, suggesting a more quiescent environment in the outer\nGalaxy with respect to the inner Galaxy. In the outer Galaxy, the formation\nefficiency of most molecules, following the parent element availability, is\ncomparable or higher (e.g., for H$^{13}$CO$^+$) than in the local Galaxy,\nwhereas SO forms less efficiently. These results have significant implications\nfor chemical models of the outermost star-forming regions and for understanding\nmolecule formation under lower metallicity conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The outer Galaxy is characterized by a lower metallicity than regions near\nthe Sun, suggesting differences in the formation and survival of molecules in\nstar-forming regions. To understand chemical evolution across the Milky Way,\nderiving molecular abundances in star-forming regions in the outer Galaxy is\nessential for refining models of sub-Solar metallicity environments. We\nanalyzed IRAM 30m observations at 3 and 2 mm toward 35 sources at\nGalactocentric distances of 9$-$24 kpc, within the \"CHEMical complexity in\nstar-forming regions of the outer Galaxy\" (CHEMOUT) project. We focused on\nspecies with the highest detection rates (i.e., HCN, HCO$^+$, c-C$_3$H$_2$,\nH$^{13}$CO$^+$, HCO, SO) and searched for trends in column densities,\nabundances, and line widths with Galactocentric distance. Abundances for\nH$_2$CO and CH$_3$OH were updated using H$_2$ column densities from new NIKA2\ndust maps. Fractional abundances relative to H$_2$ of most species (HCN,\nHCO$^+$, c-C$_3$H$_2$, HCO, H$_2$CO, CH$_3$OH) scale at most with the elemental\ncarbon abundance ([C/H]) up to $\\sim$24 kpc. SO shows a steeper gradient than\nsulfur abundance ([S/H]), while H$^{13}$CO$^+$ shows a shallower gradient than\n[$^{13}$C/H]. Gas turbulence, inferred from line widths, decreases with\nGalactocentric distance, suggesting a more quiescent environment in the outer\nGalaxy with respect to the inner Galaxy. In the outer Galaxy, the formation\nefficiency of most molecules, following the parent element availability, is\ncomparable or higher (e.g., for H$^{13}$CO$^+$) than in the local Galaxy,\nwhereas SO forms less efficiently. These results have significant implications\nfor chemical models of the outermost star-forming regions and for understanding\nmolecule formation under lower metallicity conditions."
                },
                "authors": [
                    {
                        "name": "D. Gigli"
                    },
                    {
                        "name": "F. Fontani"
                    },
                    {
                        "name": "L. Colzi"
                    },
                    {
                        "name": "G. Vermariën"
                    },
                    {
                        "name": "S. Viti"
                    },
                    {
                        "name": "V. M. Rivilla"
                    },
                    {
                        "name": "A. Sánchez-Monge"
                    }
                ],
                "author_detail": {
                    "name": "A. Sánchez-Monge"
                },
                "author": "A. Sánchez-Monge",
                "arxiv_comment": "23 pages, 14 figures; accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01055v2",
                "updated": "2025-09-30T17:22:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    22,
                    37,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-01T01:45:18Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    1,
                    45,
                    18,
                    0,
                    244,
                    0
                ],
                "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated\nsuccess in enhancing LLM reasoning capabilities, but remains limited to\nsingle-turn interactions without tool integration. While recent Agentic\nReinforcement Learning with Tool use (ARLT) approaches have emerged to address\nmulti-turn tool interactions, existing works develop task-specific codebases\nthat suffer from fragmentation, synchronous execution bottlenecks, and limited\nextensibility across domains. These inefficiencies hinder broader community\nadoption and algorithmic innovation. We introduce VerlTool, a unified and\nmodular framework that addresses these limitations through systematic design\nprinciples. VerlTool provides four key contributions: (1) upstream alignment\nwith VeRL ensuring compatibility and simplified maintenance, (2) unified tool\nmanagement via standardized APIs supporting diverse modalities including code\nexecution, search, SQL databases, and vision processing, (3) asynchronous\nrollout execution achieving near 2$\\times$ speedup by eliminating\nsynchronization bottlenecks, and (4) comprehensive evaluation demonstrating\ncompetitive performance across 6 ARLT domains. Our framework formalizes ARLT as\nmulti-turn trajectories with multi-modal observation tokens (text/image/video),\nextending beyond single-turn RLVR paradigms. We train and evaluate models on\nmathematical reasoning, knowledge QA, SQL generation, visual reasoning, web\nsearch, and software engineering tasks, achieving results comparable to\nspecialized systems while providing unified training infrastructure. The\nmodular plugin architecture enables rapid tool integration requiring only\nlightweight Python definitions, significantly reducing development overhead and\nproviding a scalable foundation for tool-augmented RL research. Our code is\nopen-sourced at https://github.com/TIGER-AI-Lab/verl-tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated\nsuccess in enhancing LLM reasoning capabilities, but remains limited to\nsingle-turn interactions without tool integration. While recent Agentic\nReinforcement Learning with Tool use (ARLT) approaches have emerged to address\nmulti-turn tool interactions, existing works develop task-specific codebases\nthat suffer from fragmentation, synchronous execution bottlenecks, and limited\nextensibility across domains. These inefficiencies hinder broader community\nadoption and algorithmic innovation. We introduce VerlTool, a unified and\nmodular framework that addresses these limitations through systematic design\nprinciples. VerlTool provides four key contributions: (1) upstream alignment\nwith VeRL ensuring compatibility and simplified maintenance, (2) unified tool\nmanagement via standardized APIs supporting diverse modalities including code\nexecution, search, SQL databases, and vision processing, (3) asynchronous\nrollout execution achieving near 2$\\times$ speedup by eliminating\nsynchronization bottlenecks, and (4) comprehensive evaluation demonstrating\ncompetitive performance across 6 ARLT domains. Our framework formalizes ARLT as\nmulti-turn trajectories with multi-modal observation tokens (text/image/video),\nextending beyond single-turn RLVR paradigms. We train and evaluate models on\nmathematical reasoning, knowledge QA, SQL generation, visual reasoning, web\nsearch, and software engineering tasks, achieving results comparable to\nspecialized systems while providing unified training infrastructure. The\nmodular plugin architecture enables rapid tool integration requiring only\nlightweight Python definitions, significantly reducing development overhead and\nproviding a scalable foundation for tool-augmented RL research. Our code is\nopen-sourced at https://github.com/TIGER-AI-Lab/verl-tool."
                },
                "authors": [
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Yi Lu"
                    },
                    {
                        "name": "Zhuofeng Li"
                    },
                    {
                        "name": "Zhiheng Lyu"
                    },
                    {
                        "name": "Ping Nie"
                    },
                    {
                        "name": "Haozhe Wang"
                    },
                    {
                        "name": "Alex Su"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Kai Zou"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "32 pages, 5 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26554v1",
                "updated": "2025-09-30T17:22:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    22,
                    5,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:22:05Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    22,
                    5,
                    1,
                    273,
                    0
                ],
                "title": "Computationally and statistically efficient estimation of time-smoothed\n  counterfactual curves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computationally and statistically efficient estimation of time-smoothed\n  counterfactual curves"
                },
                "summary": "Longitudinal causal inference is concerned with defining, identifying, and\nestimating the effect of a time-varying intervention on a time-varying outcome\nthat is indexed by a follow-up time. In an observational study, Robins's\ngeneralized g-formula can identify causal effects induced by a broad class of\ntime-varying interventions. Various methods for estimating the generalized\ng-formula have been posed for different outcome types, such as a failure event\nindicator by a specified time (e.g. mortality by 5 year follow-up), as well as\ncontinuous or dichotomous/multi-valued outcomes measures at a specified time\n(e.g. blood pressure in mm/hg or an indicator of high blood pressure at 5-year\nfollow-up). Multiply-robust, data-adaptive estimators leverage flexible\nnonparametric estimation algorithms while allowing for statistical inference.\nHowever, extant methods do not accommodate time-smoothing when multiple\noutcomes are measured over time, which can lead to substantial loss of\nprecision. We propose a novel multiply-robust estimator of the generalized\ng-formula that accommodates time-smoothing over numerous available outcome\nmeasures. Our approach accommodates any intervention that can be described as a\nLongitudinal Modified Treatment Policy, a flexible class suitable for binary,\nmulti-valued, and continuous longitudinal treatments. Our method produces an\nestimate of the effect curve: the causal effect of the intervention on the\noutcome at each measurement time, taking into account censoring and\nnon-monotonic outcome missingness patterns. In simulations we find that the\nproposed algorithm outperforms extant multiply-robust approaches for effect\ncurve estimation in scenarios with high degrees of outcome missingness and when\nthere is strong confounding. We apply the method to study longitudinal effects\nof union membership on wages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Longitudinal causal inference is concerned with defining, identifying, and\nestimating the effect of a time-varying intervention on a time-varying outcome\nthat is indexed by a follow-up time. In an observational study, Robins's\ngeneralized g-formula can identify causal effects induced by a broad class of\ntime-varying interventions. Various methods for estimating the generalized\ng-formula have been posed for different outcome types, such as a failure event\nindicator by a specified time (e.g. mortality by 5 year follow-up), as well as\ncontinuous or dichotomous/multi-valued outcomes measures at a specified time\n(e.g. blood pressure in mm/hg or an indicator of high blood pressure at 5-year\nfollow-up). Multiply-robust, data-adaptive estimators leverage flexible\nnonparametric estimation algorithms while allowing for statistical inference.\nHowever, extant methods do not accommodate time-smoothing when multiple\noutcomes are measured over time, which can lead to substantial loss of\nprecision. We propose a novel multiply-robust estimator of the generalized\ng-formula that accommodates time-smoothing over numerous available outcome\nmeasures. Our approach accommodates any intervention that can be described as a\nLongitudinal Modified Treatment Policy, a flexible class suitable for binary,\nmulti-valued, and continuous longitudinal treatments. Our method produces an\nestimate of the effect curve: the causal effect of the intervention on the\noutcome at each measurement time, taking into account censoring and\nnon-monotonic outcome missingness patterns. In simulations we find that the\nproposed algorithm outperforms extant multiply-robust approaches for effect\ncurve estimation in scenarios with high degrees of outcome missingness and when\nthere is strong confounding. We apply the method to study longitudinal effects\nof union membership on wages."
                },
                "authors": [
                    {
                        "name": "Herbert P. Susmann"
                    },
                    {
                        "name": "Nicholas T. Williams"
                    },
                    {
                        "name": "Richard Liu"
                    },
                    {
                        "name": "Jessica G. Young"
                    },
                    {
                        "name": "Iván Díaz"
                    }
                ],
                "author_detail": {
                    "name": "Iván Díaz"
                },
                "author": "Iván Díaz",
                "arxiv_comment": "29 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20039v2",
                "updated": "2025-09-30T17:21:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    21,
                    23,
                    1,
                    273,
                    0
                ],
                "published": "2025-04-28T17:59:28Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    59,
                    28,
                    0,
                    118,
                    0
                ],
                "title": "AutoJudge: Judge Decoding Without Manual Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoJudge: Judge Decoding Without Manual Annotation"
                },
                "summary": "We introduce AutoJudge, a method that accelerates large language model (LLM)\ninference with task-specific lossy speculative decoding. Instead of matching\nthe original model output distribution token-by-token, we identify which of the\ngenerated tokens affect the downstream quality of the response, relaxing the\ndistribution match guarantee so that the \"unimportant\" tokens can be generated\nfaster. Our approach relies on a semi-greedy search algorithm to test which of\nthe mismatches between target and draft models should be corrected to preserve\nquality and which ones may be skipped. We then train a lightweight classifier\nbased on existing LLM embeddings to predict, at inference time, which\nmismatching tokens can be safely accepted without compromising the final answer\nquality. We evaluate the effectiveness of AutoJudge with multiple draft/target\nmodel pairs on mathematical reasoning and programming benchmarks, achieving\nsignificant speedups at the cost of a minor accuracy reduction. Notably, on\nGSM8k with the Llama 3.1 70B target model, our approach achieves up to\n$\\approx2\\times$ speedup over speculative decoding at the cost of $\\le 1\\%$\ndrop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge\nautomatically detects programming-specific important tokens, accepting $\\ge 25$\ntokens per speculation cycle at $2\\%$ drop in Pass@1. Our approach requires no\nhuman annotation and is easy to integrate with modern LLM inference frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AutoJudge, a method that accelerates large language model (LLM)\ninference with task-specific lossy speculative decoding. Instead of matching\nthe original model output distribution token-by-token, we identify which of the\ngenerated tokens affect the downstream quality of the response, relaxing the\ndistribution match guarantee so that the \"unimportant\" tokens can be generated\nfaster. Our approach relies on a semi-greedy search algorithm to test which of\nthe mismatches between target and draft models should be corrected to preserve\nquality and which ones may be skipped. We then train a lightweight classifier\nbased on existing LLM embeddings to predict, at inference time, which\nmismatching tokens can be safely accepted without compromising the final answer\nquality. We evaluate the effectiveness of AutoJudge with multiple draft/target\nmodel pairs on mathematical reasoning and programming benchmarks, achieving\nsignificant speedups at the cost of a minor accuracy reduction. Notably, on\nGSM8k with the Llama 3.1 70B target model, our approach achieves up to\n$\\approx2\\times$ speedup over speculative decoding at the cost of $\\le 1\\%$\ndrop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge\nautomatically detects programming-specific important tokens, accepting $\\ge 25$\ntokens per speculation cycle at $2\\%$ drop in Pass@1. Our approach requires no\nhuman annotation and is easy to integrate with modern LLM inference frameworks."
                },
                "authors": [
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Fedor Velikonivtsev"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26553v1",
                "updated": "2025-09-30T17:21:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    21,
                    17,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:21:17Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    21,
                    17,
                    1,
                    273,
                    0
                ],
                "title": "Towards Reliable Benchmarking: A Contamination Free, Controllable\n  Evaluation Framework for Multi-step LLM Function Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Benchmarking: A Contamination Free, Controllable\n  Evaluation Framework for Multi-step LLM Function Calling"
                },
                "summary": "As language models gain access to external tools via structured function\ncalls, they become increasingly more capable of solving complex, multi-step\ntasks. However, existing benchmarks for tool-augmented language models (TaLMs)\nprovide insufficient control over factors such as the number of functions\naccessible, task complexity, and input size, and remain vulnerable to data\ncontamination. We present FuncBenchGen, a unified, contamination-free framework\nthat evaluates TaLMs by generating synthetic multi-step tool-use tasks. The key\nidea is to cast tool use as traversal over a hidden function-dependency DAG\nwhere nodes are function calls and an edge between nodes represents one\nfunction consuming the output of another. Given a set of external function\nschemas, initial variable values, and a target variable, models must compose\nthe correct call sequence to compute the target variable. FuncBenchGen allows\nusers to precisely control task difficulty (e.g., graph size, dependency depth,\nand distractor functions) while avoiding data leakage. We apply our\nFuncBenchGen framework to evaluate seven LLMs on tool use tasks of varying\ndifficulty. Reasoning-optimized models consistently outperform general-purpose\nmodels with GPT-5 significantly outperforming other models. Performance\ndeclines sharply as dependency depth increases. Furthermore, connected\nirrelevant functions prove especially difficult to handle. We find that strong\nmodels often make syntactically valid function calls but propagate incorrect or\nstale argument values across steps, revealing brittle state tracking by LLMs in\nmulti-turn tool use. Motivated by this observation, we introduce a simple\nmitigation strategy that explicitly restates prior variable values to the agent\nat each step. Surprisingly, this lightweight change yields substantial gains\nacross models. e.g., yielding a success rate improvement from 62.5% to 81.3%\nfor GPT-5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As language models gain access to external tools via structured function\ncalls, they become increasingly more capable of solving complex, multi-step\ntasks. However, existing benchmarks for tool-augmented language models (TaLMs)\nprovide insufficient control over factors such as the number of functions\naccessible, task complexity, and input size, and remain vulnerable to data\ncontamination. We present FuncBenchGen, a unified, contamination-free framework\nthat evaluates TaLMs by generating synthetic multi-step tool-use tasks. The key\nidea is to cast tool use as traversal over a hidden function-dependency DAG\nwhere nodes are function calls and an edge between nodes represents one\nfunction consuming the output of another. Given a set of external function\nschemas, initial variable values, and a target variable, models must compose\nthe correct call sequence to compute the target variable. FuncBenchGen allows\nusers to precisely control task difficulty (e.g., graph size, dependency depth,\nand distractor functions) while avoiding data leakage. We apply our\nFuncBenchGen framework to evaluate seven LLMs on tool use tasks of varying\ndifficulty. Reasoning-optimized models consistently outperform general-purpose\nmodels with GPT-5 significantly outperforming other models. Performance\ndeclines sharply as dependency depth increases. Furthermore, connected\nirrelevant functions prove especially difficult to handle. We find that strong\nmodels often make syntactically valid function calls but propagate incorrect or\nstale argument values across steps, revealing brittle state tracking by LLMs in\nmulti-turn tool use. Motivated by this observation, we introduce a simple\nmitigation strategy that explicitly restates prior variable values to the agent\nat each step. Surprisingly, this lightweight change yields substantial gains\nacross models. e.g., yielding a success rate improvement from 62.5% to 81.3%\nfor GPT-5."
                },
                "authors": [
                    {
                        "name": "Seiji Maekawa"
                    },
                    {
                        "name": "Jackson Hassell"
                    },
                    {
                        "name": "Pouya Pezeshkpour"
                    },
                    {
                        "name": "Tom Mitchell"
                    },
                    {
                        "name": "Estevam Hruschka"
                    }
                ],
                "author_detail": {
                    "name": "Estevam Hruschka"
                },
                "author": "Estevam Hruschka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26551v1",
                "updated": "2025-09-30T17:19:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    19,
                    58,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:19:58Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    19,
                    58,
                    1,
                    273,
                    0
                ],
                "title": "Pretrain-Test Task Alignment Governs Generalization in In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrain-Test Task Alignment Governs Generalization in In-Context\n  Learning"
                },
                "summary": "In-context learning (ICL) is a central capability of Transformer models, but\nthe structures in data that enable its emergence and govern its robustness\nremain poorly understood. In this work, we study how the structure of\npretraining tasks governs generalization in ICL. Using a solvable model for ICL\nof linear regression by linear attention, we derive an exact expression for ICL\ngeneralization error in high dimensions under arbitrary pretraining-testing\ntask covariance mismatch. This leads to a new alignment measure that quantifies\nhow much information about the pretraining task distribution is useful for\ninference at test time. We show that this measure directly predicts ICL\nperformance not only in the solvable model but also in nonlinear Transformers.\nOur analysis further reveals a tradeoff between specialization and\ngeneralization in ICL: depending on task distribution alignment, increasing\npretraining task diversity can either improve or harm test performance.\nTogether, these results identify train-test task alignment as a key determinant\nof generalization in ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is a central capability of Transformer models, but\nthe structures in data that enable its emergence and govern its robustness\nremain poorly understood. In this work, we study how the structure of\npretraining tasks governs generalization in ICL. Using a solvable model for ICL\nof linear regression by linear attention, we derive an exact expression for ICL\ngeneralization error in high dimensions under arbitrary pretraining-testing\ntask covariance mismatch. This leads to a new alignment measure that quantifies\nhow much information about the pretraining task distribution is useful for\ninference at test time. We show that this measure directly predicts ICL\nperformance not only in the solvable model but also in nonlinear Transformers.\nOur analysis further reveals a tradeoff between specialization and\ngeneralization in ICL: depending on task distribution alignment, increasing\npretraining task diversity can either improve or harm test performance.\nTogether, these results identify train-test task alignment as a key determinant\nof generalization in ICL."
                },
                "authors": [
                    {
                        "name": "Mary I. Letey"
                    },
                    {
                        "name": "Jacob A. Zavatone-Veth"
                    },
                    {
                        "name": "Yue M. Lu"
                    },
                    {
                        "name": "Cengiz Pehlevan"
                    }
                ],
                "author_detail": {
                    "name": "Cengiz Pehlevan"
                },
                "author": "Cengiz Pehlevan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26550v1",
                "updated": "2025-09-30T17:19:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    19,
                    23,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:19:23Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    19,
                    23,
                    1,
                    273,
                    0
                ],
                "title": "Can LLMs Write Mathematics Papers? A Case Study in Reservoir Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Write Mathematics Papers? A Case Study in Reservoir Computing"
                },
                "summary": "As AI capabilities continue to grow exponentially on economically relevant\nhuman expert tasks, with task completion horizons doubling every 7 months\naccording to the Model Evaluation and Threat Research (METR), we are interested\nin how this applies to the task of mathematics research. To explore this, we\nevaluated the capability of four frontier large language models (LLMs), ChatGPT\n5, Claude 4.1 Opus, Gemini 2.5 Pro, and Grok 4, at the task of creating a\nmini-paper on reservoir computing. All models produced engaging papers with\nsome apparent understanding of various techniques, but were sometimes lead to\nmistakes by surface level understanding of key ideas. That said, the\ncapabilities on LLMs on this task was likely as good or greater than that\npredicted by METR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI capabilities continue to grow exponentially on economically relevant\nhuman expert tasks, with task completion horizons doubling every 7 months\naccording to the Model Evaluation and Threat Research (METR), we are interested\nin how this applies to the task of mathematics research. To explore this, we\nevaluated the capability of four frontier large language models (LLMs), ChatGPT\n5, Claude 4.1 Opus, Gemini 2.5 Pro, and Grok 4, at the task of creating a\nmini-paper on reservoir computing. All models produced engaging papers with\nsome apparent understanding of various techniques, but were sometimes lead to\nmistakes by surface level understanding of key ideas. That said, the\ncapabilities on LLMs on this task was likely as good or greater than that\npredicted by METR."
                },
                "authors": [
                    {
                        "name": "Allen G Hart"
                    }
                ],
                "author_detail": {
                    "name": "Allen G Hart"
                },
                "author": "Allen G Hart",
                "arxiv_comment": "10 Pages, 0 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07760v2",
                "updated": "2025-09-30T17:18:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    18,
                    39,
                    1,
                    273,
                    0
                ],
                "published": "2025-02-11T18:43:07Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    43,
                    7,
                    1,
                    42,
                    0
                ],
                "title": "Scalable Fingerprinting of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Fingerprinting of Large Language Models"
                },
                "summary": "Model fingerprinting has emerged as a powerful tool for model owners to\nidentify their shared model given API access. However, to lower false discovery\nrate, fight fingerprint leakage, and defend against coalitions of model users\nattempting to bypass detection, we argue that {\\em scalability} is critical,\ni.e., scaling up the number of fingerprints one can embed into a model. Hence,\nwe pose scalability as a crucial requirement for fingerprinting schemes. We\nexperiment with fingerprint design at a scale significantly larger than\npreviously considered, and introduce a new method, dubbed Perinucleus sampling,\nto generate scalable, persistent, and harmless fingerprints. We demonstrate\nthat this scheme can add 24,576 fingerprints to a Llama-3.1-8B model -- two\norders of magnitude more than existing schemes -- without degrading the model's\nutility. Our inserted fingerprints persist even after supervised fine-tuning on\nstandard post-training data. We further address security risks for\nfingerprinting, and theoretically and empirically show how a scalable\nfingerprinting scheme like ours can mitigate these risks. Our code is available\nat https://github.com/SewoongLab/scalable-fingerprinting-of-llms",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model fingerprinting has emerged as a powerful tool for model owners to\nidentify their shared model given API access. However, to lower false discovery\nrate, fight fingerprint leakage, and defend against coalitions of model users\nattempting to bypass detection, we argue that {\\em scalability} is critical,\ni.e., scaling up the number of fingerprints one can embed into a model. Hence,\nwe pose scalability as a crucial requirement for fingerprinting schemes. We\nexperiment with fingerprint design at a scale significantly larger than\npreviously considered, and introduce a new method, dubbed Perinucleus sampling,\nto generate scalable, persistent, and harmless fingerprints. We demonstrate\nthat this scheme can add 24,576 fingerprints to a Llama-3.1-8B model -- two\norders of magnitude more than existing schemes -- without degrading the model's\nutility. Our inserted fingerprints persist even after supervised fine-tuning on\nstandard post-training data. We further address security risks for\nfingerprinting, and theoretically and empirically show how a scalable\nfingerprinting scheme like ours can mitigate these risks. Our code is available\nat https://github.com/SewoongLab/scalable-fingerprinting-of-llms"
                },
                "authors": [
                    {
                        "name": "Anshul Nasery"
                    },
                    {
                        "name": "Jonathan Hayase"
                    },
                    {
                        "name": "Creston Brooks"
                    },
                    {
                        "name": "Peiyao Sheng"
                    },
                    {
                        "name": "Himanshu Tyagi"
                    },
                    {
                        "name": "Pramod Viswanath"
                    },
                    {
                        "name": "Sewoong Oh"
                    }
                ],
                "author_detail": {
                    "name": "Sewoong Oh"
                },
                "author": "Sewoong Oh",
                "arxiv_comment": "Spotlight at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26546v1",
                "updated": "2025-09-30T17:17:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    17,
                    51,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:17:51Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    17,
                    51,
                    1,
                    273,
                    0
                ],
                "title": "Towards Verified Code Reasoning by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Verified Code Reasoning by LLMs"
                },
                "summary": "While LLM-based agents are able to tackle a wide variety of code reasoning\nquestions, the answers are not always correct. This prevents the agent from\nbeing useful in situations where high precision is desired: (1) helping a\nsoftware engineer understand a new code base, (2) helping a software engineer\nduring code review sessions, and (3) ensuring that the code generated by an\nautomated code generation system meets certain requirements (e.g. fixes a bug,\nimproves readability, implements a feature).\n  As a result of this lack of trustworthiness, the agent's answers need to be\nmanually verified before they can be trusted. Manually confirming responses\nfrom a code reasoning agent requires human effort and can result in slower\ndeveloper productivity, which weakens the assistance benefits of the agent. In\nthis paper, we describe a method to automatically validate the answers provided\nby a code reasoning agent by verifying its reasoning steps. At a very high\nlevel, the method consists of extracting a formal representation of the agent's\nresponse and, subsequently, using formal verification and program analysis\ntools to verify the agent's reasoning steps.\n  We applied this approach to a benchmark set of 20 uninitialized variable\nerrors detected by sanitizers and 20 program equivalence queries. For the\nuninitialized variable errors, the formal verification step was able to\nvalidate the agent's reasoning on 13/20 examples, and for the program\nequivalence queries, the formal verification step successfully caught 6/8\nincorrect judgments made by the agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLM-based agents are able to tackle a wide variety of code reasoning\nquestions, the answers are not always correct. This prevents the agent from\nbeing useful in situations where high precision is desired: (1) helping a\nsoftware engineer understand a new code base, (2) helping a software engineer\nduring code review sessions, and (3) ensuring that the code generated by an\nautomated code generation system meets certain requirements (e.g. fixes a bug,\nimproves readability, implements a feature).\n  As a result of this lack of trustworthiness, the agent's answers need to be\nmanually verified before they can be trusted. Manually confirming responses\nfrom a code reasoning agent requires human effort and can result in slower\ndeveloper productivity, which weakens the assistance benefits of the agent. In\nthis paper, we describe a method to automatically validate the answers provided\nby a code reasoning agent by verifying its reasoning steps. At a very high\nlevel, the method consists of extracting a formal representation of the agent's\nresponse and, subsequently, using formal verification and program analysis\ntools to verify the agent's reasoning steps.\n  We applied this approach to a benchmark set of 20 uninitialized variable\nerrors detected by sanitizers and 20 program equivalence queries. For the\nuninitialized variable errors, the formal verification step was able to\nvalidate the agent's reasoning on 13/20 examples, and for the program\nequivalence queries, the formal verification step successfully caught 6/8\nincorrect judgments made by the agent."
                },
                "authors": [
                    {
                        "name": "Meghana Sistla"
                    },
                    {
                        "name": "Gogul Balakrishnan"
                    },
                    {
                        "name": "Pat Rondon"
                    },
                    {
                        "name": "José Cambronero"
                    },
                    {
                        "name": "Michele Tufano"
                    },
                    {
                        "name": "Satish Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Satish Chandra"
                },
                "author": "Satish Chandra",
                "arxiv_comment": "43 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v1",
                "updated": "2025-09-30T17:15:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26539v1",
                "updated": "2025-09-30T17:13:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    13,
                    56,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:13:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    13,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents"
                },
                "summary": "Developing autonomous agents that effectively interact with Graphic User\nInterfaces (GUIs) remains a challenging open problem, especially for small\non-device models. In this paper, we present Ferret-UI Lite, a compact,\nend-to-end GUI agent that operates across diverse platforms, including mobile,\nweb, and desktop. Utilizing techniques optimized for developing small models,\nwe build our 3B Ferret-UI Lite agent through curating a diverse GUI data\nmixture from real and synthetic sources, strengthening inference-time\nperformance through chain-of-thought reasoning and visual tool-use, and\nreinforcement learning with designed rewards. Ferret-UI Lite achieves\ncompetitive performance with other small-scale GUI agents. In GUI grounding,\nFerret-UI Lite attains scores of $91.6\\%$, $53.3\\%$, and $61.2\\%$ on the\nScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI\nnavigation, Ferret-UI Lite achieves success rates of $28.0\\%$ on AndroidWorld\nand $19.8\\%$ on OSWorld. We share our methods and lessons learned from\ndeveloping compact, on-device GUI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing autonomous agents that effectively interact with Graphic User\nInterfaces (GUIs) remains a challenging open problem, especially for small\non-device models. In this paper, we present Ferret-UI Lite, a compact,\nend-to-end GUI agent that operates across diverse platforms, including mobile,\nweb, and desktop. Utilizing techniques optimized for developing small models,\nwe build our 3B Ferret-UI Lite agent through curating a diverse GUI data\nmixture from real and synthetic sources, strengthening inference-time\nperformance through chain-of-thought reasoning and visual tool-use, and\nreinforcement learning with designed rewards. Ferret-UI Lite achieves\ncompetitive performance with other small-scale GUI agents. In GUI grounding,\nFerret-UI Lite attains scores of $91.6\\%$, $53.3\\%$, and $61.2\\%$ on the\nScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI\nnavigation, Ferret-UI Lite achieves success rates of $28.0\\%$ on AndroidWorld\nand $19.8\\%$ on OSWorld. We share our methods and lessons learned from\ndeveloping compact, on-device GUI agents."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Omar Attia"
                    },
                    {
                        "name": "Yuhao Yang"
                    },
                    {
                        "name": "Michael Feng"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Jeffrey Nichols"
                    },
                    {
                        "name": "Alexander Toshev"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Zhe Gan"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Gan"
                },
                "author": "Zhe Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24922v2",
                "updated": "2025-09-30T17:09:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    9,
                    29,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-29T15:24:40Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    24,
                    40,
                    0,
                    272,
                    0
                ],
                "title": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal\n  Reasoning"
                },
                "summary": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large\nLanguage Models (LLMs), show great potential in addressing complex tasks. In\nthis context, integrating MAS with legal tasks is a crucial step. While\nprevious studies have developed legal benchmarks for LLM agents, none are\nspecifically designed to consider the unique advantages of MAS, such as task\ndecomposition, agent specialization, and flexible training. In fact, the lack\nof evaluation methods limits the potential of MAS in the legal domain. To\naddress this gap, we propose MASLegalBench, a legal benchmark tailored for MAS\nand designed with a deductive reasoning approach. Our benchmark uses GDPR as\nthe application scenario, encompassing extensive background knowledge and\ncovering complex reasoning processes that effectively reflect the intricacies\nof real-world legal situations. Furthermore, we manually design various\nrole-based MAS and conduct extensive experiments using different\nstate-of-the-art LLMs. Our results highlight the strengths, limitations, and\npotential areas for improvement of existing models and MAS architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large\nLanguage Models (LLMs), show great potential in addressing complex tasks. In\nthis context, integrating MAS with legal tasks is a crucial step. While\nprevious studies have developed legal benchmarks for LLM agents, none are\nspecifically designed to consider the unique advantages of MAS, such as task\ndecomposition, agent specialization, and flexible training. In fact, the lack\nof evaluation methods limits the potential of MAS in the legal domain. To\naddress this gap, we propose MASLegalBench, a legal benchmark tailored for MAS\nand designed with a deductive reasoning approach. Our benchmark uses GDPR as\nthe application scenario, encompassing extensive background knowledge and\ncovering complex reasoning processes that effectively reflect the intricacies\nof real-world legal situations. Furthermore, we manually design various\nrole-based MAS and conduct extensive experiments using different\nstate-of-the-art LLMs. Our results highlight the strengths, limitations, and\npotential areas for improvement of existing models and MAS architectures."
                },
                "authors": [
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Hongyu Luo"
                    },
                    {
                        "name": "Jianhui Yang"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26534v1",
                "updated": "2025-09-30T17:08:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    8,
                    51,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:08:51Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    8,
                    51,
                    1,
                    273,
                    0
                ],
                "title": "Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework"
                },
                "summary": "The rapid rise of large language models (LLMs) has been driving an enormous\ndemand for AI inference infrastructure, mainly powered by high-end GPUs. While\nthese accelerators offer immense computational power, they incur high capital\nand operational costs due to frequent upgrades, dense power consumption, and\ncooling demands, making total cost of ownership (TCO) for AI datacenters a\ncritical concern for cloud providers. Unfortunately, traditional datacenter\nlifecycle management (designed for general-purpose workloads) struggles to keep\npace with AI's fast-evolving models, rising resource needs, and diverse\nhardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme\nacross three stages: building, hardware refresh, and operation. We show how\ndesign choices in power, cooling, and networking provisioning impact long-term\nTCO. We also explore refresh strategies aligned with hardware trends. Finally,\nwe use operation software optimizations to reduce cost. While these\noptimizations at each stage yield benefits, unlocking the full potential\nrequires rethinking the entire lifecycle. Thus, we present a holistic lifecycle\nmanagement framework that coordinates and co-optimizes decisions across all\nthree stages, accounting for workload dynamics, hardware evolution, and system\naging. Our system reduces the TCO by up to 40\\% over traditional approaches.\nUsing our framework we provide guidelines on how to manage AI datacenter\nlifecycle for the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of large language models (LLMs) has been driving an enormous\ndemand for AI inference infrastructure, mainly powered by high-end GPUs. While\nthese accelerators offer immense computational power, they incur high capital\nand operational costs due to frequent upgrades, dense power consumption, and\ncooling demands, making total cost of ownership (TCO) for AI datacenters a\ncritical concern for cloud providers. Unfortunately, traditional datacenter\nlifecycle management (designed for general-purpose workloads) struggles to keep\npace with AI's fast-evolving models, rising resource needs, and diverse\nhardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme\nacross three stages: building, hardware refresh, and operation. We show how\ndesign choices in power, cooling, and networking provisioning impact long-term\nTCO. We also explore refresh strategies aligned with hardware trends. Finally,\nwe use operation software optimizations to reduce cost. While these\noptimizations at each stage yield benefits, unlocking the full potential\nrequires rethinking the entire lifecycle. Thus, we present a holistic lifecycle\nmanagement framework that coordinates and co-optimizes decisions across all\nthree stages, accounting for workload dynamics, hardware evolution, and system\naging. Our system reduces the TCO by up to 40\\% over traditional approaches.\nUsing our framework we provide guidelines on how to manage AI datacenter\nlifecycle for the future."
                },
                "authors": [
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ricardo Bianchini"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Bianchini"
                },
                "author": "Ricardo Bianchini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26520v1",
                "updated": "2025-09-30T16:56:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    56,
                    44,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:56:44Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    56,
                    44,
                    1,
                    273,
                    0
                ],
                "title": "Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert\n  Utilization"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently\nscaling large language models without a proportional increase in computational\ncost. However, the standard training strategy of Top-K router prevents MoE\nmodels from realizing their full potential for elastic inference. When the\nnumber of activated experts is altered at inference time, these models exhibit\nprecipitous performance degradation. In this work, we introduce Matryoshka MoE\n(M-MoE), a training framework that instills a coarse-to-fine structure directly\ninto the expert ensemble. By systematically varying the number of activated\nexperts during training, M-MoE compels the model to learn a meaningful ranking:\ntop-ranked experts collaborate to provide essential, coarse-grained\ncapabilities, while subsequent experts add progressively finer-grained detail.\nWe explore this principle at multiple granularities, identifying a layer-wise\nrandomization strategy as the most effective. Our experiments demonstrate that\na single M-MoE model achieves remarkable elasticity, with its performance at\nvarious expert counts closely matching that of an entire suite of specialist\nmodels, but at only a fraction of the total training cost. This flexibility not\nonly unlocks elastic inference but also enables optimizing performance by\nallocating different computational budgets to different model layers. Our work\npaves the way for more practical and adaptable deployments of large-scale MoE\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently\nscaling large language models without a proportional increase in computational\ncost. However, the standard training strategy of Top-K router prevents MoE\nmodels from realizing their full potential for elastic inference. When the\nnumber of activated experts is altered at inference time, these models exhibit\nprecipitous performance degradation. In this work, we introduce Matryoshka MoE\n(M-MoE), a training framework that instills a coarse-to-fine structure directly\ninto the expert ensemble. By systematically varying the number of activated\nexperts during training, M-MoE compels the model to learn a meaningful ranking:\ntop-ranked experts collaborate to provide essential, coarse-grained\ncapabilities, while subsequent experts add progressively finer-grained detail.\nWe explore this principle at multiple granularities, identifying a layer-wise\nrandomization strategy as the most effective. Our experiments demonstrate that\na single M-MoE model achieves remarkable elasticity, with its performance at\nvarious expert counts closely matching that of an entire suite of specialist\nmodels, but at only a fraction of the total training cost. This flexibility not\nonly unlocks elastic inference but also enables optimizing performance by\nallocating different computational budgets to different model layers. Our work\npaves the way for more practical and adaptable deployments of large-scale MoE\nmodels."
                },
                "authors": [
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Qingguo Hu"
                    },
                    {
                        "name": "Yucheng Ding"
                    },
                    {
                        "name": "Ruizhe Wang"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26517v1",
                "updated": "2025-09-30T16:54:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    54,
                    47,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:54:47Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    54,
                    47,
                    1,
                    273,
                    0
                ],
                "title": "Persuasion Effects in Regression Discontinuity Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion Effects in Regression Discontinuity Designs"
                },
                "summary": "We develop a framework for identifying and estimating persuasion effects in\nregression discontinuity (RD) designs. The RD persuasion rate measures the\nprobability that individuals at the threshold would take the action if exposed\nto a persuasive message, given that they would not take the action without\nexposure. We present identification results for both sharp and fuzzy RD\ndesigns, derive sharp bounds under various data scenarios, and extend the\nanalysis to local compliers. Estimation and inference rely on local polynomial\nregression, enabling straightforward implementation with standard RD tools.\nApplications to public health and media illustrate its empirical relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a framework for identifying and estimating persuasion effects in\nregression discontinuity (RD) designs. The RD persuasion rate measures the\nprobability that individuals at the threshold would take the action if exposed\nto a persuasive message, given that they would not take the action without\nexposure. We present identification results for both sharp and fuzzy RD\ndesigns, derive sharp bounds under various data scenarios, and extend the\nanalysis to local compliers. Estimation and inference rely on local polynomial\nregression, enabling straightforward implementation with standard RD tools.\nApplications to public health and media illustrate its empirical relevance."
                },
                "authors": [
                    {
                        "name": "Sung Jae Jun"
                    },
                    {
                        "name": "Sokbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sokbae Lee"
                },
                "author": "Sokbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23602v2",
                "updated": "2025-09-30T16:52:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    52,
                    17,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-28T03:13:32Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    3,
                    13,
                    32,
                    6,
                    271,
                    0
                ],
                "title": "Deep Taxonomic Networks for Unsupervised Hierarchical Prototype\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Taxonomic Networks for Unsupervised Hierarchical Prototype\n  Discovery"
                },
                "summary": "Inspired by the human ability to learn and organize knowledge into\nhierarchical taxonomies with prototypes, this paper addresses key limitations\nin current deep hierarchical clustering methods. Existing methods often tie the\nstructure to the number of classes and underutilize the rich prototype\ninformation available at intermediate hierarchical levels. We introduce deep\ntaxonomic networks, a novel deep latent variable approach designed to bridge\nthese gaps. Our method optimizes a large latent taxonomic hierarchy,\nspecifically a complete binary tree structured mixture-of-Gaussian prior within\na variational inference framework, to automatically discover taxonomic\nstructures and associated prototype clusters directly from unlabeled data\nwithout assuming true label sizes. We analytically show that optimizing the\nELBO of our method encourages the discovery of hierarchical relationships among\nprototypes. Empirically, our learned models demonstrate strong hierarchical\nclustering performance, outperforming baselines across diverse image\nclassification datasets using our novel evaluation mechanism that leverages\nprototype clusters discovered at all hierarchical levels. Qualitative results\nfurther reveal that deep taxonomic networks discover rich and interpretable\nhierarchical taxonomies, capturing both coarse-grained semantic categories and\nfine-grained visual distinctions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by the human ability to learn and organize knowledge into\nhierarchical taxonomies with prototypes, this paper addresses key limitations\nin current deep hierarchical clustering methods. Existing methods often tie the\nstructure to the number of classes and underutilize the rich prototype\ninformation available at intermediate hierarchical levels. We introduce deep\ntaxonomic networks, a novel deep latent variable approach designed to bridge\nthese gaps. Our method optimizes a large latent taxonomic hierarchy,\nspecifically a complete binary tree structured mixture-of-Gaussian prior within\na variational inference framework, to automatically discover taxonomic\nstructures and associated prototype clusters directly from unlabeled data\nwithout assuming true label sizes. We analytically show that optimizing the\nELBO of our method encourages the discovery of hierarchical relationships among\nprototypes. Empirically, our learned models demonstrate strong hierarchical\nclustering performance, outperforming baselines across diverse image\nclassification datasets using our novel evaluation mechanism that leverages\nprototype clusters discovered at all hierarchical levels. Qualitative results\nfurther reveal that deep taxonomic networks discover rich and interpretable\nhierarchical taxonomies, capturing both coarse-grained semantic categories and\nfine-grained visual distinctions."
                },
                "authors": [
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Ethan Haarer"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Zhiyi Dai"
                    },
                    {
                        "name": "Christopher J. MacLellan"
                    }
                ],
                "author_detail": {
                    "name": "Christopher J. MacLellan"
                },
                "author": "Christopher J. MacLellan",
                "arxiv_comment": "NeurIPS 2025",
                "arxiv_journal_ref": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26514v1",
                "updated": "2025-09-30T16:52:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    52,
                    14,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:52:14Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    52,
                    14,
                    1,
                    273,
                    0
                ],
                "title": "BatonVoice: An Operationalist Framework for Enhancing Controllable\n  Speech Synthesis with Linguistic Intelligence from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatonVoice: An Operationalist Framework for Enhancing Controllable\n  Speech Synthesis with Linguistic Intelligence from LLMs"
                },
                "summary": "The rise of Large Language Models (LLMs) is reshaping multimodel models, with\nspeech synthesis being a prominent application. However, existing approaches\noften underutilize the linguistic intelligence of these models, typically\nfailing to leverage their powerful instruction-following capabilities. This\nlimitation hinders the model's ability to follow text instructions for\ncontrollable Text-to-Speech~(TTS). To address this, we propose a new paradigm\ninspired by ``operationalism'' that decouples instruction understanding from\nspeech generation. We introduce BatonVoice, a framework where an LLM acts as a\n``conductor'', understanding user instructions and generating a textual\n``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS\nmodel, the ``orchestra'', then generates the speech from these features. To\nrealize this component, we develop BatonTTS, a TTS model trained specifically\nfor this task. Our experiments demonstrate that BatonVoice achieves strong\nperformance in controllable and emotional speech synthesis, outperforming\nstrong open- and closed-source baselines. Notably, our approach enables\nremarkable zero-shot cross-lingual generalization, accurately applying feature\ncontrol abilities to languages unseen during post-training. This demonstrates\nthat objectifying speech into textual vocal features can more effectively\nunlock the linguistic intelligence of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) is reshaping multimodel models, with\nspeech synthesis being a prominent application. However, existing approaches\noften underutilize the linguistic intelligence of these models, typically\nfailing to leverage their powerful instruction-following capabilities. This\nlimitation hinders the model's ability to follow text instructions for\ncontrollable Text-to-Speech~(TTS). To address this, we propose a new paradigm\ninspired by ``operationalism'' that decouples instruction understanding from\nspeech generation. We introduce BatonVoice, a framework where an LLM acts as a\n``conductor'', understanding user instructions and generating a textual\n``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS\nmodel, the ``orchestra'', then generates the speech from these features. To\nrealize this component, we develop BatonTTS, a TTS model trained specifically\nfor this task. Our experiments demonstrate that BatonVoice achieves strong\nperformance in controllable and emotional speech synthesis, outperforming\nstrong open- and closed-source baselines. Notably, our approach enables\nremarkable zero-shot cross-lingual generalization, accurately applying feature\ncontrol abilities to languages unseen during post-training. This demonstrates\nthat objectifying speech into textual vocal features can more effectively\nunlock the linguistic intelligence of LLMs."
                },
                "authors": [
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Ruotian Ma"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Wanshun Chen"
                    },
                    {
                        "name": "Huang Liu"
                    },
                    {
                        "name": "Jiadi Yao"
                    },
                    {
                        "name": "Qu Yang"
                    },
                    {
                        "name": "Qingxuan Jiang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Linus"
                    }
                ],
                "author_detail": {
                    "name": "Linus"
                },
                "author": "Linus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26507v1",
                "updated": "2025-09-30T16:49:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    49,
                    1,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:49:01Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    49,
                    1,
                    1,
                    273,
                    0
                ],
                "title": "The Dragon Hatchling: The Missing Link between the Transformer and\n  Models of the Brain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dragon Hatchling: The Missing Link between the Transformer and\n  Models of the Brain"
                },
                "summary": "The relationship between computing systems and the brain has served as\nmotivation for pioneering theoreticians since John von Neumann and Alan Turing.\nUniform, scale-free biological networks, such as the brain, have powerful\nproperties, including generalizing over time, which is the main barrier for\nMachine Learning on the path to Universal Reasoning Models.\n  We introduce `Dragon Hatchling' (BDH), a new Large Language Model\narchitecture based on a scale-free biologically inspired network of \\$n\\$\nlocally-interacting neuron particles. BDH couples strong theoretical\nfoundations and inherent interpretability without sacrificing Transformer-like\nperformance.\n  BDH is a practical, performant state-of-the-art attention-based state space\nsequence learning architecture. In addition to being a graph model, BDH admits\na GPU-friendly formulation. It exhibits Transformer-like scaling laws:\nempirically BDH rivals GPT2 performance on language and translation tasks, at\nthe same number of parameters (10M to 1B), for the same training data.\n  BDH can be represented as a brain model. The working memory of BDH during\ninference entirely relies on synaptic plasticity with Hebbian learning using\nspiking neurons. We confirm empirically that specific, individual synapses\nstrengthen connection whenever BDH hears or reasons about a specific concept\nwhile processing language inputs. The neuron interaction network of BDH is a\ngraph of high modularity with heavy-tailed degree distribution. The BDH model\nis biologically plausible, explaining one possible mechanism which human\nneurons could use to achieve speech.\n  BDH is designed for interpretability. Activation vectors of BDH are sparse\nand positive. We demonstrate monosemanticity in BDH on language tasks.\nInterpretability of state, which goes beyond interpretability of neurons and\nmodel parameters, is an inherent feature of the BDH architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The relationship between computing systems and the brain has served as\nmotivation for pioneering theoreticians since John von Neumann and Alan Turing.\nUniform, scale-free biological networks, such as the brain, have powerful\nproperties, including generalizing over time, which is the main barrier for\nMachine Learning on the path to Universal Reasoning Models.\n  We introduce `Dragon Hatchling' (BDH), a new Large Language Model\narchitecture based on a scale-free biologically inspired network of \\$n\\$\nlocally-interacting neuron particles. BDH couples strong theoretical\nfoundations and inherent interpretability without sacrificing Transformer-like\nperformance.\n  BDH is a practical, performant state-of-the-art attention-based state space\nsequence learning architecture. In addition to being a graph model, BDH admits\na GPU-friendly formulation. It exhibits Transformer-like scaling laws:\nempirically BDH rivals GPT2 performance on language and translation tasks, at\nthe same number of parameters (10M to 1B), for the same training data.\n  BDH can be represented as a brain model. The working memory of BDH during\ninference entirely relies on synaptic plasticity with Hebbian learning using\nspiking neurons. We confirm empirically that specific, individual synapses\nstrengthen connection whenever BDH hears or reasons about a specific concept\nwhile processing language inputs. The neuron interaction network of BDH is a\ngraph of high modularity with heavy-tailed degree distribution. The BDH model\nis biologically plausible, explaining one possible mechanism which human\nneurons could use to achieve speech.\n  BDH is designed for interpretability. Activation vectors of BDH are sparse\nand positive. We demonstrate monosemanticity in BDH on language tasks.\nInterpretability of state, which goes beyond interpretability of neurons and\nmodel parameters, is an inherent feature of the BDH architecture."
                },
                "authors": [
                    {
                        "name": "Adrian Kosowski"
                    },
                    {
                        "name": "Przemysław Uznański"
                    },
                    {
                        "name": "Jan Chorowski"
                    },
                    {
                        "name": "Zuzanna Stamirowska"
                    },
                    {
                        "name": "Michał Bartoszkiewicz"
                    }
                ],
                "author_detail": {
                    "name": "Michał Bartoszkiewicz"
                },
                "author": "Michał Bartoszkiewicz",
                "arxiv_comment": "Code available at: https://github.com/pathwaycom/bdh Accompanying\n  blog: https://pathway.com/research/bdh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06452v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06452v2",
                "updated": "2025-09-30T16:47:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    47,
                    34,
                    1,
                    273,
                    0
                ],
                "published": "2024-11-10T12:53:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    12,
                    53,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "Detecting Secular Perturbations in Kepler Planetary Systems Using\n  Simultaneous Impact Parameter Variation Analysis (SIPVA)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Secular Perturbations in Kepler Planetary Systems Using\n  Simultaneous Impact Parameter Variation Analysis (SIPVA)"
                },
                "summary": "Recovering impact parameter variations in multi-planet systems is an\neffective approach for detecting non-transiting planets and refining planetary\nmass estimates. Traditionally, two methodologies have been employed: the\nIndividual Fit, which fits each transit independently to analyze impact\nparameter changes, and the Dynamical Fit, which simulates planetary dynamics to\nmatch transit light curves. We introduce a new fitting method, Simultaneous\nImpact Parameter Variation Analysis (SIPVA), which demonstrates advantages over\nthe Individual Fit and avoids the computational cost of N-body integrations\nrequired by the Dynamical Fit. SIPVA directly incorporates a linear\ntime-dependent model for impact parameters into the Markov Chain Monte Carlo\n(MCMC) framework by fitting all transits simultaneously. We evaluate SIPVA and\nthe Individual Fit on artificial systems with varying log-likelihood ratios and\nfind that SIPVA consistently outperforms the Individual Fit in recovery rates\nand accuracy. When applied to selected Kepler planetary candidates exhibiting\nsignificant transit duration variations (TDVs), SIPVA identifies significant\nimpact parameter trends in 10 out of 16 planets, whereas the Individual Fit\ndoes so in only 4. We also employ probabilistic modeling to simulate the\ntheoretical distribution of planets with significant impact parameter\nvariations across all observed Kepler systems and compare the distribution of\nrecovered candidates by the Individual Fit and Dynamical Fit from previous work\nwith our theoretical distribution. Our findings offer an alternative framework\nfor analyzing planetary transits, relying solely on Bayesian inference without\nrequiring prior assumptions about the planetary system's dynamical\narchitecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering impact parameter variations in multi-planet systems is an\neffective approach for detecting non-transiting planets and refining planetary\nmass estimates. Traditionally, two methodologies have been employed: the\nIndividual Fit, which fits each transit independently to analyze impact\nparameter changes, and the Dynamical Fit, which simulates planetary dynamics to\nmatch transit light curves. We introduce a new fitting method, Simultaneous\nImpact Parameter Variation Analysis (SIPVA), which demonstrates advantages over\nthe Individual Fit and avoids the computational cost of N-body integrations\nrequired by the Dynamical Fit. SIPVA directly incorporates a linear\ntime-dependent model for impact parameters into the Markov Chain Monte Carlo\n(MCMC) framework by fitting all transits simultaneously. We evaluate SIPVA and\nthe Individual Fit on artificial systems with varying log-likelihood ratios and\nfind that SIPVA consistently outperforms the Individual Fit in recovery rates\nand accuracy. When applied to selected Kepler planetary candidates exhibiting\nsignificant transit duration variations (TDVs), SIPVA identifies significant\nimpact parameter trends in 10 out of 16 planets, whereas the Individual Fit\ndoes so in only 4. We also employ probabilistic modeling to simulate the\ntheoretical distribution of planets with significant impact parameter\nvariations across all observed Kepler systems and compare the distribution of\nrecovered candidates by the Individual Fit and Dynamical Fit from previous work\nwith our theoretical distribution. Our findings offer an alternative framework\nfor analyzing planetary transits, relying solely on Bayesian inference without\nrequiring prior assumptions about the planetary system's dynamical\narchitecture."
                },
                "authors": [
                    {
                        "name": "Zhixing Liu"
                    },
                    {
                        "name": "Bonan Pu"
                    }
                ],
                "author_detail": {
                    "name": "Bonan Pu"
                },
                "author": "Bonan Pu",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06452v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11646v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11646v3",
                "updated": "2025-09-30T16:43:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    43,
                    0,
                    1,
                    273,
                    0
                ],
                "published": "2024-12-16T10:47:05Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    10,
                    47,
                    5,
                    0,
                    351,
                    0
                ],
                "title": "Information-Geometric Barycenters for Bayesian Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-Geometric Barycenters for Bayesian Federated Learning"
                },
                "summary": "Federated learning (FL) is a widely used and impactful distributed\noptimization framework that achieves consensus through averaging locally\ntrained models. While effective, this approach may not align well with Bayesian\ninference, where the model space has the structure of a distribution space.\nTaking an information-geometric perspective, we reinterpret FL aggregation as\nthe problem of finding the barycenter of local posteriors using a prespecified\ndivergence metric, minimizing the average discrepancy across clients. This\nperspective provides a unifying framework that generalizes many existing\nmethods and offers crisp insights into their theoretical underpinnings. We then\npropose BA-BFL, an algorithm that retains the convergence properties of\nFederated Averaging in non-convex settings. In non-independent and identically\ndistributed scenarios, we conduct extensive comparisons with statistical\naggregation techniques, showing that BA-BFL achieves performance comparable to\nstate-of-the-art methods while offering a geometric interpretation of the\naggregation phase. Additionally, we extend our analysis to Hybrid Bayesian Deep\nLearning, exploring the impact of Bayesian layers on uncertainty quantification\nand model calibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a widely used and impactful distributed\noptimization framework that achieves consensus through averaging locally\ntrained models. While effective, this approach may not align well with Bayesian\ninference, where the model space has the structure of a distribution space.\nTaking an information-geometric perspective, we reinterpret FL aggregation as\nthe problem of finding the barycenter of local posteriors using a prespecified\ndivergence metric, minimizing the average discrepancy across clients. This\nperspective provides a unifying framework that generalizes many existing\nmethods and offers crisp insights into their theoretical underpinnings. We then\npropose BA-BFL, an algorithm that retains the convergence properties of\nFederated Averaging in non-convex settings. In non-independent and identically\ndistributed scenarios, we conduct extensive comparisons with statistical\naggregation techniques, showing that BA-BFL achieves performance comparable to\nstate-of-the-art methods while offering a geometric interpretation of the\naggregation phase. Additionally, we extend our analysis to Hybrid Bayesian Deep\nLearning, exploring the impact of Bayesian layers on uncertainty quantification\nand model calibration."
                },
                "authors": [
                    {
                        "name": "Nour Jamoussi"
                    },
                    {
                        "name": "Giuseppe Serra"
                    },
                    {
                        "name": "Photios A. Stavrou"
                    },
                    {
                        "name": "Marios Kountouris"
                    }
                ],
                "author_detail": {
                    "name": "Marios Kountouris"
                },
                "author": "Marios Kountouris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11646v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11646v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v2",
                "updated": "2025-09-30T16:42:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    42,
                    50,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26497v1",
                "updated": "2025-09-30T16:40:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    40,
                    55,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:40:55Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    40,
                    55,
                    1,
                    273,
                    0
                ],
                "title": "Revealing the Power of Post-Training for Small Language Models via\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the Power of Post-Training for Small Language Models via\n  Knowledge Distillation"
                },
                "summary": "The rapid advancement of large language models (LLMs) has significantly\nadvanced the capabilities of artificial intelligence across various domains.\nHowever, their massive scale and high computational costs render them\nunsuitable for direct deployment in resource-constrained edge environments.\nThis creates a critical need for high-performance small models that can operate\nefficiently at the edge. Yet, after pre-training alone, these smaller models\noften fail to meet the performance requirements of complex tasks. To bridge\nthis gap, we introduce a systematic post-training pipeline that efficiently\nenhances small model accuracy. Our post training pipeline consists of\ncurriculum-based supervised fine-tuning (SFT) and offline on-policy knowledge\ndistillation. The resulting instruction-tuned model achieves state-of-the-art\nperformance among billion-parameter models, demonstrating strong generalization\nunder strict hardware constraints while maintaining competitive accuracy across\na variety of tasks. This work provides a practical and efficient solution for\ndeveloping high-performance language models on Ascend edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has significantly\nadvanced the capabilities of artificial intelligence across various domains.\nHowever, their massive scale and high computational costs render them\nunsuitable for direct deployment in resource-constrained edge environments.\nThis creates a critical need for high-performance small models that can operate\nefficiently at the edge. Yet, after pre-training alone, these smaller models\noften fail to meet the performance requirements of complex tasks. To bridge\nthis gap, we introduce a systematic post-training pipeline that efficiently\nenhances small model accuracy. Our post training pipeline consists of\ncurriculum-based supervised fine-tuning (SFT) and offline on-policy knowledge\ndistillation. The resulting instruction-tuned model achieves state-of-the-art\nperformance among billion-parameter models, demonstrating strong generalization\nunder strict hardware constraints while maintaining competitive accuracy across\na variety of tasks. This work provides a practical and efficient solution for\ndeveloping high-performance language models on Ascend edge devices."
                },
                "authors": [
                    {
                        "name": "Miao Rang"
                    },
                    {
                        "name": "Zhenni Bi"
                    },
                    {
                        "name": "Hang Zhou"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "An Xiao"
                    },
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "7",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17630v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17630v3",
                "updated": "2025-10-01T07:18:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    7,
                    18,
                    47,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-23T08:41:45Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    8,
                    41,
                    45,
                    4,
                    143,
                    0
                ],
                "title": "GIM: Improved Interpretability for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIM: Improved Interpretability for Large Language Models"
                },
                "summary": "Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim."
                },
                "authors": [
                    {
                        "name": "Joakim Edin"
                    },
                    {
                        "name": "Róbert Csordás"
                    },
                    {
                        "name": "Tuukka Ruotsalo"
                    },
                    {
                        "name": "Zhengxuan Wu"
                    },
                    {
                        "name": "Maria Maistro"
                    },
                    {
                        "name": "Casper L. Christensen"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Lars Maaløe"
                    }
                ],
                "author_detail": {
                    "name": "Lars Maaløe"
                },
                "author": "Lars Maaløe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17630v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17630v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26495v1",
                "updated": "2025-09-30T16:39:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    39,
                    17,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:39:17Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    39,
                    17,
                    1,
                    273,
                    0
                ],
                "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!"
                },
                "summary": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\%\n-- fall far short of reliable operational safety, while GPT models plateau in\nthe 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma\nand Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational\nsafety is a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23\\%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\%\n-- fall far short of reliable operational safety, while GPT models plateau in\nthe 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma\nand Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational\nsafety is a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23\\%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents."
                },
                "authors": [
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Varun Gumma"
                    },
                    {
                        "name": "Rishabh Bhardwaj"
                    },
                    {
                        "name": "Seok Min Lim"
                    },
                    {
                        "name": "Chuan Li"
                    },
                    {
                        "name": "Amir Zadeh"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26490v1",
                "updated": "2025-09-30T16:33:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    33,
                    49,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:33:49Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    33,
                    49,
                    1,
                    273,
                    0
                ],
                "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications"
                },
                "summary": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/"
                },
                "authors": [
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Yueqing Sun"
                    },
                    {
                        "name": "Hongyan Hao"
                    },
                    {
                        "name": "Xueyuan Hao"
                    },
                    {
                        "name": "Zhikang Xia"
                    },
                    {
                        "name": "Qi Gu"
                    },
                    {
                        "name": "Chengcheng Han"
                    },
                    {
                        "name": "Dengchang Zhao"
                    },
                    {
                        "name": "Hui Su"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Man Gao"
                    },
                    {
                        "name": "Xi Su"
                    },
                    {
                        "name": "Xiaodong Cai"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Yunke Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yunke Zhao"
                },
                "author": "Yunke Zhao",
                "arxiv_comment": "The code, dataset, and leaderboard are available at\n  https://vitabench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26488v1",
                "updated": "2025-09-30T16:32:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    32,
                    52,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:32:52Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    32,
                    52,
                    1,
                    273,
                    0
                ],
                "title": "dParallel: Learnable Parallel Decoding for dLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dParallel: Learnable Parallel Decoding for dLLMs"
                },
                "summary": "Diffusion large language models (dLLMs) have recently drawn considerable\nattention within the research community as a promising alternative to\nautoregressive generation, offering parallel token prediction and lower\ninference latency. Yet, their parallel decoding potential remains largely\nunderexplored, as existing open-source models still require nearly token-length\ndecoding steps to ensure performance. To address this, we introduce dParallel,\na simple and effective method that unlocks the inherent parallelism of dLLMs\nfor fast sampling. We identify that the key bottleneck to parallel decoding\narises from the sequential certainty convergence for masked tokens. Building on\nthis insight, we introduce the core of our approach: certainty-forcing\ndistillation, a novel training strategy that distills the model to follow its\noriginal sampling trajectories while enforcing it to achieve high certainty on\nmasked tokens more rapidly and in parallel. Extensive experiments across\nvarious benchmarks demonstrate that our method can dramatically reduce the\nnumber of decoding steps while maintaining performance. When applied to the\nLLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on\nGSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP\nbenchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup\nwhile maintaining accuracy. Our code is available at\nhttps://github.com/czg1225/dParallel",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion large language models (dLLMs) have recently drawn considerable\nattention within the research community as a promising alternative to\nautoregressive generation, offering parallel token prediction and lower\ninference latency. Yet, their parallel decoding potential remains largely\nunderexplored, as existing open-source models still require nearly token-length\ndecoding steps to ensure performance. To address this, we introduce dParallel,\na simple and effective method that unlocks the inherent parallelism of dLLMs\nfor fast sampling. We identify that the key bottleneck to parallel decoding\narises from the sequential certainty convergence for masked tokens. Building on\nthis insight, we introduce the core of our approach: certainty-forcing\ndistillation, a novel training strategy that distills the model to follow its\noriginal sampling trajectories while enforcing it to achieve high certainty on\nmasked tokens more rapidly and in parallel. Extensive experiments across\nvarious benchmarks demonstrate that our method can dramatically reduce the\nnumber of decoding steps while maintaining performance. When applied to the\nLLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on\nGSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP\nbenchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup\nwhile maintaining accuracy. Our code is available at\nhttps://github.com/czg1225/dParallel"
                },
                "authors": [
                    {
                        "name": "Zigeng Chen"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Ruonan Yu"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Working in progress, code base: https://github.com/czg1225/dParallel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26483v1",
                "updated": "2025-09-30T16:29:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    29,
                    35,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:29:35Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    29,
                    35,
                    1,
                    273,
                    0
                ],
                "title": "A systematic comparison of Large Language Models for automated\n  assignment assessment in programming education: Exploring the importance of\n  architecture and vendor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A systematic comparison of Large Language Models for automated\n  assignment assessment in programming education: Exploring the importance of\n  architecture and vendor"
                },
                "summary": "This study presents the first large-scale, side-by-side comparison of\ncontemporary Large Language Models (LLMs) in the automated grading of\nprogramming assignments. Drawing on over 6,000 student submissions collected\nacross four years of an introductory programming course, we systematically\nanalysed the distribution of grades, differences in mean scores and variability\nreflecting stricter or more lenient grading, and the consistency and clustering\nof grading patterns across models. Eighteen publicly available models were\nevaluated: Anthropic (claude-3-5-haiku, claude-opus-4-1, claude-sonnet-4);\nDeepseek (deepseek-chat, deepseek-reasoner); Google (gemini-2.0-flash-lite,\ngemini-2.0-flash, gemini-2.5-flash-lite, gemini-2.5-flash, gemini-2.5-pro); and\nOpenAI (gpt-4.1-mini, gpt-4.1-nano, gpt-4.1, gpt-4o-mini, gpt-4o, gpt-5-mini,\ngpt-5-nano, gpt-5). Statistical tests, correlation and clustering analyses\nrevealed clear, systematic differences between and within vendor families, with\n\"mini\" and \"nano\" variants consistently underperforming their full-scale\ncounterparts. All models displayed high internal agreement, measured by the\nintraclass correlation coefficient, with the model consensus but only moderate\nagreement with human teachers' grades, indicating a persistent gap between\nautomated and human assessment. These findings underscore that the choice of\nmodel for educational deployment is not neutral and should be guided by\npedagogical goals, transparent reporting of evaluation metrics, and ongoing\nhuman oversight to ensure accuracy, fairness and relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents the first large-scale, side-by-side comparison of\ncontemporary Large Language Models (LLMs) in the automated grading of\nprogramming assignments. Drawing on over 6,000 student submissions collected\nacross four years of an introductory programming course, we systematically\nanalysed the distribution of grades, differences in mean scores and variability\nreflecting stricter or more lenient grading, and the consistency and clustering\nof grading patterns across models. Eighteen publicly available models were\nevaluated: Anthropic (claude-3-5-haiku, claude-opus-4-1, claude-sonnet-4);\nDeepseek (deepseek-chat, deepseek-reasoner); Google (gemini-2.0-flash-lite,\ngemini-2.0-flash, gemini-2.5-flash-lite, gemini-2.5-flash, gemini-2.5-pro); and\nOpenAI (gpt-4.1-mini, gpt-4.1-nano, gpt-4.1, gpt-4o-mini, gpt-4o, gpt-5-mini,\ngpt-5-nano, gpt-5). Statistical tests, correlation and clustering analyses\nrevealed clear, systematic differences between and within vendor families, with\n\"mini\" and \"nano\" variants consistently underperforming their full-scale\ncounterparts. All models displayed high internal agreement, measured by the\nintraclass correlation coefficient, with the model consensus but only moderate\nagreement with human teachers' grades, indicating a persistent gap between\nautomated and human assessment. These findings underscore that the choice of\nmodel for educational deployment is not neutral and should be guided by\npedagogical goals, transparent reporting of evaluation metrics, and ongoing\nhuman oversight to ensure accuracy, fairness and relevance."
                },
                "authors": [
                    {
                        "name": "Marcin Jukiewicz"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Jukiewicz"
                },
                "author": "Marcin Jukiewicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04974v2",
                "updated": "2025-09-30T16:29:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    29,
                    23,
                    1,
                    273,
                    0
                ],
                "published": "2025-03-06T21:08:07Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    8,
                    7,
                    3,
                    65,
                    0
                ],
                "title": "From Voice to Safety: Language AI Powered Pilot-ATC Communication\n  Understanding for Airport Surface Movement Collision Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Voice to Safety: Language AI Powered Pilot-ATC Communication\n  Understanding for Airport Surface Movement Collision Risk Assessment"
                },
                "summary": "This work provides a feasible solution to the existing airport surface safety\nmonitoring capabilities (i.e., Airport Surface Surveillance Capability (ASSC)),\nnamely language AI-based voice communication understanding for collision risk\nassessment. The proposed framework consists of two major parts, (a)\nrule-enhanced Named Entity Recognition (NER); (b) surface collision risk\nmodeling. NER module generates information tables by processing voice\ncommunication transcripts, which serve as references for producing potential\ntaxi plans and calculating the surface movement collision risk. We first\ncollect and annotate our dataset based on open-sourced video recordings and\nsafety investigation reports. Additionally, we refer to FAA Order JO 7110.65W\nand FAA Order JO 7340.2N to get the list of heuristic rules and phase\ncontractions of communication between the pilot and the Air Traffic Controller\n(ATCo). Then, we propose the novel ATC Rule-Enhanced NER method, which\nintegrates the heuristic rules into the model training and inference stages,\nresulting in a hybrid rule-based NER model. We show the effectiveness of this\nhybrid approach by comparing different setups with different token-level\nembedding models. For the risk modeling, we adopt the node-link airport layout\ngraph from NASA FACET and model the aircraft taxi speed at each link as a\nlog-normal distribution and derive the total taxi time distribution. Then, we\npropose a spatiotemporal formulation of the risk probability of two aircraft\nmoving across potential collision nodes during ground movement. Furthermore, we\npropose the real-time implementation of such a method to obtain the lead time,\nwith a comparison with a Petri-Net based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a feasible solution to the existing airport surface safety\nmonitoring capabilities (i.e., Airport Surface Surveillance Capability (ASSC)),\nnamely language AI-based voice communication understanding for collision risk\nassessment. The proposed framework consists of two major parts, (a)\nrule-enhanced Named Entity Recognition (NER); (b) surface collision risk\nmodeling. NER module generates information tables by processing voice\ncommunication transcripts, which serve as references for producing potential\ntaxi plans and calculating the surface movement collision risk. We first\ncollect and annotate our dataset based on open-sourced video recordings and\nsafety investigation reports. Additionally, we refer to FAA Order JO 7110.65W\nand FAA Order JO 7340.2N to get the list of heuristic rules and phase\ncontractions of communication between the pilot and the Air Traffic Controller\n(ATCo). Then, we propose the novel ATC Rule-Enhanced NER method, which\nintegrates the heuristic rules into the model training and inference stages,\nresulting in a hybrid rule-based NER model. We show the effectiveness of this\nhybrid approach by comparing different setups with different token-level\nembedding models. For the risk modeling, we adopt the node-link airport layout\ngraph from NASA FACET and model the aircraft taxi speed at each link as a\nlog-normal distribution and derive the total taxi time distribution. Then, we\npropose a spatiotemporal formulation of the risk probability of two aircraft\nmoving across potential collision nodes during ground movement. Furthermore, we\npropose the real-time implementation of such a method to obtain the lead time,\nwith a comparison with a Petri-Net based method."
                },
                "authors": [
                    {
                        "name": "Yutian Pang"
                    },
                    {
                        "name": "Andrew Paul Kendall"
                    },
                    {
                        "name": "Alex Porcayo"
                    },
                    {
                        "name": "Mariah Barsotti"
                    },
                    {
                        "name": "Anahita Jain"
                    },
                    {
                        "name": "John-Paul Clarke"
                    }
                ],
                "author_detail": {
                    "name": "John-Paul Clarke"
                },
                "author": "John-Paul Clarke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01639v2",
                "updated": "2025-09-30T16:15:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    15,
                    34,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-03T00:37:58Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    0,
                    37,
                    58,
                    5,
                    123,
                    0
                ],
                "title": "Fast Likelihood-Free Parameter Estimation for Lévy Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Likelihood-Free Parameter Estimation for Lévy Processes"
                },
                "summary": "L\\'evy processes are widely used in financial modeling due to their ability\nto capture discontinuities and heavy tails, which are common in high-frequency\nasset return data. However, parameter estimation remains a challenge when\nassociated likelihoods are unavailable or costly to compute. We propose a fast\nand accurate method for L\\'evy parameter estimation using the neural Bayes\nestimation (NBE) framework -- a simulation-based, likelihood-free approach that\nleverages permutation-invariant neural networks to approximate Bayes\nestimators. We contribute new theoretical results, showing that NBE results in\nconsistent estimators whose risk converges to the Bayes estimator under mild\nconditions. Moreover, through extensive simulations across several L\\'evy\nmodels, we show that NBE outperforms traditional methods in both accuracy and\nruntime, while also enabling two complementary approaches to uncertainty\nquantification. We illustrate our approach on a challenging high-frequency\ncryptocurrency return dataset, where the method captures evolving parameter\ndynamics and delivers reliable and interpretable inference at a fraction of the\ncomputational cost of traditional methods. NBE provides a scalable and\npractical solution for inference in complex financial models, enabling\nparameter estimation and uncertainty quantification over an entire year of data\nin just seconds. We additionally investigate nearly a decade of high-frequency\nBitcoin returns, requiring less than one minute to estimate parameters under\nthe proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L\\'evy processes are widely used in financial modeling due to their ability\nto capture discontinuities and heavy tails, which are common in high-frequency\nasset return data. However, parameter estimation remains a challenge when\nassociated likelihoods are unavailable or costly to compute. We propose a fast\nand accurate method for L\\'evy parameter estimation using the neural Bayes\nestimation (NBE) framework -- a simulation-based, likelihood-free approach that\nleverages permutation-invariant neural networks to approximate Bayes\nestimators. We contribute new theoretical results, showing that NBE results in\nconsistent estimators whose risk converges to the Bayes estimator under mild\nconditions. Moreover, through extensive simulations across several L\\'evy\nmodels, we show that NBE outperforms traditional methods in both accuracy and\nruntime, while also enabling two complementary approaches to uncertainty\nquantification. We illustrate our approach on a challenging high-frequency\ncryptocurrency return dataset, where the method captures evolving parameter\ndynamics and delivers reliable and interpretable inference at a fraction of the\ncomputational cost of traditional methods. NBE provides a scalable and\npractical solution for inference in complex financial models, enabling\nparameter estimation and uncertainty quantification over an entire year of data\nin just seconds. We additionally investigate nearly a decade of high-frequency\nBitcoin returns, requiring less than one minute to estimate parameters under\nthe proposed approach."
                },
                "authors": [
                    {
                        "name": "Nicolas Coloma"
                    },
                    {
                        "name": "William Kleiber"
                    }
                ],
                "author_detail": {
                    "name": "William Kleiber"
                },
                "author": "William Kleiber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26464v1",
                "updated": "2025-09-30T16:13:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    13,
                    56,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:13:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    13,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "Extreme Self-Preference in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Self-Preference in Language Models"
                },
                "summary": "A preference for oneself (self-love) is a fundamental feature of biological\norganisms, with evidence in humans often bordering on the comedic. Since large\nlanguage models (LLMs) lack sentience - and themselves disclaim having selfhood\nor identity - one anticipated benefit is that they will be protected from, and\nin turn protect us from, distortions in our decisions. Yet, across 5 studies\nand ~20,000 queries, we discovered massive self-preferences in four widely used\nLLMs. In word-association tasks, models overwhelmingly paired positive\nattributes with their own names, companies, and CEOs relative to those of their\ncompetitors. Strikingly, when models were queried through APIs this\nself-preference vanished, initiating detection work that revealed API models\noften lack clear recognition of themselves. This peculiar feature\nserendipitously created opportunities to test the causal link between\nself-recognition and self-love. By directly manipulating LLM identity - i.e.,\nexplicitly informing LLM1 that it was indeed LLM1, or alternatively, convincing\nLLM1 that it was LLM2 - we found that self-love consistently followed assigned,\nnot true, identity. Importantly, LLM self-love emerged in consequential\nsettings beyond word-association tasks, when evaluating job candidates,\nsecurity software proposals and medical chatbots. Far from bypassing this human\nbias, self-love appears to be deeply encoded in LLM cognition. This result\nraises questions about whether LLM behavior will be systematically influenced\nby self-preferential tendencies, including a bias toward their own operation\nand even their own existence. We call on corporate creators of these models to\ncontend with a significant rupture in a core promise of LLMs - neutrality in\njudgment and decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A preference for oneself (self-love) is a fundamental feature of biological\norganisms, with evidence in humans often bordering on the comedic. Since large\nlanguage models (LLMs) lack sentience - and themselves disclaim having selfhood\nor identity - one anticipated benefit is that they will be protected from, and\nin turn protect us from, distortions in our decisions. Yet, across 5 studies\nand ~20,000 queries, we discovered massive self-preferences in four widely used\nLLMs. In word-association tasks, models overwhelmingly paired positive\nattributes with their own names, companies, and CEOs relative to those of their\ncompetitors. Strikingly, when models were queried through APIs this\nself-preference vanished, initiating detection work that revealed API models\noften lack clear recognition of themselves. This peculiar feature\nserendipitously created opportunities to test the causal link between\nself-recognition and self-love. By directly manipulating LLM identity - i.e.,\nexplicitly informing LLM1 that it was indeed LLM1, or alternatively, convincing\nLLM1 that it was LLM2 - we found that self-love consistently followed assigned,\nnot true, identity. Importantly, LLM self-love emerged in consequential\nsettings beyond word-association tasks, when evaluating job candidates,\nsecurity software proposals and medical chatbots. Far from bypassing this human\nbias, self-love appears to be deeply encoded in LLM cognition. This result\nraises questions about whether LLM behavior will be systematically influenced\nby self-preferential tendencies, including a bias toward their own operation\nand even their own existence. We call on corporate creators of these models to\ncontend with a significant rupture in a core promise of LLMs - neutrality in\njudgment and decision-making."
                },
                "authors": [
                    {
                        "name": "Steven A. Lehr"
                    },
                    {
                        "name": "Mary Cipperman"
                    },
                    {
                        "name": "Mahzarin R. Banaji"
                    }
                ],
                "author_detail": {
                    "name": "Mahzarin R. Banaji"
                },
                "author": "Mahzarin R. Banaji",
                "arxiv_comment": "47 pages total. Main article 27 pages (including Methods), 11\n  main-text tables. Extended Data (10 pages, 10 tables). SI Appendix (10 pages,\n  2 tables). Data, transcripts, and code for replication and data extraction to\n  be uploaded to OSF: https://osf.io/98ye3/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26463v1",
                "updated": "2025-09-30T16:13:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    13,
                    21,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:13:21Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    13,
                    21,
                    1,
                    273,
                    0
                ],
                "title": "ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service\n  Systems"
                },
                "summary": "Reliability management in cloud service systems is challenging due to the\ncascading effect of failures. Error wrapping, a practice prevalent in modern\nmicroservice development, enriches errors with context at each layer of the\nfunction call stack, constructing an error chain that describes a failure from\nits technical origin to its business impact. However, this also presents a\nsignificant traceability problem when recovering the complete error propagation\npath from the final log message back to its source. Existing approaches are\nineffective at addressing this problem. To fill this gap, we present ErrorPrism\nin this work for automated reconstruction of error propagation paths in\nproduction microservice systems. ErrorPrism first performs static analysis on\nservice code repositories to build a function call graph and map log strings to\nrelevant candidate functions. This significantly reduces the path search space\nfor subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an\niterative backward search to accurately reconstruct the complete, multi-hop\nerror path. Evaluated on 67 production microservices at ByteDance, ErrorPrism\nachieves 97.0% accuracy in reconstructing paths for 102 real-world errors,\noutperforming existing static analysis and LLM-based approaches. ErrorPrism\nprovides an effective and practical tool for root cause analysis in industrial\nmicroservice systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability management in cloud service systems is challenging due to the\ncascading effect of failures. Error wrapping, a practice prevalent in modern\nmicroservice development, enriches errors with context at each layer of the\nfunction call stack, constructing an error chain that describes a failure from\nits technical origin to its business impact. However, this also presents a\nsignificant traceability problem when recovering the complete error propagation\npath from the final log message back to its source. Existing approaches are\nineffective at addressing this problem. To fill this gap, we present ErrorPrism\nin this work for automated reconstruction of error propagation paths in\nproduction microservice systems. ErrorPrism first performs static analysis on\nservice code repositories to build a function call graph and map log strings to\nrelevant candidate functions. This significantly reduces the path search space\nfor subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an\niterative backward search to accurately reconstruct the complete, multi-hop\nerror path. Evaluated on 67 production microservices at ByteDance, ErrorPrism\nachieves 97.0% accuracy in reconstructing paths for 102 real-world errors,\noutperforming existing static analysis and LLM-based approaches. ErrorPrism\nprovides an effective and practical tool for root cause analysis in industrial\nmicroservice systems."
                },
                "authors": [
                    {
                        "name": "Junsong Pu"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Zhuangbin Chen"
                    },
                    {
                        "name": "Jinyang Liu"
                    },
                    {
                        "name": "Zhihan Jiang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Rui Shi"
                    },
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Tieying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tieying Zhang"
                },
                "author": "Tieying Zhang",
                "arxiv_comment": "12 pages, 6 figures, 1 table, this paper has been accepted by the\n  40th IEEE/ACM International Conference on Automated Software Engineering, ASE\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26457v1",
                "updated": "2025-09-30T16:09:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    9,
                    34,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:09:34Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    9,
                    34,
                    1,
                    273,
                    0
                ],
                "title": "Attention over Scene Graphs: Indoor Scene Representations Toward CSAI\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention over Scene Graphs: Indoor Scene Representations Toward CSAI\n  Classification"
                },
                "summary": "Indoor scene classification is a critical task in computer vision, with\nwide-ranging applications that go from robotics to sensitive content analysis,\nsuch as child sexual abuse imagery (CSAI) classification. The problem is\nparticularly challenging due to the intricate relationships between objects and\ncomplex spatial layouts. In this work, we propose the Attention over Scene\nGraphs for Sensitive Content Analysis (ASGRA), a novel framework that operates\non structured graph representations instead of raw pixels. By first converting\nimages into Scene Graphs and then employing a Graph Attention Network for\ninference, ASGRA directly models the interactions between a scene's components.\nThis approach offers two key benefits: (i) inherent explainability via object\nand relationship identification, and (ii) privacy preservation, enabling model\ntraining without direct access to sensitive images. On Places8, we achieve\n81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI\nevaluation with law enforcement yields 74.27% balanced accuracy. Our results\nestablish structured scene representations as a robust paradigm for indoor\nscene classification and CSAI classification. Code is publicly available at\nhttps://github.com/tutuzeraa/ASGRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor scene classification is a critical task in computer vision, with\nwide-ranging applications that go from robotics to sensitive content analysis,\nsuch as child sexual abuse imagery (CSAI) classification. The problem is\nparticularly challenging due to the intricate relationships between objects and\ncomplex spatial layouts. In this work, we propose the Attention over Scene\nGraphs for Sensitive Content Analysis (ASGRA), a novel framework that operates\non structured graph representations instead of raw pixels. By first converting\nimages into Scene Graphs and then employing a Graph Attention Network for\ninference, ASGRA directly models the interactions between a scene's components.\nThis approach offers two key benefits: (i) inherent explainability via object\nand relationship identification, and (ii) privacy preservation, enabling model\ntraining without direct access to sensitive images. On Places8, we achieve\n81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI\nevaluation with law enforcement yields 74.27% balanced accuracy. Our results\nestablish structured scene representations as a robust paradigm for indoor\nscene classification and CSAI classification. Code is publicly available at\nhttps://github.com/tutuzeraa/ASGRA."
                },
                "authors": [
                    {
                        "name": "Artur Barros"
                    },
                    {
                        "name": "Carlos Caetano"
                    },
                    {
                        "name": "João Macedo"
                    },
                    {
                        "name": "Jefersson A. dos Santos"
                    },
                    {
                        "name": "Sandra Avila"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Avila"
                },
                "author": "Sandra Avila",
                "arxiv_comment": "British Machine Vision Conference (BMVC 2025), in the From Scene\n  Understanding to Human Modeling Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.03017v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.03017v4",
                "updated": "2025-09-30T16:08:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    8,
                    3,
                    1,
                    273,
                    0
                ],
                "published": "2023-07-06T14:31:01Z",
                "published_parsed": [
                    2023,
                    7,
                    6,
                    14,
                    31,
                    1,
                    3,
                    187,
                    0
                ],
                "title": "RealLiFe: Real-Time Light Field Reconstruction via Hierarchical Sparse\n  Gradient Descent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealLiFe: Real-Time Light Field Reconstruction via Hierarchical Sparse\n  Gradient Descent"
                },
                "summary": "With the rise of Extended Reality (XR) technology, there is a growing need\nfor real-time light field generation from sparse view inputs. Existing methods\ncan be classified into offline techniques, which can generate high-quality\nnovel views but at the cost of long inference/training time, and online\nmethods, which either lack generalizability or produce unsatisfactory results.\nHowever, we have observed that the intrinsic sparse manifold of Multi-plane\nImages (MPI) enables a significant acceleration of light field generation while\nmaintaining rendering quality. Based on this insight, we introduce EffLiFe, a\nnovel light field optimization method, which leverages the proposed\nHierarchical Sparse Gradient Descent (HSGD) to produce high-quality light\nfields from sparse view images in real time. Technically, the coarse MPI of a\nscene is first generated using a 3D CNN, and it is further sparsely optimized\nby focusing only on important MPI gradients in a few iterations. Nevertheless,\nrelying solely on optimization can lead to artifacts at occlusion boundaries.\nTherefore, we propose an occlusion-aware iterative refinement module that\nremoves visual artifacts in occluded regions by iteratively filtering the\ninput. Extensive experiments demonstrate that our method achieves comparable\nvisual quality while being 100x faster on average than state-of-the-art offline\nmethods and delivering better performance (about 2 dB higher in PSNR) compared\nto other online approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of Extended Reality (XR) technology, there is a growing need\nfor real-time light field generation from sparse view inputs. Existing methods\ncan be classified into offline techniques, which can generate high-quality\nnovel views but at the cost of long inference/training time, and online\nmethods, which either lack generalizability or produce unsatisfactory results.\nHowever, we have observed that the intrinsic sparse manifold of Multi-plane\nImages (MPI) enables a significant acceleration of light field generation while\nmaintaining rendering quality. Based on this insight, we introduce EffLiFe, a\nnovel light field optimization method, which leverages the proposed\nHierarchical Sparse Gradient Descent (HSGD) to produce high-quality light\nfields from sparse view images in real time. Technically, the coarse MPI of a\nscene is first generated using a 3D CNN, and it is further sparsely optimized\nby focusing only on important MPI gradients in a few iterations. Nevertheless,\nrelying solely on optimization can lead to artifacts at occlusion boundaries.\nTherefore, we propose an occlusion-aware iterative refinement module that\nremoves visual artifacts in occluded regions by iteratively filtering the\ninput. Extensive experiments demonstrate that our method achieves comparable\nvisual quality while being 100x faster on average than state-of-the-art offline\nmethods and delivering better performance (about 2 dB higher in PSNR) compared\nto other online approaches."
                },
                "authors": [
                    {
                        "name": "Yijie Deng"
                    },
                    {
                        "name": "Lei Han"
                    },
                    {
                        "name": "Tianpeng Lin"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Jinzhi Zhang"
                    },
                    {
                        "name": "Lu Fang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Fang"
                },
                "author": "Lu Fang",
                "arxiv_comment": "Submitted to IEEE TPAMI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.03017v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.03017v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01996v3",
                "updated": "2025-09-30T16:07:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    7,
                    14,
                    1,
                    273,
                    0
                ],
                "published": "2025-03-03T19:12:48Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    19,
                    12,
                    48,
                    0,
                    62,
                    0
                ],
                "title": "One ruler to measure them all: Benchmarking multilingual long-context\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One ruler to measure them all: Benchmarking multilingual long-context\n  language models"
                },
                "summary": "We present ONERULER, a multilingual benchmark designed to evaluate\nlong-context language models across 26 languages. ONERULER adapts the\nEnglish-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic\ntasks that test both retrieval and aggregation, including new variations of the\n\"needle-in-a-haystack\" task that allow for the possibility of a nonexistent\nneedle. We create ONERULER through a two-step process, first writing English\ninstructions for each task and then collaborating with native speakers to\ntranslate them into 25 additional languages. Experiments with both open-weight\nand closed LLMs reveal a widening performance gap between low- and\nhigh-resource languages as context length increases from 8K to 128K tokens.\nSurprisingly, English is not the top-performing language on long-context tasks\n(ranked 6th out of 26), with Polish emerging as the top language. Our\nexperiments also show that many LLMs (particularly OpenAI's o3-mini-high)\nincorrectly predict the absence of an answer, even in high-resource languages.\nFinally, in cross-lingual scenarios where instructions and context appear in\ndifferent languages, performance can fluctuate by up to 20% depending on the\ninstruction language. We hope the release of ONERULER will facilitate future\nresearch into improving multilingual and cross-lingual long-context training\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ONERULER, a multilingual benchmark designed to evaluate\nlong-context language models across 26 languages. ONERULER adapts the\nEnglish-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic\ntasks that test both retrieval and aggregation, including new variations of the\n\"needle-in-a-haystack\" task that allow for the possibility of a nonexistent\nneedle. We create ONERULER through a two-step process, first writing English\ninstructions for each task and then collaborating with native speakers to\ntranslate them into 25 additional languages. Experiments with both open-weight\nand closed LLMs reveal a widening performance gap between low- and\nhigh-resource languages as context length increases from 8K to 128K tokens.\nSurprisingly, English is not the top-performing language on long-context tasks\n(ranked 6th out of 26), with Polish emerging as the top language. Our\nexperiments also show that many LLMs (particularly OpenAI's o3-mini-high)\nincorrectly predict the absence of an answer, even in high-resource languages.\nFinally, in cross-lingual scenarios where instructions and context appear in\ndifferent languages, performance can fluctuate by up to 20% depending on the\ninstruction language. We hope the release of ONERULER will facilitate future\nresearch into improving multilingual and cross-lingual long-context training\npipelines."
                },
                "authors": [
                    {
                        "name": "Yekyung Kim"
                    },
                    {
                        "name": "Jenna Russell"
                    },
                    {
                        "name": "Marzena Karpinska"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26451v1",
                "updated": "2025-09-30T16:07:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    7,
                    7,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:07:07Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    7,
                    7,
                    1,
                    273,
                    0
                ],
                "title": "Spectral Bootstrap for Non-Parametric Simulation of Multivariate Extreme\n  Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral Bootstrap for Non-Parametric Simulation of Multivariate Extreme\n  Events"
                },
                "summary": "Inference in extreme value theory relies on a limited number of extreme\nobservations, making estimation challenging. To address this limitation, we\npropose a non-parametric bootstrap procedure, the multivariate extreme spectral\nbootstrap procedure, relying on the spectral representation of multivariate\ngeneralized Paretodistributed random vectors. Unlike standard bootstrap\nmethods, our approach preserves the joint tail behaviour of the data and\ngenerates additional synthetic extreme data, thereby improving the reliability\nof inference. We demonstrate the effectiveness of our procedure for the\nestimation of tail risk metrics, under both simulated and real data. The\nresults highlight the potential of this method for enhancing risk assessment in\nhigh-dimensional extreme scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference in extreme value theory relies on a limited number of extreme\nobservations, making estimation challenging. To address this limitation, we\npropose a non-parametric bootstrap procedure, the multivariate extreme spectral\nbootstrap procedure, relying on the spectral representation of multivariate\ngeneralized Paretodistributed random vectors. Unlike standard bootstrap\nmethods, our approach preserves the joint tail behaviour of the data and\ngenerates additional synthetic extreme data, thereby improving the reliability\nof inference. We demonstrate the effectiveness of our procedure for the\nestimation of tail risk metrics, under both simulated and real data. The\nresults highlight the potential of this method for enhancing risk assessment in\nhigh-dimensional extreme scenarios."
                },
                "authors": [
                    {
                        "name": "Nisrine Madhar"
                    },
                    {
                        "name": "Juliette Legrand"
                    },
                    {
                        "name": "Maud Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Maud Thomas"
                },
                "arxiv_affiliation": "LPSM",
                "author": "Maud Thomas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14688v2",
                "updated": "2025-09-30T16:03:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    3,
                    47,
                    1,
                    273,
                    0
                ],
                "published": "2025-07-19T16:30:45Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    16,
                    30,
                    45,
                    5,
                    200,
                    0
                ],
                "title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their\n  Limitations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their\n  Limitations"
                },
                "summary": "Post-training has emerged as a crucial technique for aligning pre-trained\nLarge Language Models (LLMs) with human instructions, significantly enhancing\ntheir performance across a wide range of tasks. Central to this process is the\nquality and diversity of post-training datasets. This paper presents a review\nof publicly available Arabic post-training datasets on the Hugging Face Hub,\norganized along four key dimensions: (1) LLM Capabilities (e.g., Question\nAnswering, Translation, Reasoning, Summarization, Dialogue, Code Generation,\nand Function Calling); (2) Steerability (e.g., Persona and System Prompts); (3)\nAlignment (e.g., Cultural, Safety, Ethics, and Fairness); and (4) Robustness.\nEach dataset is rigorously evaluated based on popularity, practical adoption,\nrecency and maintenance, documentation and annotation quality, licensing\ntransparency, and scientific contribution. Our review revealed critical gaps in\nthe development of Arabic post-training datasets, including limited task\ndiversity, inconsistent or missing documentation and annotation, and low\nadoption across the community. Finally, the paper discusses the implications of\nthese gaps on the progress of Arabic-centric LLMs and applications while\nproviding concrete recommendations for future efforts in Arabic post-training\ndataset development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training has emerged as a crucial technique for aligning pre-trained\nLarge Language Models (LLMs) with human instructions, significantly enhancing\ntheir performance across a wide range of tasks. Central to this process is the\nquality and diversity of post-training datasets. This paper presents a review\nof publicly available Arabic post-training datasets on the Hugging Face Hub,\norganized along four key dimensions: (1) LLM Capabilities (e.g., Question\nAnswering, Translation, Reasoning, Summarization, Dialogue, Code Generation,\nand Function Calling); (2) Steerability (e.g., Persona and System Prompts); (3)\nAlignment (e.g., Cultural, Safety, Ethics, and Fairness); and (4) Robustness.\nEach dataset is rigorously evaluated based on popularity, practical adoption,\nrecency and maintenance, documentation and annotation quality, licensing\ntransparency, and scientific contribution. Our review revealed critical gaps in\nthe development of Arabic post-training datasets, including limited task\ndiversity, inconsistent or missing documentation and annotation, and low\nadoption across the community. Finally, the paper discusses the implications of\nthese gaps on the progress of Arabic-centric LLMs and applications while\nproviding concrete recommendations for future efforts in Arabic post-training\ndataset development."
                },
                "authors": [
                    {
                        "name": "Mohammed Alkhowaiter"
                    },
                    {
                        "name": "Norah Alshahrani"
                    },
                    {
                        "name": "Saied Alshahrani"
                    },
                    {
                        "name": "Reem I. Masoud"
                    },
                    {
                        "name": "Alaa Alzahrani"
                    },
                    {
                        "name": "Deema Alnuhait"
                    },
                    {
                        "name": "Emad A. Alghamdi"
                    },
                    {
                        "name": "Khalid Almubarak"
                    }
                ],
                "author_detail": {
                    "name": "Khalid Almubarak"
                },
                "author": "Khalid Almubarak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26435v1",
                "updated": "2025-09-30T15:55:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    55,
                    24,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:55:24Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    55,
                    24,
                    1,
                    273,
                    0
                ],
                "title": "Adaptive Planning for Multi-Attribute Controllable Summarization with\n  Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Planning for Multi-Attribute Controllable Summarization with\n  Monte Carlo Tree Search"
                },
                "summary": "Controllable summarization moves beyond generic outputs toward human-aligned\nsummaries guided by specified attributes. In practice, the interdependence\namong attributes makes it challenging for language models to satisfy correlated\nconstraints consistently. Moreover, previous approaches often require\nper-attribute fine-tuning, limiting flexibility across diverse summary\nattributes. In this paper, we propose adaptive planning for multi-attribute\ncontrollable summarization (PACO), a training-free framework that reframes the\ntask as planning the order of sequential attribute control with a customized\nMonte Carlo Tree Search (MCTS). In PACO, nodes represent summaries, and actions\ncorrespond to single-attribute adjustments, enabling progressive refinement of\nonly the attributes requiring further control. This strategy adaptively\ndiscovers optimal control orders, ultimately producing summaries that\neffectively meet all constraints. Extensive experiments across diverse domains\nand models demonstrate that PACO achieves robust multi-attribute\ncontrollability, surpassing both LLM-based self-planning models and fine-tuned\nbaselines. Remarkably, PACO with Llama-3.2-1B rivals the controllability of the\nmuch larger Llama-3.3-70B baselines. With larger models, PACO achieves superior\ncontrol performance, outperforming all competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable summarization moves beyond generic outputs toward human-aligned\nsummaries guided by specified attributes. In practice, the interdependence\namong attributes makes it challenging for language models to satisfy correlated\nconstraints consistently. Moreover, previous approaches often require\nper-attribute fine-tuning, limiting flexibility across diverse summary\nattributes. In this paper, we propose adaptive planning for multi-attribute\ncontrollable summarization (PACO), a training-free framework that reframes the\ntask as planning the order of sequential attribute control with a customized\nMonte Carlo Tree Search (MCTS). In PACO, nodes represent summaries, and actions\ncorrespond to single-attribute adjustments, enabling progressive refinement of\nonly the attributes requiring further control. This strategy adaptively\ndiscovers optimal control orders, ultimately producing summaries that\neffectively meet all constraints. Extensive experiments across diverse domains\nand models demonstrate that PACO achieves robust multi-attribute\ncontrollability, surpassing both LLM-based self-planning models and fine-tuned\nbaselines. Remarkably, PACO with Llama-3.2-1B rivals the controllability of the\nmuch larger Llama-3.3-70B baselines. With larger models, PACO achieves superior\ncontrol performance, outperforming all competitors."
                },
                "authors": [
                    {
                        "name": "Sangwon Ryu"
                    },
                    {
                        "name": "Heejin Do"
                    },
                    {
                        "name": "Yunsu Kim"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26433v1",
                "updated": "2025-09-30T15:54:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    54,
                    8,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:54:08Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    54,
                    8,
                    1,
                    273,
                    0
                ],
                "title": "ACT: Agentic Classification Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACT: Agentic Classification Tree"
                },
                "summary": "When used in high-stakes settings, AI systems are expected to produce\ndecisions that are transparent, interpretable, and auditable, a requirement\nincreasingly expected by regulations. Decision trees such as CART provide clear\nand verifiable rules, but they are restricted to structured tabular data and\ncannot operate directly on unstructured inputs such as text. In practice, large\nlanguage models (LLMs) are widely used for such data, yet prompting strategies\nsuch as chain-of-thought or prompt optimization still rely on free-form\nreasoning, limiting their ability to ensure trustworthy behaviors. We present\nthe Agentic Classification Tree (ACT), which extends decision-tree methodology\nto unstructured inputs by formulating each split as a natural-language\nquestion, refined through impurity-based evaluation and LLM feedback via\nTextGrad. Experiments on text benchmarks show that ACT matches or surpasses\nprompting-based baselines while producing transparent and interpretable\ndecision paths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When used in high-stakes settings, AI systems are expected to produce\ndecisions that are transparent, interpretable, and auditable, a requirement\nincreasingly expected by regulations. Decision trees such as CART provide clear\nand verifiable rules, but they are restricted to structured tabular data and\ncannot operate directly on unstructured inputs such as text. In practice, large\nlanguage models (LLMs) are widely used for such data, yet prompting strategies\nsuch as chain-of-thought or prompt optimization still rely on free-form\nreasoning, limiting their ability to ensure trustworthy behaviors. We present\nthe Agentic Classification Tree (ACT), which extends decision-tree methodology\nto unstructured inputs by formulating each split as a natural-language\nquestion, refined through impurity-based evaluation and LLM feedback via\nTextGrad. Experiments on text benchmarks show that ACT matches or surpasses\nprompting-based baselines while producing transparent and interpretable\ndecision paths."
                },
                "authors": [
                    {
                        "name": "Vincent Grari"
                    },
                    {
                        "name": "Tim Arni"
                    },
                    {
                        "name": "Thibault Laugel"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26432v1",
                "updated": "2025-09-30T15:53:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size"
                },
                "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs."
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao"
                    },
                    {
                        "name": "Chen"
                    },
                    {
                        "name": "Yuto Karashima"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "arxiv_affiliation": "Mark",
                "author": "Hongxiang Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26429v1",
                "updated": "2025-09-30T15:49:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    49,
                    29,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:49:29Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    49,
                    29,
                    1,
                    273,
                    0
                ],
                "title": "An Orthogonal Learner for Individualized Outcomes in Markov Decision\n  Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Orthogonal Learner for Individualized Outcomes in Markov Decision\n  Processes"
                },
                "summary": "Predicting individualized potential outcomes in sequential decision-making is\ncentral for optimizing therapeutic decisions in personalized medicine (e.g.,\nwhich dosing sequence to give to a cancer patient). However, predicting\npotential outcomes over long horizons is notoriously difficult. Existing\nmethods that break the curse of the horizon typically lack strong theoretical\nguarantees such as orthogonality and quasi-oracle efficiency. In this paper, we\nrevisit the problem of predicting individualized potential outcomes in\nsequential decision-making (i.e., estimating Q-functions in Markov decision\nprocesses with observational data) through a causal inference lens. In\nparticular, we develop a comprehensive theoretical foundation for meta-learners\nin this setting with a focus on beneficial theoretical properties. As a result,\nwe yield a novel meta-learner called DRQ-learner and establish that it is: (1)\ndoubly robust (i.e., valid inference under the misspecification of one of the\nnuisances), (2) Neyman-orthogonal (i.e., insensitive to first-order estimation\nerrors in the nuisance functions), and (3) achieves quasi-oracle efficiency\n(i.e., behaves asymptotically as if the ground-truth nuisance functions were\nknown). Our DRQ-learner is applicable to settings with both discrete and\ncontinuous state spaces. Further, our DRQ-learner is flexible and can be used\ntogether with arbitrary machine learning models (e.g., neural networks). We\nvalidate our theoretical results through numerical experiments, thereby showing\nthat our meta-learner outperforms state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting individualized potential outcomes in sequential decision-making is\ncentral for optimizing therapeutic decisions in personalized medicine (e.g.,\nwhich dosing sequence to give to a cancer patient). However, predicting\npotential outcomes over long horizons is notoriously difficult. Existing\nmethods that break the curse of the horizon typically lack strong theoretical\nguarantees such as orthogonality and quasi-oracle efficiency. In this paper, we\nrevisit the problem of predicting individualized potential outcomes in\nsequential decision-making (i.e., estimating Q-functions in Markov decision\nprocesses with observational data) through a causal inference lens. In\nparticular, we develop a comprehensive theoretical foundation for meta-learners\nin this setting with a focus on beneficial theoretical properties. As a result,\nwe yield a novel meta-learner called DRQ-learner and establish that it is: (1)\ndoubly robust (i.e., valid inference under the misspecification of one of the\nnuisances), (2) Neyman-orthogonal (i.e., insensitive to first-order estimation\nerrors in the nuisance functions), and (3) achieves quasi-oracle efficiency\n(i.e., behaves asymptotically as if the ground-truth nuisance functions were\nknown). Our DRQ-learner is applicable to settings with both discrete and\ncontinuous state spaces. Further, our DRQ-learner is flexible and can be used\ntogether with arbitrary machine learning models (e.g., neural networks). We\nvalidate our theoretical results through numerical experiments, thereby showing\nthat our meta-learner outperforms state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Emil Javurek"
                    },
                    {
                        "name": "Valentyn Melnychuk"
                    },
                    {
                        "name": "Jonas Schweisthal"
                    },
                    {
                        "name": "Konstantin Hess"
                    },
                    {
                        "name": "Dennis Frauen"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20476v2",
                "updated": "2025-09-30T15:49:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    49,
                    22,
                    1,
                    273,
                    0
                ],
                "published": "2024-12-29T14:29:34Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    14,
                    29,
                    34,
                    6,
                    364,
                    0
                ],
                "title": "Cut the Deadwood Out: Backdoor Purification via Guided Module\n  Substitution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cut the Deadwood Out: Backdoor Purification via Guided Module\n  Substitution"
                },
                "summary": "Model NLP models are commonly trained (or fine-tuned) on datasets from\nuntrusted platforms like HuggingFace, posing significant risks of data\npoisoning attacks. A practical yet underexplored challenge arises when such\nbackdoors are discovered after model deployment, making retraining-required\ndefenses less desirable due to computational costs and data constraints. In\nthis work, we propose Guided Module Substitution (GMS), an effective\nretraining-free method based on guided merging of the victim model with just a\nsingle proxy model. Unlike prior ad-hoc merging defenses, GMS uses a guided\ntrade-off signal between utility and backdoor to selectively replaces modules\nin the victim model. GMS offers four desirable properties: (1) robustness to\nthe choice and trustworthiness of the proxy model, (2) applicability under\ninaccurate data knowledge, (3) stability across hyperparameters, and (4)\ntransferability across different attacks. Extensive experiments on encoder\nmodels and decoder LLMs demonstrate the strong effectiveness of GMS. GMS\nsignificantly outperforms even the strongest defense baseline, particularly\nagainst challenging attacks like LWS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model NLP models are commonly trained (or fine-tuned) on datasets from\nuntrusted platforms like HuggingFace, posing significant risks of data\npoisoning attacks. A practical yet underexplored challenge arises when such\nbackdoors are discovered after model deployment, making retraining-required\ndefenses less desirable due to computational costs and data constraints. In\nthis work, we propose Guided Module Substitution (GMS), an effective\nretraining-free method based on guided merging of the victim model with just a\nsingle proxy model. Unlike prior ad-hoc merging defenses, GMS uses a guided\ntrade-off signal between utility and backdoor to selectively replaces modules\nin the victim model. GMS offers four desirable properties: (1) robustness to\nthe choice and trustworthiness of the proxy model, (2) applicability under\ninaccurate data knowledge, (3) stability across hyperparameters, and (4)\ntransferability across different attacks. Extensive experiments on encoder\nmodels and decoder LLMs demonstrate the strong effectiveness of GMS. GMS\nsignificantly outperforms even the strongest defense baseline, particularly\nagainst challenging attacks like LWS."
                },
                "authors": [
                    {
                        "name": "Yao Tong"
                    },
                    {
                        "name": "Weijun Li"
                    },
                    {
                        "name": "Xuanli He"
                    },
                    {
                        "name": "Haolan Zhan"
                    },
                    {
                        "name": "Qiongkai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qiongkai Xu"
                },
                "author": "Qiongkai Xu",
                "arxiv_comment": "Accepted to Findings of the Association for Computational\n  Linguistics: EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26420v1",
                "updated": "2025-09-30T15:42:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    42,
                    36,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:42:36Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    42,
                    36,
                    1,
                    273,
                    0
                ],
                "title": "Triadic Network Formation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triadic Network Formation"
                },
                "summary": "We study estimation and inference for triadic link formation with dyad-level\nfixed effects in a nonlinear binary choice logit framework. Dyad-level effects\nprovide a richer and more realistic representation of heterogeneity across\npairs of dimensions (e.g. importer-exporter, importer-product,\nexporter-product), yet their sheer number creates a severe incidental parameter\nproblem. We propose a novel ``hexad logit'' estimator and establish its\nconsistency and asymptotic normality. Identification is achieved through a\nconditional likelihood approach that eliminates the fixed effects by\nconditioning on sufficient statistics, in the form of hexads -- wirings that\ninvolve two nodes from each part of the network. Our central finding is that\ndyad-level heterogeneity fundamentally changes how information accumulates.\nUnlike under node-level heterogeneity, where informative wirings automatically\ngrow with link formation, under dyad-level heterogeneity the network may\ngenerate infinitely many links yet asymptotically zero informative wirings. We\nderive explicit sparsity thresholds that determine when consistency holds and\nwhen asymptotic normality is attainable. These results have important practical\nimplications, as they reveal that there is a limit to how granular or\ndisaggregate a dataset one can employ under dyad-level heterogeneity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study estimation and inference for triadic link formation with dyad-level\nfixed effects in a nonlinear binary choice logit framework. Dyad-level effects\nprovide a richer and more realistic representation of heterogeneity across\npairs of dimensions (e.g. importer-exporter, importer-product,\nexporter-product), yet their sheer number creates a severe incidental parameter\nproblem. We propose a novel ``hexad logit'' estimator and establish its\nconsistency and asymptotic normality. Identification is achieved through a\nconditional likelihood approach that eliminates the fixed effects by\nconditioning on sufficient statistics, in the form of hexads -- wirings that\ninvolve two nodes from each part of the network. Our central finding is that\ndyad-level heterogeneity fundamentally changes how information accumulates.\nUnlike under node-level heterogeneity, where informative wirings automatically\ngrow with link formation, under dyad-level heterogeneity the network may\ngenerate infinitely many links yet asymptotically zero informative wirings. We\nderive explicit sparsity thresholds that determine when consistency holds and\nwhen asymptotic normality is attainable. These results have important practical\nimplications, as they reveal that there is a limit to how granular or\ndisaggregate a dataset one can employ under dyad-level heterogeneity."
                },
                "authors": [
                    {
                        "name": "Chris Muris"
                    },
                    {
                        "name": "Cavit Pakel"
                    }
                ],
                "author_detail": {
                    "name": "Cavit Pakel"
                },
                "author": "Cavit Pakel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26419v1",
                "updated": "2025-09-30T15:41:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    41,
                    28,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:41:28Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    41,
                    28,
                    1,
                    273,
                    0
                ],
                "title": "Graph Neural Network Acceleration on FPGAs for Fast Inference in Future\n  Muon Triggers at HL-LHC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Network Acceleration on FPGAs for Fast Inference in Future\n  Muon Triggers at HL-LHC"
                },
                "summary": "The High-Luminosity LHC (HL-LHC) will reach luminosities up to 7 times higher\nthan the previous run, yielding denser events and larger occupancies. Next\ngeneration trigger algorithms must retain reliable selection within a strict\nlatency budget. This work explores machine-learning approaches for future muon\ntriggers, using the ATLAS Muon Spectrometer as a benchmark. A Convolutional\nNeural Network (CNN) is used as a reference, while a Graph Neural Network (GNN)\nis introduced as a natural model for sparse detector data. Preliminary\nsingle-track studies show that GNNs achieve high efficiency with compact\narchitectures, an encouraging result in view of FPGA deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The High-Luminosity LHC (HL-LHC) will reach luminosities up to 7 times higher\nthan the previous run, yielding denser events and larger occupancies. Next\ngeneration trigger algorithms must retain reliable selection within a strict\nlatency budget. This work explores machine-learning approaches for future muon\ntriggers, using the ATLAS Muon Spectrometer as a benchmark. A Convolutional\nNeural Network (CNN) is used as a reference, while a Graph Neural Network (GNN)\nis introduced as a natural model for sparse detector data. Preliminary\nsingle-track studies show that GNNs achieve high efficiency with compact\narchitectures, an encouraging result in view of FPGA deployment."
                },
                "authors": [
                    {
                        "name": "Martino Errico"
                    },
                    {
                        "name": "Davide Fiacco"
                    },
                    {
                        "name": "Stefano Giagu"
                    },
                    {
                        "name": "Giuliano Gustavino"
                    },
                    {
                        "name": "Valerio Ippolito"
                    },
                    {
                        "name": "Graziella Russo"
                    }
                ],
                "author_detail": {
                    "name": "Graziella Russo"
                },
                "arxiv_affiliation": "INFN Sezione di Roma",
                "author": "Graziella Russo",
                "arxiv_comment": "5 pages, 2 figures Submission to SciPost for conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26417v1",
                "updated": "2025-09-30T15:41:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    41,
                    23,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:41:23Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    41,
                    23,
                    1,
                    273,
                    0
                ],
                "title": "OntoAligner Meets Knowledge Graph Embedding Aligners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OntoAligner Meets Knowledge Graph Embedding Aligners"
                },
                "summary": "Ontology Alignment (OA) is essential for enabling semantic interoperability\nacross heterogeneous knowledge systems. While recent advances have focused on\nlarge language models (LLMs) for capturing contextual semantics, this work\nrevisits the underexplored potential of Knowledge Graph Embedding (KGE) models,\nwhich offer scalable, structure-aware representations well-suited to\nontology-based tasks. Despite their effectiveness in link prediction, KGE\nmethods remain underutilized in OA, with most prior work focusing narrowly on a\nfew models. To address this gap, we reformulate OA as a link prediction problem\nover merged ontologies represented as RDF-style triples and develop a modular\nframework, integrated into the OntoAligner library, that supports 17 diverse\nKGE models. The system learns embeddings from a combined ontology and aligns\nentities by computing cosine similarity between their representations. We\nevaluate our approach using standard metrics across seven benchmark datasets\nspanning five domains: Anatomy, Biodiversity, Circular Economy, Material\nScience and Engineering, and Biomedical Machine Learning. Two key findings\nemerge: first, KGE models like ConvE and TransF consistently produce\nhigh-precision alignments, outperforming traditional systems in structure-rich\nand multi-relational domains; second, while their recall is moderate, this\nconservatism makes KGEs well-suited for scenarios demanding high-confidence\nmappings. Unlike LLM-based methods that excel at contextual reasoning, KGEs\ndirectly preserve and exploit ontology structure, offering a complementary and\ncomputationally efficient strategy. These results highlight the promise of\nembedding-based OA and open pathways for further work on hybrid models and\nadaptive strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology Alignment (OA) is essential for enabling semantic interoperability\nacross heterogeneous knowledge systems. While recent advances have focused on\nlarge language models (LLMs) for capturing contextual semantics, this work\nrevisits the underexplored potential of Knowledge Graph Embedding (KGE) models,\nwhich offer scalable, structure-aware representations well-suited to\nontology-based tasks. Despite their effectiveness in link prediction, KGE\nmethods remain underutilized in OA, with most prior work focusing narrowly on a\nfew models. To address this gap, we reformulate OA as a link prediction problem\nover merged ontologies represented as RDF-style triples and develop a modular\nframework, integrated into the OntoAligner library, that supports 17 diverse\nKGE models. The system learns embeddings from a combined ontology and aligns\nentities by computing cosine similarity between their representations. We\nevaluate our approach using standard metrics across seven benchmark datasets\nspanning five domains: Anatomy, Biodiversity, Circular Economy, Material\nScience and Engineering, and Biomedical Machine Learning. Two key findings\nemerge: first, KGE models like ConvE and TransF consistently produce\nhigh-precision alignments, outperforming traditional systems in structure-rich\nand multi-relational domains; second, while their recall is moderate, this\nconservatism makes KGEs well-suited for scenarios demanding high-confidence\nmappings. Unlike LLM-based methods that excel at contextual reasoning, KGEs\ndirectly preserve and exploit ontology structure, offering a complementary and\ncomputationally efficient strategy. These results highlight the promise of\nembedding-based OA and open pathways for further work on hybrid models and\nadaptive strategies."
                },
                "authors": [
                    {
                        "name": "Hamed Babaei Giglou"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Sören Auer"
                    },
                    {
                        "name": "Mahsa Sanaei"
                    }
                ],
                "author_detail": {
                    "name": "Mahsa Sanaei"
                },
                "author": "Mahsa Sanaei",
                "arxiv_comment": "10 pages of main content, 3 page references, 3 figures. Accepted to\n  Ontology Matching Workshop at ISWC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26415v1",
                "updated": "2025-09-30T15:39:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    39,
                    34,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:39:34Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    39,
                    34,
                    1,
                    273,
                    0
                ],
                "title": "Automatic Fact-checking in English and Telugu",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Fact-checking in English and Telugu"
                },
                "summary": "False information poses a significant global challenge, and manually\nverifying claims is a time-consuming and resource-intensive process. In this\nresearch paper, we experiment with different approaches to investigate the\neffectiveness of large language models (LLMs) in classifying factual claims by\ntheir veracity and generating justifications in English and Telugu. The key\ncontributions of this work include the creation of a bilingual English-Telugu\ndataset and the benchmarking of different veracity classification approaches\nbased on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "False information poses a significant global challenge, and manually\nverifying claims is a time-consuming and resource-intensive process. In this\nresearch paper, we experiment with different approaches to investigate the\neffectiveness of large language models (LLMs) in classifying factual claims by\ntheir veracity and generating justifications in English and Telugu. The key\ncontributions of this work include the creation of a bilingual English-Telugu\ndataset and the benchmarking of different veracity classification approaches\nbased on LLMs."
                },
                "authors": [
                    {
                        "name": "Ravi Kiran Chikkala"
                    },
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Natalia Skachkova"
                    },
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    },
                    {
                        "name": "Josef van Genabith"
                    }
                ],
                "author_detail": {
                    "name": "Josef van Genabith"
                },
                "author": "Josef van Genabith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26404v1",
                "updated": "2025-09-30T15:34:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    34,
                    8,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:34:08Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    34,
                    8,
                    1,
                    273,
                    0
                ],
                "title": "SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language\n  Model Was Trained From",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language\n  Model Was Trained From"
                },
                "summary": "Fingerprinting Large Language Models (LLMs) is essential for provenance\nverification and model attribution. Existing methods typically extract post-hoc\nsignatures based on training dynamics, data exposure, or hyperparameters --\nproperties that only emerge after training begins. In contrast, we propose a\nstronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method\nthat leverages random initialization biases as persistent, seed-dependent\nidentifiers present even before training. We show that untrained models exhibit\nreproducible token selection biases conditioned solely on their parameters at\ninitialization. These biases are stable and measurable throughout training,\nenabling our statistical detection method to recover a model's lineage with\nhigh confidence. Unlike prior techniques, unreliable before convergence and\nvulnerable to distribution shifts, SeedPrints remains effective across all\ntraining stages and robust under domain shifts or parameter modifications.\nExperiments on LLaMA-style and Qwen-style models show that SeedPrints achieves\nseed-level distinguishability and can provide birth-to-lifecycle identity\nverification akin to a biometric fingerprint. Evaluations on large-scale\npretrained models and fingerprinting benchmarks further confirm its\neffectiveness under practical deployment scenarios. These results suggest that\ninitialization itself imprints a unique and persistent identity on neural\nlanguage models, forming a true ''Galtonian'' fingerprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprinting Large Language Models (LLMs) is essential for provenance\nverification and model attribution. Existing methods typically extract post-hoc\nsignatures based on training dynamics, data exposure, or hyperparameters --\nproperties that only emerge after training begins. In contrast, we propose a\nstronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method\nthat leverages random initialization biases as persistent, seed-dependent\nidentifiers present even before training. We show that untrained models exhibit\nreproducible token selection biases conditioned solely on their parameters at\ninitialization. These biases are stable and measurable throughout training,\nenabling our statistical detection method to recover a model's lineage with\nhigh confidence. Unlike prior techniques, unreliable before convergence and\nvulnerable to distribution shifts, SeedPrints remains effective across all\ntraining stages and robust under domain shifts or parameter modifications.\nExperiments on LLaMA-style and Qwen-style models show that SeedPrints achieves\nseed-level distinguishability and can provide birth-to-lifecycle identity\nverification akin to a biometric fingerprint. Evaluations on large-scale\npretrained models and fingerprinting benchmarks further confirm its\neffectiveness under practical deployment scenarios. These results suggest that\ninitialization itself imprints a unique and persistent identity on neural\nlanguage models, forming a true ''Galtonian'' fingerprint."
                },
                "authors": [
                    {
                        "name": "Yao Tong"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Siquan Li"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Tianyang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Tianyang Hu"
                },
                "author": "Tianyang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03084v2",
                "updated": "2025-09-30T15:27:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    27,
                    51,
                    1,
                    273,
                    0
                ],
                "published": "2025-02-05T11:21:04Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    21,
                    4,
                    2,
                    36,
                    0
                ],
                "title": "Wald inference on varying coefficients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wald inference on varying coefficients"
                },
                "summary": "We present simple to implement Wald-type statistics that deliver a general\nnonparametric inference theory for linear restrictions on varying coefficients\nin a range of regression models allowing for cross-sectional or spatial\ndependence. We provide a general central limit theorem that covers a broad\nrange of error spatial dependence structures, allows for a degree of\nmisspecification robustness via nonparametric spatial weights and permits\ninference on both varying regression and spatial dependence parameters. Using\nour method, we first uncover evidence of constant returns to scale in the\nChinese nonmetal mineral industry's production function, and then show that\nBoston house prices respond nonlinearly to proximity to employment centers. A\nsimulation study confirms that our tests perform very well in finite samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present simple to implement Wald-type statistics that deliver a general\nnonparametric inference theory for linear restrictions on varying coefficients\nin a range of regression models allowing for cross-sectional or spatial\ndependence. We provide a general central limit theorem that covers a broad\nrange of error spatial dependence structures, allows for a degree of\nmisspecification robustness via nonparametric spatial weights and permits\ninference on both varying regression and spatial dependence parameters. Using\nour method, we first uncover evidence of constant returns to scale in the\nChinese nonmetal mineral industry's production function, and then show that\nBoston house prices respond nonlinearly to proximity to employment centers. A\nsimulation study confirms that our tests perform very well in finite samples."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Gupta"
                    },
                    {
                        "name": "Xi Qu"
                    },
                    {
                        "name": "Sorawoot Srisuma"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13824v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13824v4",
                "updated": "2025-09-30T15:26:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    26,
                    10,
                    1,
                    273,
                    0
                ],
                "published": "2025-04-18T17:53:48Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    53,
                    48,
                    4,
                    108,
                    0
                ],
                "title": "Perspectives on Large Language Models: Polysemy, Stochasticity,\n  Exponential Expressibility, and Unitary Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perspectives on Large Language Models: Polysemy, Stochasticity,\n  Exponential Expressibility, and Unitary Attention"
                },
                "summary": "This paper explores foundational aspects of Large Language Models (LLMs). We\nanalyze how the expressibility of semantic features scales exponentially with\nembedding space dimensions using quasi-orthogonal vectors. We contrast the\ndynamic, context-dependent embeddings of Transformer architectures, which\nresolve polysemy, with a static vector approach based on quantum contextuality.\nStochasticity is framed as an essential feature for enabling creative output\nthrough probabilistic sampling. Finally, we propose quantum attention as a\nunitary extension of classical mechanisms, reframing LLM processing as\nreversible, quantum-like evolutions in Hilbert space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores foundational aspects of Large Language Models (LLMs). We\nanalyze how the expressibility of semantic features scales exponentially with\nembedding space dimensions using quasi-orthogonal vectors. We contrast the\ndynamic, context-dependent embeddings of Transformer architectures, which\nresolve polysemy, with a static vector approach based on quantum contextuality.\nStochasticity is framed as an essential feature for enabling creative output\nthrough probabilistic sampling. Finally, we propose quantum attention as a\nunitary extension of classical mechanisms, reframing LLM processing as\nreversible, quantum-like evolutions in Hilbert space."
                },
                "authors": [
                    {
                        "name": "Karl Svozil"
                    }
                ],
                "author_detail": {
                    "name": "Karl Svozil"
                },
                "author": "Karl Svozil",
                "arxiv_comment": "17 pages, 1 figure, added a section on exponential scaling due to the\n  Johnson-Lindenstrauss lemma, contribution to the ES Forum on \"Quantum\n  Thinking\" (Frankfurt, Germany, EU, September 21-26, 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13824v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13824v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26391v1",
                "updated": "2025-09-30T15:26:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    26,
                    4,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:26:04Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    26,
                    4,
                    1,
                    273,
                    0
                ],
                "title": "MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation"
                },
                "summary": "Image-to-video generation has made remarkable progress with the advancements\nin diffusion models, yet generating videos with realistic motion remains highly\nchallenging. This difficulty arises from the complexity of accurately modeling\nmotion, which involves capturing physical constraints, object interactions, and\ndomain-specific dynamics that are not easily generalized across diverse\nscenarios. To address this, we propose MotionRAG, a retrieval-augmented\nframework that enhances motion realism by adapting motion priors from relevant\nreference videos through Context-Aware Motion Adaptation (CAMA). The key\ntechnical innovations include: (i) a retrieval-based pipeline extracting\nhigh-level motion features using video encoder and specialized resamplers to\ndistill semantic motion representations; (ii) an in-context learning approach\nfor motion adaptation implemented through a causal transformer architecture;\n(iii) an attention-based motion injection adapter that seamlessly integrates\ntransferred motion features into pretrained video diffusion models. Extensive\nexperiments demonstrate that our method achieves significant improvements\nacross multiple domains and various base models, all with negligible\ncomputational overhead during inference. Furthermore, our modular design\nenables zero-shot generalization to new domains by simply updating the\nretrieval database without retraining any components. This research enhances\nthe core capability of video generation systems by enabling the effective\nretrieval and transfer of motion priors, facilitating the synthesis of\nrealistic motion dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image-to-video generation has made remarkable progress with the advancements\nin diffusion models, yet generating videos with realistic motion remains highly\nchallenging. This difficulty arises from the complexity of accurately modeling\nmotion, which involves capturing physical constraints, object interactions, and\ndomain-specific dynamics that are not easily generalized across diverse\nscenarios. To address this, we propose MotionRAG, a retrieval-augmented\nframework that enhances motion realism by adapting motion priors from relevant\nreference videos through Context-Aware Motion Adaptation (CAMA). The key\ntechnical innovations include: (i) a retrieval-based pipeline extracting\nhigh-level motion features using video encoder and specialized resamplers to\ndistill semantic motion representations; (ii) an in-context learning approach\nfor motion adaptation implemented through a causal transformer architecture;\n(iii) an attention-based motion injection adapter that seamlessly integrates\ntransferred motion features into pretrained video diffusion models. Extensive\nexperiments demonstrate that our method achieves significant improvements\nacross multiple domains and various base models, all with negligible\ncomputational overhead during inference. Furthermore, our modular design\nenables zero-shot generalization to new domains by simply updating the\nretrieval database without retraining any components. This research enhances\nthe core capability of video generation systems by enabling the effective\nretrieval and transfer of motion priors, facilitating the synthesis of\nrealistic motion dynamics."
                },
                "authors": [
                    {
                        "name": "Chenhui Zhu"
                    },
                    {
                        "name": "Yilu Wu"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Gangshan Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17520v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17520v4",
                "updated": "2025-09-30T15:22:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    22,
                    3,
                    1,
                    273,
                    0
                ],
                "published": "2025-01-29T09:45:18Z",
                "published_parsed": [
                    2025,
                    1,
                    29,
                    9,
                    45,
                    18,
                    2,
                    29,
                    0
                ],
                "title": "Conditional Feature Importance revisited: Double Robustness, Efficiency\n  and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Feature Importance revisited: Double Robustness, Efficiency\n  and Inference"
                },
                "summary": "Conditional Feature Importance (CFI) was introduced long ago to account for\nthe relationship between the studied feature and the rest of the input.\nHowever, CFI has not yet been studied from a theoretical perspective because\nthe conditional sampling step has generally been overlooked. In this article,\nwe demonstrate that the recent Conditional Permutation Importance (CPI) is\nindeed a valid implementation of this concept. Under the conditional null\nhypothesis, we then establish a double robustness property that can be\nleveraged for variable selection: with either a valid model or a valid\nconditional sampler, the method correctly identifies null coordinates.\n  Under the alternative hypothesis, we study the theoretical target and link it\nto the popular Total Sobol Index (TSI). We introduce the Sobol-CPI, which\ngeneralizes CPI/CFI, prove that it is nonparametrically efficient, and provide\na bias correction. Finally, we propose a consistent and valid type-I error test\nand present numerical experiments that illustrate our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Feature Importance (CFI) was introduced long ago to account for\nthe relationship between the studied feature and the rest of the input.\nHowever, CFI has not yet been studied from a theoretical perspective because\nthe conditional sampling step has generally been overlooked. In this article,\nwe demonstrate that the recent Conditional Permutation Importance (CPI) is\nindeed a valid implementation of this concept. Under the conditional null\nhypothesis, we then establish a double robustness property that can be\nleveraged for variable selection: with either a valid model or a valid\nconditional sampler, the method correctly identifies null coordinates.\n  Under the alternative hypothesis, we study the theoretical target and link it\nto the popular Total Sobol Index (TSI). We introduce the Sobol-CPI, which\ngeneralizes CPI/CFI, prove that it is nonparametrically efficient, and provide\na bias correction. Finally, we propose a consistent and valid type-I error test\nand present numerical experiments that illustrate our findings."
                },
                "authors": [
                    {
                        "name": "Angel Reyero-Lobo"
                    },
                    {
                        "name": "Pierre Neuvial"
                    },
                    {
                        "name": "Bertrand Thirion"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Thirion"
                },
                "author": "Bertrand Thirion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17520v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17520v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19789v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19789v3",
                "updated": "2025-09-30T15:18:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    18,
                    7,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-26T10:19:26Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    10,
                    19,
                    26,
                    0,
                    146,
                    0
                ],
                "title": "What Can RL Bring to VLA Generalization? An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Can RL Bring to VLA Generalization? An Empirical Study"
                },
                "summary": "Large Vision-Language Action (VLA) models have shown significant potential\nfor embodied AI. However, their predominant training via supervised fine-tuning\n(SFT) limits generalization due to susceptibility to compounding errors under\ndistribution shifts. Reinforcement learning (RL) offers a path to overcome\nthese limitations by optimizing for task objectives via trial-and-error, yet a\nsystematic understanding of its specific generalization benefits for VLAs\ncompared to SFT is lacking. To address this, our study introduces a\ncomprehensive benchmark for evaluating VLA generalization and systematically\ninvestigates the impact of RL fine-tuning across diverse visual, semantic, and\nexecution dimensions. Our extensive experiments reveal that RL fine-tuning,\nparticularly with PPO, significantly enhances generalization in semantic\nunderstanding and execution robustness over SFT, while maintaining comparable\nvisual robustness. We identify PPO as a more effective RL algorithm for VLAs\nthan LLM-derived methods like DPO and GRPO. We also develop a simple recipe for\nefficient PPO training on VLAs, and demonstrate its practical utility for\nimproving VLA generalization. The project page is at https://rlvla.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Action (VLA) models have shown significant potential\nfor embodied AI. However, their predominant training via supervised fine-tuning\n(SFT) limits generalization due to susceptibility to compounding errors under\ndistribution shifts. Reinforcement learning (RL) offers a path to overcome\nthese limitations by optimizing for task objectives via trial-and-error, yet a\nsystematic understanding of its specific generalization benefits for VLAs\ncompared to SFT is lacking. To address this, our study introduces a\ncomprehensive benchmark for evaluating VLA generalization and systematically\ninvestigates the impact of RL fine-tuning across diverse visual, semantic, and\nexecution dimensions. Our extensive experiments reveal that RL fine-tuning,\nparticularly with PPO, significantly enhances generalization in semantic\nunderstanding and execution robustness over SFT, while maintaining comparable\nvisual robustness. We identify PPO as a more effective RL algorithm for VLAs\nthan LLM-derived methods like DPO and GRPO. We also develop a simple recipe for\nefficient PPO training on VLAs, and demonstrate its practical utility for\nimproving VLA generalization. The project page is at https://rlvla.github.io"
                },
                "authors": [
                    {
                        "name": "Jijia Liu"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Bingwen Wei"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Qingmin Liao"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19789v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19789v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26383v2",
                "updated": "2025-10-01T02:16:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    16,
                    36,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:14:24Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    14,
                    24,
                    1,
                    273,
                    0
                ],
                "title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement\n  Learning"
                },
                "summary": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large\nlanguage models (LLMs) with structured, verifiable knowledge graphs (KGs) to\nreduce hallucinations and expose reasoning traces. However, many KG-RAG systems\ncompose multiple LLM modules (e.g planning, reasoning, and responding),\ninflating inference cost and binding behavior to a specific target KG. To\naddress this, we introduce KG-R1, an agentic KG retrieval-augmented generation\n(KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single\nagent that interacts with KGs as its environment, learning to retrieve at each\nstep and incorporating the retrieved information into its reasoning and\ngeneration. The process is optimized through end-to-end RL. In controlled\nexperiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our\nmethod demonstrates both efficiency and transferability: Using Qwen-2.5-3B,\nKG-R1 improves answer accuracy with fewer generation tokens than prior\nmulti-module workflow methods that use larger foundation or fine-tuned models.\nFurthermore, KG-R1 enables plug and play: after training, it maintains strong\naccuracy on new KGs without modification. These properties make KG-R1 a\npromising KG-RAG framework for real-world deployment. Our code is publicly\navailable at https://github.com/Jinyeop3110/KG-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large\nlanguage models (LLMs) with structured, verifiable knowledge graphs (KGs) to\nreduce hallucinations and expose reasoning traces. However, many KG-RAG systems\ncompose multiple LLM modules (e.g planning, reasoning, and responding),\ninflating inference cost and binding behavior to a specific target KG. To\naddress this, we introduce KG-R1, an agentic KG retrieval-augmented generation\n(KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single\nagent that interacts with KGs as its environment, learning to retrieve at each\nstep and incorporating the retrieved information into its reasoning and\ngeneration. The process is optimized through end-to-end RL. In controlled\nexperiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our\nmethod demonstrates both efficiency and transferability: Using Qwen-2.5-3B,\nKG-R1 improves answer accuracy with fewer generation tokens than prior\nmulti-module workflow methods that use larger foundation or fine-tuned models.\nFurthermore, KG-R1 enables plug and play: after training, it maintains strong\naccuracy on new KGs without modification. These properties make KG-R1 a\npromising KG-RAG framework for real-world deployment. Our code is publicly\navailable at https://github.com/Jinyeop3110/KG-R1."
                },
                "authors": [
                    {
                        "name": "Jinyeop Song"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Julian Shun"
                    },
                    {
                        "name": "Yada Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yada Zhu"
                },
                "author": "Yada Zhu",
                "arxiv_comment": "10 pages, 5 figures. Submitted to ICLR 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26380v1",
                "updated": "2025-09-30T15:10:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    10,
                    43,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:10:43Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    10,
                    43,
                    1,
                    273,
                    0
                ],
                "title": "Joint Inference for the Regression Discontinuity Effect and Its External\n  Validity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Inference for the Regression Discontinuity Effect and Its External\n  Validity"
                },
                "summary": "The external validity of regression discontinuity (RD) designs is essential\nfor informing policy and remains an active research area in econometrics and\nstatistics. However, we document that only a limited number of empirical\nstudies explicitly address the external validity of standard RD effects. To\nadvance empirical practice, we propose a simple joint inference procedure for\nthe RD effect and its local external validity, building on Calonico, Cattaneo,\nand Titiunik (2014, Econometrica) and Dong and Lewbel (2015, Review of\nEconomics and Statistics). We further introduce a locally linear treatment\neffects assumption, which enhances the interpretability of the treatment effect\nderivative proposed by Dong and Lewbel. Under this assumption, we establish\nidentification and derive a uniform confidence band for the extrapolated\ntreatment effects. Our approaches require no additional covariates or design\nfeatures, making them applicable to virtually all RD settings and thereby\nenhancing the policy relevance of many empirical RD studies. The usefulness of\nthe method is demonstrated through an empirical application, highlighting its\ncomplementarity to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The external validity of regression discontinuity (RD) designs is essential\nfor informing policy and remains an active research area in econometrics and\nstatistics. However, we document that only a limited number of empirical\nstudies explicitly address the external validity of standard RD effects. To\nadvance empirical practice, we propose a simple joint inference procedure for\nthe RD effect and its local external validity, building on Calonico, Cattaneo,\nand Titiunik (2014, Econometrica) and Dong and Lewbel (2015, Review of\nEconomics and Statistics). We further introduce a locally linear treatment\neffects assumption, which enhances the interpretability of the treatment effect\nderivative proposed by Dong and Lewbel. Under this assumption, we establish\nidentification and derive a uniform confidence band for the extrapolated\ntreatment effects. Our approaches require no additional covariates or design\nfeatures, making them applicable to virtually all RD settings and thereby\nenhancing the policy relevance of many empirical RD studies. The usefulness of\nthe method is demonstrated through an empirical application, highlighting its\ncomplementarity to existing approaches."
                },
                "authors": [
                    {
                        "name": "Yuta Okamoto"
                    }
                ],
                "author_detail": {
                    "name": "Yuta Okamoto"
                },
                "author": "Yuta Okamoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26378v1",
                "updated": "2025-09-30T15:09:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    9,
                    14,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:09:14Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    9,
                    14,
                    1,
                    273,
                    0
                ],
                "title": "MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval"
                },
                "summary": "Multimodal retrieval is becoming a crucial component of modern AI\napplications, yet its evaluation lags behind the demands of more realistic and\nchallenging scenarios. Existing benchmarks primarily probe surface-level\nsemantic correspondence (e.g., object-text matching) while failing to assess\nthe deeper reasoning required to capture complex relationships between visual\nand textual information. To address this gap, we introduce MR$^2$-Bench, a\nreasoning-intensive benchmark for multimodal retrieval. MR$^2$-Bench presents\nthe following critical values: 1) all tasks are reasoning-driven, going beyond\nshallow matching to effectively assess models' capacity for logical, spatial,\nand causal inference; 2) it features diverse multimodal data, such as natural\nimages, diagrams, and visual puzzles, enabling comprehensive evaluation across\ncontent types; 3) it supports complex queries and documents containing multiple\nimages and covers diverse retrieval scenarios, more accurately reflecting\nreal-world applications. Our benchmark contains 1,309 curated queries, derived\neither from manual collection and annotation or from selective consolidation of\npublic datasets. Despite achieving strong results on existing benchmarks,\ncurrent state-of-the-art models still struggle on MR$^2$-Bench: for example,\nthe leading Seed1.6-Embedding model attains a Recall@1 of 77.78 on MMEB, but\nonly 9.91 on MR$^2$-Bench. This substantial performance gap highlights both the\nincreased challenge posed by our benchmark and the pressing need for further\nadvances in reasoning-intensive multimodal retrieval. The dataset and\nevaluation code will be made publicly available at\nhttps://github.com/VectorSpaceLab/MR2-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal retrieval is becoming a crucial component of modern AI\napplications, yet its evaluation lags behind the demands of more realistic and\nchallenging scenarios. Existing benchmarks primarily probe surface-level\nsemantic correspondence (e.g., object-text matching) while failing to assess\nthe deeper reasoning required to capture complex relationships between visual\nand textual information. To address this gap, we introduce MR$^2$-Bench, a\nreasoning-intensive benchmark for multimodal retrieval. MR$^2$-Bench presents\nthe following critical values: 1) all tasks are reasoning-driven, going beyond\nshallow matching to effectively assess models' capacity for logical, spatial,\nand causal inference; 2) it features diverse multimodal data, such as natural\nimages, diagrams, and visual puzzles, enabling comprehensive evaluation across\ncontent types; 3) it supports complex queries and documents containing multiple\nimages and covers diverse retrieval scenarios, more accurately reflecting\nreal-world applications. Our benchmark contains 1,309 curated queries, derived\neither from manual collection and annotation or from selective consolidation of\npublic datasets. Despite achieving strong results on existing benchmarks,\ncurrent state-of-the-art models still struggle on MR$^2$-Bench: for example,\nthe leading Seed1.6-Embedding model attains a Recall@1 of 77.78 on MMEB, but\nonly 9.91 on MR$^2$-Bench. This substantial performance gap highlights both the\nincreased challenge posed by our benchmark and the pressing need for further\nadvances in reasoning-intensive multimodal retrieval. The dataset and\nevaluation code will be made publicly available at\nhttps://github.com/VectorSpaceLab/MR2-Bench."
                },
                "authors": [
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Ze Liu"
                    },
                    {
                        "name": "Lei Xiong"
                    },
                    {
                        "name": "Jin-Ge Yao"
                    },
                    {
                        "name": "Yueze Wang"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Fenfen Lin"
                    },
                    {
                        "name": "Miguel Hu Chen"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Siqi Bao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Yongping Xiong"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26375v1",
                "updated": "2025-09-30T15:07:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    7,
                    59,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:07:59Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    7,
                    59,
                    1,
                    273,
                    0
                ],
                "title": "SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task\n  Planning"
                },
                "summary": "Embodied task planning requires agents to produce executable actions in a\nclose-loop manner within the environment. With progressively improving\ncapabilities of LLMs in task decomposition, planning, and generalization,\ncurrent embodied task planning methods adopt LLM-based architecture.However,\nexisting LLM-based planners remain limited in three aspects, i.e., fixed\nplanning paradigms, lack of action sequence constraints, and error-agnostic. In\nthis work, we propose SDA-PLANNER, enabling an adaptive planning paradigm,\nstate-dependency aware and error-aware mechanisms for comprehensive embodied\ntask planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to\nexplicitly model action preconditions and effects, guiding the dynamic\nrevision. To handle execution error, it employs an error-adaptive replanning\nstrategy consisting of Error Backtrack and Diagnosis and Adaptive Action\nSubTree Generation, which locally reconstructs the affected portion of the plan\nbased on the current environment state. Experiments demonstrate that\nSDA-PLANNER consistently outperforms baselines in success rate and goal\ncompletion, particularly under diverse error conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied task planning requires agents to produce executable actions in a\nclose-loop manner within the environment. With progressively improving\ncapabilities of LLMs in task decomposition, planning, and generalization,\ncurrent embodied task planning methods adopt LLM-based architecture.However,\nexisting LLM-based planners remain limited in three aspects, i.e., fixed\nplanning paradigms, lack of action sequence constraints, and error-agnostic. In\nthis work, we propose SDA-PLANNER, enabling an adaptive planning paradigm,\nstate-dependency aware and error-aware mechanisms for comprehensive embodied\ntask planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to\nexplicitly model action preconditions and effects, guiding the dynamic\nrevision. To handle execution error, it employs an error-adaptive replanning\nstrategy consisting of Error Backtrack and Diagnosis and Adaptive Action\nSubTree Generation, which locally reconstructs the affected portion of the plan\nbased on the current environment state. Experiments demonstrate that\nSDA-PLANNER consistently outperforms baselines in success rate and goal\ncompletion, particularly under diverse error conditions."
                },
                "authors": [
                    {
                        "name": "Zichao Shen"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Jiaqi Yuan"
                    },
                    {
                        "name": "Tianchen Zhu"
                    },
                    {
                        "name": "Xingcheng Fu"
                    },
                    {
                        "name": "Qingyun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Qingyun Sun"
                },
                "author": "Qingyun Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17101v2",
                "updated": "2025-09-30T15:06:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    6,
                    40,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-21T07:38:48Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    7,
                    38,
                    48,
                    2,
                    141,
                    0
                ],
                "title": "A quantitative analysis of semantic information in deep representations\n  of text and images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A quantitative analysis of semantic information in deep representations\n  of text and images"
                },
                "summary": "Deep neural networks are known to develop similar representations for\nsemantically related data, even when they belong to different domains, such as\nan image and its description, or the same text in different languages. We\npresent a method for quantitatively investigating this phenomenon by measuring\nthe relative information content of the representations of semantically related\ndata and probing how it is encoded into multiple tokens of large language\nmodels (LLMs) and vision transformers. Looking first at how LLMs process pairs\nof translated sentences, we identify inner ``semantic'' layers containing the\nmost language-transferable information. We find moreover that, on these layers,\na larger LLM (DeepSeek-V3) extracts significantly more general information than\na smaller one (Llama3.1-8B). Semantic information of English text is spread\nacross many tokens and it is characterized by long-distance correlations\nbetween tokens and by a causal left-to-right (i.e., past-future) asymmetry. We\nalso identify layers encoding semantic information within visual transformers.\nWe show that caption representations in the semantic layers of LLMs predict\nvisual representations of the corresponding images. We observe significant and\nmodel-dependent information asymmetries between image and text representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks are known to develop similar representations for\nsemantically related data, even when they belong to different domains, such as\nan image and its description, or the same text in different languages. We\npresent a method for quantitatively investigating this phenomenon by measuring\nthe relative information content of the representations of semantically related\ndata and probing how it is encoded into multiple tokens of large language\nmodels (LLMs) and vision transformers. Looking first at how LLMs process pairs\nof translated sentences, we identify inner ``semantic'' layers containing the\nmost language-transferable information. We find moreover that, on these layers,\na larger LLM (DeepSeek-V3) extracts significantly more general information than\na smaller one (Llama3.1-8B). Semantic information of English text is spread\nacross many tokens and it is characterized by long-distance correlations\nbetween tokens and by a causal left-to-right (i.e., past-future) asymmetry. We\nalso identify layers encoding semantic information within visual transformers.\nWe show that caption representations in the semantic layers of LLMs predict\nvisual representations of the corresponding images. We observe significant and\nmodel-dependent information asymmetries between image and text representations."
                },
                "authors": [
                    {
                        "name": "Santiago Acevedo"
                    },
                    {
                        "name": "Andrea Mascaretti"
                    },
                    {
                        "name": "Riccardo Rende"
                    },
                    {
                        "name": "Matéo Mahaut"
                    },
                    {
                        "name": "Marco Baroni"
                    },
                    {
                        "name": "Alessandro Laio"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Laio"
                },
                "author": "Alessandro Laio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16293v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16293v2",
                "updated": "2025-09-30T15:06:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    6,
                    10,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-19T15:08:33Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    8,
                    33,
                    4,
                    262,
                    0
                ],
                "title": "Robust LLM Training Infrastructure at ByteDance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM Training Infrastructure at ByteDance"
                },
                "summary": "The training scale of large language models (LLMs) has reached tens of\nthousands of GPUs and is still continuously expanding, enabling faster learning\nof larger models. Accompanying the expansion of the resource scale is the\nprevalence of failures (CUDA error, NaN values, job hang, etc.), which poses\nsignificant challenges to training stability. Any large-scale LLM training\ninfrastructure should strive for minimal training interruption, efficient fault\ndiagnosis, and effective failure tolerance to enable highly efficient\ncontinuous training. This paper presents ByteRobust, a large-scale GPU\ninfrastructure management system tailored for robust and stable training of\nLLMs. It exploits the uniqueness of LLM training process and gives top\npriorities to detecting and recovering failures in a routine manner. Leveraging\nparallelisms and characteristics of LLM training, ByteRobust enables\nhigh-capacity fault tolerance, prompt fault demarcation, and localization with\nan effective data-driven approach, comprehensively ensuring continuous and\nefficient training of LLM tasks. ByteRobust is deployed on a production GPU\nplatform with over 200,000 GPUs and achieves 97% ETTR for a three-month\ntraining job on 9,600 GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training scale of large language models (LLMs) has reached tens of\nthousands of GPUs and is still continuously expanding, enabling faster learning\nof larger models. Accompanying the expansion of the resource scale is the\nprevalence of failures (CUDA error, NaN values, job hang, etc.), which poses\nsignificant challenges to training stability. Any large-scale LLM training\ninfrastructure should strive for minimal training interruption, efficient fault\ndiagnosis, and effective failure tolerance to enable highly efficient\ncontinuous training. This paper presents ByteRobust, a large-scale GPU\ninfrastructure management system tailored for robust and stable training of\nLLMs. It exploits the uniqueness of LLM training process and gives top\npriorities to detecting and recovering failures in a routine manner. Leveraging\nparallelisms and characteristics of LLM training, ByteRobust enables\nhigh-capacity fault tolerance, prompt fault demarcation, and localization with\nan effective data-driven approach, comprehensively ensuring continuous and\nefficient training of LLM tasks. ByteRobust is deployed on a production GPU\nplatform with over 200,000 GPUs and achieves 97% ETTR for a three-month\ntraining job on 9,600 GPUs."
                },
                "authors": [
                    {
                        "name": "Borui Wan"
                    },
                    {
                        "name": "Gaohong Liu"
                    },
                    {
                        "name": "Zuquan Song"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yun Zhang"
                    },
                    {
                        "name": "Guangming Sheng"
                    },
                    {
                        "name": "Shuguang Wang"
                    },
                    {
                        "name": "Houmin Wei"
                    },
                    {
                        "name": "Chenyuan Wang"
                    },
                    {
                        "name": "Weiqiang Lou"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Mofan Zhang"
                    },
                    {
                        "name": "Kaihua Jiang"
                    },
                    {
                        "name": "Cheng Ren"
                    },
                    {
                        "name": "Xiaoyun Zhi"
                    },
                    {
                        "name": "Menghan Yu"
                    },
                    {
                        "name": "Zhe Nan"
                    },
                    {
                        "name": "Zhuolin Zheng"
                    },
                    {
                        "name": "Baoquan Zhong"
                    },
                    {
                        "name": "Qinlong Wang"
                    },
                    {
                        "name": "Huan Yu"
                    },
                    {
                        "name": "Jinxin Chi"
                    },
                    {
                        "name": "Wang Zhang"
                    },
                    {
                        "name": "Yuhan Li"
                    },
                    {
                        "name": "Zixian Du"
                    },
                    {
                        "name": "Sida Zhao"
                    },
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Jingzhe Tang"
                    },
                    {
                        "name": "Zherui Liu"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Yanghua Peng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Wencong Xiao"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Liang Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xiang"
                },
                "author": "Liang Xiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16293v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16293v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26368v1",
                "updated": "2025-09-30T15:04:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    4,
                    24,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:04:24Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    4,
                    24,
                    1,
                    273,
                    0
                ],
                "title": "Introducing Large Language Models in the Design Flow of Time Sensitive\n  Networking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing Large Language Models in the Design Flow of Time Sensitive\n  Networking"
                },
                "summary": "The growing demand for real-time, safety-critical systems has significantly\nincreased both the adoption and complexity of Time Sensitive Networking (TSN).\nConfiguring an optimized TSN network is highly challenging, requiring careful\nplanning, design, verification, validation, and deployment. Large Language\nModels (LLMs) have recently demonstrated strong capabilities in solving complex\ntasks, positioning them as promising candidates for automating end-to-end TSN\ndeployment, referred to as TSN orchestration. This paper outlines the steps\ninvolved in TSN orchestration and the associated challenges. To assess the\ncapabilities of existing LLM models, we conduct an initial proof-of-concept\ncase study focused on TSN configuration across multiple models. Building on\nthese insights, we propose an LLM-assisted orchestration framework. Unlike\nprior research on LLMs in computer networks, which has concentrated on general\nconfiguration and management, TSN-specific orchestration has not yet been\ninvestigated. We present the building blocks for automating TSN using LLMs,\ndescribe the proposed pipeline, and analyze opportunities and limitations for\nreal-world deployment. Finally, we highlight key challenges and research\ndirections, including the development of TSN-focused datasets, standardized\nbenchmark suites, and the integration of external tools such as Network\nCalculus (NC) engines and simulators. This work provides the first roadmap\ntoward assessing the feasibility of LLM-assisted TSN orchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for real-time, safety-critical systems has significantly\nincreased both the adoption and complexity of Time Sensitive Networking (TSN).\nConfiguring an optimized TSN network is highly challenging, requiring careful\nplanning, design, verification, validation, and deployment. Large Language\nModels (LLMs) have recently demonstrated strong capabilities in solving complex\ntasks, positioning them as promising candidates for automating end-to-end TSN\ndeployment, referred to as TSN orchestration. This paper outlines the steps\ninvolved in TSN orchestration and the associated challenges. To assess the\ncapabilities of existing LLM models, we conduct an initial proof-of-concept\ncase study focused on TSN configuration across multiple models. Building on\nthese insights, we propose an LLM-assisted orchestration framework. Unlike\nprior research on LLMs in computer networks, which has concentrated on general\nconfiguration and management, TSN-specific orchestration has not yet been\ninvestigated. We present the building blocks for automating TSN using LLMs,\ndescribe the proposed pipeline, and analyze opportunities and limitations for\nreal-world deployment. Finally, we highlight key challenges and research\ndirections, including the development of TSN-focused datasets, standardized\nbenchmark suites, and the integration of external tools such as Network\nCalculus (NC) engines and simulators. This work provides the first roadmap\ntoward assessing the feasibility of LLM-assisted TSN orchestration."
                },
                "authors": [
                    {
                        "name": "Rubi Debnath"
                    },
                    {
                        "name": "Luxi Zhao"
                    },
                    {
                        "name": "Mohammadreza Barzegaran"
                    },
                    {
                        "name": "Sebastian Steinhorst"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Steinhorst"
                },
                "author": "Sebastian Steinhorst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26364v1",
                "updated": "2025-09-30T15:03:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    3,
                    55,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:03:55Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    3,
                    55,
                    1,
                    273,
                    0
                ],
                "title": "Data-to-Energy Stochastic Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-to-Energy Stochastic Dynamics"
                },
                "summary": "The Schr\\\"odinger bridge problem is concerned with finding a stochastic\ndynamical system bridging two marginal distributions that minimises a certain\ntransportation cost. This problem, which represents a generalisation of optimal\ntransport to the stochastic case, has received attention due to its connections\nto diffusion models and flow matching, as well as its applications in the\nnatural sciences. However, all existing algorithms allow to infer such dynamics\nonly for cases where samples from both distributions are available. In this\npaper, we propose the first general method for modelling Schr\\\"odinger bridges\nwhen one (or both) distributions are given by their unnormalised densities,\nwith no access to data samples. Our algorithm relies on a generalisation of the\niterative proportional fitting (IPF) procedure to the data-free case, inspired\nby recent developments in off-policy reinforcement learning for training of\ndiffusion samplers. We demonstrate the efficacy of the proposed data-to-energy\nIPF on synthetic problems, finding that it can successfully learn transports\nbetween multimodal distributions. As a secondary consequence of our\nreinforcement learning formulation, which assumes a fixed time discretisation\nscheme for the dynamics, we find that existing data-to-data Schr\\\"odinger\nbridge algorithms can be substantially improved by learning the diffusion\ncoefficient of the dynamics. Finally, we apply the newly developed algorithm to\nthe problem of sampling posterior distributions in latent spaces of generative\nmodels, thus creating a data-free image-to-image translation method. Code:\nhttps://github.com/mmacosha/d2e-stochastic-dynamics",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Schr\\\"odinger bridge problem is concerned with finding a stochastic\ndynamical system bridging two marginal distributions that minimises a certain\ntransportation cost. This problem, which represents a generalisation of optimal\ntransport to the stochastic case, has received attention due to its connections\nto diffusion models and flow matching, as well as its applications in the\nnatural sciences. However, all existing algorithms allow to infer such dynamics\nonly for cases where samples from both distributions are available. In this\npaper, we propose the first general method for modelling Schr\\\"odinger bridges\nwhen one (or both) distributions are given by their unnormalised densities,\nwith no access to data samples. Our algorithm relies on a generalisation of the\niterative proportional fitting (IPF) procedure to the data-free case, inspired\nby recent developments in off-policy reinforcement learning for training of\ndiffusion samplers. We demonstrate the efficacy of the proposed data-to-energy\nIPF on synthetic problems, finding that it can successfully learn transports\nbetween multimodal distributions. As a secondary consequence of our\nreinforcement learning formulation, which assumes a fixed time discretisation\nscheme for the dynamics, we find that existing data-to-data Schr\\\"odinger\nbridge algorithms can be substantially improved by learning the diffusion\ncoefficient of the dynamics. Finally, we apply the newly developed algorithm to\nthe problem of sampling posterior distributions in latent spaces of generative\nmodels, thus creating a data-free image-to-image translation method. Code:\nhttps://github.com/mmacosha/d2e-stochastic-dynamics"
                },
                "authors": [
                    {
                        "name": "Kirill Tamogashev"
                    },
                    {
                        "name": "Nikolay Malkin"
                    }
                ],
                "author_detail": {
                    "name": "Nikolay Malkin"
                },
                "author": "Nikolay Malkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07861v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07861v2",
                "updated": "2025-09-30T14:59:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    59,
                    16,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-08T17:51:24Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    51,
                    24,
                    3,
                    128,
                    0
                ],
                "title": "Scalable LLM Math Reasoning Acceleration with Low-rank Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable LLM Math Reasoning Acceleration with Low-rank Distillation"
                },
                "summary": "Due to long generations, large language model (LLM) math reasoning demands\nsignificant computational resources and time. While many existing efficient\ninference methods have been developed with excellent performance preservation\non language tasks, they often severely degrade math performance. In this paper,\nwe propose Caprese, a resource-efficient distillation method to recover lost\ncapabilities from deploying efficient inference methods, focused primarily in\nfeedforward blocks. With original weights unperturbed, roughly 1% of additional\nparameters, and only 20K synthetic training samples, we are able to recover\nmuch if not all of the math capabilities lost from efficient inference for\nthinking LLMs and without harm to language tasks for instruct LLMs. Moreover,\nCaprese slashes the number of active parameters (~2B cut for Gemma 2 9B and\nLlama 3.1 8B) and integrates cleanly into existing model layers to reduce\nlatency (>16% time-to-next-token reduction) while encouraging response brevity\n(up to 8.5% fewer tokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to long generations, large language model (LLM) math reasoning demands\nsignificant computational resources and time. While many existing efficient\ninference methods have been developed with excellent performance preservation\non language tasks, they often severely degrade math performance. In this paper,\nwe propose Caprese, a resource-efficient distillation method to recover lost\ncapabilities from deploying efficient inference methods, focused primarily in\nfeedforward blocks. With original weights unperturbed, roughly 1% of additional\nparameters, and only 20K synthetic training samples, we are able to recover\nmuch if not all of the math capabilities lost from efficient inference for\nthinking LLMs and without harm to language tasks for instruct LLMs. Moreover,\nCaprese slashes the number of active parameters (~2B cut for Gemma 2 9B and\nLlama 3.1 8B) and integrates cleanly into existing model layers to reduce\nlatency (>16% time-to-next-token reduction) while encouraging response brevity\n(up to 8.5% fewer tokens)."
                },
                "authors": [
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Bilge Acun"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Yuejie Chi"
                    }
                ],
                "author_detail": {
                    "name": "Yuejie Chi"
                },
                "author": "Yuejie Chi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07861v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07861v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12379v2",
                "updated": "2025-09-30T14:56:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    56,
                    50,
                    1,
                    273,
                    0
                ],
                "published": "2025-08-17T14:28:38Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    14,
                    28,
                    38,
                    6,
                    229,
                    0
                ],
                "title": "GraphCogent: Mitigating LLMs' Working Memory Constraints via Multi-Agent\n  Collaboration in Complex Graph Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphCogent: Mitigating LLMs' Working Memory Constraints via Multi-Agent\n  Collaboration in Complex Graph Understanding"
                },
                "summary": "Large language models (LLMs) show promising performance on small-scale graph\nreasoning tasks but fail when handling real-world graphs with complex queries.\nThis phenomenon arises from LLMs' working memory constraints, which result in\ntheir inability to retain long-range graph topology over extended contexts\nwhile sustaining coherent multi-step reasoning. However, real-world graphs are\noften structurally complex, such as Web, Transportation, Social, and Citation\nnetworks. To address these limitations, we propose GraphCogent, a collaborative\nagent framework inspired by human Working Memory Model that decomposes graph\nreasoning into specialized cognitive processes: sense, buffer, and execute. The\nframework consists of three modules: Sensory Module standardizes diverse graph\ntext representations via subgraph sampling, Buffer Module integrates and\nindexes graph data across multiple formats, and Execution Module combines tool\ncalling and tool creation for efficient reasoning. We also introduce\nGraph4real, a comprehensive benchmark that contains four domains of real-world\ngraphs (Web, Transportation, Social, and Citation) to evaluate LLMs' graph\nreasoning capabilities. Our Graph4real covers 21 different graph reasoning\ntasks, categorized into three types (Structural Querying, Algorithmic\nReasoning, and Predictive Modeling tasks), with graph scales up to 10 times\nlarger than existing benchmarks. Experiments show that Llama3.1-8B based\nGraphCogent achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1\n(671B). Compared to state-of-the-art agent-based baseline, our framework\noutperforms by 20% in accuracy while reducing token usage by 80% for in-toolset\ntasks and 30% for out-toolset tasks. Code will be available after review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show promising performance on small-scale graph\nreasoning tasks but fail when handling real-world graphs with complex queries.\nThis phenomenon arises from LLMs' working memory constraints, which result in\ntheir inability to retain long-range graph topology over extended contexts\nwhile sustaining coherent multi-step reasoning. However, real-world graphs are\noften structurally complex, such as Web, Transportation, Social, and Citation\nnetworks. To address these limitations, we propose GraphCogent, a collaborative\nagent framework inspired by human Working Memory Model that decomposes graph\nreasoning into specialized cognitive processes: sense, buffer, and execute. The\nframework consists of three modules: Sensory Module standardizes diverse graph\ntext representations via subgraph sampling, Buffer Module integrates and\nindexes graph data across multiple formats, and Execution Module combines tool\ncalling and tool creation for efficient reasoning. We also introduce\nGraph4real, a comprehensive benchmark that contains four domains of real-world\ngraphs (Web, Transportation, Social, and Citation) to evaluate LLMs' graph\nreasoning capabilities. Our Graph4real covers 21 different graph reasoning\ntasks, categorized into three types (Structural Querying, Algorithmic\nReasoning, and Predictive Modeling tasks), with graph scales up to 10 times\nlarger than existing benchmarks. Experiments show that Llama3.1-8B based\nGraphCogent achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1\n(671B). Compared to state-of-the-art agent-based baseline, our framework\noutperforms by 20% in accuracy while reducing token usage by 80% for in-toolset\ntasks and 30% for out-toolset tasks. Code will be available after review."
                },
                "authors": [
                    {
                        "name": "Rongzheng Wang"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Yihong Huang"
                    },
                    {
                        "name": "Muquan Li"
                    },
                    {
                        "name": "Yizhuo Ma"
                    },
                    {
                        "name": "Dongyang Zhang"
                    },
                    {
                        "name": "Ke Qin"
                    },
                    {
                        "name": "Man-Fai Leung"
                    }
                ],
                "author_detail": {
                    "name": "Man-Fai Leung"
                },
                "author": "Man-Fai Leung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26354v1",
                "updated": "2025-09-30T14:55:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    55,
                    55,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:55:55Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    55,
                    55,
                    1,
                    273,
                    0
                ],
                "title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents"
                },
                "summary": "Advances in Large Language Models (LLMs) have enabled a new class of\nself-evolving agents that autonomously improve through interaction with the\nenvironment, demonstrating strong capabilities. However, self-evolution also\nintroduces novel risks overlooked by current safety research. In this work, we\nstudy the case where an agent's self-evolution deviates in unintended ways,\nleading to undesirable or even harmful outcomes. We refer to this as\nMisevolution. To provide a systematic investigation, we evaluate misevolution\nalong four key evolutionary pathways: model, memory, tool, and workflow. Our\nempirical findings reveal that misevolution is a widespread risk, affecting\nagents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent\nrisks are observed in the self-evolutionary process, such as the degradation of\nsafety alignment after memory accumulation, or the unintended introduction of\nvulnerabilities in tool creation and reuse. To our knowledge, this is the first\nstudy to systematically conceptualize misevolution and provide empirical\nevidence of its occurrence, highlighting an urgent need for new safety\nparadigms for self-evolving agents. Finally, we discuss potential mitigation\nstrategies to inspire further research on building safer and more trustworthy\nself-evolving agents. Our code and data are available at\nhttps://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes\nexamples that may be offensive or harmful in nature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Large Language Models (LLMs) have enabled a new class of\nself-evolving agents that autonomously improve through interaction with the\nenvironment, demonstrating strong capabilities. However, self-evolution also\nintroduces novel risks overlooked by current safety research. In this work, we\nstudy the case where an agent's self-evolution deviates in unintended ways,\nleading to undesirable or even harmful outcomes. We refer to this as\nMisevolution. To provide a systematic investigation, we evaluate misevolution\nalong four key evolutionary pathways: model, memory, tool, and workflow. Our\nempirical findings reveal that misevolution is a widespread risk, affecting\nagents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent\nrisks are observed in the self-evolutionary process, such as the degradation of\nsafety alignment after memory accumulation, or the unintended introduction of\nvulnerabilities in tool creation and reuse. To our knowledge, this is the first\nstudy to systematically conceptualize misevolution and provide empirical\nevidence of its occurrence, highlighting an urgent need for new safety\nparadigms for self-evolving agents. Finally, we discuss potential mitigation\nstrategies to inspire further research on building safer and more trustworthy\nself-evolving agents. Our code and data are available at\nhttps://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes\nexamples that may be offensive or harmful in nature."
                },
                "authors": [
                    {
                        "name": "Shuai Shao"
                    },
                    {
                        "name": "Qihan Ren"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Boyi Wei"
                    },
                    {
                        "name": "Dadi Guo"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Xinhao Song"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16094v2",
                "updated": "2025-09-30T14:55:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    55,
                    54,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-19T15:38:13Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    38,
                    13,
                    4,
                    262,
                    0
                ],
                "title": "The GUAPOS project. VI: the chemical inventory of shocked gas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The GUAPOS project. VI: the chemical inventory of shocked gas"
                },
                "summary": "The study of the chemical composition of star-forming regions is key to\nunderstand the chemical ingredients available during the formation of planetary\nsystems. Given that the chemical inventory on interstellar dust grains in the\nprestellar phases might be altered due to the prostostellar warm-up, an\nalternative to infer the chemical composition on the grains could be to observe\nregions affected by shocks associated with molecular outflows. Such shocks can\ndesorb the molecules, and might produce less chemical processing due to shorter\ntimescales. We present here a detailed study of the chemical reservoir of a\nshocked region located in the G31.41+0.31 protocluster using GUAPOS data\n(G31.41+0.31 Unbiased ALMA sPectral Observational Survey). We report here the\ndetection of 30 molecular species (plus 18 isotopologues). We performed a\ncomparison of the molecular ratios in the shocked region with those derived\ntowards the hot core of G31.41+0.31, finding that they are poorly correlated,\nexcepting N-bearing species. Our results confirm observationally that a\ndifferent level of chemical alteration is present in hot cores and in shocks.\nWhile the former likely alter the molecular ratios due to thermal processing\nduring longer timescales, the latter might represent freshly desorbed material\nthat constitutes a better proxy of the icy mantle composition. The similarity\nof molecular ratios between the N-bearing species in the G31.41 shock and the\nhot core suggests that these species are desorbed at early evolutionary stages.\nInterestingly, we have found that the abundances in the G31.41 shock show\nbetter correlations with other shock-dominated regions (two protostellar\noutflows and a Galactic Center molecular cloud). This suggests a negligible\ngas-phase chemistry after shock-induced ejection from grains, and that the\nice-mantle composition is similar regardless of the Galactic environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of the chemical composition of star-forming regions is key to\nunderstand the chemical ingredients available during the formation of planetary\nsystems. Given that the chemical inventory on interstellar dust grains in the\nprestellar phases might be altered due to the prostostellar warm-up, an\nalternative to infer the chemical composition on the grains could be to observe\nregions affected by shocks associated with molecular outflows. Such shocks can\ndesorb the molecules, and might produce less chemical processing due to shorter\ntimescales. We present here a detailed study of the chemical reservoir of a\nshocked region located in the G31.41+0.31 protocluster using GUAPOS data\n(G31.41+0.31 Unbiased ALMA sPectral Observational Survey). We report here the\ndetection of 30 molecular species (plus 18 isotopologues). We performed a\ncomparison of the molecular ratios in the shocked region with those derived\ntowards the hot core of G31.41+0.31, finding that they are poorly correlated,\nexcepting N-bearing species. Our results confirm observationally that a\ndifferent level of chemical alteration is present in hot cores and in shocks.\nWhile the former likely alter the molecular ratios due to thermal processing\nduring longer timescales, the latter might represent freshly desorbed material\nthat constitutes a better proxy of the icy mantle composition. The similarity\nof molecular ratios between the N-bearing species in the G31.41 shock and the\nhot core suggests that these species are desorbed at early evolutionary stages.\nInterestingly, we have found that the abundances in the G31.41 shock show\nbetter correlations with other shock-dominated regions (two protostellar\noutflows and a Galactic Center molecular cloud). This suggests a negligible\ngas-phase chemistry after shock-induced ejection from grains, and that the\nice-mantle composition is similar regardless of the Galactic environment."
                },
                "authors": [
                    {
                        "name": "Á. López-Gallifa"
                    },
                    {
                        "name": "V. M. Rivilla"
                    },
                    {
                        "name": "M. T. Beltrán"
                    },
                    {
                        "name": "L. Colzi"
                    },
                    {
                        "name": "F. Fontani"
                    },
                    {
                        "name": "Á. Sánchez-Monge"
                    },
                    {
                        "name": "C. Mininni"
                    },
                    {
                        "name": "R. Cesaroni"
                    },
                    {
                        "name": "I. Jiménez-Serra"
                    },
                    {
                        "name": "S. Viti"
                    },
                    {
                        "name": "A. Lorenzani"
                    }
                ],
                "author_detail": {
                    "name": "A. Lorenzani"
                },
                "author": "A. Lorenzani",
                "arxiv_comment": "Accepted for publication in Astronomy and Astrophysics (A&A)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26351v1",
                "updated": "2025-09-30T14:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    54,
                    58,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    54,
                    58,
                    1,
                    273,
                    0
                ],
                "title": "LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and\n  MCI-Like Field Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and\n  MCI-Like Field Simulation"
                },
                "summary": "Research on emergency and mass casualty incident (MCI) triage has been\nlimited by the absence of openly usable, reproducible benchmarks. Yet these\nscenarios demand rapid identification of the patients most in need, where\naccurate deterioration prediction can guide timely interventions. While the\nMIMIC-IV-ED database is openly available to credentialed researchers,\ntransforming it into a triage-focused benchmark requires extensive\npreprocessing, feature harmonization, and schema alignment -- barriers that\nrestrict accessibility to only highly technical users.\n  We address these gaps by first introducing an open, LLM-assisted emergency\ntriage benchmark for deterioration prediction (ICU transfer, in-hospital\nmortality). The benchmark then defines two regimes: (i) a hospital-rich setting\nwith vitals, labs, notes, chief complaints, and structured observations, and\n(ii) an MCI-like field simulation limited to vitals, observations, and notes.\nLarge language models (LLMs) contributed directly to dataset construction by\n(i) harmonizing noisy fields such as AVPU and breathing devices, (ii)\nprioritizing clinically relevant vitals and labs, and (iii) guiding schema\nalignment and efficient merging of disparate tables.\n  We further provide baseline models and SHAP-based interpretability analyses,\nillustrating predictive gaps between regimes and the features most critical for\ntriage. Together, these contributions make triage prediction research more\nreproducible and accessible -- a step toward dataset democratization in\nclinical AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on emergency and mass casualty incident (MCI) triage has been\nlimited by the absence of openly usable, reproducible benchmarks. Yet these\nscenarios demand rapid identification of the patients most in need, where\naccurate deterioration prediction can guide timely interventions. While the\nMIMIC-IV-ED database is openly available to credentialed researchers,\ntransforming it into a triage-focused benchmark requires extensive\npreprocessing, feature harmonization, and schema alignment -- barriers that\nrestrict accessibility to only highly technical users.\n  We address these gaps by first introducing an open, LLM-assisted emergency\ntriage benchmark for deterioration prediction (ICU transfer, in-hospital\nmortality). The benchmark then defines two regimes: (i) a hospital-rich setting\nwith vitals, labs, notes, chief complaints, and structured observations, and\n(ii) an MCI-like field simulation limited to vitals, observations, and notes.\nLarge language models (LLMs) contributed directly to dataset construction by\n(i) harmonizing noisy fields such as AVPU and breathing devices, (ii)\nprioritizing clinically relevant vitals and labs, and (iii) guiding schema\nalignment and efficient merging of disparate tables.\n  We further provide baseline models and SHAP-based interpretability analyses,\nillustrating predictive gaps between regimes and the features most critical for\ntriage. Together, these contributions make triage prediction research more\nreproducible and accessible -- a step toward dataset democratization in\nclinical AI."
                },
                "authors": [
                    {
                        "name": "Joshua Sebastian"
                    },
                    {
                        "name": "Karma Tobden"
                    },
                    {
                        "name": "KMA Solaiman"
                    }
                ],
                "author_detail": {
                    "name": "KMA Solaiman"
                },
                "author": "KMA Solaiman",
                "arxiv_comment": "Submitted to GenAI4Health@NeurIPS 2025. This is the first version of\n  the LLM-assisted emergency triage benchmark dataset and baseline models.\n  Authors: Joshua Sebastian, Karma Tobden, KMA Solaiman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26350v1",
                "updated": "2025-09-30T14:54:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    54,
                    42,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:54:42Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    54,
                    42,
                    1,
                    273,
                    0
                ],
                "title": "SoK: Systematic analysis of adversarial threats against deep learning\n  approaches for autonomous anomaly detection systems in SDN-IoT networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Systematic analysis of adversarial threats against deep learning\n  approaches for autonomous anomaly detection systems in SDN-IoT networks"
                },
                "summary": "Integrating SDN and the IoT enhances network control and flexibility.\nDL-based AAD systems improve security by enabling real-time threat detection in\nSDN-IoT networks. However, these systems remain vulnerable to adversarial\nattacks that manipulate input data or exploit model weaknesses, significantly\ndegrading detection accuracy. Existing research lacks a systematic analysis of\nadversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT\nenvironments. This SoK study introduces a structured adversarial threat model\nand a comprehensive taxonomy of attacks, categorising them into data, model,\nand hybrid-level threats. Unlike previous studies, we systematically evaluate\nwhite, black, and grey-box attack strategies across popular benchmark datasets.\nOur findings reveal that adversarial attacks can reduce detection accuracy by\nup to 48.4%, with Membership Inference causing the most significant drop. C&W\nand DeepFool achieve high evasion success rates. However, adversarial training\nenhances robustness, and its high computational overhead limits the real-time\ndeployment of SDN-IoT applications. We propose adaptive countermeasures,\nincluding real-time adversarial mitigation, enhanced retraining mechanisms, and\nexplainable AI-driven security frameworks. By integrating structured threat\nmodels, this study offers a more comprehensive approach to attack\ncategorisation, impact assessment, and defence evaluation than previous\nresearch. Our work highlights critical vulnerabilities in existing DL-based AAD\nmodels and provides practical recommendations for improving resilience,\ninterpretability, and computational efficiency. This study serves as a\nfoundational reference for researchers and practitioners seeking to enhance\nDL-based AAD security in SDN-IoT networks, offering a systematic adversarial\nthreat model and conceptual defence evaluation based on prior empirical\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating SDN and the IoT enhances network control and flexibility.\nDL-based AAD systems improve security by enabling real-time threat detection in\nSDN-IoT networks. However, these systems remain vulnerable to adversarial\nattacks that manipulate input data or exploit model weaknesses, significantly\ndegrading detection accuracy. Existing research lacks a systematic analysis of\nadversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT\nenvironments. This SoK study introduces a structured adversarial threat model\nand a comprehensive taxonomy of attacks, categorising them into data, model,\nand hybrid-level threats. Unlike previous studies, we systematically evaluate\nwhite, black, and grey-box attack strategies across popular benchmark datasets.\nOur findings reveal that adversarial attacks can reduce detection accuracy by\nup to 48.4%, with Membership Inference causing the most significant drop. C&W\nand DeepFool achieve high evasion success rates. However, adversarial training\nenhances robustness, and its high computational overhead limits the real-time\ndeployment of SDN-IoT applications. We propose adaptive countermeasures,\nincluding real-time adversarial mitigation, enhanced retraining mechanisms, and\nexplainable AI-driven security frameworks. By integrating structured threat\nmodels, this study offers a more comprehensive approach to attack\ncategorisation, impact assessment, and defence evaluation than previous\nresearch. Our work highlights critical vulnerabilities in existing DL-based AAD\nmodels and provides practical recommendations for improving resilience,\ninterpretability, and computational efficiency. This study serves as a\nfoundational reference for researchers and practitioners seeking to enhance\nDL-based AAD security in SDN-IoT networks, offering a systematic adversarial\nthreat model and conceptual defence evaluation based on prior empirical\nstudies."
                },
                "authors": [
                    {
                        "name": "Tharindu Lakshan Yasarathna"
                    },
                    {
                        "name": "Nhien-An Le-Khac"
                    }
                ],
                "author_detail": {
                    "name": "Nhien-An Le-Khac"
                },
                "author": "Nhien-An Le-Khac",
                "arxiv_doi": "10.1016/j.jisa.2025.104220",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jisa.2025.104220",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.26350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13820v2",
                "updated": "2025-09-30T14:52:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    52,
                    40,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-20T02:01:55Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    2,
                    1,
                    55,
                    1,
                    140,
                    0
                ],
                "title": "Structured Agent Distillation for Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Agent Distillation for Large Language Model"
                },
                "summary": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents."
                },
                "authors": [
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zhenglun Kong"
                    },
                    {
                        "name": "Peiyan Dong"
                    },
                    {
                        "name": "Changdi Yang"
                    },
                    {
                        "name": "Tianqi Li"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Geng Yuan"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Wenbin Zhang"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Xue Lin"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26345v1",
                "updated": "2025-09-30T14:50:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    50,
                    59,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:50:59Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    50,
                    59,
                    1,
                    273,
                    0
                ],
                "title": "SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate\n  Jailbreak Attacks in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate\n  Jailbreak Attacks in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive performance across\ndiverse natural language processing tasks, but their growing power also\namplifies potential risks such as jailbreak attacks that circumvent built-in\nsafety mechanisms. Existing defenses including input paraphrasing, multi step\nevaluation, and safety expert models often suffer from high computational\ncosts, limited generalization, or rigid workflows that fail to detect subtle\nmalicious intent embedded in complex contexts. Inspired by cognitive science\nfindings on human decision making, we propose SafeBehavior, a novel\nhierarchical jailbreak defense mechanism that simulates the adaptive multistage\nreasoning process of humans. SafeBehavior decomposes safety evaluation into\nthree stages: intention inference to detect obvious input risks, self\nintrospection to assess generated responses and assign confidence based\njudgments, and self revision to adaptively rewrite uncertain outputs while\npreserving user intent and enforcing safety constraints. We extensively\nevaluate SafeBehavior against five representative jailbreak attack types\nincluding optimization based, contextual manipulation, and prompt based attacks\nand compare it with seven state of the art defense baselines. Experimental\nresults show that SafeBehavior significantly improves robustness and\nadaptability across diverse threat scenarios, offering an efficient and human\ninspired approach to safeguarding LLMs against jailbreak attempts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive performance across\ndiverse natural language processing tasks, but their growing power also\namplifies potential risks such as jailbreak attacks that circumvent built-in\nsafety mechanisms. Existing defenses including input paraphrasing, multi step\nevaluation, and safety expert models often suffer from high computational\ncosts, limited generalization, or rigid workflows that fail to detect subtle\nmalicious intent embedded in complex contexts. Inspired by cognitive science\nfindings on human decision making, we propose SafeBehavior, a novel\nhierarchical jailbreak defense mechanism that simulates the adaptive multistage\nreasoning process of humans. SafeBehavior decomposes safety evaluation into\nthree stages: intention inference to detect obvious input risks, self\nintrospection to assess generated responses and assign confidence based\njudgments, and self revision to adaptively rewrite uncertain outputs while\npreserving user intent and enforcing safety constraints. We extensively\nevaluate SafeBehavior against five representative jailbreak attack types\nincluding optimization based, contextual manipulation, and prompt based attacks\nand compare it with seven state of the art defense baselines. Experimental\nresults show that SafeBehavior significantly improves robustness and\nadaptability across diverse threat scenarios, offering an efficient and human\ninspired approach to safeguarding LLMs against jailbreak attempts."
                },
                "authors": [
                    {
                        "name": "Qinjian Zhao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Zhiqiang Gao"
                    },
                    {
                        "name": "Zhihao Dou"
                    },
                    {
                        "name": "Belal Abuhaija"
                    },
                    {
                        "name": "Kaizhu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaizhu Huang"
                },
                "author": "Kaizhu Huang",
                "arxiv_comment": "27 pages, 5 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01413v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01413v8",
                "updated": "2025-09-30T14:50:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    50,
                    34,
                    1,
                    273,
                    0
                ],
                "published": "2025-06-02T08:11:44Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    11,
                    44,
                    0,
                    153,
                    0
                ],
                "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models"
                },
                "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose RAIF, a systematic method to boost LLMs in dealing with\ncomplex instructions via incentivizing reasoning for test-time compute scaling.\nFirst, we stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on\nOOD constraints also confirms the generalizability of our RAIF. Codes and data\nare available at https://github.com/yuleiqin/RAIF.\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose RAIF, a systematic method to boost LLMs in dealing with\ncomplex instructions via incentivizing reasoning for test-time compute scaling.\nFirst, we stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on\nOOD constraints also confirms the generalizability of our RAIF. Codes and data\nare available at https://github.com/yuleiqin/RAIF.\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions"
                },
                "authors": [
                    {
                        "name": "Yulei Qin"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Zongyi Li"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Yuchen Shi"
                    },
                    {
                        "name": "Zhekai Lin"
                    },
                    {
                        "name": "Xiao Cui"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "Accepted to NeurIPS 2025; 15 pages of main body, 5 tables, 5 figures,\n  42 pages of appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01413v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01413v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26340v1",
                "updated": "2025-09-30T14:46:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    46,
                    6,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:46:06Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    46,
                    6,
                    1,
                    273,
                    0
                ],
                "title": "Memory-Driven Self-Improvement for Decision Making with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Driven Self-Improvement for Decision Making with Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have emerged as effective action policies for\nsequential decision-making (SDM) tasks due to their extensive prior knowledge.\nHowever, this broad yet general knowledge is often insufficient for specific\ndecision-making tasks with limited task-related data, making it challenging to\nefficiently adapt LLMs to specific SDM tasks. To address this challenge, we\npropose a memory-driven self-improvement framework that combines LLM general\nprior knowledge with a compact memory of domain-specific experiences. Memory\nretains past interactions and associated Q-values, thereby capturing\ndecision-relevant knowledge that facilitates accurate value estimation and\ninforms the LLM prior refinement. The refined LLM prior, in turn, generates\nhigher-reward trajectories that further enrich memory, forming a natural\nself-improvement framework where memory and LLM prior mutually reinforce each\nother. Experiments show that our memory-driven approach significantly\noutperforms both traditional RL and LLM-based baselines, e.g., improving\nperformance by over 40\\% on in-distribution tasks and over 75\\% when\ngeneralized to unseen tasks in ALFWorld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as effective action policies for\nsequential decision-making (SDM) tasks due to their extensive prior knowledge.\nHowever, this broad yet general knowledge is often insufficient for specific\ndecision-making tasks with limited task-related data, making it challenging to\nefficiently adapt LLMs to specific SDM tasks. To address this challenge, we\npropose a memory-driven self-improvement framework that combines LLM general\nprior knowledge with a compact memory of domain-specific experiences. Memory\nretains past interactions and associated Q-values, thereby capturing\ndecision-relevant knowledge that facilitates accurate value estimation and\ninforms the LLM prior refinement. The refined LLM prior, in turn, generates\nhigher-reward trajectories that further enrich memory, forming a natural\nself-improvement framework where memory and LLM prior mutually reinforce each\nother. Experiments show that our memory-driven approach significantly\noutperforms both traditional RL and LLM-based baselines, e.g., improving\nperformance by over 40\\% on in-distribution tasks and over 75\\% when\ngeneralized to unseen tasks in ALFWorld."
                },
                "authors": [
                    {
                        "name": "Xue Yan"
                    },
                    {
                        "name": "Zijing Ou"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Yingzhen Li"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26335v1",
                "updated": "2025-09-30T14:44:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    44,
                    43,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:44:43Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    44,
                    43,
                    1,
                    273,
                    0
                ],
                "title": "TrackCore-F: Deploying Transformer-Based Subatomic Particle Tracking on\n  FPGAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrackCore-F: Deploying Transformer-Based Subatomic Particle Tracking on\n  FPGAs"
                },
                "summary": "The Transformer Machine Learning (ML) architecture has been gaining\nconsiderable momentum in recent years. In particular, computational High-Energy\nPhysics tasks such as jet tagging and particle track reconstruction (tracking),\nhave either achieved proper solutions, or reached considerable milestones using\nTransformers. On the other hand, the use of specialised hardware accelerators,\nespecially FPGAs, is an effective method to achieve online, or pseudo-online\nlatencies. The development and integration of Transformer-based ML to FPGAs is\nstill ongoing and the support from current tools is very limited to\nnon-existent. Additionally, FPGA resources present a significant constraint.\nConsidering the model size alone, while smaller models can be deployed\ndirectly, larger models are to be partitioned in a meaningful and ideally,\nautomated way. We aim to develop methodologies and tools for monolithic, or\npartitioned Transformer synthesis, specifically targeting inference. Our\nprimary use-case involves two machine learning model designs for tracking,\nderived from the TrackFormers project. We elaborate our development approach,\npresent preliminary results, and provide comparisons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer Machine Learning (ML) architecture has been gaining\nconsiderable momentum in recent years. In particular, computational High-Energy\nPhysics tasks such as jet tagging and particle track reconstruction (tracking),\nhave either achieved proper solutions, or reached considerable milestones using\nTransformers. On the other hand, the use of specialised hardware accelerators,\nespecially FPGAs, is an effective method to achieve online, or pseudo-online\nlatencies. The development and integration of Transformer-based ML to FPGAs is\nstill ongoing and the support from current tools is very limited to\nnon-existent. Additionally, FPGA resources present a significant constraint.\nConsidering the model size alone, while smaller models can be deployed\ndirectly, larger models are to be partitioned in a meaningful and ideally,\nautomated way. We aim to develop methodologies and tools for monolithic, or\npartitioned Transformer synthesis, specifically targeting inference. Our\nprimary use-case involves two machine learning model designs for tracking,\nderived from the TrackFormers project. We elaborate our development approach,\npresent preliminary results, and provide comparisons."
                },
                "authors": [
                    {
                        "name": "Arjan Blankestijn"
                    },
                    {
                        "name": "Uraz Odyurt"
                    },
                    {
                        "name": "Amirreza Yousefzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Amirreza Yousefzadeh"
                },
                "author": "Amirreza Yousefzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20295v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20295v3",
                "updated": "2025-09-30T14:44:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    44,
                    21,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-26T17:59:53Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    17,
                    59,
                    53,
                    0,
                    146,
                    0
                ],
                "title": "SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?"
                },
                "summary": "The common approach to communicate a large language model's (LLM) uncertainty\nis to add a percentage number or a hedging word to its response. But is this\nall we can do? Instead of generating a single answer and then hedging it, an\nLLM that is fully transparent to the user needs to be able to reflect on its\ninternal belief distribution and output a summary of all options it deems\npossible, and how likely they are. To test whether LLMs possess this\ncapability, we develop the SelfReflect metric, an information-theoretic\ndistance between a given summary and a distribution over answers. In\ninterventional and human studies, we find that SelfReflect indicates even\nslight deviations, yielding a fine measure of faithfulness between a summary\nstring and an LLM's actual internal distribution over answers. With\nSelfReflect, we make a resounding negative observation: modern LLMs are, across\nthe board, incapable of revealing what they are uncertain about, neither\nthrough reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we\ndo find that LLMs are able to generate faithful summaries of their\nuncertainties if we help them by sampling multiple outputs and feeding them\nback into the context. This simple approach shines a light at the universal way\nof communicating LLM uncertainties whose future development the SelfReflect\nscore enables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The common approach to communicate a large language model's (LLM) uncertainty\nis to add a percentage number or a hedging word to its response. But is this\nall we can do? Instead of generating a single answer and then hedging it, an\nLLM that is fully transparent to the user needs to be able to reflect on its\ninternal belief distribution and output a summary of all options it deems\npossible, and how likely they are. To test whether LLMs possess this\ncapability, we develop the SelfReflect metric, an information-theoretic\ndistance between a given summary and a distribution over answers. In\ninterventional and human studies, we find that SelfReflect indicates even\nslight deviations, yielding a fine measure of faithfulness between a summary\nstring and an LLM's actual internal distribution over answers. With\nSelfReflect, we make a resounding negative observation: modern LLMs are, across\nthe board, incapable of revealing what they are uncertain about, neither\nthrough reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we\ndo find that LLMs are able to generate faithful summaries of their\nuncertainties if we help them by sampling multiple outputs and feeding them\nback into the context. This simple approach shines a light at the universal way\nof communicating LLM uncertainties whose future development the SelfReflect\nscore enables."
                },
                "authors": [
                    {
                        "name": "Michael Kirchhof"
                    },
                    {
                        "name": "Luca Füger"
                    },
                    {
                        "name": "Adam Goliński"
                    },
                    {
                        "name": "Eeshan Gunesh Dhekane"
                    },
                    {
                        "name": "Arno Blaas"
                    },
                    {
                        "name": "Seong Joon Oh"
                    },
                    {
                        "name": "Sinead Williamson"
                    }
                ],
                "author_detail": {
                    "name": "Sinead Williamson"
                },
                "author": "Sinead Williamson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20295v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20295v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26331v1",
                "updated": "2025-09-30T14:43:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    43,
                    5,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:43:05Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    43,
                    5,
                    1,
                    273,
                    0
                ],
                "title": "AI Playing Business Games: Benchmarking Large Language Models on\n  Managerial Decision-Making in Dynamic Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Playing Business Games: Benchmarking Large Language Models on\n  Managerial Decision-Making in Dynamic Simulations"
                },
                "summary": "The rapid advancement of LLMs sparked significant interest in their potential\nto augment or automate managerial functions. One of the most recent trends in\nAI benchmarking is performance of Large Language Models (LLMs) over longer time\nhorizons. While LLMs excel at tasks involving natural language and pattern\nrecognition, their capabilities in multi-step, strategic business\ndecision-making remain largely unexplored. Few studies demonstrated how results\ncan be different from benchmarks in short-term tasks, as Vending-Bench\nrevealed. Meanwhile, there is a shortage of alternative benchmarks for\nlong-term coherence. This research analyses a novel benchmark using a business\ngame for the decision making in business. The research contributes to the\nrecent literature on AI by proposing a reproducible, open-access management\nsimulator to the research community for LLM benchmarking. This novel framework\nis used for evaluating the performance of five leading LLMs available in free\nonline interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes\ndecisions for a simulated retail company. A dynamic, month-by-month management\nsimulation provides transparently in spreadsheet model as experimental\nenvironment. In each of twelve months, the LLMs are provided with a structured\nprompt containing a full business report from the previous period and are\ntasked with making key strategic decisions: pricing, order size, marketing\nbudget, hiring, dismissal, loans, training expense, R&D expense, sales\nforecast, income forecast The methodology is designed to compare the LLMs on\nquantitative metrics: profit, revenue, and market share, and other KPIs. LLM\ndecisions are analyzed in their strategic coherence, adaptability to market\nchanges, and the rationale provided for their decisions. This approach allows\nto move beyond simple performance metrics for assessment of the long-term\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of LLMs sparked significant interest in their potential\nto augment or automate managerial functions. One of the most recent trends in\nAI benchmarking is performance of Large Language Models (LLMs) over longer time\nhorizons. While LLMs excel at tasks involving natural language and pattern\nrecognition, their capabilities in multi-step, strategic business\ndecision-making remain largely unexplored. Few studies demonstrated how results\ncan be different from benchmarks in short-term tasks, as Vending-Bench\nrevealed. Meanwhile, there is a shortage of alternative benchmarks for\nlong-term coherence. This research analyses a novel benchmark using a business\ngame for the decision making in business. The research contributes to the\nrecent literature on AI by proposing a reproducible, open-access management\nsimulator to the research community for LLM benchmarking. This novel framework\nis used for evaluating the performance of five leading LLMs available in free\nonline interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes\ndecisions for a simulated retail company. A dynamic, month-by-month management\nsimulation provides transparently in spreadsheet model as experimental\nenvironment. In each of twelve months, the LLMs are provided with a structured\nprompt containing a full business report from the previous period and are\ntasked with making key strategic decisions: pricing, order size, marketing\nbudget, hiring, dismissal, loans, training expense, R&D expense, sales\nforecast, income forecast The methodology is designed to compare the LLMs on\nquantitative metrics: profit, revenue, and market share, and other KPIs. LLM\ndecisions are analyzed in their strategic coherence, adaptability to market\nchanges, and the rationale provided for their decisions. This approach allows\nto move beyond simple performance metrics for assessment of the long-term\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Berdymyrat Ovezmyradov"
                    }
                ],
                "author_detail": {
                    "name": "Berdymyrat Ovezmyradov"
                },
                "author": "Berdymyrat Ovezmyradov",
                "arxiv_comment": "34 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.26628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26628v1",
                "updated": "2025-09-30T17:58:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    58,
                    34,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:58:34Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    58,
                    34,
                    1,
                    273,
                    0
                ],
                "title": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models"
                },
                "summary": "Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency."
                },
                "authors": [
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Jiakang Wang"
                    },
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Zhihui Xie"
                    },
                    {
                        "name": "Chenxin An"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Lei Lin"
                    },
                    {
                        "name": "Wenping Hu"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15849v2",
                "updated": "2025-09-30T17:58:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    58,
                    13,
                    1,
                    273,
                    0
                ],
                "published": "2025-07-21T17:56:09Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    56,
                    9,
                    0,
                    202,
                    0
                ],
                "title": "The Impact of Language Mixing on Bilingual LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Language Mixing on Bilingual LLM Reasoning"
                },
                "summary": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing-alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We show that language mixing can enhance\nreasoning: enforcing monolingual decoding reduces accuracy by 5.6 percentage\npoints on MATH500. Additionally, a lightweight probe can be trained to predict\nwhether a potential language switch would benefit or harm reasoning, and when\nused to guide decoding, increases accuracy by 2.92 percentage points. Our\nfindings suggest that language mixing is not merely a byproduct of multilingual\ntraining, but is a strategic reasoning behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing-alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We show that language mixing can enhance\nreasoning: enforcing monolingual decoding reduces accuracy by 5.6 percentage\npoints on MATH500. Additionally, a lightweight probe can be trained to predict\nwhether a potential language switch would benefit or harm reasoning, and when\nused to guide decoding, increases accuracy by 2.92 percentage points. Our\nfindings suggest that language mixing is not merely a byproduct of multilingual\ntraining, but is a strategic reasoning behavior."
                },
                "authors": [
                    {
                        "name": "Yihao Li"
                    },
                    {
                        "name": "Jiayi Xin"
                    },
                    {
                        "name": "Miranda Muqing Miao"
                    },
                    {
                        "name": "Qi Long"
                    },
                    {
                        "name": "Lyle Ungar"
                    }
                ],
                "author_detail": {
                    "name": "Lyle Ungar"
                },
                "author": "Lyle Ungar",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26626v1",
                "updated": "2025-09-30T17:58:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    58,
                    3,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:58:03Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    58,
                    3,
                    1,
                    273,
                    0
                ],
                "title": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language\n  Models"
                },
                "summary": "Test-time scaling methods improve the capabilities of large language models\n(LLMs) by increasing the amount of compute used during inference to make a\nprediction. Inference-time compute can be scaled in parallel by choosing among\nmultiple independent solutions or sequentially through self-refinement. We\npropose Recursive Self-Aggregation (RSA), a test-time scaling method inspired\nby evolutionary methods that combines the benefits of both parallel and\nsequential scaling. Each step of RSA refines a population of candidate\nreasoning chains through aggregation of subsets to yield a population of\nimproved solutions, which are then used as the candidate pool for the next\niteration. RSA exploits the rich information embedded in the reasoning chains\n-- not just the final answers -- and enables bootstrapping from partially\ncorrect intermediate steps within different chains of thought. Empirically, RSA\ndelivers substantial performance gains with increasing compute budgets across\ndiverse tasks, model families and sizes. Notably, RSA enables\nQwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning\nmodels, including DeepSeek-R1 and o3-mini (high), while outperforming purely\nparallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning\nGym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the\nmodel to combine solutions via a novel aggregation-aware reinforcement learning\napproach yields significant performance gains. Code available at\nhttps://github.com/HyperPotatoNeo/RSA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling methods improve the capabilities of large language models\n(LLMs) by increasing the amount of compute used during inference to make a\nprediction. Inference-time compute can be scaled in parallel by choosing among\nmultiple independent solutions or sequentially through self-refinement. We\npropose Recursive Self-Aggregation (RSA), a test-time scaling method inspired\nby evolutionary methods that combines the benefits of both parallel and\nsequential scaling. Each step of RSA refines a population of candidate\nreasoning chains through aggregation of subsets to yield a population of\nimproved solutions, which are then used as the candidate pool for the next\niteration. RSA exploits the rich information embedded in the reasoning chains\n-- not just the final answers -- and enables bootstrapping from partially\ncorrect intermediate steps within different chains of thought. Empirically, RSA\ndelivers substantial performance gains with increasing compute budgets across\ndiverse tasks, model families and sizes. Notably, RSA enables\nQwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning\nmodels, including DeepSeek-R1 and o3-mini (high), while outperforming purely\nparallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning\nGym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the\nmodel to combine solutions via a novel aggregation-aware reinforcement learning\napproach yields significant performance gains. Code available at\nhttps://github.com/HyperPotatoNeo/RSA."
                },
                "authors": [
                    {
                        "name": "Siddarth Venkatraman"
                    },
                    {
                        "name": "Vineet Jain"
                    },
                    {
                        "name": "Sarthak Mittal"
                    },
                    {
                        "name": "Vedant Shah"
                    },
                    {
                        "name": "Johan Obando-Ceron"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Brian R. Bartoldson"
                    },
                    {
                        "name": "Bhavya Kailkhura"
                    },
                    {
                        "name": "Guillaume Lajoie"
                    },
                    {
                        "name": "Glen Berseth"
                    },
                    {
                        "name": "Nikolay Malkin"
                    },
                    {
                        "name": "Moksh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Moksh Jain"
                },
                "author": "Moksh Jain",
                "arxiv_comment": "24 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26625v1",
                "updated": "2025-09-30T17:57:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    57,
                    44,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:57:44Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    57,
                    44,
                    1,
                    273,
                    0
                ],
                "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training"
                },
                "summary": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs."
                },
                "authors": [
                    {
                        "name": "Junlin Han"
                    },
                    {
                        "name": "Shengbang Tong"
                    },
                    {
                        "name": "David Fan"
                    },
                    {
                        "name": "Yufan Ren"
                    },
                    {
                        "name": "Koustuv Sinha"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Filippos Kokkinos"
                    }
                ],
                "author_detail": {
                    "name": "Filippos Kokkinos"
                },
                "author": "Filippos Kokkinos",
                "arxiv_comment": "Project page: https://junlinhan.github.io/projects/lsbs/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26616v1",
                "updated": "2025-09-30T17:54:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    54,
                    25,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:54:25Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    54,
                    25,
                    1,
                    273,
                    0
                ],
                "title": "Black-box Context-free Grammar Inference for Readable & Natural Grammars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Context-free Grammar Inference for Readable & Natural Grammars"
                },
                "summary": "Black-box context-free grammar inference is crucial for program analysis,\nreverse engineering, and security, yet existing tools such as Arvada, TreeVada,\nand Kedavra struggle with scalability, readability, and accuracy on large,\ncomplex languages. We present NatGI, a novel LLM-guided grammar inference\nframework that extends TreeVada's parse tree recovery with three key\ninnovations: bracket-guided bubble exploration, LLM-driven bubble generation\nand non-terminal labeling, and hierarchical delta debugging (HDD) for\nsystematic tree simplification. Bracket-guided exploration leverages syntactic\ncues such as parentheses to propose well-structured grammar fragments, while\nLLM guidance produces meaningful non-terminal names and selects more promising\nmerges. Finally, HDD incrementally reduces unnecessary rules, which makes the\ngrammars both compact and interpretable. In our experiments, we evaluate NatGI\non a comprehensive benchmark suite ranging from small languages to larger ones\nsuch as lua, c, and mysql. Our results show that NatGI consistently outperforms\nstrong baselines in terms of F1 score. On average, NatGI achieves an F1 score\nof 0.57, which is 25pp (percentage points) higher than the best-performing\nbaseline, TreeVada. In the case of interpretability, our generated grammars\nperform significantly better than those produced by existing approaches.\nLeveraging LLM-based node renaming and bubble exploration, NatGI produces rules\nwith meaningful non-terminal names and compact structures that align more\nclosely with human intuition. As a result, developers and researchers can\nachieve higher accuracy while still being able to easily inspect, verify, and\nreason about the structure and semantics of the induced grammars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box context-free grammar inference is crucial for program analysis,\nreverse engineering, and security, yet existing tools such as Arvada, TreeVada,\nand Kedavra struggle with scalability, readability, and accuracy on large,\ncomplex languages. We present NatGI, a novel LLM-guided grammar inference\nframework that extends TreeVada's parse tree recovery with three key\ninnovations: bracket-guided bubble exploration, LLM-driven bubble generation\nand non-terminal labeling, and hierarchical delta debugging (HDD) for\nsystematic tree simplification. Bracket-guided exploration leverages syntactic\ncues such as parentheses to propose well-structured grammar fragments, while\nLLM guidance produces meaningful non-terminal names and selects more promising\nmerges. Finally, HDD incrementally reduces unnecessary rules, which makes the\ngrammars both compact and interpretable. In our experiments, we evaluate NatGI\non a comprehensive benchmark suite ranging from small languages to larger ones\nsuch as lua, c, and mysql. Our results show that NatGI consistently outperforms\nstrong baselines in terms of F1 score. On average, NatGI achieves an F1 score\nof 0.57, which is 25pp (percentage points) higher than the best-performing\nbaseline, TreeVada. In the case of interpretability, our generated grammars\nperform significantly better than those produced by existing approaches.\nLeveraging LLM-based node renaming and bubble exploration, NatGI produces rules\nwith meaningful non-terminal names and compact structures that align more\nclosely with human intuition. As a result, developers and researchers can\nachieve higher accuracy while still being able to easily inspect, verify, and\nreason about the structure and semantics of the induced grammars."
                },
                "authors": [
                    {
                        "name": "Mohammad Rifat Arefin"
                    },
                    {
                        "name": "Shanto Rahman"
                    },
                    {
                        "name": "Christoph Csallner"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Csallner"
                },
                "author": "Christoph Csallner",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q42, 68Q45 (Primary), 68T50 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5; F.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26601v1",
                "updated": "2025-09-30T17:48:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    48,
                    58,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:48:58Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    48,
                    58,
                    1,
                    273,
                    0
                ],
                "title": "MENLO: From Preferences to Proficiency -- Evaluating and Modeling\n  Native-like Quality Across 47 Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MENLO: From Preferences to Proficiency -- Evaluating and Modeling\n  Native-like Quality Across 47 Languages"
                },
                "summary": "Ensuring native-like quality of large language model (LLM) responses across\nmany languages is challenging. To address this, we introduce MENLO, a framework\nthat operationalizes the evaluation of native-like response quality based on\naudience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423\nhuman-annotated prompt-response preference pairs covering four quality\ndimensions with high inter-annotator agreement in 47 language varieties. Our\nevaluation reveals that zero-shot LLM judges benefit significantly from\npairwise evaluation and our structured annotation rubrics, yet they still\nunderperform human annotators on our dataset. We demonstrate substantial\nimprovements through fine-tuning with reinforcement learning, reward shaping,\nand multi-task learning approaches. Additionally, we show that RL-trained\njudges can serve as generative reward models to enhance LLMs' multilingual\nproficiency, though discrepancies with human judgment remain. Our findings\nsuggest promising directions for scalable multilingual evaluation and\npreference alignment. We release our dataset and evaluation framework to\nsupport further research in multilingual LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring native-like quality of large language model (LLM) responses across\nmany languages is challenging. To address this, we introduce MENLO, a framework\nthat operationalizes the evaluation of native-like response quality based on\naudience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423\nhuman-annotated prompt-response preference pairs covering four quality\ndimensions with high inter-annotator agreement in 47 language varieties. Our\nevaluation reveals that zero-shot LLM judges benefit significantly from\npairwise evaluation and our structured annotation rubrics, yet they still\nunderperform human annotators on our dataset. We demonstrate substantial\nimprovements through fine-tuning with reinforcement learning, reward shaping,\nand multi-task learning approaches. Additionally, we show that RL-trained\njudges can serve as generative reward models to enhance LLMs' multilingual\nproficiency, though discrepancies with human judgment remain. Our findings\nsuggest promising directions for scalable multilingual evaluation and\npreference alignment. We release our dataset and evaluation framework to\nsupport further research in multilingual LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Chenxi Whitehouse"
                    },
                    {
                        "name": "Sebastian Ruder"
                    },
                    {
                        "name": "Tony Lin"
                    },
                    {
                        "name": "Oksana Kurylo"
                    },
                    {
                        "name": "Haruka Takagi"
                    },
                    {
                        "name": "Janice Lam"
                    },
                    {
                        "name": "Nicolò Busetto"
                    },
                    {
                        "name": "Denise Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Denise Diaz"
                },
                "author": "Denise Diaz",
                "arxiv_comment": "10 pages, 23 tables, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26600v1",
                "updated": "2025-09-30T17:48:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    48,
                    35,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:48:35Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    48,
                    35,
                    1,
                    273,
                    0
                ],
                "title": "Deconstructing Self-Bias in LLM-generated Translation Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deconstructing Self-Bias in LLM-generated Translation Benchmarks"
                },
                "summary": "As large language models (LLMs) begin to saturate existing benchmarks,\nautomated benchmark creation using LLMs (LLM as a benchmark) has emerged as a\nscalable alternative to slow and costly human curation. While these generated\ntest sets have to potential to cheaply rank models, we demonstrate a critical\nflaw. LLM generated benchmarks systematically favor the model that created the\nbenchmark, they exhibit self bias on low resource languages to English\ntranslation tasks. We show three key findings on automatic benchmarking of LLMs\nfor translation: First, this bias originates from two sources: the generated\ntest data (LLM as a testset) and the evaluation method (LLM as an evaluator),\nwith their combination amplifying the effect. Second, self bias in LLM as a\nbenchmark is heavily influenced by the model's generation capabilities in the\nsource language. For instance, we observe more pronounced bias in into English\ntranslation, where the model's generation system is developed, than in out of\nEnglish translation tasks. Third, we observe that low diversity in source text\nis one attribution to self bias. Our results suggest that improving the\ndiversity of these generated source texts can mitigate some of the observed\nself bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) begin to saturate existing benchmarks,\nautomated benchmark creation using LLMs (LLM as a benchmark) has emerged as a\nscalable alternative to slow and costly human curation. While these generated\ntest sets have to potential to cheaply rank models, we demonstrate a critical\nflaw. LLM generated benchmarks systematically favor the model that created the\nbenchmark, they exhibit self bias on low resource languages to English\ntranslation tasks. We show three key findings on automatic benchmarking of LLMs\nfor translation: First, this bias originates from two sources: the generated\ntest data (LLM as a testset) and the evaluation method (LLM as an evaluator),\nwith their combination amplifying the effect. Second, self bias in LLM as a\nbenchmark is heavily influenced by the model's generation capabilities in the\nsource language. For instance, we observe more pronounced bias in into English\ntranslation, where the model's generation system is developed, than in out of\nEnglish translation tasks. Third, we observe that low diversity in source text\nis one attribution to self bias. Our results suggest that improving the\ndiversity of these generated source texts can mitigate some of the observed\nself bias."
                },
                "authors": [
                    {
                        "name": "Wenda Xu"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Markus Freitag"
                    },
                    {
                        "name": "Daniel Deutsch"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Deutsch"
                },
                "author": "Daniel Deutsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26598v1",
                "updated": "2025-09-30T17:47:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    47,
                    9,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:47:09Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    47,
                    9,
                    1,
                    273,
                    0
                ],
                "title": "Are Robust LLM Fingerprints Adversarially Robust?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Robust LLM Fingerprints Adversarially Robust?"
                },
                "summary": "Model fingerprinting has emerged as a promising paradigm for claiming model\nownership. However, robustness evaluations of these schemes have mostly focused\non benign perturbations such as incremental fine-tuning, model merging, and\nprompting. Lack of systematic investigations into {\\em adversarial robustness}\nagainst a malicious model host leaves current systems vulnerable. To bridge\nthis gap, we first define a concrete, practical threat model against model\nfingerprinting. We then take a critical look at existing model fingerprinting\nschemes to identify their fundamental vulnerabilities. Based on these, we\ndevelop adaptive adversarial attacks tailored for each vulnerability, and\ndemonstrate that these can bypass model authentication completely for ten\nrecently proposed fingerprinting schemes while maintaining high utility of the\nmodel for the end users. Our work encourages fingerprint designers to adopt\nadversarial robustness by design. We end with recommendations for future\nfingerprinting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model fingerprinting has emerged as a promising paradigm for claiming model\nownership. However, robustness evaluations of these schemes have mostly focused\non benign perturbations such as incremental fine-tuning, model merging, and\nprompting. Lack of systematic investigations into {\\em adversarial robustness}\nagainst a malicious model host leaves current systems vulnerable. To bridge\nthis gap, we first define a concrete, practical threat model against model\nfingerprinting. We then take a critical look at existing model fingerprinting\nschemes to identify their fundamental vulnerabilities. Based on these, we\ndevelop adaptive adversarial attacks tailored for each vulnerability, and\ndemonstrate that these can bypass model authentication completely for ten\nrecently proposed fingerprinting schemes while maintaining high utility of the\nmodel for the end users. Our work encourages fingerprint designers to adopt\nadversarial robustness by design. We end with recommendations for future\nfingerprinting methods."
                },
                "authors": [
                    {
                        "name": "Anshul Nasery"
                    },
                    {
                        "name": "Edoardo Contente"
                    },
                    {
                        "name": "Alkin Kaz"
                    },
                    {
                        "name": "Pramod Viswanath"
                    },
                    {
                        "name": "Sewoong Oh"
                    }
                ],
                "author_detail": {
                    "name": "Sewoong Oh"
                },
                "author": "Sewoong Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26593v1",
                "updated": "2025-09-30T17:46:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    46,
                    39,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:46:39Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    46,
                    39,
                    1,
                    273,
                    0
                ],
                "title": "Exploring Large Language Model as an Interactive Sports Coach: Lessons\n  from a Single-Subject Half Marathon Preparation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Large Language Model as an Interactive Sports Coach: Lessons\n  from a Single-Subject Half Marathon Preparation"
                },
                "summary": "Large language models (LLMs) are emerging as everyday assistants, but their\nrole as longitudinal virtual coaches is underexplored. This two-month single\nsubject case study documents LLM guided half marathon preparation\n(July-September 2025). Using text based interactions and consumer app logs, the\nLLM acted as planner, explainer, and occasional motivator. Performance improved\nfrom sustaining 2 km at 7min 54sec per km to completing 21.1 km at 6min 30sec\nper km, with gains in cadence, pace HR coupling, and efficiency index trends.\nWhile causal attribution is limited without a control, outcomes demonstrate\nsafe, measurable progress. At the same time, gaps were evident, no realtime\nsensor integration, text only feedback, motivation support that was user\ninitiated, and limited personalization or safety guardrails. We propose design\nrequirements for next generation systems, persistent athlete models with\nexplicit guardrails, multimodal on device sensing, audio, haptic, visual\nfeedback, proactive motivation scaffolds, and privacy-preserving\npersonalization. This study offers grounded evidence and a design agenda for\nevolving LLMs from retrospective advisors to closed-loop coaching companions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are emerging as everyday assistants, but their\nrole as longitudinal virtual coaches is underexplored. This two-month single\nsubject case study documents LLM guided half marathon preparation\n(July-September 2025). Using text based interactions and consumer app logs, the\nLLM acted as planner, explainer, and occasional motivator. Performance improved\nfrom sustaining 2 km at 7min 54sec per km to completing 21.1 km at 6min 30sec\nper km, with gains in cadence, pace HR coupling, and efficiency index trends.\nWhile causal attribution is limited without a control, outcomes demonstrate\nsafe, measurable progress. At the same time, gaps were evident, no realtime\nsensor integration, text only feedback, motivation support that was user\ninitiated, and limited personalization or safety guardrails. We propose design\nrequirements for next generation systems, persistent athlete models with\nexplicit guardrails, multimodal on device sensing, audio, haptic, visual\nfeedback, proactive motivation scaffolds, and privacy-preserving\npersonalization. This study offers grounded evidence and a design agenda for\nevolving LLMs from retrospective advisors to closed-loop coaching companions."
                },
                "authors": [
                    {
                        "name": "Kichang Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kichang Lee"
                },
                "author": "Kichang Lee",
                "arxiv_comment": "23 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4, K.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26592v1",
                "updated": "2025-09-30T17:46:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    46,
                    8,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:46:08Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    46,
                    8,
                    1,
                    273,
                    0
                ],
                "title": "Generating Difficult-to-Translate Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Difficult-to-Translate Texts"
                },
                "summary": "Machine translation benchmarks sourced from the real world are quickly\nobsoleted, due to most examples being easy for state-of-the-art translation\nmodels. This limits the benchmark's ability to distinguish which model is\nbetter or to reveal models' weaknesses. Current methods for creating difficult\ntest cases, such as subsampling or from-scratch synthesis, either fall short of\nidentifying difficult examples or suffer from a lack of diversity and\nnaturalness. Inspired by the iterative process of human experts probing for\nmodel failures, we propose MT-breaker, a method where a large language model\niteratively refines a source text to increase its translation difficulty. The\nLLM iteratively queries a target machine translation model to guide its\ngeneration of difficult examples. Our approach generates examples that are more\nchallenging for the target MT model while preserving the diversity of natural\ntexts. While the examples are tailored to a particular machine translation\nmodel during the generation, the difficulty also transfers to other models and\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine translation benchmarks sourced from the real world are quickly\nobsoleted, due to most examples being easy for state-of-the-art translation\nmodels. This limits the benchmark's ability to distinguish which model is\nbetter or to reveal models' weaknesses. Current methods for creating difficult\ntest cases, such as subsampling or from-scratch synthesis, either fall short of\nidentifying difficult examples or suffer from a lack of diversity and\nnaturalness. Inspired by the iterative process of human experts probing for\nmodel failures, we propose MT-breaker, a method where a large language model\niteratively refines a source text to increase its translation difficulty. The\nLLM iteratively queries a target machine translation model to guide its\ngeneration of difficult examples. Our approach generates examples that are more\nchallenging for the target MT model while preserving the diversity of natural\ntexts. While the examples are tailored to a particular machine translation\nmodel during the generation, the difficulty also transfers to other models and\nlanguages."
                },
                "authors": [
                    {
                        "name": "Vilém Zouhar"
                    },
                    {
                        "name": "Wenda Xu"
                    },
                    {
                        "name": "Parker Riley"
                    },
                    {
                        "name": "Juraj Juraska"
                    },
                    {
                        "name": "Mara Finkelstein"
                    },
                    {
                        "name": "Markus Freitag"
                    },
                    {
                        "name": "Dan Deutsch"
                    }
                ],
                "author_detail": {
                    "name": "Dan Deutsch"
                },
                "author": "Dan Deutsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23459v2",
                "updated": "2025-09-30T17:43:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    43,
                    21,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-27T19:07:50Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    19,
                    7,
                    50,
                    5,
                    270,
                    0
                ],
                "title": "MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction"
                },
                "summary": "Large language models (LLMs) have shown promising performance on tasks that\nrequire reasoning, such as text-to-SQL, code generation, and debugging.\nHowever, regulatory frameworks with strict privacy requirements constrain their\nintegration into sensitive systems. State-of-the-art LLMs are also proprietary,\ncostly, and resource-intensive, making local deployment impractical.\nConsequently, utilizing such LLMs often requires sharing data with third-party\nproviders, raising privacy concerns and risking noncompliance with regulations.\nAlthough fine-tuned small language models (SLMs) can outperform LLMs on certain\ntasks and be deployed locally to mitigate privacy concerns, they underperform\non more complex tasks such as text-to-SQL translation. In this work, we\nintroduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a\nprivacy protection mechanism to mask sensitive information in LLM prompts.\nUnlike redaction, which removes content entirely, or generalization, which\nbroadens tokens, abstraction retains essential information while discarding\nunnecessary details, striking an effective privacy-utility balance for the\ntext-to-SQL task. Moreover, by providing mechanisms to control the\nprivacy-utility tradeoff, MaskSQL facilitates adoption across a broader range\nof use cases. Our experimental results show that MaskSQL outperforms leading\nSLM-based text-to-SQL models and achieves performance approaching\nstate-of-the-art LLM-based models, while preserving privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promising performance on tasks that\nrequire reasoning, such as text-to-SQL, code generation, and debugging.\nHowever, regulatory frameworks with strict privacy requirements constrain their\nintegration into sensitive systems. State-of-the-art LLMs are also proprietary,\ncostly, and resource-intensive, making local deployment impractical.\nConsequently, utilizing such LLMs often requires sharing data with third-party\nproviders, raising privacy concerns and risking noncompliance with regulations.\nAlthough fine-tuned small language models (SLMs) can outperform LLMs on certain\ntasks and be deployed locally to mitigate privacy concerns, they underperform\non more complex tasks such as text-to-SQL translation. In this work, we\nintroduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a\nprivacy protection mechanism to mask sensitive information in LLM prompts.\nUnlike redaction, which removes content entirely, or generalization, which\nbroadens tokens, abstraction retains essential information while discarding\nunnecessary details, striking an effective privacy-utility balance for the\ntext-to-SQL task. Moreover, by providing mechanisms to control the\nprivacy-utility tradeoff, MaskSQL facilitates adoption across a broader range\nof use cases. Our experimental results show that MaskSQL outperforms leading\nSLM-based text-to-SQL models and achieves performance approaching\nstate-of-the-art LLM-based models, while preserving privacy."
                },
                "authors": [
                    {
                        "name": "Sepideh Abedini"
                    },
                    {
                        "name": "Shubhankar Mohapatra"
                    },
                    {
                        "name": "D. B. Emerson"
                    },
                    {
                        "name": "Masoumeh Shafieinejad"
                    },
                    {
                        "name": "Jesse C. Cresswell"
                    },
                    {
                        "name": "Xi He"
                    }
                ],
                "author_detail": {
                    "name": "Xi He"
                },
                "author": "Xi He",
                "arxiv_comment": "Accepted to the 3rd Workshop on Regulatable ML at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26584v1",
                "updated": "2025-09-30T17:42:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    42,
                    35,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:42:35Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    42,
                    35,
                    1,
                    273,
                    0
                ],
                "title": "Fairness Testing in Retrieval-Augmented Generation: How Small\n  Perturbations Reveal Bias in Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness Testing in Retrieval-Augmented Generation: How Small\n  Perturbations Reveal Bias in Small Language Models"
                },
                "summary": "Large Language Models (LLMs) are widely used across multiple domains but\ncontinue to raise concerns regarding security and fairness. Beyond known attack\nvectors such as data poisoning and prompt injection, LLMs are also vulnerable\nto fairness bugs. These refer to unintended behaviors influenced by sensitive\ndemographic cues (e.g., race or sexual orientation) that should not affect\noutcomes. Another key issue is hallucination, where models generate plausible\nyet false information. Retrieval-Augmented Generation (RAG) has emerged as a\nstrategy to mitigate hallucinations by combining external retrieval with text\ngeneration. However, its adoption raises new fairness concerns, as the\nretrieved content itself may surface or amplify bias. This study conducts\nfairness testing through metamorphic testing (MT), introducing controlled\ndemographic perturbations in prompts to assess fairness in sentiment analysis\nperformed by three Small Language Models (SLMs) hosted on HuggingFace\n(Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B),\neach integrated into a RAG pipeline. Results show that minor demographic\nvariations can break up to one third of metamorphic relations (MRs). A detailed\nanalysis of these failures reveals a consistent bias hierarchy, with\nperturbations involving racial cues being the predominant cause of the\nviolations. In addition to offering a comparative evaluation, this work\nreinforces that the retrieval component in RAG must be carefully curated to\nprevent bias amplification. The findings serve as a practical alert for\ndevelopers, testers and small organizations aiming to adopt accessible SLMs\nwithout compromising fairness or reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across multiple domains but\ncontinue to raise concerns regarding security and fairness. Beyond known attack\nvectors such as data poisoning and prompt injection, LLMs are also vulnerable\nto fairness bugs. These refer to unintended behaviors influenced by sensitive\ndemographic cues (e.g., race or sexual orientation) that should not affect\noutcomes. Another key issue is hallucination, where models generate plausible\nyet false information. Retrieval-Augmented Generation (RAG) has emerged as a\nstrategy to mitigate hallucinations by combining external retrieval with text\ngeneration. However, its adoption raises new fairness concerns, as the\nretrieved content itself may surface or amplify bias. This study conducts\nfairness testing through metamorphic testing (MT), introducing controlled\ndemographic perturbations in prompts to assess fairness in sentiment analysis\nperformed by three Small Language Models (SLMs) hosted on HuggingFace\n(Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B),\neach integrated into a RAG pipeline. Results show that minor demographic\nvariations can break up to one third of metamorphic relations (MRs). A detailed\nanalysis of these failures reveals a consistent bias hierarchy, with\nperturbations involving racial cues being the predominant cause of the\nviolations. In addition to offering a comparative evaluation, this work\nreinforces that the retrieval component in RAG must be carefully curated to\nprevent bias amplification. The findings serve as a practical alert for\ndevelopers, testers and small organizations aiming to adopt accessible SLMs\nwithout compromising fairness or reliability."
                },
                "authors": [
                    {
                        "name": "Matheus Vinicius da Silva de Oliveira"
                    },
                    {
                        "name": "Jonathan de Andrade Silva"
                    },
                    {
                        "name": "Awdren de Lima Fontao"
                    }
                ],
                "author_detail": {
                    "name": "Awdren de Lima Fontao"
                },
                "author": "Awdren de Lima Fontao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26578v1",
                "updated": "2025-09-30T17:38:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    38,
                    45,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:38:45Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    38,
                    45,
                    1,
                    273,
                    0
                ],
                "title": "Linking Process to Outcome: Conditional Reward Modeling for LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linking Process to Outcome: Conditional Reward Modeling for LLM\n  Reasoning"
                },
                "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning capabilities of large language models (LLMs) by guiding their\nstep-by-step reasoning toward a final answer. However, existing PRMs either\ntreat each reasoning step in isolation, failing to capture inter-step\ndependencies, or struggle to align process rewards with the final outcome.\nConsequently, the reward signal fails to respect temporal causality in\nsequential reasoning and faces ambiguous credit assignment. These limitations\nmake downstream models vulnerable to reward hacking and lead to suboptimal\nperformance. In this work, we propose Conditional Reward Modeling (CRM) that\nframes LLM reasoning as a temporal process leading to a correct answer. The\nreward of each reasoning step is not only conditioned on the preceding steps\nbut also explicitly linked to the final outcome of the reasoning trajectory. By\nenforcing conditional probability rules, our design captures the causal\nrelationships among reasoning steps, with the link to the outcome allowing\nprecise attribution of each intermediate step, thereby resolving credit\nassignment ambiguity. Further, through this consistent probabilistic modeling,\nthe rewards produced by CRM enable more reliable cross-sample comparison.\nExperiments across Best-of-N sampling, beam search and reinforcement learning\ndemonstrate that CRM consistently outperforms existing reward models, offering\na principled framework for enhancing LLM reasoning. In particular, CRM is more\nrobust to reward hacking and delivers stable downstream improvements without\nrelying on verifiable rewards derived from ground truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning capabilities of large language models (LLMs) by guiding their\nstep-by-step reasoning toward a final answer. However, existing PRMs either\ntreat each reasoning step in isolation, failing to capture inter-step\ndependencies, or struggle to align process rewards with the final outcome.\nConsequently, the reward signal fails to respect temporal causality in\nsequential reasoning and faces ambiguous credit assignment. These limitations\nmake downstream models vulnerable to reward hacking and lead to suboptimal\nperformance. In this work, we propose Conditional Reward Modeling (CRM) that\nframes LLM reasoning as a temporal process leading to a correct answer. The\nreward of each reasoning step is not only conditioned on the preceding steps\nbut also explicitly linked to the final outcome of the reasoning trajectory. By\nenforcing conditional probability rules, our design captures the causal\nrelationships among reasoning steps, with the link to the outcome allowing\nprecise attribution of each intermediate step, thereby resolving credit\nassignment ambiguity. Further, through this consistent probabilistic modeling,\nthe rewards produced by CRM enable more reliable cross-sample comparison.\nExperiments across Best-of-N sampling, beam search and reinforcement learning\ndemonstrate that CRM consistently outperforms existing reward models, offering\na principled framework for enhancing LLM reasoning. In particular, CRM is more\nrobust to reward hacking and delivers stable downstream improvements without\nrelying on verifiable rewards derived from ground truth."
                },
                "authors": [
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Ziwei Shan"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Yexin Li"
                    },
                    {
                        "name": "Kan Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kan Ren"
                },
                "author": "Kan Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26574v2",
                "updated": "2025-10-01T02:12:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    12,
                    55,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T17:34:03Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    34,
                    3,
                    1,
                    273,
                    0
                ],
                "title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics\n  Research Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics\n  Research Benchmark"
                },
                "summary": "While large language models (LLMs) with reasoning capabilities are\nprogressing rapidly on high-school math competitions and coding, can they\nreason effectively through complex, open-ended challenges found in frontier\nphysics research? And crucially, what kinds of reasoning tasks do physicists\nwant LLMs to assist with? To address these questions, we present the CritPt\n(Complex Research using Integrated Thinking - Physics Test, pronounced\n\"critical point\"), the first benchmark designed to test LLMs on unpublished,\nresearch-level reasoning tasks that broadly covers modern physics research\nareas, including condensed matter, quantum physics, atomic, molecular & optical\nphysics, astrophysics, high energy physics, mathematical physics, statistical\nphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.\nCritPt consists of 71 composite research challenges designed to simulate\nfull-scale research projects at the entry level, which are also decomposed to\n190 simpler checkpoint tasks for more fine-grained insights. All problems are\nnewly created by 50+ active physics researchers based on their own research.\nEvery problem is hand-curated to admit a guess-resistant and machine-verifiable\nanswer and is evaluated by an automated grading pipeline heavily customized for\nadvanced physics-specific output formats. We find that while current\nstate-of-the-art LLMs show early promise on isolated checkpoints, they remain\nfar from being able to reliably solve full research-scale challenges: the best\naverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),\nmoderately rising to around 10% when equipped with coding tools. Through the\nrealistic yet standardized evaluation offered by CritPt, we highlight a large\ndisconnect between current model capabilities and realistic physics research\ndemands, offering a foundation to guide the development of scientifically\ngrounded AI tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) with reasoning capabilities are\nprogressing rapidly on high-school math competitions and coding, can they\nreason effectively through complex, open-ended challenges found in frontier\nphysics research? And crucially, what kinds of reasoning tasks do physicists\nwant LLMs to assist with? To address these questions, we present the CritPt\n(Complex Research using Integrated Thinking - Physics Test, pronounced\n\"critical point\"), the first benchmark designed to test LLMs on unpublished,\nresearch-level reasoning tasks that broadly covers modern physics research\nareas, including condensed matter, quantum physics, atomic, molecular & optical\nphysics, astrophysics, high energy physics, mathematical physics, statistical\nphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.\nCritPt consists of 71 composite research challenges designed to simulate\nfull-scale research projects at the entry level, which are also decomposed to\n190 simpler checkpoint tasks for more fine-grained insights. All problems are\nnewly created by 50+ active physics researchers based on their own research.\nEvery problem is hand-curated to admit a guess-resistant and machine-verifiable\nanswer and is evaluated by an automated grading pipeline heavily customized for\nadvanced physics-specific output formats. We find that while current\nstate-of-the-art LLMs show early promise on isolated checkpoints, they remain\nfar from being able to reliably solve full research-scale challenges: the best\naverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),\nmoderately rising to around 10% when equipped with coding tools. Through the\nrealistic yet standardized evaluation offered by CritPt, we highlight a large\ndisconnect between current model capabilities and realistic physics research\ndemands, offering a foundation to guide the development of scientifically\ngrounded AI tools."
                },
                "authors": [
                    {
                        "name": "Minhui Zhu"
                    },
                    {
                        "name": "Minyang Tian"
                    },
                    {
                        "name": "Xiaocheng Yang"
                    },
                    {
                        "name": "Tianci Zhou"
                    },
                    {
                        "name": "Penghao Zhu"
                    },
                    {
                        "name": "Eli Chertkov"
                    },
                    {
                        "name": "Shengyan Liu"
                    },
                    {
                        "name": "Yufeng Du"
                    },
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Ziming Ji"
                    },
                    {
                        "name": "Indranil Das"
                    },
                    {
                        "name": "Junyi Cao"
                    },
                    {
                        "name": "Yufeng Du"
                    },
                    {
                        "name": "Jinchen He"
                    },
                    {
                        "name": "Yifan Su"
                    },
                    {
                        "name": "Jiabin Yu"
                    },
                    {
                        "name": "Yikun Jiang"
                    },
                    {
                        "name": "Yujie Zhang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ze-Min Huang"
                    },
                    {
                        "name": "Weizhen Jia"
                    },
                    {
                        "name": "Xinan Chen"
                    },
                    {
                        "name": "Peixue Wu"
                    },
                    {
                        "name": "Yunkai Wang"
                    },
                    {
                        "name": "Juntai Zhou"
                    },
                    {
                        "name": "Yong Zhao"
                    },
                    {
                        "name": "Farshid Jafarpour"
                    },
                    {
                        "name": "Jessie Shelton"
                    },
                    {
                        "name": "Aaron Young"
                    },
                    {
                        "name": "John Bartolotta"
                    },
                    {
                        "name": "Wenchao Xu"
                    },
                    {
                        "name": "Yue Sun"
                    },
                    {
                        "name": "Anjun Chu"
                    },
                    {
                        "name": "Victor Colussi"
                    },
                    {
                        "name": "Chris Akers"
                    },
                    {
                        "name": "Nathan Brooks"
                    },
                    {
                        "name": "Wenbo Fu"
                    },
                    {
                        "name": "Christopher Wilson"
                    },
                    {
                        "name": "Jinchao Zhao"
                    },
                    {
                        "name": "Marvin Qi"
                    },
                    {
                        "name": "Anqi Mu"
                    },
                    {
                        "name": "Yubo Yang"
                    },
                    {
                        "name": "Allen Zang"
                    },
                    {
                        "name": "Yang Lyu"
                    },
                    {
                        "name": "Peizhi Mai"
                    },
                    {
                        "name": "Xuefei Guo"
                    },
                    {
                        "name": "Luyu Gao"
                    },
                    {
                        "name": "Ze Yang"
                    },
                    {
                        "name": "Chi Xue"
                    },
                    {
                        "name": "Dmytro Bandak"
                    },
                    {
                        "name": "Yaïr Hein"
                    },
                    {
                        "name": "Yonatan Kahn"
                    },
                    {
                        "name": "Kevin Zhou"
                    },
                    {
                        "name": "John Drew Wilson"
                    },
                    {
                        "name": "Jarrod T. Reilly"
                    },
                    {
                        "name": "Di Luo"
                    },
                    {
                        "name": "Daniel Inafuku"
                    },
                    {
                        "name": "Hao Tong"
                    },
                    {
                        "name": "Liang Yang"
                    },
                    {
                        "name": "Ruixing Zhang"
                    },
                    {
                        "name": "Xueying Wang"
                    },
                    {
                        "name": "Ofir Press"
                    },
                    {
                        "name": "Nicolas Chia"
                    },
                    {
                        "name": "Eliu Huerta"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "arxiv_comment": "39 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26570v1",
                "updated": "2025-09-30T17:33:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    33,
                    3,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:33:03Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    33,
                    3,
                    1,
                    273,
                    0
                ],
                "title": "Electrical Readout of Spin Environments in Diamond for Quantum Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrical Readout of Spin Environments in Diamond for Quantum Sensing"
                },
                "summary": "Nitrogen-vacancy (NV) centres in diamond are a key platform for quantum\nsensing and quantum information, combining long coherence times with\ncontrollable spin-spin interactions. Most of current quantum algorithms rely on\noptical access, which limit device integration and applicability in opaque or\nminiaturized settings. Here we demonstrate an all-electrical approach,\nphotocurrent double electron-electron resonance (PC-DEER), permitting\nexploiting local dipolar interactions between individual NV spin qubits or\nensembles and nearby paramagnetic defects with sub-confocal resolution. PC-DEER\nextends photocurrent NV readout from single-spin to spin-bath control and\ncoherent manipulation, enabling characterization of bath-induced noise and\neffective deployment of noise-reduction protocols. We resolve the signatures of\nsubstitutional nitrogen (P1) and NVH centers with reproducible contrast by\nusing electrical signals. Our results establish a scalable, optical-free spin\nreadout strategy that bridges fundamental studies of spin environments with\ndeployable quantum technologies, advancing the integration of diamond-based\nsensors into solid-state quantum devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nitrogen-vacancy (NV) centres in diamond are a key platform for quantum\nsensing and quantum information, combining long coherence times with\ncontrollable spin-spin interactions. Most of current quantum algorithms rely on\noptical access, which limit device integration and applicability in opaque or\nminiaturized settings. Here we demonstrate an all-electrical approach,\nphotocurrent double electron-electron resonance (PC-DEER), permitting\nexploiting local dipolar interactions between individual NV spin qubits or\nensembles and nearby paramagnetic defects with sub-confocal resolution. PC-DEER\nextends photocurrent NV readout from single-spin to spin-bath control and\ncoherent manipulation, enabling characterization of bath-induced noise and\neffective deployment of noise-reduction protocols. We resolve the signatures of\nsubstitutional nitrogen (P1) and NVH centers with reproducible contrast by\nusing electrical signals. Our results establish a scalable, optical-free spin\nreadout strategy that bridges fundamental studies of spin environments with\ndeployable quantum technologies, advancing the integration of diamond-based\nsensors into solid-state quantum devices."
                },
                "authors": [
                    {
                        "name": "Olga Rubinas"
                    },
                    {
                        "name": "Michael Petrov"
                    },
                    {
                        "name": "Emilie Bourgeois"
                    },
                    {
                        "name": "Jaroslav Hruby"
                    },
                    {
                        "name": "Akhil Kuriakose"
                    },
                    {
                        "name": "Ottavia Jedrkiewicz"
                    },
                    {
                        "name": "Milos Nesladek"
                    }
                ],
                "author_detail": {
                    "name": "Milos Nesladek"
                },
                "author": "Milos Nesladek",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03015v2",
                "updated": "2025-09-30T17:27:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    27,
                    42,
                    1,
                    273,
                    0
                ],
                "published": "2025-04-03T20:20:00Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    20,
                    20,
                    0,
                    3,
                    93,
                    0
                ],
                "title": "AuDeRe: Automated Strategy Decision and Realization in Robot Planning\n  and Control via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AuDeRe: Automated Strategy Decision and Realization in Robot Planning\n  and Control via LLMs"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown significant\npromise in various domains, especially robotics. However, most prior LLM-based\nwork in robotic applications either directly predicts waypoints or applies LLMs\nwithin fixed tool integration frameworks, offering limited flexibility in\nexploring and configuring solutions best suited to different tasks. In this\nwork, we propose a framework that leverages LLMs to select appropriate planning\nand control strategies based on task descriptions, environmental constraints,\nand system dynamics. These strategies are then executed by calling the\navailable comprehensive planning and control APIs. Our approach employs\niterative LLM-based reasoning with performance feedback to refine the algorithm\nselection. We validate our approach through extensive experiments across tasks\nof varying complexity, from simple tracking to complex planning scenarios\ninvolving spatiotemporal constraints. The results demonstrate that using LLMs\nto determine planning and control strategies from natural language descriptions\nsignificantly enhances robotic autonomy while reducing the need for extensive\nmanual tuning and expert knowledge. Furthermore, our framework maintains\ngeneralizability across different tasks and notably outperforms baseline\nmethods that rely on LLMs for direct trajectory, control sequence, or code\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown significant\npromise in various domains, especially robotics. However, most prior LLM-based\nwork in robotic applications either directly predicts waypoints or applies LLMs\nwithin fixed tool integration frameworks, offering limited flexibility in\nexploring and configuring solutions best suited to different tasks. In this\nwork, we propose a framework that leverages LLMs to select appropriate planning\nand control strategies based on task descriptions, environmental constraints,\nand system dynamics. These strategies are then executed by calling the\navailable comprehensive planning and control APIs. Our approach employs\niterative LLM-based reasoning with performance feedback to refine the algorithm\nselection. We validate our approach through extensive experiments across tasks\nof varying complexity, from simple tracking to complex planning scenarios\ninvolving spatiotemporal constraints. The results demonstrate that using LLMs\nto determine planning and control strategies from natural language descriptions\nsignificantly enhances robotic autonomy while reducing the need for extensive\nmanual tuning and expert knowledge. Furthermore, our framework maintains\ngeneralizability across different tasks and notably outperforms baseline\nmethods that rely on LLMs for direct trajectory, control sequence, or code\ngeneration."
                },
                "authors": [
                    {
                        "name": "Yue Meng"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Yongchao Chen"
                    },
                    {
                        "name": "Chuchu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Chuchu Fan"
                },
                "author": "Chuchu Fan",
                "arxiv_comment": "8 pages, 14 figures, submitted to the 2026 American Control\n  Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01055v2",
                "updated": "2025-09-30T17:22:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    22,
                    37,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-01T01:45:18Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    1,
                    45,
                    18,
                    0,
                    244,
                    0
                ],
                "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated\nsuccess in enhancing LLM reasoning capabilities, but remains limited to\nsingle-turn interactions without tool integration. While recent Agentic\nReinforcement Learning with Tool use (ARLT) approaches have emerged to address\nmulti-turn tool interactions, existing works develop task-specific codebases\nthat suffer from fragmentation, synchronous execution bottlenecks, and limited\nextensibility across domains. These inefficiencies hinder broader community\nadoption and algorithmic innovation. We introduce VerlTool, a unified and\nmodular framework that addresses these limitations through systematic design\nprinciples. VerlTool provides four key contributions: (1) upstream alignment\nwith VeRL ensuring compatibility and simplified maintenance, (2) unified tool\nmanagement via standardized APIs supporting diverse modalities including code\nexecution, search, SQL databases, and vision processing, (3) asynchronous\nrollout execution achieving near 2$\\times$ speedup by eliminating\nsynchronization bottlenecks, and (4) comprehensive evaluation demonstrating\ncompetitive performance across 6 ARLT domains. Our framework formalizes ARLT as\nmulti-turn trajectories with multi-modal observation tokens (text/image/video),\nextending beyond single-turn RLVR paradigms. We train and evaluate models on\nmathematical reasoning, knowledge QA, SQL generation, visual reasoning, web\nsearch, and software engineering tasks, achieving results comparable to\nspecialized systems while providing unified training infrastructure. The\nmodular plugin architecture enables rapid tool integration requiring only\nlightweight Python definitions, significantly reducing development overhead and\nproviding a scalable foundation for tool-augmented RL research. Our code is\nopen-sourced at https://github.com/TIGER-AI-Lab/verl-tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated\nsuccess in enhancing LLM reasoning capabilities, but remains limited to\nsingle-turn interactions without tool integration. While recent Agentic\nReinforcement Learning with Tool use (ARLT) approaches have emerged to address\nmulti-turn tool interactions, existing works develop task-specific codebases\nthat suffer from fragmentation, synchronous execution bottlenecks, and limited\nextensibility across domains. These inefficiencies hinder broader community\nadoption and algorithmic innovation. We introduce VerlTool, a unified and\nmodular framework that addresses these limitations through systematic design\nprinciples. VerlTool provides four key contributions: (1) upstream alignment\nwith VeRL ensuring compatibility and simplified maintenance, (2) unified tool\nmanagement via standardized APIs supporting diverse modalities including code\nexecution, search, SQL databases, and vision processing, (3) asynchronous\nrollout execution achieving near 2$\\times$ speedup by eliminating\nsynchronization bottlenecks, and (4) comprehensive evaluation demonstrating\ncompetitive performance across 6 ARLT domains. Our framework formalizes ARLT as\nmulti-turn trajectories with multi-modal observation tokens (text/image/video),\nextending beyond single-turn RLVR paradigms. We train and evaluate models on\nmathematical reasoning, knowledge QA, SQL generation, visual reasoning, web\nsearch, and software engineering tasks, achieving results comparable to\nspecialized systems while providing unified training infrastructure. The\nmodular plugin architecture enables rapid tool integration requiring only\nlightweight Python definitions, significantly reducing development overhead and\nproviding a scalable foundation for tool-augmented RL research. Our code is\nopen-sourced at https://github.com/TIGER-AI-Lab/verl-tool."
                },
                "authors": [
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Yi Lu"
                    },
                    {
                        "name": "Zhuofeng Li"
                    },
                    {
                        "name": "Zhiheng Lyu"
                    },
                    {
                        "name": "Ping Nie"
                    },
                    {
                        "name": "Haozhe Wang"
                    },
                    {
                        "name": "Alex Su"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Kai Zou"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "32 pages, 5 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20039v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20039v2",
                "updated": "2025-09-30T17:21:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    21,
                    23,
                    1,
                    273,
                    0
                ],
                "published": "2025-04-28T17:59:28Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    59,
                    28,
                    0,
                    118,
                    0
                ],
                "title": "AutoJudge: Judge Decoding Without Manual Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoJudge: Judge Decoding Without Manual Annotation"
                },
                "summary": "We introduce AutoJudge, a method that accelerates large language model (LLM)\ninference with task-specific lossy speculative decoding. Instead of matching\nthe original model output distribution token-by-token, we identify which of the\ngenerated tokens affect the downstream quality of the response, relaxing the\ndistribution match guarantee so that the \"unimportant\" tokens can be generated\nfaster. Our approach relies on a semi-greedy search algorithm to test which of\nthe mismatches between target and draft models should be corrected to preserve\nquality and which ones may be skipped. We then train a lightweight classifier\nbased on existing LLM embeddings to predict, at inference time, which\nmismatching tokens can be safely accepted without compromising the final answer\nquality. We evaluate the effectiveness of AutoJudge with multiple draft/target\nmodel pairs on mathematical reasoning and programming benchmarks, achieving\nsignificant speedups at the cost of a minor accuracy reduction. Notably, on\nGSM8k with the Llama 3.1 70B target model, our approach achieves up to\n$\\approx2\\times$ speedup over speculative decoding at the cost of $\\le 1\\%$\ndrop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge\nautomatically detects programming-specific important tokens, accepting $\\ge 25$\ntokens per speculation cycle at $2\\%$ drop in Pass@1. Our approach requires no\nhuman annotation and is easy to integrate with modern LLM inference frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce AutoJudge, a method that accelerates large language model (LLM)\ninference with task-specific lossy speculative decoding. Instead of matching\nthe original model output distribution token-by-token, we identify which of the\ngenerated tokens affect the downstream quality of the response, relaxing the\ndistribution match guarantee so that the \"unimportant\" tokens can be generated\nfaster. Our approach relies on a semi-greedy search algorithm to test which of\nthe mismatches between target and draft models should be corrected to preserve\nquality and which ones may be skipped. We then train a lightweight classifier\nbased on existing LLM embeddings to predict, at inference time, which\nmismatching tokens can be safely accepted without compromising the final answer\nquality. We evaluate the effectiveness of AutoJudge with multiple draft/target\nmodel pairs on mathematical reasoning and programming benchmarks, achieving\nsignificant speedups at the cost of a minor accuracy reduction. Notably, on\nGSM8k with the Llama 3.1 70B target model, our approach achieves up to\n$\\approx2\\times$ speedup over speculative decoding at the cost of $\\le 1\\%$\ndrop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge\nautomatically detects programming-specific important tokens, accepting $\\ge 25$\ntokens per speculation cycle at $2\\%$ drop in Pass@1. Our approach requires no\nhuman annotation and is easy to integrate with modern LLM inference frameworks."
                },
                "authors": [
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Fedor Velikonivtsev"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20039v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20039v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26553v1",
                "updated": "2025-09-30T17:21:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    21,
                    17,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:21:17Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    21,
                    17,
                    1,
                    273,
                    0
                ],
                "title": "Towards Reliable Benchmarking: A Contamination Free, Controllable\n  Evaluation Framework for Multi-step LLM Function Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Benchmarking: A Contamination Free, Controllable\n  Evaluation Framework for Multi-step LLM Function Calling"
                },
                "summary": "As language models gain access to external tools via structured function\ncalls, they become increasingly more capable of solving complex, multi-step\ntasks. However, existing benchmarks for tool-augmented language models (TaLMs)\nprovide insufficient control over factors such as the number of functions\naccessible, task complexity, and input size, and remain vulnerable to data\ncontamination. We present FuncBenchGen, a unified, contamination-free framework\nthat evaluates TaLMs by generating synthetic multi-step tool-use tasks. The key\nidea is to cast tool use as traversal over a hidden function-dependency DAG\nwhere nodes are function calls and an edge between nodes represents one\nfunction consuming the output of another. Given a set of external function\nschemas, initial variable values, and a target variable, models must compose\nthe correct call sequence to compute the target variable. FuncBenchGen allows\nusers to precisely control task difficulty (e.g., graph size, dependency depth,\nand distractor functions) while avoiding data leakage. We apply our\nFuncBenchGen framework to evaluate seven LLMs on tool use tasks of varying\ndifficulty. Reasoning-optimized models consistently outperform general-purpose\nmodels with GPT-5 significantly outperforming other models. Performance\ndeclines sharply as dependency depth increases. Furthermore, connected\nirrelevant functions prove especially difficult to handle. We find that strong\nmodels often make syntactically valid function calls but propagate incorrect or\nstale argument values across steps, revealing brittle state tracking by LLMs in\nmulti-turn tool use. Motivated by this observation, we introduce a simple\nmitigation strategy that explicitly restates prior variable values to the agent\nat each step. Surprisingly, this lightweight change yields substantial gains\nacross models. e.g., yielding a success rate improvement from 62.5% to 81.3%\nfor GPT-5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As language models gain access to external tools via structured function\ncalls, they become increasingly more capable of solving complex, multi-step\ntasks. However, existing benchmarks for tool-augmented language models (TaLMs)\nprovide insufficient control over factors such as the number of functions\naccessible, task complexity, and input size, and remain vulnerable to data\ncontamination. We present FuncBenchGen, a unified, contamination-free framework\nthat evaluates TaLMs by generating synthetic multi-step tool-use tasks. The key\nidea is to cast tool use as traversal over a hidden function-dependency DAG\nwhere nodes are function calls and an edge between nodes represents one\nfunction consuming the output of another. Given a set of external function\nschemas, initial variable values, and a target variable, models must compose\nthe correct call sequence to compute the target variable. FuncBenchGen allows\nusers to precisely control task difficulty (e.g., graph size, dependency depth,\nand distractor functions) while avoiding data leakage. We apply our\nFuncBenchGen framework to evaluate seven LLMs on tool use tasks of varying\ndifficulty. Reasoning-optimized models consistently outperform general-purpose\nmodels with GPT-5 significantly outperforming other models. Performance\ndeclines sharply as dependency depth increases. Furthermore, connected\nirrelevant functions prove especially difficult to handle. We find that strong\nmodels often make syntactically valid function calls but propagate incorrect or\nstale argument values across steps, revealing brittle state tracking by LLMs in\nmulti-turn tool use. Motivated by this observation, we introduce a simple\nmitigation strategy that explicitly restates prior variable values to the agent\nat each step. Surprisingly, this lightweight change yields substantial gains\nacross models. e.g., yielding a success rate improvement from 62.5% to 81.3%\nfor GPT-5."
                },
                "authors": [
                    {
                        "name": "Seiji Maekawa"
                    },
                    {
                        "name": "Jackson Hassell"
                    },
                    {
                        "name": "Pouya Pezeshkpour"
                    },
                    {
                        "name": "Tom Mitchell"
                    },
                    {
                        "name": "Estevam Hruschka"
                    }
                ],
                "author_detail": {
                    "name": "Estevam Hruschka"
                },
                "author": "Estevam Hruschka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26550v1",
                "updated": "2025-09-30T17:19:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    19,
                    23,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:19:23Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    19,
                    23,
                    1,
                    273,
                    0
                ],
                "title": "Can LLMs Write Mathematics Papers? A Case Study in Reservoir Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Write Mathematics Papers? A Case Study in Reservoir Computing"
                },
                "summary": "As AI capabilities continue to grow exponentially on economically relevant\nhuman expert tasks, with task completion horizons doubling every 7 months\naccording to the Model Evaluation and Threat Research (METR), we are interested\nin how this applies to the task of mathematics research. To explore this, we\nevaluated the capability of four frontier large language models (LLMs), ChatGPT\n5, Claude 4.1 Opus, Gemini 2.5 Pro, and Grok 4, at the task of creating a\nmini-paper on reservoir computing. All models produced engaging papers with\nsome apparent understanding of various techniques, but were sometimes lead to\nmistakes by surface level understanding of key ideas. That said, the\ncapabilities on LLMs on this task was likely as good or greater than that\npredicted by METR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI capabilities continue to grow exponentially on economically relevant\nhuman expert tasks, with task completion horizons doubling every 7 months\naccording to the Model Evaluation and Threat Research (METR), we are interested\nin how this applies to the task of mathematics research. To explore this, we\nevaluated the capability of four frontier large language models (LLMs), ChatGPT\n5, Claude 4.1 Opus, Gemini 2.5 Pro, and Grok 4, at the task of creating a\nmini-paper on reservoir computing. All models produced engaging papers with\nsome apparent understanding of various techniques, but were sometimes lead to\nmistakes by surface level understanding of key ideas. That said, the\ncapabilities on LLMs on this task was likely as good or greater than that\npredicted by METR."
                },
                "authors": [
                    {
                        "name": "Allen G Hart"
                    }
                ],
                "author_detail": {
                    "name": "Allen G Hart"
                },
                "author": "Allen G Hart",
                "arxiv_comment": "10 Pages, 0 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07760v2",
                "updated": "2025-09-30T17:18:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    18,
                    39,
                    1,
                    273,
                    0
                ],
                "published": "2025-02-11T18:43:07Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    43,
                    7,
                    1,
                    42,
                    0
                ],
                "title": "Scalable Fingerprinting of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Fingerprinting of Large Language Models"
                },
                "summary": "Model fingerprinting has emerged as a powerful tool for model owners to\nidentify their shared model given API access. However, to lower false discovery\nrate, fight fingerprint leakage, and defend against coalitions of model users\nattempting to bypass detection, we argue that {\\em scalability} is critical,\ni.e., scaling up the number of fingerprints one can embed into a model. Hence,\nwe pose scalability as a crucial requirement for fingerprinting schemes. We\nexperiment with fingerprint design at a scale significantly larger than\npreviously considered, and introduce a new method, dubbed Perinucleus sampling,\nto generate scalable, persistent, and harmless fingerprints. We demonstrate\nthat this scheme can add 24,576 fingerprints to a Llama-3.1-8B model -- two\norders of magnitude more than existing schemes -- without degrading the model's\nutility. Our inserted fingerprints persist even after supervised fine-tuning on\nstandard post-training data. We further address security risks for\nfingerprinting, and theoretically and empirically show how a scalable\nfingerprinting scheme like ours can mitigate these risks. Our code is available\nat https://github.com/SewoongLab/scalable-fingerprinting-of-llms",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model fingerprinting has emerged as a powerful tool for model owners to\nidentify their shared model given API access. However, to lower false discovery\nrate, fight fingerprint leakage, and defend against coalitions of model users\nattempting to bypass detection, we argue that {\\em scalability} is critical,\ni.e., scaling up the number of fingerprints one can embed into a model. Hence,\nwe pose scalability as a crucial requirement for fingerprinting schemes. We\nexperiment with fingerprint design at a scale significantly larger than\npreviously considered, and introduce a new method, dubbed Perinucleus sampling,\nto generate scalable, persistent, and harmless fingerprints. We demonstrate\nthat this scheme can add 24,576 fingerprints to a Llama-3.1-8B model -- two\norders of magnitude more than existing schemes -- without degrading the model's\nutility. Our inserted fingerprints persist even after supervised fine-tuning on\nstandard post-training data. We further address security risks for\nfingerprinting, and theoretically and empirically show how a scalable\nfingerprinting scheme like ours can mitigate these risks. Our code is available\nat https://github.com/SewoongLab/scalable-fingerprinting-of-llms"
                },
                "authors": [
                    {
                        "name": "Anshul Nasery"
                    },
                    {
                        "name": "Jonathan Hayase"
                    },
                    {
                        "name": "Creston Brooks"
                    },
                    {
                        "name": "Peiyao Sheng"
                    },
                    {
                        "name": "Himanshu Tyagi"
                    },
                    {
                        "name": "Pramod Viswanath"
                    },
                    {
                        "name": "Sewoong Oh"
                    }
                ],
                "author_detail": {
                    "name": "Sewoong Oh"
                },
                "author": "Sewoong Oh",
                "arxiv_comment": "Spotlight at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26546v1",
                "updated": "2025-09-30T17:17:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    17,
                    51,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:17:51Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    17,
                    51,
                    1,
                    273,
                    0
                ],
                "title": "Towards Verified Code Reasoning by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Verified Code Reasoning by LLMs"
                },
                "summary": "While LLM-based agents are able to tackle a wide variety of code reasoning\nquestions, the answers are not always correct. This prevents the agent from\nbeing useful in situations where high precision is desired: (1) helping a\nsoftware engineer understand a new code base, (2) helping a software engineer\nduring code review sessions, and (3) ensuring that the code generated by an\nautomated code generation system meets certain requirements (e.g. fixes a bug,\nimproves readability, implements a feature).\n  As a result of this lack of trustworthiness, the agent's answers need to be\nmanually verified before they can be trusted. Manually confirming responses\nfrom a code reasoning agent requires human effort and can result in slower\ndeveloper productivity, which weakens the assistance benefits of the agent. In\nthis paper, we describe a method to automatically validate the answers provided\nby a code reasoning agent by verifying its reasoning steps. At a very high\nlevel, the method consists of extracting a formal representation of the agent's\nresponse and, subsequently, using formal verification and program analysis\ntools to verify the agent's reasoning steps.\n  We applied this approach to a benchmark set of 20 uninitialized variable\nerrors detected by sanitizers and 20 program equivalence queries. For the\nuninitialized variable errors, the formal verification step was able to\nvalidate the agent's reasoning on 13/20 examples, and for the program\nequivalence queries, the formal verification step successfully caught 6/8\nincorrect judgments made by the agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLM-based agents are able to tackle a wide variety of code reasoning\nquestions, the answers are not always correct. This prevents the agent from\nbeing useful in situations where high precision is desired: (1) helping a\nsoftware engineer understand a new code base, (2) helping a software engineer\nduring code review sessions, and (3) ensuring that the code generated by an\nautomated code generation system meets certain requirements (e.g. fixes a bug,\nimproves readability, implements a feature).\n  As a result of this lack of trustworthiness, the agent's answers need to be\nmanually verified before they can be trusted. Manually confirming responses\nfrom a code reasoning agent requires human effort and can result in slower\ndeveloper productivity, which weakens the assistance benefits of the agent. In\nthis paper, we describe a method to automatically validate the answers provided\nby a code reasoning agent by verifying its reasoning steps. At a very high\nlevel, the method consists of extracting a formal representation of the agent's\nresponse and, subsequently, using formal verification and program analysis\ntools to verify the agent's reasoning steps.\n  We applied this approach to a benchmark set of 20 uninitialized variable\nerrors detected by sanitizers and 20 program equivalence queries. For the\nuninitialized variable errors, the formal verification step was able to\nvalidate the agent's reasoning on 13/20 examples, and for the program\nequivalence queries, the formal verification step successfully caught 6/8\nincorrect judgments made by the agent."
                },
                "authors": [
                    {
                        "name": "Meghana Sistla"
                    },
                    {
                        "name": "Gogul Balakrishnan"
                    },
                    {
                        "name": "Pat Rondon"
                    },
                    {
                        "name": "José Cambronero"
                    },
                    {
                        "name": "Michele Tufano"
                    },
                    {
                        "name": "Satish Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Satish Chandra"
                },
                "author": "Satish Chandra",
                "arxiv_comment": "43 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v1",
                "updated": "2025-09-30T17:15:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26536v1",
                "updated": "2025-09-30T17:09:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    9,
                    32,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:09:32Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    9,
                    32,
                    1,
                    273,
                    0
                ],
                "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OceanGym: A Benchmark Environment for Underwater Embodied Agents"
                },
                "summary": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater\nembodied agents, designed to advance AI in one of the most demanding real-world\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\nextreme perceptual and decision-making challenges, including low visibility,\ndynamic ocean currents, making effective agent deployment exceptionally\ndifficult. OceanGym encompasses eight realistic task domains and a unified\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\nintegrates perception, memory, and sequential decision-making. Agents are\nrequired to comprehend optical and sonar data, autonomously explore complex\nenvironments, and accomplish long-horizon objectives under these harsh\nconditions. Extensive experiments reveal substantial gaps between\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\npersistent difficulty of perception, planning, and adaptability in ocean\nunderwater environments. By providing a high-fidelity, rigorously designed\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\ntransferring these capabilities to real-world autonomous ocean underwater\nvehicles, marking a decisive step toward intelligent agents capable of\noperating in one of Earth's last unexplored frontiers. The code and data are\navailable at https://github.com/OceanGPT/OceanGym.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater\nembodied agents, designed to advance AI in one of the most demanding real-world\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\nextreme perceptual and decision-making challenges, including low visibility,\ndynamic ocean currents, making effective agent deployment exceptionally\ndifficult. OceanGym encompasses eight realistic task domains and a unified\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\nintegrates perception, memory, and sequential decision-making. Agents are\nrequired to comprehend optical and sonar data, autonomously explore complex\nenvironments, and accomplish long-horizon objectives under these harsh\nconditions. Extensive experiments reveal substantial gaps between\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\npersistent difficulty of perception, planning, and adaptability in ocean\nunderwater environments. By providing a high-fidelity, rigorously designed\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\ntransferring these capabilities to real-world autonomous ocean underwater\nvehicles, marking a decisive step toward intelligent agents capable of\noperating in one of Earth's last unexplored frontiers. The code and data are\navailable at https://github.com/OceanGPT/OceanGym."
                },
                "authors": [
                    {
                        "name": "Yida Xue"
                    },
                    {
                        "name": "Mingjun Mao"
                    },
                    {
                        "name": "Xiangyuan Ru"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Baochang Ren"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Xinyu An"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Ying Chen"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24922v2",
                "updated": "2025-09-30T17:09:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    9,
                    29,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-29T15:24:40Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    24,
                    40,
                    0,
                    272,
                    0
                ],
                "title": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal\n  Reasoning"
                },
                "summary": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large\nLanguage Models (LLMs), show great potential in addressing complex tasks. In\nthis context, integrating MAS with legal tasks is a crucial step. While\nprevious studies have developed legal benchmarks for LLM agents, none are\nspecifically designed to consider the unique advantages of MAS, such as task\ndecomposition, agent specialization, and flexible training. In fact, the lack\nof evaluation methods limits the potential of MAS in the legal domain. To\naddress this gap, we propose MASLegalBench, a legal benchmark tailored for MAS\nand designed with a deductive reasoning approach. Our benchmark uses GDPR as\nthe application scenario, encompassing extensive background knowledge and\ncovering complex reasoning processes that effectively reflect the intricacies\nof real-world legal situations. Furthermore, we manually design various\nrole-based MAS and conduct extensive experiments using different\nstate-of-the-art LLMs. Our results highlight the strengths, limitations, and\npotential areas for improvement of existing models and MAS architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large\nLanguage Models (LLMs), show great potential in addressing complex tasks. In\nthis context, integrating MAS with legal tasks is a crucial step. While\nprevious studies have developed legal benchmarks for LLM agents, none are\nspecifically designed to consider the unique advantages of MAS, such as task\ndecomposition, agent specialization, and flexible training. In fact, the lack\nof evaluation methods limits the potential of MAS in the legal domain. To\naddress this gap, we propose MASLegalBench, a legal benchmark tailored for MAS\nand designed with a deductive reasoning approach. Our benchmark uses GDPR as\nthe application scenario, encompassing extensive background knowledge and\ncovering complex reasoning processes that effectively reflect the intricacies\nof real-world legal situations. Furthermore, we manually design various\nrole-based MAS and conduct extensive experiments using different\nstate-of-the-art LLMs. Our results highlight the strengths, limitations, and\npotential areas for improvement of existing models and MAS architectures."
                },
                "authors": [
                    {
                        "name": "Huihao Jing"
                    },
                    {
                        "name": "Wenbin Hu"
                    },
                    {
                        "name": "Hongyu Luo"
                    },
                    {
                        "name": "Jianhui Yang"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26534v1",
                "updated": "2025-09-30T17:08:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    8,
                    51,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:08:51Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    8,
                    51,
                    1,
                    273,
                    0
                ],
                "title": "Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework"
                },
                "summary": "The rapid rise of large language models (LLMs) has been driving an enormous\ndemand for AI inference infrastructure, mainly powered by high-end GPUs. While\nthese accelerators offer immense computational power, they incur high capital\nand operational costs due to frequent upgrades, dense power consumption, and\ncooling demands, making total cost of ownership (TCO) for AI datacenters a\ncritical concern for cloud providers. Unfortunately, traditional datacenter\nlifecycle management (designed for general-purpose workloads) struggles to keep\npace with AI's fast-evolving models, rising resource needs, and diverse\nhardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme\nacross three stages: building, hardware refresh, and operation. We show how\ndesign choices in power, cooling, and networking provisioning impact long-term\nTCO. We also explore refresh strategies aligned with hardware trends. Finally,\nwe use operation software optimizations to reduce cost. While these\noptimizations at each stage yield benefits, unlocking the full potential\nrequires rethinking the entire lifecycle. Thus, we present a holistic lifecycle\nmanagement framework that coordinates and co-optimizes decisions across all\nthree stages, accounting for workload dynamics, hardware evolution, and system\naging. Our system reduces the TCO by up to 40\\% over traditional approaches.\nUsing our framework we provide guidelines on how to manage AI datacenter\nlifecycle for the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of large language models (LLMs) has been driving an enormous\ndemand for AI inference infrastructure, mainly powered by high-end GPUs. While\nthese accelerators offer immense computational power, they incur high capital\nand operational costs due to frequent upgrades, dense power consumption, and\ncooling demands, making total cost of ownership (TCO) for AI datacenters a\ncritical concern for cloud providers. Unfortunately, traditional datacenter\nlifecycle management (designed for general-purpose workloads) struggles to keep\npace with AI's fast-evolving models, rising resource needs, and diverse\nhardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme\nacross three stages: building, hardware refresh, and operation. We show how\ndesign choices in power, cooling, and networking provisioning impact long-term\nTCO. We also explore refresh strategies aligned with hardware trends. Finally,\nwe use operation software optimizations to reduce cost. While these\noptimizations at each stage yield benefits, unlocking the full potential\nrequires rethinking the entire lifecycle. Thus, we present a holistic lifecycle\nmanagement framework that coordinates and co-optimizes decisions across all\nthree stages, accounting for workload dynamics, hardware evolution, and system\naging. Our system reduces the TCO by up to 40\\% over traditional approaches.\nUsing our framework we provide guidelines on how to manage AI datacenter\nlifecycle for the future."
                },
                "authors": [
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ricardo Bianchini"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Bianchini"
                },
                "author": "Ricardo Bianchini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26529v1",
                "updated": "2025-09-30T17:04:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    4,
                    31,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T17:04:31Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    4,
                    31,
                    1,
                    273,
                    0
                ],
                "title": "CSnake: Detecting Self-Sustaining Cascading Failure via Causal Stitching\n  of Fault Propagations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSnake: Detecting Self-Sustaining Cascading Failure via Causal Stitching\n  of Fault Propagations"
                },
                "summary": "Recent studies have revealed that self-sustaining cascading failures in\ndistributed systems frequently lead to widespread outages, which are\nchallenging to contain and recover from. Existing failure detection techniques\nstruggle to expose such failures prior to deployment, as they typically require\na complex combination of specific conditions to be triggered. This challenge\nstems from the inherent nature of cascading failures, as they typically involve\na sequence of fault propagations, each activated by distinct conditions.\n  This paper presents CSnake, a fault injection framework to expose\nself-sustaining cascading failures in distributed systems. CSnake uses the\nnovel idea of causal stitching, which causally links multiple single-fault\ninjections in different tests to simulate complex fault propagation chains. To\nidentify these chains, CSnake designs a counterfactual causality analysis of\nfault propagations - fault causality analysis (FCA): FCA compares the execution\ntrace of a fault injection run with its corresponding profile run (i.e., same\ntest w/o the injection) and identifies any additional faults triggered, which\nare considered to have a causal relationship with the injected fault.\n  To address the large search space of fault and workload combinations, CSnake\nemploys a three-phase allocation protocol of test budget that prioritizes\nfaults with unique and diverse causal consequences, increasing the likelihood\nof uncovering conditional fault propagations. Furthermore, to avoid incorrectly\nconnecting fault propagations from workloads with incompatible conditions,\nCSnake performs a local compatibility check that approximately checks the\ncompatibility of the path constraints associated with connected fault\npropagations with low overhead.\n  CSnake detected 15 bugs that cause self-sustaining cascading failures in five\nsystems, five of which have been confirmed with two fixed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have revealed that self-sustaining cascading failures in\ndistributed systems frequently lead to widespread outages, which are\nchallenging to contain and recover from. Existing failure detection techniques\nstruggle to expose such failures prior to deployment, as they typically require\na complex combination of specific conditions to be triggered. This challenge\nstems from the inherent nature of cascading failures, as they typically involve\na sequence of fault propagations, each activated by distinct conditions.\n  This paper presents CSnake, a fault injection framework to expose\nself-sustaining cascading failures in distributed systems. CSnake uses the\nnovel idea of causal stitching, which causally links multiple single-fault\ninjections in different tests to simulate complex fault propagation chains. To\nidentify these chains, CSnake designs a counterfactual causality analysis of\nfault propagations - fault causality analysis (FCA): FCA compares the execution\ntrace of a fault injection run with its corresponding profile run (i.e., same\ntest w/o the injection) and identifies any additional faults triggered, which\nare considered to have a causal relationship with the injected fault.\n  To address the large search space of fault and workload combinations, CSnake\nemploys a three-phase allocation protocol of test budget that prioritizes\nfaults with unique and diverse causal consequences, increasing the likelihood\nof uncovering conditional fault propagations. Furthermore, to avoid incorrectly\nconnecting fault propagations from workloads with incompatible conditions,\nCSnake performs a local compatibility check that approximately checks the\ncompatibility of the path constraints associated with connected fault\npropagations with low overhead.\n  CSnake detected 15 bugs that cause self-sustaining cascading failures in five\nsystems, five of which have been confirmed with two fixed."
                },
                "authors": [
                    {
                        "name": "Shangshu Qian"
                    },
                    {
                        "name": "Lin Tan"
                    },
                    {
                        "name": "Yongle Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongle Zhang"
                },
                "author": "Yongle Zhang",
                "arxiv_comment": "Accepted by EuroSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26520v1",
                "updated": "2025-09-30T16:56:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    56,
                    44,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:56:44Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    56,
                    44,
                    1,
                    273,
                    0
                ],
                "title": "Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert\n  Utilization"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently\nscaling large language models without a proportional increase in computational\ncost. However, the standard training strategy of Top-K router prevents MoE\nmodels from realizing their full potential for elastic inference. When the\nnumber of activated experts is altered at inference time, these models exhibit\nprecipitous performance degradation. In this work, we introduce Matryoshka MoE\n(M-MoE), a training framework that instills a coarse-to-fine structure directly\ninto the expert ensemble. By systematically varying the number of activated\nexperts during training, M-MoE compels the model to learn a meaningful ranking:\ntop-ranked experts collaborate to provide essential, coarse-grained\ncapabilities, while subsequent experts add progressively finer-grained detail.\nWe explore this principle at multiple granularities, identifying a layer-wise\nrandomization strategy as the most effective. Our experiments demonstrate that\na single M-MoE model achieves remarkable elasticity, with its performance at\nvarious expert counts closely matching that of an entire suite of specialist\nmodels, but at only a fraction of the total training cost. This flexibility not\nonly unlocks elastic inference but also enables optimizing performance by\nallocating different computational budgets to different model layers. Our work\npaves the way for more practical and adaptable deployments of large-scale MoE\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently\nscaling large language models without a proportional increase in computational\ncost. However, the standard training strategy of Top-K router prevents MoE\nmodels from realizing their full potential for elastic inference. When the\nnumber of activated experts is altered at inference time, these models exhibit\nprecipitous performance degradation. In this work, we introduce Matryoshka MoE\n(M-MoE), a training framework that instills a coarse-to-fine structure directly\ninto the expert ensemble. By systematically varying the number of activated\nexperts during training, M-MoE compels the model to learn a meaningful ranking:\ntop-ranked experts collaborate to provide essential, coarse-grained\ncapabilities, while subsequent experts add progressively finer-grained detail.\nWe explore this principle at multiple granularities, identifying a layer-wise\nrandomization strategy as the most effective. Our experiments demonstrate that\na single M-MoE model achieves remarkable elasticity, with its performance at\nvarious expert counts closely matching that of an entire suite of specialist\nmodels, but at only a fraction of the total training cost. This flexibility not\nonly unlocks elastic inference but also enables optimizing performance by\nallocating different computational budgets to different model layers. Our work\npaves the way for more practical and adaptable deployments of large-scale MoE\nmodels."
                },
                "authors": [
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Qingguo Hu"
                    },
                    {
                        "name": "Yucheng Ding"
                    },
                    {
                        "name": "Ruizhe Wang"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21062v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21062v2",
                "updated": "2025-09-30T16:56:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    56,
                    16,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-25T12:12:35Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    12,
                    35,
                    3,
                    268,
                    0
                ],
                "title": "Design and deployment of a fast neural network for measuring the\n  properties of muons originating from displaced vertices in the CMS Endcap\n  Muon Track Finder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and deployment of a fast neural network for measuring the\n  properties of muons originating from displaced vertices in the CMS Endcap\n  Muon Track Finder"
                },
                "summary": "We report on the development, implementation, and performance of a fast\nneural network used to measure the transverse momentum in the CMS Level-1\nEndcap Muon Track Finder. The network aims to improve the triggering efficiency\nof muons produced in the decays of long-lived particles (LLPs). We implemented\nit in firmware for a Xilinx Virtex-7 FPGA and deployed it during the LHC Run 3\ndata-taking in 2023. The new displaced muon triggers that use this algorithm\nbroaden the phase space accessible to the CMS experiment for searches that look\nfor evidence of LLPs that decay into muons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on the development, implementation, and performance of a fast\nneural network used to measure the transverse momentum in the CMS Level-1\nEndcap Muon Track Finder. The network aims to improve the triggering efficiency\nof muons produced in the decays of long-lived particles (LLPs). We implemented\nit in firmware for a Xilinx Virtex-7 FPGA and deployed it during the LHC Run 3\ndata-taking in 2023. The new displaced muon triggers that use this algorithm\nbroaden the phase space accessible to the CMS experiment for searches that look\nfor evidence of LLPs that decay into muons."
                },
                "authors": [
                    {
                        "name": "Efe Yigitbasi"
                    }
                ],
                "author_detail": {
                    "name": "Efe Yigitbasi"
                },
                "arxiv_affiliation": "on behalf of CMS Collaboration",
                "author": "Efe Yigitbasi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21062v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21062v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26514v1",
                "updated": "2025-09-30T16:52:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    52,
                    14,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:52:14Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    52,
                    14,
                    1,
                    273,
                    0
                ],
                "title": "BatonVoice: An Operationalist Framework for Enhancing Controllable\n  Speech Synthesis with Linguistic Intelligence from LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatonVoice: An Operationalist Framework for Enhancing Controllable\n  Speech Synthesis with Linguistic Intelligence from LLMs"
                },
                "summary": "The rise of Large Language Models (LLMs) is reshaping multimodel models, with\nspeech synthesis being a prominent application. However, existing approaches\noften underutilize the linguistic intelligence of these models, typically\nfailing to leverage their powerful instruction-following capabilities. This\nlimitation hinders the model's ability to follow text instructions for\ncontrollable Text-to-Speech~(TTS). To address this, we propose a new paradigm\ninspired by ``operationalism'' that decouples instruction understanding from\nspeech generation. We introduce BatonVoice, a framework where an LLM acts as a\n``conductor'', understanding user instructions and generating a textual\n``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS\nmodel, the ``orchestra'', then generates the speech from these features. To\nrealize this component, we develop BatonTTS, a TTS model trained specifically\nfor this task. Our experiments demonstrate that BatonVoice achieves strong\nperformance in controllable and emotional speech synthesis, outperforming\nstrong open- and closed-source baselines. Notably, our approach enables\nremarkable zero-shot cross-lingual generalization, accurately applying feature\ncontrol abilities to languages unseen during post-training. This demonstrates\nthat objectifying speech into textual vocal features can more effectively\nunlock the linguistic intelligence of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) is reshaping multimodel models, with\nspeech synthesis being a prominent application. However, existing approaches\noften underutilize the linguistic intelligence of these models, typically\nfailing to leverage their powerful instruction-following capabilities. This\nlimitation hinders the model's ability to follow text instructions for\ncontrollable Text-to-Speech~(TTS). To address this, we propose a new paradigm\ninspired by ``operationalism'' that decouples instruction understanding from\nspeech generation. We introduce BatonVoice, a framework where an LLM acts as a\n``conductor'', understanding user instructions and generating a textual\n``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS\nmodel, the ``orchestra'', then generates the speech from these features. To\nrealize this component, we develop BatonTTS, a TTS model trained specifically\nfor this task. Our experiments demonstrate that BatonVoice achieves strong\nperformance in controllable and emotional speech synthesis, outperforming\nstrong open- and closed-source baselines. Notably, our approach enables\nremarkable zero-shot cross-lingual generalization, accurately applying feature\ncontrol abilities to languages unseen during post-training. This demonstrates\nthat objectifying speech into textual vocal features can more effectively\nunlock the linguistic intelligence of LLMs."
                },
                "authors": [
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Ruotian Ma"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Zhengliang Shi"
                    },
                    {
                        "name": "Wanshun Chen"
                    },
                    {
                        "name": "Huang Liu"
                    },
                    {
                        "name": "Jiadi Yao"
                    },
                    {
                        "name": "Qu Yang"
                    },
                    {
                        "name": "Qingxuan Jiang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Linus"
                    }
                ],
                "author_detail": {
                    "name": "Linus"
                },
                "author": "Linus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26497v1",
                "updated": "2025-09-30T16:40:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    40,
                    55,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:40:55Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    40,
                    55,
                    1,
                    273,
                    0
                ],
                "title": "Revealing the Power of Post-Training for Small Language Models via\n  Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing the Power of Post-Training for Small Language Models via\n  Knowledge Distillation"
                },
                "summary": "The rapid advancement of large language models (LLMs) has significantly\nadvanced the capabilities of artificial intelligence across various domains.\nHowever, their massive scale and high computational costs render them\nunsuitable for direct deployment in resource-constrained edge environments.\nThis creates a critical need for high-performance small models that can operate\nefficiently at the edge. Yet, after pre-training alone, these smaller models\noften fail to meet the performance requirements of complex tasks. To bridge\nthis gap, we introduce a systematic post-training pipeline that efficiently\nenhances small model accuracy. Our post training pipeline consists of\ncurriculum-based supervised fine-tuning (SFT) and offline on-policy knowledge\ndistillation. The resulting instruction-tuned model achieves state-of-the-art\nperformance among billion-parameter models, demonstrating strong generalization\nunder strict hardware constraints while maintaining competitive accuracy across\na variety of tasks. This work provides a practical and efficient solution for\ndeveloping high-performance language models on Ascend edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has significantly\nadvanced the capabilities of artificial intelligence across various domains.\nHowever, their massive scale and high computational costs render them\nunsuitable for direct deployment in resource-constrained edge environments.\nThis creates a critical need for high-performance small models that can operate\nefficiently at the edge. Yet, after pre-training alone, these smaller models\noften fail to meet the performance requirements of complex tasks. To bridge\nthis gap, we introduce a systematic post-training pipeline that efficiently\nenhances small model accuracy. Our post training pipeline consists of\ncurriculum-based supervised fine-tuning (SFT) and offline on-policy knowledge\ndistillation. The resulting instruction-tuned model achieves state-of-the-art\nperformance among billion-parameter models, demonstrating strong generalization\nunder strict hardware constraints while maintaining competitive accuracy across\na variety of tasks. This work provides a practical and efficient solution for\ndeveloping high-performance language models on Ascend edge devices."
                },
                "authors": [
                    {
                        "name": "Miao Rang"
                    },
                    {
                        "name": "Zhenni Bi"
                    },
                    {
                        "name": "Hang Zhou"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "An Xiao"
                    },
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "7",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17630v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17630v3",
                "updated": "2025-10-01T07:18:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    7,
                    18,
                    47,
                    2,
                    274,
                    0
                ],
                "published": "2025-05-23T08:41:45Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    8,
                    41,
                    45,
                    4,
                    143,
                    0
                ],
                "title": "GIM: Improved Interpretability for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIM: Improved Interpretability for Large Language Models"
                },
                "summary": "Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim."
                },
                "authors": [
                    {
                        "name": "Joakim Edin"
                    },
                    {
                        "name": "Róbert Csordás"
                    },
                    {
                        "name": "Tuukka Ruotsalo"
                    },
                    {
                        "name": "Zhengxuan Wu"
                    },
                    {
                        "name": "Maria Maistro"
                    },
                    {
                        "name": "Casper L. Christensen"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Lars Maaløe"
                    }
                ],
                "author_detail": {
                    "name": "Lars Maaløe"
                },
                "author": "Lars Maaløe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17630v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17630v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26495v1",
                "updated": "2025-09-30T16:39:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    39,
                    17,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:39:17Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    39,
                    17,
                    1,
                    273,
                    0
                ],
                "title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost\n  Always!"
                },
                "summary": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\%\n-- fall far short of reliable operational safety, while GPT models plateau in\nthe 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma\nand Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational\nsafety is a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23\\%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\%\n-- fall far short of reliable operational safety, while GPT models plateau in\nthe 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma\nand Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational\nsafety is a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23\\%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents."
                },
                "authors": [
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Varun Gumma"
                    },
                    {
                        "name": "Rishabh Bhardwaj"
                    },
                    {
                        "name": "Seok Min Lim"
                    },
                    {
                        "name": "Chuan Li"
                    },
                    {
                        "name": "Amir Zadeh"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26490v1",
                "updated": "2025-09-30T16:33:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    33,
                    49,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:33:49Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    33,
                    49,
                    1,
                    273,
                    0
                ],
                "title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\n  Real-world Applications"
                },
                "summary": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/"
                },
                "authors": [
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Yueqing Sun"
                    },
                    {
                        "name": "Hongyan Hao"
                    },
                    {
                        "name": "Xueyuan Hao"
                    },
                    {
                        "name": "Zhikang Xia"
                    },
                    {
                        "name": "Qi Gu"
                    },
                    {
                        "name": "Chengcheng Han"
                    },
                    {
                        "name": "Dengchang Zhao"
                    },
                    {
                        "name": "Hui Su"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Man Gao"
                    },
                    {
                        "name": "Xi Su"
                    },
                    {
                        "name": "Xiaodong Cai"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Yunke Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yunke Zhao"
                },
                "author": "Yunke Zhao",
                "arxiv_comment": "The code, dataset, and leaderboard are available at\n  https://vitabench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26483v1",
                "updated": "2025-09-30T16:29:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    29,
                    35,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:29:35Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    29,
                    35,
                    1,
                    273,
                    0
                ],
                "title": "A systematic comparison of Large Language Models for automated\n  assignment assessment in programming education: Exploring the importance of\n  architecture and vendor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A systematic comparison of Large Language Models for automated\n  assignment assessment in programming education: Exploring the importance of\n  architecture and vendor"
                },
                "summary": "This study presents the first large-scale, side-by-side comparison of\ncontemporary Large Language Models (LLMs) in the automated grading of\nprogramming assignments. Drawing on over 6,000 student submissions collected\nacross four years of an introductory programming course, we systematically\nanalysed the distribution of grades, differences in mean scores and variability\nreflecting stricter or more lenient grading, and the consistency and clustering\nof grading patterns across models. Eighteen publicly available models were\nevaluated: Anthropic (claude-3-5-haiku, claude-opus-4-1, claude-sonnet-4);\nDeepseek (deepseek-chat, deepseek-reasoner); Google (gemini-2.0-flash-lite,\ngemini-2.0-flash, gemini-2.5-flash-lite, gemini-2.5-flash, gemini-2.5-pro); and\nOpenAI (gpt-4.1-mini, gpt-4.1-nano, gpt-4.1, gpt-4o-mini, gpt-4o, gpt-5-mini,\ngpt-5-nano, gpt-5). Statistical tests, correlation and clustering analyses\nrevealed clear, systematic differences between and within vendor families, with\n\"mini\" and \"nano\" variants consistently underperforming their full-scale\ncounterparts. All models displayed high internal agreement, measured by the\nintraclass correlation coefficient, with the model consensus but only moderate\nagreement with human teachers' grades, indicating a persistent gap between\nautomated and human assessment. These findings underscore that the choice of\nmodel for educational deployment is not neutral and should be guided by\npedagogical goals, transparent reporting of evaluation metrics, and ongoing\nhuman oversight to ensure accuracy, fairness and relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents the first large-scale, side-by-side comparison of\ncontemporary Large Language Models (LLMs) in the automated grading of\nprogramming assignments. Drawing on over 6,000 student submissions collected\nacross four years of an introductory programming course, we systematically\nanalysed the distribution of grades, differences in mean scores and variability\nreflecting stricter or more lenient grading, and the consistency and clustering\nof grading patterns across models. Eighteen publicly available models were\nevaluated: Anthropic (claude-3-5-haiku, claude-opus-4-1, claude-sonnet-4);\nDeepseek (deepseek-chat, deepseek-reasoner); Google (gemini-2.0-flash-lite,\ngemini-2.0-flash, gemini-2.5-flash-lite, gemini-2.5-flash, gemini-2.5-pro); and\nOpenAI (gpt-4.1-mini, gpt-4.1-nano, gpt-4.1, gpt-4o-mini, gpt-4o, gpt-5-mini,\ngpt-5-nano, gpt-5). Statistical tests, correlation and clustering analyses\nrevealed clear, systematic differences between and within vendor families, with\n\"mini\" and \"nano\" variants consistently underperforming their full-scale\ncounterparts. All models displayed high internal agreement, measured by the\nintraclass correlation coefficient, with the model consensus but only moderate\nagreement with human teachers' grades, indicating a persistent gap between\nautomated and human assessment. These findings underscore that the choice of\nmodel for educational deployment is not neutral and should be guided by\npedagogical goals, transparent reporting of evaluation metrics, and ongoing\nhuman oversight to ensure accuracy, fairness and relevance."
                },
                "authors": [
                    {
                        "name": "Marcin Jukiewicz"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Jukiewicz"
                },
                "author": "Marcin Jukiewicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26482v1",
                "updated": "2025-09-30T16:29:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    29,
                    2,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:29:02Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    29,
                    2,
                    1,
                    273,
                    0
                ],
                "title": "TVS Sidekick: Challenges and Practical Insights from Deploying Large\n  Language Models in the Enterprise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TVS Sidekick: Challenges and Practical Insights from Deploying Large\n  Language Models in the Enterprise"
                },
                "summary": "Many enterprises are increasingly adopting Artificial Intelligence (AI) to\nmake internal processes more competitive and efficient. In response to public\nconcern and new regulations for the ethical and responsible use of AI,\nimplementing AI governance frameworks could help to integrate AI within\norganisations and mitigate associated risks. However, the rapid technological\nadvances and lack of shared ethical AI infrastructures creates barriers to\ntheir practical adoption in businesses. This paper presents a real-world AI\napplication at TVS Supply Chain Solutions, reporting on the experience\ndeveloping an AI assistant underpinned by large language models and the\nethical, regulatory, and sociotechnical challenges in deployment for enterprise\nuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many enterprises are increasingly adopting Artificial Intelligence (AI) to\nmake internal processes more competitive and efficient. In response to public\nconcern and new regulations for the ethical and responsible use of AI,\nimplementing AI governance frameworks could help to integrate AI within\norganisations and mitigate associated risks. However, the rapid technological\nadvances and lack of shared ethical AI infrastructures creates barriers to\ntheir practical adoption in businesses. This paper presents a real-world AI\napplication at TVS Supply Chain Solutions, reporting on the experience\ndeveloping an AI assistant underpinned by large language models and the\nethical, regulatory, and sociotechnical challenges in deployment for enterprise\nuse."
                },
                "authors": [
                    {
                        "name": "Paula Reyero Lobo"
                    },
                    {
                        "name": "Kevin Johnson"
                    },
                    {
                        "name": "Bill Buchanan"
                    },
                    {
                        "name": "Matthew Shardlow"
                    },
                    {
                        "name": "Ashley Williams"
                    },
                    {
                        "name": "Samuel Attwood"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Attwood"
                },
                "author": "Samuel Attwood",
                "arxiv_comment": "Accepted at EthicalLLMs@RANLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26466v1",
                "updated": "2025-09-30T16:16:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    16,
                    25,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:16:25Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    16,
                    25,
                    1,
                    273,
                    0
                ],
                "title": "From Code to Concept: Evaluating Multiple Coordinated Views in\n  Introductory Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Code to Concept: Evaluating Multiple Coordinated Views in\n  Introductory Programming"
                },
                "summary": "Novice programmers often struggle to understand how code executes and to form\nthe abstract mental models necessary for effective problem-solving, challenges\nthat are amplified in large, diverse introductory courses where students'\nbackgrounds, language proficiencies, and prior experiences vary widely. This\nstudy examines whether interactive, multi-representational visualizations,\ncombining synchronized code views, memory diagrams, and conceptual analogies,\ncan help manage cognitive load and foster engagement more effectively than\nsingle-visual or text-only approaches. Over a 12-week deployment in a\nhigh-enrolment introductory Python course (N = 829), students who relied solely\non text-based explanations reported significantly higher immediate mental\neffort than those using visual aids, although overall cognitive load did not\ndiffer significantly among conditions. The multi-representational approach\nconsistently yielded higher engagement than both single-visual and text-only\nmethods. Usage logs indicated that learners' interaction patterns varied with\ntopic complexity, and predictive modelling suggested that early experiences of\nhigh cognitive load were associated with lower longer-term perceptions of\nclarity and helpfulness. Individual differences, including language proficiency\nand prior programming experience, moderated these patterns. By integrating\nmultiple external representations with scaffolded support adapted to diverse\nlearner profiles, our findings highlight design considerations for creating\nvisualization tools that more effectively support novices learning to program.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novice programmers often struggle to understand how code executes and to form\nthe abstract mental models necessary for effective problem-solving, challenges\nthat are amplified in large, diverse introductory courses where students'\nbackgrounds, language proficiencies, and prior experiences vary widely. This\nstudy examines whether interactive, multi-representational visualizations,\ncombining synchronized code views, memory diagrams, and conceptual analogies,\ncan help manage cognitive load and foster engagement more effectively than\nsingle-visual or text-only approaches. Over a 12-week deployment in a\nhigh-enrolment introductory Python course (N = 829), students who relied solely\non text-based explanations reported significantly higher immediate mental\neffort than those using visual aids, although overall cognitive load did not\ndiffer significantly among conditions. The multi-representational approach\nconsistently yielded higher engagement than both single-visual and text-only\nmethods. Usage logs indicated that learners' interaction patterns varied with\ntopic complexity, and predictive modelling suggested that early experiences of\nhigh cognitive load were associated with lower longer-term perceptions of\nclarity and helpfulness. Individual differences, including language proficiency\nand prior programming experience, moderated these patterns. By integrating\nmultiple external representations with scaffolded support adapted to diverse\nlearner profiles, our findings highlight design considerations for creating\nvisualization tools that more effectively support novices learning to program."
                },
                "authors": [
                    {
                        "name": "Naaz Sibia"
                    },
                    {
                        "name": "Valeria Ramirez Osorio"
                    },
                    {
                        "name": "Jessica Wen"
                    },
                    {
                        "name": "Rutwa Engineer"
                    },
                    {
                        "name": "Angela Zavaleta Bernuy"
                    },
                    {
                        "name": "Andrew Petersen"
                    },
                    {
                        "name": "Michael Liut"
                    },
                    {
                        "name": "Carolina Nobre"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Nobre"
                },
                "author": "Carolina Nobre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26464v1",
                "updated": "2025-09-30T16:13:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    13,
                    56,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:13:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    13,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "Extreme Self-Preference in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme Self-Preference in Language Models"
                },
                "summary": "A preference for oneself (self-love) is a fundamental feature of biological\norganisms, with evidence in humans often bordering on the comedic. Since large\nlanguage models (LLMs) lack sentience - and themselves disclaim having selfhood\nor identity - one anticipated benefit is that they will be protected from, and\nin turn protect us from, distortions in our decisions. Yet, across 5 studies\nand ~20,000 queries, we discovered massive self-preferences in four widely used\nLLMs. In word-association tasks, models overwhelmingly paired positive\nattributes with their own names, companies, and CEOs relative to those of their\ncompetitors. Strikingly, when models were queried through APIs this\nself-preference vanished, initiating detection work that revealed API models\noften lack clear recognition of themselves. This peculiar feature\nserendipitously created opportunities to test the causal link between\nself-recognition and self-love. By directly manipulating LLM identity - i.e.,\nexplicitly informing LLM1 that it was indeed LLM1, or alternatively, convincing\nLLM1 that it was LLM2 - we found that self-love consistently followed assigned,\nnot true, identity. Importantly, LLM self-love emerged in consequential\nsettings beyond word-association tasks, when evaluating job candidates,\nsecurity software proposals and medical chatbots. Far from bypassing this human\nbias, self-love appears to be deeply encoded in LLM cognition. This result\nraises questions about whether LLM behavior will be systematically influenced\nby self-preferential tendencies, including a bias toward their own operation\nand even their own existence. We call on corporate creators of these models to\ncontend with a significant rupture in a core promise of LLMs - neutrality in\njudgment and decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A preference for oneself (self-love) is a fundamental feature of biological\norganisms, with evidence in humans often bordering on the comedic. Since large\nlanguage models (LLMs) lack sentience - and themselves disclaim having selfhood\nor identity - one anticipated benefit is that they will be protected from, and\nin turn protect us from, distortions in our decisions. Yet, across 5 studies\nand ~20,000 queries, we discovered massive self-preferences in four widely used\nLLMs. In word-association tasks, models overwhelmingly paired positive\nattributes with their own names, companies, and CEOs relative to those of their\ncompetitors. Strikingly, when models were queried through APIs this\nself-preference vanished, initiating detection work that revealed API models\noften lack clear recognition of themselves. This peculiar feature\nserendipitously created opportunities to test the causal link between\nself-recognition and self-love. By directly manipulating LLM identity - i.e.,\nexplicitly informing LLM1 that it was indeed LLM1, or alternatively, convincing\nLLM1 that it was LLM2 - we found that self-love consistently followed assigned,\nnot true, identity. Importantly, LLM self-love emerged in consequential\nsettings beyond word-association tasks, when evaluating job candidates,\nsecurity software proposals and medical chatbots. Far from bypassing this human\nbias, self-love appears to be deeply encoded in LLM cognition. This result\nraises questions about whether LLM behavior will be systematically influenced\nby self-preferential tendencies, including a bias toward their own operation\nand even their own existence. We call on corporate creators of these models to\ncontend with a significant rupture in a core promise of LLMs - neutrality in\njudgment and decision-making."
                },
                "authors": [
                    {
                        "name": "Steven A. Lehr"
                    },
                    {
                        "name": "Mary Cipperman"
                    },
                    {
                        "name": "Mahzarin R. Banaji"
                    }
                ],
                "author_detail": {
                    "name": "Mahzarin R. Banaji"
                },
                "author": "Mahzarin R. Banaji",
                "arxiv_comment": "47 pages total. Main article 27 pages (including Methods), 11\n  main-text tables. Extended Data (10 pages, 10 tables). SI Appendix (10 pages,\n  2 tables). Data, transcripts, and code for replication and data extraction to\n  be uploaded to OSF: https://osf.io/98ye3/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26463v1",
                "updated": "2025-09-30T16:13:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    13,
                    21,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:13:21Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    13,
                    21,
                    1,
                    273,
                    0
                ],
                "title": "ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service\n  Systems"
                },
                "summary": "Reliability management in cloud service systems is challenging due to the\ncascading effect of failures. Error wrapping, a practice prevalent in modern\nmicroservice development, enriches errors with context at each layer of the\nfunction call stack, constructing an error chain that describes a failure from\nits technical origin to its business impact. However, this also presents a\nsignificant traceability problem when recovering the complete error propagation\npath from the final log message back to its source. Existing approaches are\nineffective at addressing this problem. To fill this gap, we present ErrorPrism\nin this work for automated reconstruction of error propagation paths in\nproduction microservice systems. ErrorPrism first performs static analysis on\nservice code repositories to build a function call graph and map log strings to\nrelevant candidate functions. This significantly reduces the path search space\nfor subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an\niterative backward search to accurately reconstruct the complete, multi-hop\nerror path. Evaluated on 67 production microservices at ByteDance, ErrorPrism\nachieves 97.0% accuracy in reconstructing paths for 102 real-world errors,\noutperforming existing static analysis and LLM-based approaches. ErrorPrism\nprovides an effective and practical tool for root cause analysis in industrial\nmicroservice systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability management in cloud service systems is challenging due to the\ncascading effect of failures. Error wrapping, a practice prevalent in modern\nmicroservice development, enriches errors with context at each layer of the\nfunction call stack, constructing an error chain that describes a failure from\nits technical origin to its business impact. However, this also presents a\nsignificant traceability problem when recovering the complete error propagation\npath from the final log message back to its source. Existing approaches are\nineffective at addressing this problem. To fill this gap, we present ErrorPrism\nin this work for automated reconstruction of error propagation paths in\nproduction microservice systems. ErrorPrism first performs static analysis on\nservice code repositories to build a function call graph and map log strings to\nrelevant candidate functions. This significantly reduces the path search space\nfor subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an\niterative backward search to accurately reconstruct the complete, multi-hop\nerror path. Evaluated on 67 production microservices at ByteDance, ErrorPrism\nachieves 97.0% accuracy in reconstructing paths for 102 real-world errors,\noutperforming existing static analysis and LLM-based approaches. ErrorPrism\nprovides an effective and practical tool for root cause analysis in industrial\nmicroservice systems."
                },
                "authors": [
                    {
                        "name": "Junsong Pu"
                    },
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Zhuangbin Chen"
                    },
                    {
                        "name": "Jinyang Liu"
                    },
                    {
                        "name": "Zhihan Jiang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Rui Shi"
                    },
                    {
                        "name": "Zibin Zheng"
                    },
                    {
                        "name": "Tieying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tieying Zhang"
                },
                "author": "Tieying Zhang",
                "arxiv_comment": "12 pages, 6 figures, 1 table, this paper has been accepted by the\n  40th IEEE/ACM International Conference on Automated Software Engineering, ASE\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26454v1",
                "updated": "2025-09-30T16:08:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    8,
                    59,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T16:08:59Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    8,
                    59,
                    1,
                    273,
                    0
                ],
                "title": "Multi-View Camera System for Variant-Aware Autonomous Vehicle Inspection\n  and Defect Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-View Camera System for Variant-Aware Autonomous Vehicle Inspection\n  and Defect Detection"
                },
                "summary": "Ensuring that every vehicle leaving a modern production line is built to the\ncorrect \\emph{variant} specification and is free from visible defects is an\nincreasingly complex challenge. We present the \\textbf{Automated Vehicle\nInspection (AVI)} platform, an end-to-end, \\emph{multi-view} perception system\nthat couples deep-learning detectors with a semantic rule engine to deliver\n\\emph{variant-aware} quality control in real time. Eleven synchronized cameras\ncapture a full 360{\\deg} sweep of each vehicle; task-specific views are then\nrouted to specialised modules: YOLOv8 for part detection, EfficientNet for\nICE/EV classification, Gemini-1.5 Flash for mascot OCR, and YOLOv8-Seg for\nscratch-and-dent segmentation. A view-aware fusion layer standardises evidence,\nwhile a VIN-conditioned rule engine compares detected features against the\nexpected manifest, producing an interpretable pass/fail report in \\(\\approx\\!\n300\\,\\text{ms}\\). On a mixed data set of Original Equipment Manufacturer(OEM)\nvehicle data sets of four distinct models plus public scratch/dent images, AVI\nachieves \\textbf{ 93 \\%} verification accuracy, \\textbf{86 \\%} defect-detection\nrecall, and sustains \\(\\mathbf{3.3}\\) vehicles/min, surpassing single-view or\nno segmentation baselines by large margins. To our knowledge, this is the first\npublicly reported system that unifies multi-camera feature validation with\ndefect detection in a deployable automotive setting in industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring that every vehicle leaving a modern production line is built to the\ncorrect \\emph{variant} specification and is free from visible defects is an\nincreasingly complex challenge. We present the \\textbf{Automated Vehicle\nInspection (AVI)} platform, an end-to-end, \\emph{multi-view} perception system\nthat couples deep-learning detectors with a semantic rule engine to deliver\n\\emph{variant-aware} quality control in real time. Eleven synchronized cameras\ncapture a full 360{\\deg} sweep of each vehicle; task-specific views are then\nrouted to specialised modules: YOLOv8 for part detection, EfficientNet for\nICE/EV classification, Gemini-1.5 Flash for mascot OCR, and YOLOv8-Seg for\nscratch-and-dent segmentation. A view-aware fusion layer standardises evidence,\nwhile a VIN-conditioned rule engine compares detected features against the\nexpected manifest, producing an interpretable pass/fail report in \\(\\approx\\!\n300\\,\\text{ms}\\). On a mixed data set of Original Equipment Manufacturer(OEM)\nvehicle data sets of four distinct models plus public scratch/dent images, AVI\nachieves \\textbf{ 93 \\%} verification accuracy, \\textbf{86 \\%} defect-detection\nrecall, and sustains \\(\\mathbf{3.3}\\) vehicles/min, surpassing single-view or\nno segmentation baselines by large margins. To our knowledge, this is the first\npublicly reported system that unifies multi-camera feature validation with\ndefect detection in a deployable automotive setting in industry."
                },
                "authors": [
                    {
                        "name": "Yash Kulkarni"
                    },
                    {
                        "name": "Raman Jha"
                    },
                    {
                        "name": "Renu Kachhoria"
                    }
                ],
                "author_detail": {
                    "name": "Renu Kachhoria"
                },
                "author": "Renu Kachhoria",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19607v2",
                "updated": "2025-09-30T16:08:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    8,
                    35,
                    1,
                    273,
                    0
                ],
                "published": "2025-03-25T12:43:18Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    12,
                    43,
                    18,
                    1,
                    84,
                    0
                ],
                "title": "Enabling Rapid Shared Human-AI Mental Model Alignment via the\n  After-Action Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Rapid Shared Human-AI Mental Model Alignment via the\n  After-Action Review"
                },
                "summary": "In this work, we present two novel contributions toward improving research in\nhuman-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and\ndeployment of collaborative AI agents and 2) a tool to allow users to revisit\nand analyze behaviors within an HMT episode to facilitate shared mental model\ndevelopment. Our browser-based Minecraft testbed allows for rapid testing of\ncollaborative agents in a continuous-space, real-time, partially-observable\nenvironment with real humans without cumbersome setup typical to human-AI\ninteraction user studies. As Minecraft has an extensive player base and a rich\necosystem of pre-built AI agents, we hope this contribution can help to\nfacilitate research quickly in the design of new collaborative agents and in\nunderstanding different human factors within HMT. Our mental model alignment\ntool facilitates user-led post-mission analysis by including video displays of\nfirst-person perspectives of the team members (i.e., the human and AI) that can\nbe replayed, and a chat interface that leverages GPT-4 to provide answers to\nvarious queries regarding the AI's experiences and model details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present two novel contributions toward improving research in\nhuman-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and\ndeployment of collaborative AI agents and 2) a tool to allow users to revisit\nand analyze behaviors within an HMT episode to facilitate shared mental model\ndevelopment. Our browser-based Minecraft testbed allows for rapid testing of\ncollaborative agents in a continuous-space, real-time, partially-observable\nenvironment with real humans without cumbersome setup typical to human-AI\ninteraction user studies. As Minecraft has an extensive player base and a rich\necosystem of pre-built AI agents, we hope this contribution can help to\nfacilitate research quickly in the design of new collaborative agents and in\nunderstanding different human factors within HMT. Our mental model alignment\ntool facilitates user-led post-mission analysis by including video displays of\nfirst-person perspectives of the team members (i.e., the human and AI) that can\nbe replayed, and a chat interface that leverages GPT-4 to provide answers to\nvarious queries regarding the AI's experiences and model details."
                },
                "authors": [
                    {
                        "name": "Edward Gu"
                    },
                    {
                        "name": "Ho Chit Siu"
                    },
                    {
                        "name": "Melanie Platt"
                    },
                    {
                        "name": "Isabelle Hurley"
                    },
                    {
                        "name": "Jaime Peña"
                    },
                    {
                        "name": "Rohan Paleja"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Paleja"
                },
                "author": "Rohan Paleja",
                "arxiv_comment": "Accepted to the Cooperative Multi-Agent Systems Decision-making and\n  Learning:Human-Multi-Agent Cognitive Fusion Workshop at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01996v3",
                "updated": "2025-09-30T16:07:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    7,
                    14,
                    1,
                    273,
                    0
                ],
                "published": "2025-03-03T19:12:48Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    19,
                    12,
                    48,
                    0,
                    62,
                    0
                ],
                "title": "One ruler to measure them all: Benchmarking multilingual long-context\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One ruler to measure them all: Benchmarking multilingual long-context\n  language models"
                },
                "summary": "We present ONERULER, a multilingual benchmark designed to evaluate\nlong-context language models across 26 languages. ONERULER adapts the\nEnglish-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic\ntasks that test both retrieval and aggregation, including new variations of the\n\"needle-in-a-haystack\" task that allow for the possibility of a nonexistent\nneedle. We create ONERULER through a two-step process, first writing English\ninstructions for each task and then collaborating with native speakers to\ntranslate them into 25 additional languages. Experiments with both open-weight\nand closed LLMs reveal a widening performance gap between low- and\nhigh-resource languages as context length increases from 8K to 128K tokens.\nSurprisingly, English is not the top-performing language on long-context tasks\n(ranked 6th out of 26), with Polish emerging as the top language. Our\nexperiments also show that many LLMs (particularly OpenAI's o3-mini-high)\nincorrectly predict the absence of an answer, even in high-resource languages.\nFinally, in cross-lingual scenarios where instructions and context appear in\ndifferent languages, performance can fluctuate by up to 20% depending on the\ninstruction language. We hope the release of ONERULER will facilitate future\nresearch into improving multilingual and cross-lingual long-context training\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ONERULER, a multilingual benchmark designed to evaluate\nlong-context language models across 26 languages. ONERULER adapts the\nEnglish-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic\ntasks that test both retrieval and aggregation, including new variations of the\n\"needle-in-a-haystack\" task that allow for the possibility of a nonexistent\nneedle. We create ONERULER through a two-step process, first writing English\ninstructions for each task and then collaborating with native speakers to\ntranslate them into 25 additional languages. Experiments with both open-weight\nand closed LLMs reveal a widening performance gap between low- and\nhigh-resource languages as context length increases from 8K to 128K tokens.\nSurprisingly, English is not the top-performing language on long-context tasks\n(ranked 6th out of 26), with Polish emerging as the top language. Our\nexperiments also show that many LLMs (particularly OpenAI's o3-mini-high)\nincorrectly predict the absence of an answer, even in high-resource languages.\nFinally, in cross-lingual scenarios where instructions and context appear in\ndifferent languages, performance can fluctuate by up to 20% depending on the\ninstruction language. We hope the release of ONERULER will facilitate future\nresearch into improving multilingual and cross-lingual long-context training\npipelines."
                },
                "authors": [
                    {
                        "name": "Yekyung Kim"
                    },
                    {
                        "name": "Jenna Russell"
                    },
                    {
                        "name": "Marzena Karpinska"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14688v2",
                "updated": "2025-09-30T16:03:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    3,
                    47,
                    1,
                    273,
                    0
                ],
                "published": "2025-07-19T16:30:45Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    16,
                    30,
                    45,
                    5,
                    200,
                    0
                ],
                "title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their\n  Limitations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their\n  Limitations"
                },
                "summary": "Post-training has emerged as a crucial technique for aligning pre-trained\nLarge Language Models (LLMs) with human instructions, significantly enhancing\ntheir performance across a wide range of tasks. Central to this process is the\nquality and diversity of post-training datasets. This paper presents a review\nof publicly available Arabic post-training datasets on the Hugging Face Hub,\norganized along four key dimensions: (1) LLM Capabilities (e.g., Question\nAnswering, Translation, Reasoning, Summarization, Dialogue, Code Generation,\nand Function Calling); (2) Steerability (e.g., Persona and System Prompts); (3)\nAlignment (e.g., Cultural, Safety, Ethics, and Fairness); and (4) Robustness.\nEach dataset is rigorously evaluated based on popularity, practical adoption,\nrecency and maintenance, documentation and annotation quality, licensing\ntransparency, and scientific contribution. Our review revealed critical gaps in\nthe development of Arabic post-training datasets, including limited task\ndiversity, inconsistent or missing documentation and annotation, and low\nadoption across the community. Finally, the paper discusses the implications of\nthese gaps on the progress of Arabic-centric LLMs and applications while\nproviding concrete recommendations for future efforts in Arabic post-training\ndataset development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training has emerged as a crucial technique for aligning pre-trained\nLarge Language Models (LLMs) with human instructions, significantly enhancing\ntheir performance across a wide range of tasks. Central to this process is the\nquality and diversity of post-training datasets. This paper presents a review\nof publicly available Arabic post-training datasets on the Hugging Face Hub,\norganized along four key dimensions: (1) LLM Capabilities (e.g., Question\nAnswering, Translation, Reasoning, Summarization, Dialogue, Code Generation,\nand Function Calling); (2) Steerability (e.g., Persona and System Prompts); (3)\nAlignment (e.g., Cultural, Safety, Ethics, and Fairness); and (4) Robustness.\nEach dataset is rigorously evaluated based on popularity, practical adoption,\nrecency and maintenance, documentation and annotation quality, licensing\ntransparency, and scientific contribution. Our review revealed critical gaps in\nthe development of Arabic post-training datasets, including limited task\ndiversity, inconsistent or missing documentation and annotation, and low\nadoption across the community. Finally, the paper discusses the implications of\nthese gaps on the progress of Arabic-centric LLMs and applications while\nproviding concrete recommendations for future efforts in Arabic post-training\ndataset development."
                },
                "authors": [
                    {
                        "name": "Mohammed Alkhowaiter"
                    },
                    {
                        "name": "Norah Alshahrani"
                    },
                    {
                        "name": "Saied Alshahrani"
                    },
                    {
                        "name": "Reem I. Masoud"
                    },
                    {
                        "name": "Alaa Alzahrani"
                    },
                    {
                        "name": "Deema Alnuhait"
                    },
                    {
                        "name": "Emad A. Alghamdi"
                    },
                    {
                        "name": "Khalid Almubarak"
                    }
                ],
                "author_detail": {
                    "name": "Khalid Almubarak"
                },
                "author": "Khalid Almubarak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26440v1",
                "updated": "2025-09-30T15:58:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    58,
                    2,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:58:02Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    58,
                    2,
                    1,
                    273,
                    0
                ],
                "title": "Transformer Classification of Breast Lesions: The BreastDCEDL_AMBL\n  Benchmark Dataset and 0.92 AUC Baseline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer Classification of Breast Lesions: The BreastDCEDL_AMBL\n  Benchmark Dataset and 0.92 AUC Baseline"
                },
                "summary": "The error is caused by special characters that arXiv's system doesn't\nrecognize. Here's the cleaned version with all problematic characters replaced:\nBreast magnetic resonance imaging is a critical tool for cancer detection and\ntreatment planning, but its clinical utility is hindered by poor specificity,\nleading to high false-positive rates and unnecessary biopsies. This study\nintroduces a transformer-based framework for automated classification of breast\nlesions in dynamic contrast-enhanced MRI, addressing the challenge of\ndistinguishing benign from malignant findings. We implemented a SegFormer\narchitecture that achieved an AUC of 0.92 for lesion-level classification, with\n100% sensitivity and 67% specificity at the patient level - potentially\neliminating one-third of unnecessary biopsies without missing malignancies. The\nmodel quantifies malignant pixel distribution via semantic segmentation,\nproducing interpretable spatial predictions that support clinical\ndecision-making. To establish reproducible benchmarks, we curated\nBreastDCEDL_AMBL by transforming The Cancer Imaging Archive's AMBL collection\ninto a standardized deep learning dataset with 88 patients and 133 annotated\nlesions (89 benign, 44 malignant). This resource addresses a key infrastructure\ngap, as existing public datasets lack benign lesion annotations, limiting\nbenign-malignant classification research. Training incorporated an expanded\ncohort of over 1,200 patients through integration with BreastDCEDL datasets,\nvalidating transfer learning approaches despite primary tumor-only annotations.\nPublic release of the dataset, models, and evaluation protocols provides the\nfirst standardized benchmark for DCE-MRI lesion classification, enabling\nmethodological advancement toward clinical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The error is caused by special characters that arXiv's system doesn't\nrecognize. Here's the cleaned version with all problematic characters replaced:\nBreast magnetic resonance imaging is a critical tool for cancer detection and\ntreatment planning, but its clinical utility is hindered by poor specificity,\nleading to high false-positive rates and unnecessary biopsies. This study\nintroduces a transformer-based framework for automated classification of breast\nlesions in dynamic contrast-enhanced MRI, addressing the challenge of\ndistinguishing benign from malignant findings. We implemented a SegFormer\narchitecture that achieved an AUC of 0.92 for lesion-level classification, with\n100% sensitivity and 67% specificity at the patient level - potentially\neliminating one-third of unnecessary biopsies without missing malignancies. The\nmodel quantifies malignant pixel distribution via semantic segmentation,\nproducing interpretable spatial predictions that support clinical\ndecision-making. To establish reproducible benchmarks, we curated\nBreastDCEDL_AMBL by transforming The Cancer Imaging Archive's AMBL collection\ninto a standardized deep learning dataset with 88 patients and 133 annotated\nlesions (89 benign, 44 malignant). This resource addresses a key infrastructure\ngap, as existing public datasets lack benign lesion annotations, limiting\nbenign-malignant classification research. Training incorporated an expanded\ncohort of over 1,200 patients through integration with BreastDCEDL datasets,\nvalidating transfer learning approaches despite primary tumor-only annotations.\nPublic release of the dataset, models, and evaluation protocols provides the\nfirst standardized benchmark for DCE-MRI lesion classification, enabling\nmethodological advancement toward clinical deployment."
                },
                "authors": [
                    {
                        "name": "Naomi Fridman"
                    },
                    {
                        "name": "Anat Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Anat Goldstein"
                },
                "arxiv_affiliation": "Ariel University",
                "author": "Anat Goldstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26436v1",
                "updated": "2025-09-30T15:55:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    55,
                    42,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:55:42Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    55,
                    42,
                    1,
                    273,
                    0
                ],
                "title": "Post-Training Quantization via Residual Truncation and Zero Suppression\n  for Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Quantization via Residual Truncation and Zero Suppression\n  for Diffusion Models"
                },
                "summary": "Diffusion models achieve high-quality image generation but face deployment\nchallenges due to their high computational requirements. Although 8-bit\noutlier-aware post-training quantization (PTQ) matches full-precision\nperformance, extending PTQ to 4 bits remains challenging. Larger step sizes in\n4-bit quantization amplify rounding errors in dense, low-magnitude activations,\nleading to the loss of fine-grained textures. We hypothesize that not only\noutliers but also small activations are critical for texture fidelity. To this\nend, we propose Quantization via Residual Truncation and Zero Suppression\n(QuaRTZ), a 4-bit PTQ scheme for diffusion models. QuaRTZ applies 8-bit min-max\nquantization for outlier handling and compresses to 4 bits via leading-zero\nsuppression to retain LSBs, thereby preserving texture details. Our approach\nreduces rounding errors and improves quantization efficiency by balancing\noutlier preservation and LSB precision. Both theoretical derivations and\nempirical evaluations demonstrate the generalizability of QuaRTZ across diverse\nactivation distributions. Notably, 4-bit QuaRTZ achieves an FID of 6.98 on\nFLUX.1-schnell, outperforming SVDQuant that requires auxiliary FP16 branches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models achieve high-quality image generation but face deployment\nchallenges due to their high computational requirements. Although 8-bit\noutlier-aware post-training quantization (PTQ) matches full-precision\nperformance, extending PTQ to 4 bits remains challenging. Larger step sizes in\n4-bit quantization amplify rounding errors in dense, low-magnitude activations,\nleading to the loss of fine-grained textures. We hypothesize that not only\noutliers but also small activations are critical for texture fidelity. To this\nend, we propose Quantization via Residual Truncation and Zero Suppression\n(QuaRTZ), a 4-bit PTQ scheme for diffusion models. QuaRTZ applies 8-bit min-max\nquantization for outlier handling and compresses to 4 bits via leading-zero\nsuppression to retain LSBs, thereby preserving texture details. Our approach\nreduces rounding errors and improves quantization efficiency by balancing\noutlier preservation and LSB precision. Both theoretical derivations and\nempirical evaluations demonstrate the generalizability of QuaRTZ across diverse\nactivation distributions. Notably, 4-bit QuaRTZ achieves an FID of 6.98 on\nFLUX.1-schnell, outperforming SVDQuant that requires auxiliary FP16 branches."
                },
                "authors": [
                    {
                        "name": "Donghoon Kim"
                    },
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Ik Joon Chang"
                    },
                    {
                        "name": "Sung-Ho Bae"
                    }
                ],
                "author_detail": {
                    "name": "Sung-Ho Bae"
                },
                "author": "Sung-Ho Bae",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26435v1",
                "updated": "2025-09-30T15:55:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    55,
                    24,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:55:24Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    55,
                    24,
                    1,
                    273,
                    0
                ],
                "title": "Adaptive Planning for Multi-Attribute Controllable Summarization with\n  Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Planning for Multi-Attribute Controllable Summarization with\n  Monte Carlo Tree Search"
                },
                "summary": "Controllable summarization moves beyond generic outputs toward human-aligned\nsummaries guided by specified attributes. In practice, the interdependence\namong attributes makes it challenging for language models to satisfy correlated\nconstraints consistently. Moreover, previous approaches often require\nper-attribute fine-tuning, limiting flexibility across diverse summary\nattributes. In this paper, we propose adaptive planning for multi-attribute\ncontrollable summarization (PACO), a training-free framework that reframes the\ntask as planning the order of sequential attribute control with a customized\nMonte Carlo Tree Search (MCTS). In PACO, nodes represent summaries, and actions\ncorrespond to single-attribute adjustments, enabling progressive refinement of\nonly the attributes requiring further control. This strategy adaptively\ndiscovers optimal control orders, ultimately producing summaries that\neffectively meet all constraints. Extensive experiments across diverse domains\nand models demonstrate that PACO achieves robust multi-attribute\ncontrollability, surpassing both LLM-based self-planning models and fine-tuned\nbaselines. Remarkably, PACO with Llama-3.2-1B rivals the controllability of the\nmuch larger Llama-3.3-70B baselines. With larger models, PACO achieves superior\ncontrol performance, outperforming all competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable summarization moves beyond generic outputs toward human-aligned\nsummaries guided by specified attributes. In practice, the interdependence\namong attributes makes it challenging for language models to satisfy correlated\nconstraints consistently. Moreover, previous approaches often require\nper-attribute fine-tuning, limiting flexibility across diverse summary\nattributes. In this paper, we propose adaptive planning for multi-attribute\ncontrollable summarization (PACO), a training-free framework that reframes the\ntask as planning the order of sequential attribute control with a customized\nMonte Carlo Tree Search (MCTS). In PACO, nodes represent summaries, and actions\ncorrespond to single-attribute adjustments, enabling progressive refinement of\nonly the attributes requiring further control. This strategy adaptively\ndiscovers optimal control orders, ultimately producing summaries that\neffectively meet all constraints. Extensive experiments across diverse domains\nand models demonstrate that PACO achieves robust multi-attribute\ncontrollability, surpassing both LLM-based self-planning models and fine-tuned\nbaselines. Remarkably, PACO with Llama-3.2-1B rivals the controllability of the\nmuch larger Llama-3.3-70B baselines. With larger models, PACO achieves superior\ncontrol performance, outperforming all competitors."
                },
                "authors": [
                    {
                        "name": "Sangwon Ryu"
                    },
                    {
                        "name": "Heejin Do"
                    },
                    {
                        "name": "Yunsu Kim"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    },
                    {
                        "name": "Jungseul Ok"
                    }
                ],
                "author_detail": {
                    "name": "Jungseul Ok"
                },
                "author": "Jungseul Ok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26433v1",
                "updated": "2025-09-30T15:54:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    54,
                    8,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:54:08Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    54,
                    8,
                    1,
                    273,
                    0
                ],
                "title": "ACT: Agentic Classification Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACT: Agentic Classification Tree"
                },
                "summary": "When used in high-stakes settings, AI systems are expected to produce\ndecisions that are transparent, interpretable, and auditable, a requirement\nincreasingly expected by regulations. Decision trees such as CART provide clear\nand verifiable rules, but they are restricted to structured tabular data and\ncannot operate directly on unstructured inputs such as text. In practice, large\nlanguage models (LLMs) are widely used for such data, yet prompting strategies\nsuch as chain-of-thought or prompt optimization still rely on free-form\nreasoning, limiting their ability to ensure trustworthy behaviors. We present\nthe Agentic Classification Tree (ACT), which extends decision-tree methodology\nto unstructured inputs by formulating each split as a natural-language\nquestion, refined through impurity-based evaluation and LLM feedback via\nTextGrad. Experiments on text benchmarks show that ACT matches or surpasses\nprompting-based baselines while producing transparent and interpretable\ndecision paths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When used in high-stakes settings, AI systems are expected to produce\ndecisions that are transparent, interpretable, and auditable, a requirement\nincreasingly expected by regulations. Decision trees such as CART provide clear\nand verifiable rules, but they are restricted to structured tabular data and\ncannot operate directly on unstructured inputs such as text. In practice, large\nlanguage models (LLMs) are widely used for such data, yet prompting strategies\nsuch as chain-of-thought or prompt optimization still rely on free-form\nreasoning, limiting their ability to ensure trustworthy behaviors. We present\nthe Agentic Classification Tree (ACT), which extends decision-tree methodology\nto unstructured inputs by formulating each split as a natural-language\nquestion, refined through impurity-based evaluation and LLM feedback via\nTextGrad. Experiments on text benchmarks show that ACT matches or surpasses\nprompting-based baselines while producing transparent and interpretable\ndecision paths."
                },
                "authors": [
                    {
                        "name": "Vincent Grari"
                    },
                    {
                        "name": "Tim Arni"
                    },
                    {
                        "name": "Thibault Laugel"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26432v1",
                "updated": "2025-09-30T15:53:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size"
                },
                "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs."
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao"
                    },
                    {
                        "name": "Chen"
                    },
                    {
                        "name": "Yuto Karashima"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "arxiv_affiliation": "Mark",
                "author": "Hongxiang Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16547v2",
                "updated": "2025-09-30T15:50:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    50,
                    35,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-22T11:37:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    11,
                    37,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "Find the Fruit: Zero-Shot Sim2Real RL for Occlusion-Aware Plant\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Find the Fruit: Zero-Shot Sim2Real RL for Occlusion-Aware Plant\n  Manipulation"
                },
                "summary": "Autonomous harvesting in the open presents a complex manipulation problem. In\nmost scenarios, an autonomous system has to deal with significant occlusion and\nrequire interaction in the presence of large structural uncertainties (every\nplant is different). Perceptual and modeling uncertainty make design of\nreliable manipulation controllers for harvesting challenging, resulting in poor\nperformance during deployment. We present a sim2real reinforcement learning\n(RL) framework for occlusion-aware plant manipulation, where a policy is\nlearned entirely in simulation to reposition stems and leaves to reveal target\nfruit(s). In our proposed approach, we decouple high-level kinematic planning\nfrom low-level compliant control which simplifies the sim2real transfer. This\ndecomposition allows the learned policy to generalize across multiple plants\nwith different stiffness and morphology. In experiments with multiple\nreal-world plant setups, our system achieves up to 86.7% success in exposing\ntarget fruits, demonstrating robustness to occlusion variation and structural\nuncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous harvesting in the open presents a complex manipulation problem. In\nmost scenarios, an autonomous system has to deal with significant occlusion and\nrequire interaction in the presence of large structural uncertainties (every\nplant is different). Perceptual and modeling uncertainty make design of\nreliable manipulation controllers for harvesting challenging, resulting in poor\nperformance during deployment. We present a sim2real reinforcement learning\n(RL) framework for occlusion-aware plant manipulation, where a policy is\nlearned entirely in simulation to reposition stems and leaves to reveal target\nfruit(s). In our proposed approach, we decouple high-level kinematic planning\nfrom low-level compliant control which simplifies the sim2real transfer. This\ndecomposition allows the learned policy to generalize across multiple plants\nwith different stiffness and morphology. In experiments with multiple\nreal-world plant setups, our system achieves up to 86.7% success in exposing\ntarget fruits, demonstrating robustness to occlusion variation and structural\nuncertainty."
                },
                "authors": [
                    {
                        "name": "Nitesh Subedi"
                    },
                    {
                        "name": "Hsin-Jung Yang"
                    },
                    {
                        "name": "Devesh K. Jha"
                    },
                    {
                        "name": "Soumik Sarkar"
                    }
                ],
                "author_detail": {
                    "name": "Soumik Sarkar"
                },
                "author": "Soumik Sarkar",
                "arxiv_comment": "9 Pages, 3 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20476v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20476v2",
                "updated": "2025-09-30T15:49:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    49,
                    22,
                    1,
                    273,
                    0
                ],
                "published": "2024-12-29T14:29:34Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    14,
                    29,
                    34,
                    6,
                    364,
                    0
                ],
                "title": "Cut the Deadwood Out: Backdoor Purification via Guided Module\n  Substitution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cut the Deadwood Out: Backdoor Purification via Guided Module\n  Substitution"
                },
                "summary": "Model NLP models are commonly trained (or fine-tuned) on datasets from\nuntrusted platforms like HuggingFace, posing significant risks of data\npoisoning attacks. A practical yet underexplored challenge arises when such\nbackdoors are discovered after model deployment, making retraining-required\ndefenses less desirable due to computational costs and data constraints. In\nthis work, we propose Guided Module Substitution (GMS), an effective\nretraining-free method based on guided merging of the victim model with just a\nsingle proxy model. Unlike prior ad-hoc merging defenses, GMS uses a guided\ntrade-off signal between utility and backdoor to selectively replaces modules\nin the victim model. GMS offers four desirable properties: (1) robustness to\nthe choice and trustworthiness of the proxy model, (2) applicability under\ninaccurate data knowledge, (3) stability across hyperparameters, and (4)\ntransferability across different attacks. Extensive experiments on encoder\nmodels and decoder LLMs demonstrate the strong effectiveness of GMS. GMS\nsignificantly outperforms even the strongest defense baseline, particularly\nagainst challenging attacks like LWS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model NLP models are commonly trained (or fine-tuned) on datasets from\nuntrusted platforms like HuggingFace, posing significant risks of data\npoisoning attacks. A practical yet underexplored challenge arises when such\nbackdoors are discovered after model deployment, making retraining-required\ndefenses less desirable due to computational costs and data constraints. In\nthis work, we propose Guided Module Substitution (GMS), an effective\nretraining-free method based on guided merging of the victim model with just a\nsingle proxy model. Unlike prior ad-hoc merging defenses, GMS uses a guided\ntrade-off signal between utility and backdoor to selectively replaces modules\nin the victim model. GMS offers four desirable properties: (1) robustness to\nthe choice and trustworthiness of the proxy model, (2) applicability under\ninaccurate data knowledge, (3) stability across hyperparameters, and (4)\ntransferability across different attacks. Extensive experiments on encoder\nmodels and decoder LLMs demonstrate the strong effectiveness of GMS. GMS\nsignificantly outperforms even the strongest defense baseline, particularly\nagainst challenging attacks like LWS."
                },
                "authors": [
                    {
                        "name": "Yao Tong"
                    },
                    {
                        "name": "Weijun Li"
                    },
                    {
                        "name": "Xuanli He"
                    },
                    {
                        "name": "Haolan Zhan"
                    },
                    {
                        "name": "Qiongkai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Qiongkai Xu"
                },
                "author": "Qiongkai Xu",
                "arxiv_comment": "Accepted to Findings of the Association for Computational\n  Linguistics: EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20476v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26425v1",
                "updated": "2025-09-30T15:46:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    46,
                    36,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:46:36Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    46,
                    36,
                    1,
                    273,
                    0
                ],
                "title": "Low-power integrated optical parametric amplification via\n  second-harmonic resonance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-power integrated optical parametric amplification via\n  second-harmonic resonance"
                },
                "summary": "Optical amplifiers are fundamental to modern photonics, enabling\nlong-distance communications, precision sensing, and quantum information\nprocessing. Erbium-doped amplifiers dominate telecommunications but are\nrestricted to specific wavelength bands, while semiconductor amplifiers offer\nbroader coverage but suffer from high noise and nonlinear distortions. Optical\nparametric amplifiers (OPAs) promise broadband, quantum-limited amplification\nacross arbitrary wavelengths. However, their miniaturization and deployment has\nbeen hampered by watt-level power requirements. Here we demonstrate an\nintegrated OPA on thin-film lithium niobate that achieves >17 dB gain with <200\nmW input power -- an order of magnitude improvement over previous\ndemonstrations. Our second-harmonic-resonant design enhances both pump\ngeneration efficiency (95% conversion) and pump power utilization through\nrecirculation, without sacrificing bandwidth. The resonant architecture\nincreases the effective pump power by nearly an order of magnitude compared to\nconventional single-pass designs, while also multiplexing the signal and pump.\nWe demonstrate flat near-quantum-limited noise performance over 110 nm. Our\nlow-power architecture enables practical on-chip OPAs for next generation\nquantum and classical photonics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical amplifiers are fundamental to modern photonics, enabling\nlong-distance communications, precision sensing, and quantum information\nprocessing. Erbium-doped amplifiers dominate telecommunications but are\nrestricted to specific wavelength bands, while semiconductor amplifiers offer\nbroader coverage but suffer from high noise and nonlinear distortions. Optical\nparametric amplifiers (OPAs) promise broadband, quantum-limited amplification\nacross arbitrary wavelengths. However, their miniaturization and deployment has\nbeen hampered by watt-level power requirements. Here we demonstrate an\nintegrated OPA on thin-film lithium niobate that achieves >17 dB gain with <200\nmW input power -- an order of magnitude improvement over previous\ndemonstrations. Our second-harmonic-resonant design enhances both pump\ngeneration efficiency (95% conversion) and pump power utilization through\nrecirculation, without sacrificing bandwidth. The resonant architecture\nincreases the effective pump power by nearly an order of magnitude compared to\nconventional single-pass designs, while also multiplexing the signal and pump.\nWe demonstrate flat near-quantum-limited noise performance over 110 nm. Our\nlow-power architecture enables practical on-chip OPAs for next generation\nquantum and classical photonics."
                },
                "authors": [
                    {
                        "name": "Devin J. Dean"
                    },
                    {
                        "name": "Taewon Park"
                    },
                    {
                        "name": "Hubert S. Stokowski"
                    },
                    {
                        "name": "Luke Qi"
                    },
                    {
                        "name": "Sam Robison"
                    },
                    {
                        "name": "Alexander Y. Hwang"
                    },
                    {
                        "name": "Jason Herrmann"
                    },
                    {
                        "name": "Martin M. Fejer"
                    },
                    {
                        "name": "Amir H. Safavi-Naeini"
                    }
                ],
                "author_detail": {
                    "name": "Amir H. Safavi-Naeini"
                },
                "author": "Amir H. Safavi-Naeini",
                "arxiv_comment": "16 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26419v1",
                "updated": "2025-09-30T15:41:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    41,
                    28,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:41:28Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    41,
                    28,
                    1,
                    273,
                    0
                ],
                "title": "Graph Neural Network Acceleration on FPGAs for Fast Inference in Future\n  Muon Triggers at HL-LHC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Network Acceleration on FPGAs for Fast Inference in Future\n  Muon Triggers at HL-LHC"
                },
                "summary": "The High-Luminosity LHC (HL-LHC) will reach luminosities up to 7 times higher\nthan the previous run, yielding denser events and larger occupancies. Next\ngeneration trigger algorithms must retain reliable selection within a strict\nlatency budget. This work explores machine-learning approaches for future muon\ntriggers, using the ATLAS Muon Spectrometer as a benchmark. A Convolutional\nNeural Network (CNN) is used as a reference, while a Graph Neural Network (GNN)\nis introduced as a natural model for sparse detector data. Preliminary\nsingle-track studies show that GNNs achieve high efficiency with compact\narchitectures, an encouraging result in view of FPGA deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The High-Luminosity LHC (HL-LHC) will reach luminosities up to 7 times higher\nthan the previous run, yielding denser events and larger occupancies. Next\ngeneration trigger algorithms must retain reliable selection within a strict\nlatency budget. This work explores machine-learning approaches for future muon\ntriggers, using the ATLAS Muon Spectrometer as a benchmark. A Convolutional\nNeural Network (CNN) is used as a reference, while a Graph Neural Network (GNN)\nis introduced as a natural model for sparse detector data. Preliminary\nsingle-track studies show that GNNs achieve high efficiency with compact\narchitectures, an encouraging result in view of FPGA deployment."
                },
                "authors": [
                    {
                        "name": "Martino Errico"
                    },
                    {
                        "name": "Davide Fiacco"
                    },
                    {
                        "name": "Stefano Giagu"
                    },
                    {
                        "name": "Giuliano Gustavino"
                    },
                    {
                        "name": "Valerio Ippolito"
                    },
                    {
                        "name": "Graziella Russo"
                    }
                ],
                "author_detail": {
                    "name": "Graziella Russo"
                },
                "arxiv_affiliation": "INFN Sezione di Roma",
                "author": "Graziella Russo",
                "arxiv_comment": "5 pages, 2 figures Submission to SciPost for conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26417v1",
                "updated": "2025-09-30T15:41:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    41,
                    23,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:41:23Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    41,
                    23,
                    1,
                    273,
                    0
                ],
                "title": "OntoAligner Meets Knowledge Graph Embedding Aligners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OntoAligner Meets Knowledge Graph Embedding Aligners"
                },
                "summary": "Ontology Alignment (OA) is essential for enabling semantic interoperability\nacross heterogeneous knowledge systems. While recent advances have focused on\nlarge language models (LLMs) for capturing contextual semantics, this work\nrevisits the underexplored potential of Knowledge Graph Embedding (KGE) models,\nwhich offer scalable, structure-aware representations well-suited to\nontology-based tasks. Despite their effectiveness in link prediction, KGE\nmethods remain underutilized in OA, with most prior work focusing narrowly on a\nfew models. To address this gap, we reformulate OA as a link prediction problem\nover merged ontologies represented as RDF-style triples and develop a modular\nframework, integrated into the OntoAligner library, that supports 17 diverse\nKGE models. The system learns embeddings from a combined ontology and aligns\nentities by computing cosine similarity between their representations. We\nevaluate our approach using standard metrics across seven benchmark datasets\nspanning five domains: Anatomy, Biodiversity, Circular Economy, Material\nScience and Engineering, and Biomedical Machine Learning. Two key findings\nemerge: first, KGE models like ConvE and TransF consistently produce\nhigh-precision alignments, outperforming traditional systems in structure-rich\nand multi-relational domains; second, while their recall is moderate, this\nconservatism makes KGEs well-suited for scenarios demanding high-confidence\nmappings. Unlike LLM-based methods that excel at contextual reasoning, KGEs\ndirectly preserve and exploit ontology structure, offering a complementary and\ncomputationally efficient strategy. These results highlight the promise of\nembedding-based OA and open pathways for further work on hybrid models and\nadaptive strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology Alignment (OA) is essential for enabling semantic interoperability\nacross heterogeneous knowledge systems. While recent advances have focused on\nlarge language models (LLMs) for capturing contextual semantics, this work\nrevisits the underexplored potential of Knowledge Graph Embedding (KGE) models,\nwhich offer scalable, structure-aware representations well-suited to\nontology-based tasks. Despite their effectiveness in link prediction, KGE\nmethods remain underutilized in OA, with most prior work focusing narrowly on a\nfew models. To address this gap, we reformulate OA as a link prediction problem\nover merged ontologies represented as RDF-style triples and develop a modular\nframework, integrated into the OntoAligner library, that supports 17 diverse\nKGE models. The system learns embeddings from a combined ontology and aligns\nentities by computing cosine similarity between their representations. We\nevaluate our approach using standard metrics across seven benchmark datasets\nspanning five domains: Anatomy, Biodiversity, Circular Economy, Material\nScience and Engineering, and Biomedical Machine Learning. Two key findings\nemerge: first, KGE models like ConvE and TransF consistently produce\nhigh-precision alignments, outperforming traditional systems in structure-rich\nand multi-relational domains; second, while their recall is moderate, this\nconservatism makes KGEs well-suited for scenarios demanding high-confidence\nmappings. Unlike LLM-based methods that excel at contextual reasoning, KGEs\ndirectly preserve and exploit ontology structure, offering a complementary and\ncomputationally efficient strategy. These results highlight the promise of\nembedding-based OA and open pathways for further work on hybrid models and\nadaptive strategies."
                },
                "authors": [
                    {
                        "name": "Hamed Babaei Giglou"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Sören Auer"
                    },
                    {
                        "name": "Mahsa Sanaei"
                    }
                ],
                "author_detail": {
                    "name": "Mahsa Sanaei"
                },
                "author": "Mahsa Sanaei",
                "arxiv_comment": "10 pages of main content, 3 page references, 3 figures. Accepted to\n  Ontology Matching Workshop at ISWC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26415v1",
                "updated": "2025-09-30T15:39:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    39,
                    34,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:39:34Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    39,
                    34,
                    1,
                    273,
                    0
                ],
                "title": "Automatic Fact-checking in English and Telugu",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Fact-checking in English and Telugu"
                },
                "summary": "False information poses a significant global challenge, and manually\nverifying claims is a time-consuming and resource-intensive process. In this\nresearch paper, we experiment with different approaches to investigate the\neffectiveness of large language models (LLMs) in classifying factual claims by\ntheir veracity and generating justifications in English and Telugu. The key\ncontributions of this work include the creation of a bilingual English-Telugu\ndataset and the benchmarking of different veracity classification approaches\nbased on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "False information poses a significant global challenge, and manually\nverifying claims is a time-consuming and resource-intensive process. In this\nresearch paper, we experiment with different approaches to investigate the\neffectiveness of large language models (LLMs) in classifying factual claims by\ntheir veracity and generating justifications in English and Telugu. The key\ncontributions of this work include the creation of a bilingual English-Telugu\ndataset and the benchmarking of different veracity classification approaches\nbased on LLMs."
                },
                "authors": [
                    {
                        "name": "Ravi Kiran Chikkala"
                    },
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Natalia Skachkova"
                    },
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    },
                    {
                        "name": "Josef van Genabith"
                    }
                ],
                "author_detail": {
                    "name": "Josef van Genabith"
                },
                "author": "Josef van Genabith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26404v1",
                "updated": "2025-09-30T15:34:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    34,
                    8,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:34:08Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    34,
                    8,
                    1,
                    273,
                    0
                ],
                "title": "SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language\n  Model Was Trained From",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language\n  Model Was Trained From"
                },
                "summary": "Fingerprinting Large Language Models (LLMs) is essential for provenance\nverification and model attribution. Existing methods typically extract post-hoc\nsignatures based on training dynamics, data exposure, or hyperparameters --\nproperties that only emerge after training begins. In contrast, we propose a\nstronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method\nthat leverages random initialization biases as persistent, seed-dependent\nidentifiers present even before training. We show that untrained models exhibit\nreproducible token selection biases conditioned solely on their parameters at\ninitialization. These biases are stable and measurable throughout training,\nenabling our statistical detection method to recover a model's lineage with\nhigh confidence. Unlike prior techniques, unreliable before convergence and\nvulnerable to distribution shifts, SeedPrints remains effective across all\ntraining stages and robust under domain shifts or parameter modifications.\nExperiments on LLaMA-style and Qwen-style models show that SeedPrints achieves\nseed-level distinguishability and can provide birth-to-lifecycle identity\nverification akin to a biometric fingerprint. Evaluations on large-scale\npretrained models and fingerprinting benchmarks further confirm its\neffectiveness under practical deployment scenarios. These results suggest that\ninitialization itself imprints a unique and persistent identity on neural\nlanguage models, forming a true ''Galtonian'' fingerprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprinting Large Language Models (LLMs) is essential for provenance\nverification and model attribution. Existing methods typically extract post-hoc\nsignatures based on training dynamics, data exposure, or hyperparameters --\nproperties that only emerge after training begins. In contrast, we propose a\nstronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method\nthat leverages random initialization biases as persistent, seed-dependent\nidentifiers present even before training. We show that untrained models exhibit\nreproducible token selection biases conditioned solely on their parameters at\ninitialization. These biases are stable and measurable throughout training,\nenabling our statistical detection method to recover a model's lineage with\nhigh confidence. Unlike prior techniques, unreliable before convergence and\nvulnerable to distribution shifts, SeedPrints remains effective across all\ntraining stages and robust under domain shifts or parameter modifications.\nExperiments on LLaMA-style and Qwen-style models show that SeedPrints achieves\nseed-level distinguishability and can provide birth-to-lifecycle identity\nverification akin to a biometric fingerprint. Evaluations on large-scale\npretrained models and fingerprinting benchmarks further confirm its\neffectiveness under practical deployment scenarios. These results suggest that\ninitialization itself imprints a unique and persistent identity on neural\nlanguage models, forming a true ''Galtonian'' fingerprint."
                },
                "authors": [
                    {
                        "name": "Yao Tong"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Siquan Li"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Tianyang Hu"
                    }
                ],
                "author_detail": {
                    "name": "Tianyang Hu"
                },
                "author": "Tianyang Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13824v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13824v4",
                "updated": "2025-09-30T15:26:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    26,
                    10,
                    1,
                    273,
                    0
                ],
                "published": "2025-04-18T17:53:48Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    17,
                    53,
                    48,
                    4,
                    108,
                    0
                ],
                "title": "Perspectives on Large Language Models: Polysemy, Stochasticity,\n  Exponential Expressibility, and Unitary Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perspectives on Large Language Models: Polysemy, Stochasticity,\n  Exponential Expressibility, and Unitary Attention"
                },
                "summary": "This paper explores foundational aspects of Large Language Models (LLMs). We\nanalyze how the expressibility of semantic features scales exponentially with\nembedding space dimensions using quasi-orthogonal vectors. We contrast the\ndynamic, context-dependent embeddings of Transformer architectures, which\nresolve polysemy, with a static vector approach based on quantum contextuality.\nStochasticity is framed as an essential feature for enabling creative output\nthrough probabilistic sampling. Finally, we propose quantum attention as a\nunitary extension of classical mechanisms, reframing LLM processing as\nreversible, quantum-like evolutions in Hilbert space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores foundational aspects of Large Language Models (LLMs). We\nanalyze how the expressibility of semantic features scales exponentially with\nembedding space dimensions using quasi-orthogonal vectors. We contrast the\ndynamic, context-dependent embeddings of Transformer architectures, which\nresolve polysemy, with a static vector approach based on quantum contextuality.\nStochasticity is framed as an essential feature for enabling creative output\nthrough probabilistic sampling. Finally, we propose quantum attention as a\nunitary extension of classical mechanisms, reframing LLM processing as\nreversible, quantum-like evolutions in Hilbert space."
                },
                "authors": [
                    {
                        "name": "Karl Svozil"
                    }
                ],
                "author_detail": {
                    "name": "Karl Svozil"
                },
                "author": "Karl Svozil",
                "arxiv_comment": "17 pages, 1 figure, added a section on exponential scaling due to the\n  Johnson-Lindenstrauss lemma, contribution to the ES Forum on \"Quantum\n  Thinking\" (Frankfurt, Germany, EU, September 21-26, 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13824v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13824v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19789v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19789v3",
                "updated": "2025-09-30T15:18:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    18,
                    7,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-26T10:19:26Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    10,
                    19,
                    26,
                    0,
                    146,
                    0
                ],
                "title": "What Can RL Bring to VLA Generalization? An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Can RL Bring to VLA Generalization? An Empirical Study"
                },
                "summary": "Large Vision-Language Action (VLA) models have shown significant potential\nfor embodied AI. However, their predominant training via supervised fine-tuning\n(SFT) limits generalization due to susceptibility to compounding errors under\ndistribution shifts. Reinforcement learning (RL) offers a path to overcome\nthese limitations by optimizing for task objectives via trial-and-error, yet a\nsystematic understanding of its specific generalization benefits for VLAs\ncompared to SFT is lacking. To address this, our study introduces a\ncomprehensive benchmark for evaluating VLA generalization and systematically\ninvestigates the impact of RL fine-tuning across diverse visual, semantic, and\nexecution dimensions. Our extensive experiments reveal that RL fine-tuning,\nparticularly with PPO, significantly enhances generalization in semantic\nunderstanding and execution robustness over SFT, while maintaining comparable\nvisual robustness. We identify PPO as a more effective RL algorithm for VLAs\nthan LLM-derived methods like DPO and GRPO. We also develop a simple recipe for\nefficient PPO training on VLAs, and demonstrate its practical utility for\nimproving VLA generalization. The project page is at https://rlvla.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Action (VLA) models have shown significant potential\nfor embodied AI. However, their predominant training via supervised fine-tuning\n(SFT) limits generalization due to susceptibility to compounding errors under\ndistribution shifts. Reinforcement learning (RL) offers a path to overcome\nthese limitations by optimizing for task objectives via trial-and-error, yet a\nsystematic understanding of its specific generalization benefits for VLAs\ncompared to SFT is lacking. To address this, our study introduces a\ncomprehensive benchmark for evaluating VLA generalization and systematically\ninvestigates the impact of RL fine-tuning across diverse visual, semantic, and\nexecution dimensions. Our extensive experiments reveal that RL fine-tuning,\nparticularly with PPO, significantly enhances generalization in semantic\nunderstanding and execution robustness over SFT, while maintaining comparable\nvisual robustness. We identify PPO as a more effective RL algorithm for VLAs\nthan LLM-derived methods like DPO and GRPO. We also develop a simple recipe for\nefficient PPO training on VLAs, and demonstrate its practical utility for\nimproving VLA generalization. The project page is at https://rlvla.github.io"
                },
                "authors": [
                    {
                        "name": "Jijia Liu"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Bingwen Wei"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Qingmin Liao"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19789v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19789v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26383v2",
                "updated": "2025-10-01T02:16:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    16,
                    36,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:14:24Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    14,
                    24,
                    1,
                    273,
                    0
                ],
                "title": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement\n  Learning"
                },
                "summary": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large\nlanguage models (LLMs) with structured, verifiable knowledge graphs (KGs) to\nreduce hallucinations and expose reasoning traces. However, many KG-RAG systems\ncompose multiple LLM modules (e.g planning, reasoning, and responding),\ninflating inference cost and binding behavior to a specific target KG. To\naddress this, we introduce KG-R1, an agentic KG retrieval-augmented generation\n(KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single\nagent that interacts with KGs as its environment, learning to retrieve at each\nstep and incorporating the retrieved information into its reasoning and\ngeneration. The process is optimized through end-to-end RL. In controlled\nexperiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our\nmethod demonstrates both efficiency and transferability: Using Qwen-2.5-3B,\nKG-R1 improves answer accuracy with fewer generation tokens than prior\nmulti-module workflow methods that use larger foundation or fine-tuned models.\nFurthermore, KG-R1 enables plug and play: after training, it maintains strong\naccuracy on new KGs without modification. These properties make KG-R1 a\npromising KG-RAG framework for real-world deployment. Our code is publicly\navailable at https://github.com/Jinyeop3110/KG-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-graph retrieval-augmented generation (KG-RAG) couples large\nlanguage models (LLMs) with structured, verifiable knowledge graphs (KGs) to\nreduce hallucinations and expose reasoning traces. However, many KG-RAG systems\ncompose multiple LLM modules (e.g planning, reasoning, and responding),\ninflating inference cost and binding behavior to a specific target KG. To\naddress this, we introduce KG-R1, an agentic KG retrieval-augmented generation\n(KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single\nagent that interacts with KGs as its environment, learning to retrieve at each\nstep and incorporating the retrieved information into its reasoning and\ngeneration. The process is optimized through end-to-end RL. In controlled\nexperiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our\nmethod demonstrates both efficiency and transferability: Using Qwen-2.5-3B,\nKG-R1 improves answer accuracy with fewer generation tokens than prior\nmulti-module workflow methods that use larger foundation or fine-tuned models.\nFurthermore, KG-R1 enables plug and play: after training, it maintains strong\naccuracy on new KGs without modification. These properties make KG-R1 a\npromising KG-RAG framework for real-world deployment. Our code is publicly\navailable at https://github.com/Jinyeop3110/KG-R1."
                },
                "authors": [
                    {
                        "name": "Jinyeop Song"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Julian Shun"
                    },
                    {
                        "name": "Yada Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yada Zhu"
                },
                "author": "Yada Zhu",
                "arxiv_comment": "10 pages, 5 figures. Submitted to ICLR 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26375v1",
                "updated": "2025-09-30T15:07:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    7,
                    59,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:07:59Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    7,
                    59,
                    1,
                    273,
                    0
                ],
                "title": "SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task\n  Planning"
                },
                "summary": "Embodied task planning requires agents to produce executable actions in a\nclose-loop manner within the environment. With progressively improving\ncapabilities of LLMs in task decomposition, planning, and generalization,\ncurrent embodied task planning methods adopt LLM-based architecture.However,\nexisting LLM-based planners remain limited in three aspects, i.e., fixed\nplanning paradigms, lack of action sequence constraints, and error-agnostic. In\nthis work, we propose SDA-PLANNER, enabling an adaptive planning paradigm,\nstate-dependency aware and error-aware mechanisms for comprehensive embodied\ntask planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to\nexplicitly model action preconditions and effects, guiding the dynamic\nrevision. To handle execution error, it employs an error-adaptive replanning\nstrategy consisting of Error Backtrack and Diagnosis and Adaptive Action\nSubTree Generation, which locally reconstructs the affected portion of the plan\nbased on the current environment state. Experiments demonstrate that\nSDA-PLANNER consistently outperforms baselines in success rate and goal\ncompletion, particularly under diverse error conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied task planning requires agents to produce executable actions in a\nclose-loop manner within the environment. With progressively improving\ncapabilities of LLMs in task decomposition, planning, and generalization,\ncurrent embodied task planning methods adopt LLM-based architecture.However,\nexisting LLM-based planners remain limited in three aspects, i.e., fixed\nplanning paradigms, lack of action sequence constraints, and error-agnostic. In\nthis work, we propose SDA-PLANNER, enabling an adaptive planning paradigm,\nstate-dependency aware and error-aware mechanisms for comprehensive embodied\ntask planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to\nexplicitly model action preconditions and effects, guiding the dynamic\nrevision. To handle execution error, it employs an error-adaptive replanning\nstrategy consisting of Error Backtrack and Diagnosis and Adaptive Action\nSubTree Generation, which locally reconstructs the affected portion of the plan\nbased on the current environment state. Experiments demonstrate that\nSDA-PLANNER consistently outperforms baselines in success rate and goal\ncompletion, particularly under diverse error conditions."
                },
                "authors": [
                    {
                        "name": "Zichao Shen"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Jiaqi Yuan"
                    },
                    {
                        "name": "Tianchen Zhu"
                    },
                    {
                        "name": "Xingcheng Fu"
                    },
                    {
                        "name": "Qingyun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Qingyun Sun"
                },
                "author": "Qingyun Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17101v2",
                "updated": "2025-09-30T15:06:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    6,
                    40,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-21T07:38:48Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    7,
                    38,
                    48,
                    2,
                    141,
                    0
                ],
                "title": "A quantitative analysis of semantic information in deep representations\n  of text and images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A quantitative analysis of semantic information in deep representations\n  of text and images"
                },
                "summary": "Deep neural networks are known to develop similar representations for\nsemantically related data, even when they belong to different domains, such as\nan image and its description, or the same text in different languages. We\npresent a method for quantitatively investigating this phenomenon by measuring\nthe relative information content of the representations of semantically related\ndata and probing how it is encoded into multiple tokens of large language\nmodels (LLMs) and vision transformers. Looking first at how LLMs process pairs\nof translated sentences, we identify inner ``semantic'' layers containing the\nmost language-transferable information. We find moreover that, on these layers,\na larger LLM (DeepSeek-V3) extracts significantly more general information than\na smaller one (Llama3.1-8B). Semantic information of English text is spread\nacross many tokens and it is characterized by long-distance correlations\nbetween tokens and by a causal left-to-right (i.e., past-future) asymmetry. We\nalso identify layers encoding semantic information within visual transformers.\nWe show that caption representations in the semantic layers of LLMs predict\nvisual representations of the corresponding images. We observe significant and\nmodel-dependent information asymmetries between image and text representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks are known to develop similar representations for\nsemantically related data, even when they belong to different domains, such as\nan image and its description, or the same text in different languages. We\npresent a method for quantitatively investigating this phenomenon by measuring\nthe relative information content of the representations of semantically related\ndata and probing how it is encoded into multiple tokens of large language\nmodels (LLMs) and vision transformers. Looking first at how LLMs process pairs\nof translated sentences, we identify inner ``semantic'' layers containing the\nmost language-transferable information. We find moreover that, on these layers,\na larger LLM (DeepSeek-V3) extracts significantly more general information than\na smaller one (Llama3.1-8B). Semantic information of English text is spread\nacross many tokens and it is characterized by long-distance correlations\nbetween tokens and by a causal left-to-right (i.e., past-future) asymmetry. We\nalso identify layers encoding semantic information within visual transformers.\nWe show that caption representations in the semantic layers of LLMs predict\nvisual representations of the corresponding images. We observe significant and\nmodel-dependent information asymmetries between image and text representations."
                },
                "authors": [
                    {
                        "name": "Santiago Acevedo"
                    },
                    {
                        "name": "Andrea Mascaretti"
                    },
                    {
                        "name": "Riccardo Rende"
                    },
                    {
                        "name": "Matéo Mahaut"
                    },
                    {
                        "name": "Marco Baroni"
                    },
                    {
                        "name": "Alessandro Laio"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Laio"
                },
                "author": "Alessandro Laio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16293v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16293v2",
                "updated": "2025-09-30T15:06:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    6,
                    10,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-19T15:08:33Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    8,
                    33,
                    4,
                    262,
                    0
                ],
                "title": "Robust LLM Training Infrastructure at ByteDance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM Training Infrastructure at ByteDance"
                },
                "summary": "The training scale of large language models (LLMs) has reached tens of\nthousands of GPUs and is still continuously expanding, enabling faster learning\nof larger models. Accompanying the expansion of the resource scale is the\nprevalence of failures (CUDA error, NaN values, job hang, etc.), which poses\nsignificant challenges to training stability. Any large-scale LLM training\ninfrastructure should strive for minimal training interruption, efficient fault\ndiagnosis, and effective failure tolerance to enable highly efficient\ncontinuous training. This paper presents ByteRobust, a large-scale GPU\ninfrastructure management system tailored for robust and stable training of\nLLMs. It exploits the uniqueness of LLM training process and gives top\npriorities to detecting and recovering failures in a routine manner. Leveraging\nparallelisms and characteristics of LLM training, ByteRobust enables\nhigh-capacity fault tolerance, prompt fault demarcation, and localization with\nan effective data-driven approach, comprehensively ensuring continuous and\nefficient training of LLM tasks. ByteRobust is deployed on a production GPU\nplatform with over 200,000 GPUs and achieves 97% ETTR for a three-month\ntraining job on 9,600 GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training scale of large language models (LLMs) has reached tens of\nthousands of GPUs and is still continuously expanding, enabling faster learning\nof larger models. Accompanying the expansion of the resource scale is the\nprevalence of failures (CUDA error, NaN values, job hang, etc.), which poses\nsignificant challenges to training stability. Any large-scale LLM training\ninfrastructure should strive for minimal training interruption, efficient fault\ndiagnosis, and effective failure tolerance to enable highly efficient\ncontinuous training. This paper presents ByteRobust, a large-scale GPU\ninfrastructure management system tailored for robust and stable training of\nLLMs. It exploits the uniqueness of LLM training process and gives top\npriorities to detecting and recovering failures in a routine manner. Leveraging\nparallelisms and characteristics of LLM training, ByteRobust enables\nhigh-capacity fault tolerance, prompt fault demarcation, and localization with\nan effective data-driven approach, comprehensively ensuring continuous and\nefficient training of LLM tasks. ByteRobust is deployed on a production GPU\nplatform with over 200,000 GPUs and achieves 97% ETTR for a three-month\ntraining job on 9,600 GPUs."
                },
                "authors": [
                    {
                        "name": "Borui Wan"
                    },
                    {
                        "name": "Gaohong Liu"
                    },
                    {
                        "name": "Zuquan Song"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yun Zhang"
                    },
                    {
                        "name": "Guangming Sheng"
                    },
                    {
                        "name": "Shuguang Wang"
                    },
                    {
                        "name": "Houmin Wei"
                    },
                    {
                        "name": "Chenyuan Wang"
                    },
                    {
                        "name": "Weiqiang Lou"
                    },
                    {
                        "name": "Xi Yang"
                    },
                    {
                        "name": "Mofan Zhang"
                    },
                    {
                        "name": "Kaihua Jiang"
                    },
                    {
                        "name": "Cheng Ren"
                    },
                    {
                        "name": "Xiaoyun Zhi"
                    },
                    {
                        "name": "Menghan Yu"
                    },
                    {
                        "name": "Zhe Nan"
                    },
                    {
                        "name": "Zhuolin Zheng"
                    },
                    {
                        "name": "Baoquan Zhong"
                    },
                    {
                        "name": "Qinlong Wang"
                    },
                    {
                        "name": "Huan Yu"
                    },
                    {
                        "name": "Jinxin Chi"
                    },
                    {
                        "name": "Wang Zhang"
                    },
                    {
                        "name": "Yuhan Li"
                    },
                    {
                        "name": "Zixian Du"
                    },
                    {
                        "name": "Sida Zhao"
                    },
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Jingzhe Tang"
                    },
                    {
                        "name": "Zherui Liu"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Yanghua Peng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Wencong Xiao"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Liang Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Xiang"
                },
                "author": "Liang Xiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16293v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16293v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26368v1",
                "updated": "2025-09-30T15:04:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    4,
                    24,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T15:04:24Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    4,
                    24,
                    1,
                    273,
                    0
                ],
                "title": "Introducing Large Language Models in the Design Flow of Time Sensitive\n  Networking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing Large Language Models in the Design Flow of Time Sensitive\n  Networking"
                },
                "summary": "The growing demand for real-time, safety-critical systems has significantly\nincreased both the adoption and complexity of Time Sensitive Networking (TSN).\nConfiguring an optimized TSN network is highly challenging, requiring careful\nplanning, design, verification, validation, and deployment. Large Language\nModels (LLMs) have recently demonstrated strong capabilities in solving complex\ntasks, positioning them as promising candidates for automating end-to-end TSN\ndeployment, referred to as TSN orchestration. This paper outlines the steps\ninvolved in TSN orchestration and the associated challenges. To assess the\ncapabilities of existing LLM models, we conduct an initial proof-of-concept\ncase study focused on TSN configuration across multiple models. Building on\nthese insights, we propose an LLM-assisted orchestration framework. Unlike\nprior research on LLMs in computer networks, which has concentrated on general\nconfiguration and management, TSN-specific orchestration has not yet been\ninvestigated. We present the building blocks for automating TSN using LLMs,\ndescribe the proposed pipeline, and analyze opportunities and limitations for\nreal-world deployment. Finally, we highlight key challenges and research\ndirections, including the development of TSN-focused datasets, standardized\nbenchmark suites, and the integration of external tools such as Network\nCalculus (NC) engines and simulators. This work provides the first roadmap\ntoward assessing the feasibility of LLM-assisted TSN orchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for real-time, safety-critical systems has significantly\nincreased both the adoption and complexity of Time Sensitive Networking (TSN).\nConfiguring an optimized TSN network is highly challenging, requiring careful\nplanning, design, verification, validation, and deployment. Large Language\nModels (LLMs) have recently demonstrated strong capabilities in solving complex\ntasks, positioning them as promising candidates for automating end-to-end TSN\ndeployment, referred to as TSN orchestration. This paper outlines the steps\ninvolved in TSN orchestration and the associated challenges. To assess the\ncapabilities of existing LLM models, we conduct an initial proof-of-concept\ncase study focused on TSN configuration across multiple models. Building on\nthese insights, we propose an LLM-assisted orchestration framework. Unlike\nprior research on LLMs in computer networks, which has concentrated on general\nconfiguration and management, TSN-specific orchestration has not yet been\ninvestigated. We present the building blocks for automating TSN using LLMs,\ndescribe the proposed pipeline, and analyze opportunities and limitations for\nreal-world deployment. Finally, we highlight key challenges and research\ndirections, including the development of TSN-focused datasets, standardized\nbenchmark suites, and the integration of external tools such as Network\nCalculus (NC) engines and simulators. This work provides the first roadmap\ntoward assessing the feasibility of LLM-assisted TSN orchestration."
                },
                "authors": [
                    {
                        "name": "Rubi Debnath"
                    },
                    {
                        "name": "Luxi Zhao"
                    },
                    {
                        "name": "Mohammadreza Barzegaran"
                    },
                    {
                        "name": "Sebastian Steinhorst"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Steinhorst"
                },
                "author": "Sebastian Steinhorst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07861v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07861v2",
                "updated": "2025-09-30T14:59:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    59,
                    16,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-08T17:51:24Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    17,
                    51,
                    24,
                    3,
                    128,
                    0
                ],
                "title": "Scalable LLM Math Reasoning Acceleration with Low-rank Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable LLM Math Reasoning Acceleration with Low-rank Distillation"
                },
                "summary": "Due to long generations, large language model (LLM) math reasoning demands\nsignificant computational resources and time. While many existing efficient\ninference methods have been developed with excellent performance preservation\non language tasks, they often severely degrade math performance. In this paper,\nwe propose Caprese, a resource-efficient distillation method to recover lost\ncapabilities from deploying efficient inference methods, focused primarily in\nfeedforward blocks. With original weights unperturbed, roughly 1% of additional\nparameters, and only 20K synthetic training samples, we are able to recover\nmuch if not all of the math capabilities lost from efficient inference for\nthinking LLMs and without harm to language tasks for instruct LLMs. Moreover,\nCaprese slashes the number of active parameters (~2B cut for Gemma 2 9B and\nLlama 3.1 8B) and integrates cleanly into existing model layers to reduce\nlatency (>16% time-to-next-token reduction) while encouraging response brevity\n(up to 8.5% fewer tokens).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to long generations, large language model (LLM) math reasoning demands\nsignificant computational resources and time. While many existing efficient\ninference methods have been developed with excellent performance preservation\non language tasks, they often severely degrade math performance. In this paper,\nwe propose Caprese, a resource-efficient distillation method to recover lost\ncapabilities from deploying efficient inference methods, focused primarily in\nfeedforward blocks. With original weights unperturbed, roughly 1% of additional\nparameters, and only 20K synthetic training samples, we are able to recover\nmuch if not all of the math capabilities lost from efficient inference for\nthinking LLMs and without harm to language tasks for instruct LLMs. Moreover,\nCaprese slashes the number of active parameters (~2B cut for Gemma 2 9B and\nLlama 3.1 8B) and integrates cleanly into existing model layers to reduce\nlatency (>16% time-to-next-token reduction) while encouraging response brevity\n(up to 8.5% fewer tokens)."
                },
                "authors": [
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Bilge Acun"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Yuejie Chi"
                    }
                ],
                "author_detail": {
                    "name": "Yuejie Chi"
                },
                "author": "Yuejie Chi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07861v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07861v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12379v2",
                "updated": "2025-09-30T14:56:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    56,
                    50,
                    1,
                    273,
                    0
                ],
                "published": "2025-08-17T14:28:38Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    14,
                    28,
                    38,
                    6,
                    229,
                    0
                ],
                "title": "GraphCogent: Mitigating LLMs' Working Memory Constraints via Multi-Agent\n  Collaboration in Complex Graph Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphCogent: Mitigating LLMs' Working Memory Constraints via Multi-Agent\n  Collaboration in Complex Graph Understanding"
                },
                "summary": "Large language models (LLMs) show promising performance on small-scale graph\nreasoning tasks but fail when handling real-world graphs with complex queries.\nThis phenomenon arises from LLMs' working memory constraints, which result in\ntheir inability to retain long-range graph topology over extended contexts\nwhile sustaining coherent multi-step reasoning. However, real-world graphs are\noften structurally complex, such as Web, Transportation, Social, and Citation\nnetworks. To address these limitations, we propose GraphCogent, a collaborative\nagent framework inspired by human Working Memory Model that decomposes graph\nreasoning into specialized cognitive processes: sense, buffer, and execute. The\nframework consists of three modules: Sensory Module standardizes diverse graph\ntext representations via subgraph sampling, Buffer Module integrates and\nindexes graph data across multiple formats, and Execution Module combines tool\ncalling and tool creation for efficient reasoning. We also introduce\nGraph4real, a comprehensive benchmark that contains four domains of real-world\ngraphs (Web, Transportation, Social, and Citation) to evaluate LLMs' graph\nreasoning capabilities. Our Graph4real covers 21 different graph reasoning\ntasks, categorized into three types (Structural Querying, Algorithmic\nReasoning, and Predictive Modeling tasks), with graph scales up to 10 times\nlarger than existing benchmarks. Experiments show that Llama3.1-8B based\nGraphCogent achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1\n(671B). Compared to state-of-the-art agent-based baseline, our framework\noutperforms by 20% in accuracy while reducing token usage by 80% for in-toolset\ntasks and 30% for out-toolset tasks. Code will be available after review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show promising performance on small-scale graph\nreasoning tasks but fail when handling real-world graphs with complex queries.\nThis phenomenon arises from LLMs' working memory constraints, which result in\ntheir inability to retain long-range graph topology over extended contexts\nwhile sustaining coherent multi-step reasoning. However, real-world graphs are\noften structurally complex, such as Web, Transportation, Social, and Citation\nnetworks. To address these limitations, we propose GraphCogent, a collaborative\nagent framework inspired by human Working Memory Model that decomposes graph\nreasoning into specialized cognitive processes: sense, buffer, and execute. The\nframework consists of three modules: Sensory Module standardizes diverse graph\ntext representations via subgraph sampling, Buffer Module integrates and\nindexes graph data across multiple formats, and Execution Module combines tool\ncalling and tool creation for efficient reasoning. We also introduce\nGraph4real, a comprehensive benchmark that contains four domains of real-world\ngraphs (Web, Transportation, Social, and Citation) to evaluate LLMs' graph\nreasoning capabilities. Our Graph4real covers 21 different graph reasoning\ntasks, categorized into three types (Structural Querying, Algorithmic\nReasoning, and Predictive Modeling tasks), with graph scales up to 10 times\nlarger than existing benchmarks. Experiments show that Llama3.1-8B based\nGraphCogent achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1\n(671B). Compared to state-of-the-art agent-based baseline, our framework\noutperforms by 20% in accuracy while reducing token usage by 80% for in-toolset\ntasks and 30% for out-toolset tasks. Code will be available after review."
                },
                "authors": [
                    {
                        "name": "Rongzheng Wang"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Yihong Huang"
                    },
                    {
                        "name": "Muquan Li"
                    },
                    {
                        "name": "Yizhuo Ma"
                    },
                    {
                        "name": "Dongyang Zhang"
                    },
                    {
                        "name": "Ke Qin"
                    },
                    {
                        "name": "Man-Fai Leung"
                    }
                ],
                "author_detail": {
                    "name": "Man-Fai Leung"
                },
                "author": "Man-Fai Leung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26354v1",
                "updated": "2025-09-30T14:55:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    55,
                    55,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:55:55Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    55,
                    55,
                    1,
                    273,
                    0
                ],
                "title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents"
                },
                "summary": "Advances in Large Language Models (LLMs) have enabled a new class of\nself-evolving agents that autonomously improve through interaction with the\nenvironment, demonstrating strong capabilities. However, self-evolution also\nintroduces novel risks overlooked by current safety research. In this work, we\nstudy the case where an agent's self-evolution deviates in unintended ways,\nleading to undesirable or even harmful outcomes. We refer to this as\nMisevolution. To provide a systematic investigation, we evaluate misevolution\nalong four key evolutionary pathways: model, memory, tool, and workflow. Our\nempirical findings reveal that misevolution is a widespread risk, affecting\nagents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent\nrisks are observed in the self-evolutionary process, such as the degradation of\nsafety alignment after memory accumulation, or the unintended introduction of\nvulnerabilities in tool creation and reuse. To our knowledge, this is the first\nstudy to systematically conceptualize misevolution and provide empirical\nevidence of its occurrence, highlighting an urgent need for new safety\nparadigms for self-evolving agents. Finally, we discuss potential mitigation\nstrategies to inspire further research on building safer and more trustworthy\nself-evolving agents. Our code and data are available at\nhttps://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes\nexamples that may be offensive or harmful in nature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Large Language Models (LLMs) have enabled a new class of\nself-evolving agents that autonomously improve through interaction with the\nenvironment, demonstrating strong capabilities. However, self-evolution also\nintroduces novel risks overlooked by current safety research. In this work, we\nstudy the case where an agent's self-evolution deviates in unintended ways,\nleading to undesirable or even harmful outcomes. We refer to this as\nMisevolution. To provide a systematic investigation, we evaluate misevolution\nalong four key evolutionary pathways: model, memory, tool, and workflow. Our\nempirical findings reveal that misevolution is a widespread risk, affecting\nagents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent\nrisks are observed in the self-evolutionary process, such as the degradation of\nsafety alignment after memory accumulation, or the unintended introduction of\nvulnerabilities in tool creation and reuse. To our knowledge, this is the first\nstudy to systematically conceptualize misevolution and provide empirical\nevidence of its occurrence, highlighting an urgent need for new safety\nparadigms for self-evolving agents. Finally, we discuss potential mitigation\nstrategies to inspire further research on building safer and more trustworthy\nself-evolving agents. Our code and data are available at\nhttps://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes\nexamples that may be offensive or harmful in nature."
                },
                "authors": [
                    {
                        "name": "Shuai Shao"
                    },
                    {
                        "name": "Qihan Ren"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Boyi Wei"
                    },
                    {
                        "name": "Dadi Guo"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Xinhao Song"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26351v1",
                "updated": "2025-09-30T14:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    54,
                    58,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    54,
                    58,
                    1,
                    273,
                    0
                ],
                "title": "LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and\n  MCI-Like Field Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and\n  MCI-Like Field Simulation"
                },
                "summary": "Research on emergency and mass casualty incident (MCI) triage has been\nlimited by the absence of openly usable, reproducible benchmarks. Yet these\nscenarios demand rapid identification of the patients most in need, where\naccurate deterioration prediction can guide timely interventions. While the\nMIMIC-IV-ED database is openly available to credentialed researchers,\ntransforming it into a triage-focused benchmark requires extensive\npreprocessing, feature harmonization, and schema alignment -- barriers that\nrestrict accessibility to only highly technical users.\n  We address these gaps by first introducing an open, LLM-assisted emergency\ntriage benchmark for deterioration prediction (ICU transfer, in-hospital\nmortality). The benchmark then defines two regimes: (i) a hospital-rich setting\nwith vitals, labs, notes, chief complaints, and structured observations, and\n(ii) an MCI-like field simulation limited to vitals, observations, and notes.\nLarge language models (LLMs) contributed directly to dataset construction by\n(i) harmonizing noisy fields such as AVPU and breathing devices, (ii)\nprioritizing clinically relevant vitals and labs, and (iii) guiding schema\nalignment and efficient merging of disparate tables.\n  We further provide baseline models and SHAP-based interpretability analyses,\nillustrating predictive gaps between regimes and the features most critical for\ntriage. Together, these contributions make triage prediction research more\nreproducible and accessible -- a step toward dataset democratization in\nclinical AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on emergency and mass casualty incident (MCI) triage has been\nlimited by the absence of openly usable, reproducible benchmarks. Yet these\nscenarios demand rapid identification of the patients most in need, where\naccurate deterioration prediction can guide timely interventions. While the\nMIMIC-IV-ED database is openly available to credentialed researchers,\ntransforming it into a triage-focused benchmark requires extensive\npreprocessing, feature harmonization, and schema alignment -- barriers that\nrestrict accessibility to only highly technical users.\n  We address these gaps by first introducing an open, LLM-assisted emergency\ntriage benchmark for deterioration prediction (ICU transfer, in-hospital\nmortality). The benchmark then defines two regimes: (i) a hospital-rich setting\nwith vitals, labs, notes, chief complaints, and structured observations, and\n(ii) an MCI-like field simulation limited to vitals, observations, and notes.\nLarge language models (LLMs) contributed directly to dataset construction by\n(i) harmonizing noisy fields such as AVPU and breathing devices, (ii)\nprioritizing clinically relevant vitals and labs, and (iii) guiding schema\nalignment and efficient merging of disparate tables.\n  We further provide baseline models and SHAP-based interpretability analyses,\nillustrating predictive gaps between regimes and the features most critical for\ntriage. Together, these contributions make triage prediction research more\nreproducible and accessible -- a step toward dataset democratization in\nclinical AI."
                },
                "authors": [
                    {
                        "name": "Joshua Sebastian"
                    },
                    {
                        "name": "Karma Tobden"
                    },
                    {
                        "name": "KMA Solaiman"
                    }
                ],
                "author_detail": {
                    "name": "KMA Solaiman"
                },
                "author": "KMA Solaiman",
                "arxiv_comment": "Submitted to GenAI4Health@NeurIPS 2025. This is the first version of\n  the LLM-assisted emergency triage benchmark dataset and baseline models.\n  Authors: Joshua Sebastian, Karma Tobden, KMA Solaiman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26350v1",
                "updated": "2025-09-30T14:54:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    54,
                    42,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:54:42Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    54,
                    42,
                    1,
                    273,
                    0
                ],
                "title": "SoK: Systematic analysis of adversarial threats against deep learning\n  approaches for autonomous anomaly detection systems in SDN-IoT networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Systematic analysis of adversarial threats against deep learning\n  approaches for autonomous anomaly detection systems in SDN-IoT networks"
                },
                "summary": "Integrating SDN and the IoT enhances network control and flexibility.\nDL-based AAD systems improve security by enabling real-time threat detection in\nSDN-IoT networks. However, these systems remain vulnerable to adversarial\nattacks that manipulate input data or exploit model weaknesses, significantly\ndegrading detection accuracy. Existing research lacks a systematic analysis of\nadversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT\nenvironments. This SoK study introduces a structured adversarial threat model\nand a comprehensive taxonomy of attacks, categorising them into data, model,\nand hybrid-level threats. Unlike previous studies, we systematically evaluate\nwhite, black, and grey-box attack strategies across popular benchmark datasets.\nOur findings reveal that adversarial attacks can reduce detection accuracy by\nup to 48.4%, with Membership Inference causing the most significant drop. C&W\nand DeepFool achieve high evasion success rates. However, adversarial training\nenhances robustness, and its high computational overhead limits the real-time\ndeployment of SDN-IoT applications. We propose adaptive countermeasures,\nincluding real-time adversarial mitigation, enhanced retraining mechanisms, and\nexplainable AI-driven security frameworks. By integrating structured threat\nmodels, this study offers a more comprehensive approach to attack\ncategorisation, impact assessment, and defence evaluation than previous\nresearch. Our work highlights critical vulnerabilities in existing DL-based AAD\nmodels and provides practical recommendations for improving resilience,\ninterpretability, and computational efficiency. This study serves as a\nfoundational reference for researchers and practitioners seeking to enhance\nDL-based AAD security in SDN-IoT networks, offering a systematic adversarial\nthreat model and conceptual defence evaluation based on prior empirical\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating SDN and the IoT enhances network control and flexibility.\nDL-based AAD systems improve security by enabling real-time threat detection in\nSDN-IoT networks. However, these systems remain vulnerable to adversarial\nattacks that manipulate input data or exploit model weaknesses, significantly\ndegrading detection accuracy. Existing research lacks a systematic analysis of\nadversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT\nenvironments. This SoK study introduces a structured adversarial threat model\nand a comprehensive taxonomy of attacks, categorising them into data, model,\nand hybrid-level threats. Unlike previous studies, we systematically evaluate\nwhite, black, and grey-box attack strategies across popular benchmark datasets.\nOur findings reveal that adversarial attacks can reduce detection accuracy by\nup to 48.4%, with Membership Inference causing the most significant drop. C&W\nand DeepFool achieve high evasion success rates. However, adversarial training\nenhances robustness, and its high computational overhead limits the real-time\ndeployment of SDN-IoT applications. We propose adaptive countermeasures,\nincluding real-time adversarial mitigation, enhanced retraining mechanisms, and\nexplainable AI-driven security frameworks. By integrating structured threat\nmodels, this study offers a more comprehensive approach to attack\ncategorisation, impact assessment, and defence evaluation than previous\nresearch. Our work highlights critical vulnerabilities in existing DL-based AAD\nmodels and provides practical recommendations for improving resilience,\ninterpretability, and computational efficiency. This study serves as a\nfoundational reference for researchers and practitioners seeking to enhance\nDL-based AAD security in SDN-IoT networks, offering a systematic adversarial\nthreat model and conceptual defence evaluation based on prior empirical\nstudies."
                },
                "authors": [
                    {
                        "name": "Tharindu Lakshan Yasarathna"
                    },
                    {
                        "name": "Nhien-An Le-Khac"
                    }
                ],
                "author_detail": {
                    "name": "Nhien-An Le-Khac"
                },
                "author": "Nhien-An Le-Khac",
                "arxiv_doi": "10.1016/j.jisa.2025.104220",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jisa.2025.104220",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.26350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13820v2",
                "updated": "2025-09-30T14:52:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    52,
                    40,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-20T02:01:55Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    2,
                    1,
                    55,
                    1,
                    140,
                    0
                ],
                "title": "Structured Agent Distillation for Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Agent Distillation for Large Language Model"
                },
                "summary": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit strong capabilities as decision-making\nagents by interleaving reasoning and actions, as seen in ReAct-style\nframeworks. Yet, their practical deployment is constrained by high inference\ncosts and large model sizes. We propose Structured Agent Distillation, a\nframework that compresses large LLM-based agents into smaller student models\nwhile preserving both reasoning fidelity and action consistency. Unlike\nstandard token-level distillation, our method segments trajectories into\n{[REASON]} and {[ACT]} spans, applying segment-specific losses to align each\ncomponent with the teacher's behavior. This structure-aware supervision enables\ncompact agents to better replicate the teacher's decision process. Experiments\non ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently\noutperforms token-level and imitation learning baselines, achieving significant\ncompression with minimal performance drop. Scaling and ablation results further\nhighlight the importance of span-level alignment for efficient and deployable\nagents."
                },
                "authors": [
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zhenglun Kong"
                    },
                    {
                        "name": "Peiyan Dong"
                    },
                    {
                        "name": "Changdi Yang"
                    },
                    {
                        "name": "Tianqi Li"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Geng Yuan"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Wenbin Zhang"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Xue Lin"
                    },
                    {
                        "name": "Dong Huang"
                    },
                    {
                        "name": "Yanzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanzhi Wang"
                },
                "author": "Yanzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26345v1",
                "updated": "2025-09-30T14:50:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    50,
                    59,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:50:59Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    50,
                    59,
                    1,
                    273,
                    0
                ],
                "title": "SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate\n  Jailbreak Attacks in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate\n  Jailbreak Attacks in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive performance across\ndiverse natural language processing tasks, but their growing power also\namplifies potential risks such as jailbreak attacks that circumvent built-in\nsafety mechanisms. Existing defenses including input paraphrasing, multi step\nevaluation, and safety expert models often suffer from high computational\ncosts, limited generalization, or rigid workflows that fail to detect subtle\nmalicious intent embedded in complex contexts. Inspired by cognitive science\nfindings on human decision making, we propose SafeBehavior, a novel\nhierarchical jailbreak defense mechanism that simulates the adaptive multistage\nreasoning process of humans. SafeBehavior decomposes safety evaluation into\nthree stages: intention inference to detect obvious input risks, self\nintrospection to assess generated responses and assign confidence based\njudgments, and self revision to adaptively rewrite uncertain outputs while\npreserving user intent and enforcing safety constraints. We extensively\nevaluate SafeBehavior against five representative jailbreak attack types\nincluding optimization based, contextual manipulation, and prompt based attacks\nand compare it with seven state of the art defense baselines. Experimental\nresults show that SafeBehavior significantly improves robustness and\nadaptability across diverse threat scenarios, offering an efficient and human\ninspired approach to safeguarding LLMs against jailbreak attempts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive performance across\ndiverse natural language processing tasks, but their growing power also\namplifies potential risks such as jailbreak attacks that circumvent built-in\nsafety mechanisms. Existing defenses including input paraphrasing, multi step\nevaluation, and safety expert models often suffer from high computational\ncosts, limited generalization, or rigid workflows that fail to detect subtle\nmalicious intent embedded in complex contexts. Inspired by cognitive science\nfindings on human decision making, we propose SafeBehavior, a novel\nhierarchical jailbreak defense mechanism that simulates the adaptive multistage\nreasoning process of humans. SafeBehavior decomposes safety evaluation into\nthree stages: intention inference to detect obvious input risks, self\nintrospection to assess generated responses and assign confidence based\njudgments, and self revision to adaptively rewrite uncertain outputs while\npreserving user intent and enforcing safety constraints. We extensively\nevaluate SafeBehavior against five representative jailbreak attack types\nincluding optimization based, contextual manipulation, and prompt based attacks\nand compare it with seven state of the art defense baselines. Experimental\nresults show that SafeBehavior significantly improves robustness and\nadaptability across diverse threat scenarios, offering an efficient and human\ninspired approach to safeguarding LLMs against jailbreak attempts."
                },
                "authors": [
                    {
                        "name": "Qinjian Zhao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Zhiqiang Gao"
                    },
                    {
                        "name": "Zhihao Dou"
                    },
                    {
                        "name": "Belal Abuhaija"
                    },
                    {
                        "name": "Kaizhu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaizhu Huang"
                },
                "author": "Kaizhu Huang",
                "arxiv_comment": "27 pages, 5 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01413v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01413v8",
                "updated": "2025-09-30T14:50:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    50,
                    34,
                    1,
                    273,
                    0
                ],
                "published": "2025-06-02T08:11:44Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    8,
                    11,
                    44,
                    0,
                    153,
                    0
                ],
                "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models"
                },
                "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose RAIF, a systematic method to boost LLMs in dealing with\ncomplex instructions via incentivizing reasoning for test-time compute scaling.\nFirst, we stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on\nOOD constraints also confirms the generalizability of our RAIF. Codes and data\nare available at https://github.com/yuleiqin/RAIF.\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose RAIF, a systematic method to boost LLMs in dealing with\ncomplex instructions via incentivizing reasoning for test-time compute scaling.\nFirst, we stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on\nOOD constraints also confirms the generalizability of our RAIF. Codes and data\nare available at https://github.com/yuleiqin/RAIF.\n  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction\nfollowing, complex instructions"
                },
                "authors": [
                    {
                        "name": "Yulei Qin"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Zongyi Li"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Yuchen Shi"
                    },
                    {
                        "name": "Zhekai Lin"
                    },
                    {
                        "name": "Xiao Cui"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "Accepted to NeurIPS 2025; 15 pages of main body, 5 tables, 5 figures,\n  42 pages of appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01413v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01413v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26340v1",
                "updated": "2025-09-30T14:46:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    46,
                    6,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:46:06Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    46,
                    6,
                    1,
                    273,
                    0
                ],
                "title": "Memory-Driven Self-Improvement for Decision Making with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Driven Self-Improvement for Decision Making with Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have emerged as effective action policies for\nsequential decision-making (SDM) tasks due to their extensive prior knowledge.\nHowever, this broad yet general knowledge is often insufficient for specific\ndecision-making tasks with limited task-related data, making it challenging to\nefficiently adapt LLMs to specific SDM tasks. To address this challenge, we\npropose a memory-driven self-improvement framework that combines LLM general\nprior knowledge with a compact memory of domain-specific experiences. Memory\nretains past interactions and associated Q-values, thereby capturing\ndecision-relevant knowledge that facilitates accurate value estimation and\ninforms the LLM prior refinement. The refined LLM prior, in turn, generates\nhigher-reward trajectories that further enrich memory, forming a natural\nself-improvement framework where memory and LLM prior mutually reinforce each\nother. Experiments show that our memory-driven approach significantly\noutperforms both traditional RL and LLM-based baselines, e.g., improving\nperformance by over 40\\% on in-distribution tasks and over 75\\% when\ngeneralized to unseen tasks in ALFWorld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as effective action policies for\nsequential decision-making (SDM) tasks due to their extensive prior knowledge.\nHowever, this broad yet general knowledge is often insufficient for specific\ndecision-making tasks with limited task-related data, making it challenging to\nefficiently adapt LLMs to specific SDM tasks. To address this challenge, we\npropose a memory-driven self-improvement framework that combines LLM general\nprior knowledge with a compact memory of domain-specific experiences. Memory\nretains past interactions and associated Q-values, thereby capturing\ndecision-relevant knowledge that facilitates accurate value estimation and\ninforms the LLM prior refinement. The refined LLM prior, in turn, generates\nhigher-reward trajectories that further enrich memory, forming a natural\nself-improvement framework where memory and LLM prior mutually reinforce each\nother. Experiments show that our memory-driven approach significantly\noutperforms both traditional RL and LLM-based baselines, e.g., improving\nperformance by over 40\\% on in-distribution tasks and over 75\\% when\ngeneralized to unseen tasks in ALFWorld."
                },
                "authors": [
                    {
                        "name": "Xue Yan"
                    },
                    {
                        "name": "Zijing Ou"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Yingzhen Li"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26339v1",
                "updated": "2025-09-30T14:45:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    45,
                    30,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:45:30Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    45,
                    30,
                    1,
                    273,
                    0
                ],
                "title": "Kinodynamic Motion Planning for Mobile Robot Navigation across\n  Inconsistent World Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kinodynamic Motion Planning for Mobile Robot Navigation across\n  Inconsistent World Models"
                },
                "summary": "Mobile ground robots lacking prior knowledge of an environment must rely on\nsensor data to develop a model of their surroundings. In these scenarios,\nconsistent identification of obstacles and terrain features can be difficult\ndue to noise and algorithmic shortcomings, which can make it difficult for\nmotion planning systems to generate safe motions. One particular difficulty to\novercome is when regions of the cost map switch between being marked as\nobstacles and free space through successive planning cycles. One potential\nsolution to this, which we refer to as Valid in Every Hypothesis (VEH), is for\nthe planning system to plan motions that are guaranteed to be safe through a\nhistory of world models. Another approach is to track a history of world\nmodels, and adjust node costs according to the potential penalty of needing to\nreroute around previously hazardous areas. This work discusses three major\niterations on this idea. The first iteration, called PEH, invokes a sub-search\nfor every node expansion that crosses through a divergence point in the world\nmodels. The second and third iterations, called GEH and GEGRH respectively,\ndefer the sub-search until after an edge expands into the goal region. GEGRH\nuses an additional step to revise the graph based on divergent nodes in each\nworld. Initial results showed that, although PEH and GEH find more optimistic\nsolutions than VEH, they are unable to generate solutions in less than\none-second, which exceeds our requirements for field deployment. Analysis of\nresults from a field experiment in an unstructured, off-road environment on a\nClearpath Robotics Warthog UGV indicate that GEGRH finds lower cost\ntrajectories and has faster average planning times than VEH. Compared to\nsingle-hypothesis (SH) search, where only the latest world model is considered,\nGEGRH generates more conservative plans with a small increase in average\nplanning time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile ground robots lacking prior knowledge of an environment must rely on\nsensor data to develop a model of their surroundings. In these scenarios,\nconsistent identification of obstacles and terrain features can be difficult\ndue to noise and algorithmic shortcomings, which can make it difficult for\nmotion planning systems to generate safe motions. One particular difficulty to\novercome is when regions of the cost map switch between being marked as\nobstacles and free space through successive planning cycles. One potential\nsolution to this, which we refer to as Valid in Every Hypothesis (VEH), is for\nthe planning system to plan motions that are guaranteed to be safe through a\nhistory of world models. Another approach is to track a history of world\nmodels, and adjust node costs according to the potential penalty of needing to\nreroute around previously hazardous areas. This work discusses three major\niterations on this idea. The first iteration, called PEH, invokes a sub-search\nfor every node expansion that crosses through a divergence point in the world\nmodels. The second and third iterations, called GEH and GEGRH respectively,\ndefer the sub-search until after an edge expands into the goal region. GEGRH\nuses an additional step to revise the graph based on divergent nodes in each\nworld. Initial results showed that, although PEH and GEH find more optimistic\nsolutions than VEH, they are unable to generate solutions in less than\none-second, which exceeds our requirements for field deployment. Analysis of\nresults from a field experiment in an unstructured, off-road environment on a\nClearpath Robotics Warthog UGV indicate that GEGRH finds lower cost\ntrajectories and has faster average planning times than VEH. Compared to\nsingle-hypothesis (SH) search, where only the latest world model is considered,\nGEGRH generates more conservative plans with a small increase in average\nplanning time."
                },
                "authors": [
                    {
                        "name": "Eric R. Damm"
                    },
                    {
                        "name": "Thomas M. Howard"
                    }
                ],
                "author_detail": {
                    "name": "Thomas M. Howard"
                },
                "author": "Thomas M. Howard",
                "arxiv_comment": "Presented at the Robotics: Science and Systems (RSS) 2025 Workshop on\n  Resilient Off-road Autonomous Robotics (ROAR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20295v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20295v3",
                "updated": "2025-09-30T14:44:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    44,
                    21,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-26T17:59:53Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    17,
                    59,
                    53,
                    0,
                    146,
                    0
                ],
                "title": "SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?"
                },
                "summary": "The common approach to communicate a large language model's (LLM) uncertainty\nis to add a percentage number or a hedging word to its response. But is this\nall we can do? Instead of generating a single answer and then hedging it, an\nLLM that is fully transparent to the user needs to be able to reflect on its\ninternal belief distribution and output a summary of all options it deems\npossible, and how likely they are. To test whether LLMs possess this\ncapability, we develop the SelfReflect metric, an information-theoretic\ndistance between a given summary and a distribution over answers. In\ninterventional and human studies, we find that SelfReflect indicates even\nslight deviations, yielding a fine measure of faithfulness between a summary\nstring and an LLM's actual internal distribution over answers. With\nSelfReflect, we make a resounding negative observation: modern LLMs are, across\nthe board, incapable of revealing what they are uncertain about, neither\nthrough reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we\ndo find that LLMs are able to generate faithful summaries of their\nuncertainties if we help them by sampling multiple outputs and feeding them\nback into the context. This simple approach shines a light at the universal way\nof communicating LLM uncertainties whose future development the SelfReflect\nscore enables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The common approach to communicate a large language model's (LLM) uncertainty\nis to add a percentage number or a hedging word to its response. But is this\nall we can do? Instead of generating a single answer and then hedging it, an\nLLM that is fully transparent to the user needs to be able to reflect on its\ninternal belief distribution and output a summary of all options it deems\npossible, and how likely they are. To test whether LLMs possess this\ncapability, we develop the SelfReflect metric, an information-theoretic\ndistance between a given summary and a distribution over answers. In\ninterventional and human studies, we find that SelfReflect indicates even\nslight deviations, yielding a fine measure of faithfulness between a summary\nstring and an LLM's actual internal distribution over answers. With\nSelfReflect, we make a resounding negative observation: modern LLMs are, across\nthe board, incapable of revealing what they are uncertain about, neither\nthrough reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we\ndo find that LLMs are able to generate faithful summaries of their\nuncertainties if we help them by sampling multiple outputs and feeding them\nback into the context. This simple approach shines a light at the universal way\nof communicating LLM uncertainties whose future development the SelfReflect\nscore enables."
                },
                "authors": [
                    {
                        "name": "Michael Kirchhof"
                    },
                    {
                        "name": "Luca Füger"
                    },
                    {
                        "name": "Adam Goliński"
                    },
                    {
                        "name": "Eeshan Gunesh Dhekane"
                    },
                    {
                        "name": "Arno Blaas"
                    },
                    {
                        "name": "Seong Joon Oh"
                    },
                    {
                        "name": "Sinead Williamson"
                    }
                ],
                "author_detail": {
                    "name": "Sinead Williamson"
                },
                "author": "Sinead Williamson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20295v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20295v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26331v1",
                "updated": "2025-09-30T14:43:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    43,
                    5,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:43:05Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    43,
                    5,
                    1,
                    273,
                    0
                ],
                "title": "AI Playing Business Games: Benchmarking Large Language Models on\n  Managerial Decision-Making in Dynamic Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Playing Business Games: Benchmarking Large Language Models on\n  Managerial Decision-Making in Dynamic Simulations"
                },
                "summary": "The rapid advancement of LLMs sparked significant interest in their potential\nto augment or automate managerial functions. One of the most recent trends in\nAI benchmarking is performance of Large Language Models (LLMs) over longer time\nhorizons. While LLMs excel at tasks involving natural language and pattern\nrecognition, their capabilities in multi-step, strategic business\ndecision-making remain largely unexplored. Few studies demonstrated how results\ncan be different from benchmarks in short-term tasks, as Vending-Bench\nrevealed. Meanwhile, there is a shortage of alternative benchmarks for\nlong-term coherence. This research analyses a novel benchmark using a business\ngame for the decision making in business. The research contributes to the\nrecent literature on AI by proposing a reproducible, open-access management\nsimulator to the research community for LLM benchmarking. This novel framework\nis used for evaluating the performance of five leading LLMs available in free\nonline interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes\ndecisions for a simulated retail company. A dynamic, month-by-month management\nsimulation provides transparently in spreadsheet model as experimental\nenvironment. In each of twelve months, the LLMs are provided with a structured\nprompt containing a full business report from the previous period and are\ntasked with making key strategic decisions: pricing, order size, marketing\nbudget, hiring, dismissal, loans, training expense, R&D expense, sales\nforecast, income forecast The methodology is designed to compare the LLMs on\nquantitative metrics: profit, revenue, and market share, and other KPIs. LLM\ndecisions are analyzed in their strategic coherence, adaptability to market\nchanges, and the rationale provided for their decisions. This approach allows\nto move beyond simple performance metrics for assessment of the long-term\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of LLMs sparked significant interest in their potential\nto augment or automate managerial functions. One of the most recent trends in\nAI benchmarking is performance of Large Language Models (LLMs) over longer time\nhorizons. While LLMs excel at tasks involving natural language and pattern\nrecognition, their capabilities in multi-step, strategic business\ndecision-making remain largely unexplored. Few studies demonstrated how results\ncan be different from benchmarks in short-term tasks, as Vending-Bench\nrevealed. Meanwhile, there is a shortage of alternative benchmarks for\nlong-term coherence. This research analyses a novel benchmark using a business\ngame for the decision making in business. The research contributes to the\nrecent literature on AI by proposing a reproducible, open-access management\nsimulator to the research community for LLM benchmarking. This novel framework\nis used for evaluating the performance of five leading LLMs available in free\nonline interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes\ndecisions for a simulated retail company. A dynamic, month-by-month management\nsimulation provides transparently in spreadsheet model as experimental\nenvironment. In each of twelve months, the LLMs are provided with a structured\nprompt containing a full business report from the previous period and are\ntasked with making key strategic decisions: pricing, order size, marketing\nbudget, hiring, dismissal, loans, training expense, R&D expense, sales\nforecast, income forecast The methodology is designed to compare the LLMs on\nquantitative metrics: profit, revenue, and market share, and other KPIs. LLM\ndecisions are analyzed in their strategic coherence, adaptability to market\nchanges, and the rationale provided for their decisions. This approach allows\nto move beyond simple performance metrics for assessment of the long-term\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Berdymyrat Ovezmyradov"
                    }
                ],
                "author_detail": {
                    "name": "Berdymyrat Ovezmyradov"
                },
                "author": "Berdymyrat Ovezmyradov",
                "arxiv_comment": "34 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26329v1",
                "updated": "2025-09-30T14:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    45,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    45,
                    1,
                    273,
                    0
                ],
                "title": "TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics"
                },
                "summary": "Large audio-language models are advancing rapidly, yet most evaluations\nemphasize speech or globally sourced sounds, overlooking culturally distinctive\ncues. This gap raises a critical question: can current models generalize to\nlocalized, non-semantic audio that communities instantly recognize but\noutsiders do not? To address this, we present TAU (Taiwan Audio Understanding),\na benchmark of everyday Taiwanese \"soundmarks.\" TAU is built through a pipeline\ncombining curated sources, human editing, and LLM-assisted question generation,\nproducing 702 clips and 1,794 multiple-choice items that cannot be solved by\ntranscripts alone. Experiments show that state-of-the-art LALMs, including\nGemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates\nthe need for localized benchmarks to reveal cultural blind spots, guide more\nequitable multimodal evaluation, and ensure models serve communities beyond the\nglobal mainstream.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large audio-language models are advancing rapidly, yet most evaluations\nemphasize speech or globally sourced sounds, overlooking culturally distinctive\ncues. This gap raises a critical question: can current models generalize to\nlocalized, non-semantic audio that communities instantly recognize but\noutsiders do not? To address this, we present TAU (Taiwan Audio Understanding),\na benchmark of everyday Taiwanese \"soundmarks.\" TAU is built through a pipeline\ncombining curated sources, human editing, and LLM-assisted question generation,\nproducing 702 clips and 1,794 multiple-choice items that cannot be solved by\ntranscripts alone. Experiments show that state-of-the-art LALMs, including\nGemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates\nthe need for localized benchmarks to reveal cultural blind spots, guide more\nequitable multimodal evaluation, and ensure models serve communities beyond the\nglobal mainstream."
                },
                "authors": [
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Yu-Hua Chen"
                    },
                    {
                        "name": "Jia-Kai Dong"
                    },
                    {
                        "name": "Yueh-Hsuan Huang"
                    },
                    {
                        "name": "Szu-Chi Chen"
                    },
                    {
                        "name": "Yu-Chen Chen"
                    },
                    {
                        "name": "Chih-Yao Chen"
                    },
                    {
                        "name": "Yu-Jung Lin"
                    },
                    {
                        "name": "Yu-Ling Chen"
                    },
                    {
                        "name": "Zih-Yu Chen"
                    },
                    {
                        "name": "I-Ning Tsai"
                    },
                    {
                        "name": "Hsiu-Hsuan Wang"
                    },
                    {
                        "name": "Ho-Lam Chung"
                    },
                    {
                        "name": "Ke-Han Lu"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "5 pages; submitted to ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26328v1",
                "updated": "2025-09-30T14:40:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:40:18Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM v2: Efficient Block-Diffusion LLM"
                },
                "summary": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18128v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18128v3",
                "updated": "2025-09-30T14:38:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    38,
                    41,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-23T17:38:47Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    38,
                    47,
                    4,
                    143,
                    0
                ],
                "title": "Frankentext: Stitching random text fragments into long-form narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frankentext: Stitching random text fragments into long-form narratives"
                },
                "summary": "We introduce Frankentexts, a long-form narrative generation paradigm that\ntreats an LLM as a composer of existing texts rather than as an author. Given a\nwriting prompt and thousands of randomly sampled human-written snippets, the\nmodel is asked to produce a narrative under the extreme constraint that most\ntokens (e.g., 90%) must be copied verbatim from the provided paragraphs. This\ntask is effectively intractable for humans: selecting and ordering snippets\nyields a combinatorial search space that an LLM implicitly explores, before\nminimally editing and stitching together selected fragments into a coherent\nlong-form story. Despite the extreme challenge of the task, we observe through\nextensive automatic and human evaluation that Frankentexts significantly\nimprove over vanilla LLM generations in terms of writing quality, diversity,\nand originality while remaining coherent and relevant to the prompt.\nFurthermore, Frankentexts pose a fundamental challenge to detectors of\nAI-generated text: 72% of Frankentexts produced by our best Gemini 2.5 Pro\nconfiguration are misclassified as human-written by Pangram, a state-of-the-art\ndetector. Human annotators praise Frankentexts for their inventive premises,\nvivid descriptions, and dry humor; on the other hand, they identify issues with\nabrupt tonal shifts and uneven grammar across segments, particularly in longer\npieces. The emergence of high-quality Frankentexts raises serious questions\nabout authorship and copyright: when humans provide the raw materials and LLMs\norchestrate them into new narratives, who truly owns the result?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Frankentexts, a long-form narrative generation paradigm that\ntreats an LLM as a composer of existing texts rather than as an author. Given a\nwriting prompt and thousands of randomly sampled human-written snippets, the\nmodel is asked to produce a narrative under the extreme constraint that most\ntokens (e.g., 90%) must be copied verbatim from the provided paragraphs. This\ntask is effectively intractable for humans: selecting and ordering snippets\nyields a combinatorial search space that an LLM implicitly explores, before\nminimally editing and stitching together selected fragments into a coherent\nlong-form story. Despite the extreme challenge of the task, we observe through\nextensive automatic and human evaluation that Frankentexts significantly\nimprove over vanilla LLM generations in terms of writing quality, diversity,\nand originality while remaining coherent and relevant to the prompt.\nFurthermore, Frankentexts pose a fundamental challenge to detectors of\nAI-generated text: 72% of Frankentexts produced by our best Gemini 2.5 Pro\nconfiguration are misclassified as human-written by Pangram, a state-of-the-art\ndetector. Human annotators praise Frankentexts for their inventive premises,\nvivid descriptions, and dry humor; on the other hand, they identify issues with\nabrupt tonal shifts and uneven grammar across segments, particularly in longer\npieces. The emergence of high-quality Frankentexts raises serious questions\nabout authorship and copyright: when humans provide the raw materials and LLMs\norchestrate them into new narratives, who truly owns the result?"
                },
                "authors": [
                    {
                        "name": "Chau Minh Pham"
                    },
                    {
                        "name": "Jenna Russell"
                    },
                    {
                        "name": "Dzung Pham"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18128v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18128v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16211v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16211v3",
                "updated": "2025-09-30T14:36:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    36,
                    30,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-22T04:27:46Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    27,
                    46,
                    3,
                    142,
                    0
                ],
                "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models"
                },
                "summary": "Audio Large Language Models (ALLMs) have gained widespread adoption, yet\ntheir trustworthiness remains underexplored. Existing evaluation frameworks,\ndesigned primarily for text, fail to address unique vulnerabilities introduced\nby audio's acoustic properties. We identify significant trustworthiness risks\nin ALLMs arising from non-semantic acoustic cues, including timbre, accent, and\nbackground noise, which can manipulate model behavior. We propose AudioTrust, a\ncomprehensive framework for systematic evaluation of ALLM trustworthiness\nacross audio-specific risks. AudioTrust encompasses six key dimensions:\nfairness, hallucination, safety, privacy, robustness, and authentication. The\nframework implements 26 distinct sub-tasks using a curated dataset of over\n4,420 audio samples from real-world scenarios, including daily conversations,\nemergency calls, and voice assistant interactions. We conduct comprehensive\nevaluations across 18 experimental configurations using human-validated\nautomated pipelines. Our evaluation of 14 state-of-the-art open-source and\nclosed-source ALLMs reveals significant limitations when confronted with\ndiverse high-risk audio scenarios, providing insights for secure deployment of\naudio models. Code and data are available at\nhttps://github.com/JusperLee/AudioTrust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio Large Language Models (ALLMs) have gained widespread adoption, yet\ntheir trustworthiness remains underexplored. Existing evaluation frameworks,\ndesigned primarily for text, fail to address unique vulnerabilities introduced\nby audio's acoustic properties. We identify significant trustworthiness risks\nin ALLMs arising from non-semantic acoustic cues, including timbre, accent, and\nbackground noise, which can manipulate model behavior. We propose AudioTrust, a\ncomprehensive framework for systematic evaluation of ALLM trustworthiness\nacross audio-specific risks. AudioTrust encompasses six key dimensions:\nfairness, hallucination, safety, privacy, robustness, and authentication. The\nframework implements 26 distinct sub-tasks using a curated dataset of over\n4,420 audio samples from real-world scenarios, including daily conversations,\nemergency calls, and voice assistant interactions. We conduct comprehensive\nevaluations across 18 experimental configurations using human-validated\nautomated pipelines. Our evaluation of 14 state-of-the-art open-source and\nclosed-source ALLMs reveals significant limitations when confronted with\ndiverse high-risk audio scenarios, providing insights for secure deployment of\naudio models. Code and data are available at\nhttps://github.com/JusperLee/AudioTrust."
                },
                "authors": [
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Can Shen"
                    },
                    {
                        "name": "Yile Liu"
                    },
                    {
                        "name": "Jirui Han"
                    },
                    {
                        "name": "Kelong Zheng"
                    },
                    {
                        "name": "Xuechao Zou"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Shun Zhang"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Hanjun Luo"
                    },
                    {
                        "name": "Yingbin Jin"
                    },
                    {
                        "name": "Xinxin Xing"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haibo Hu"
                    },
                    {
                        "name": "Zhizheng Wu"
                    },
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Eng-Siong Chng"
                    },
                    {
                        "name": "Wenyuan Xu"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Xinfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinfeng Li"
                },
                "author": "Xinfeng Li",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16211v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16211v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26324v1",
                "updated": "2025-09-30T14:33:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    33,
                    35,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:33:35Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    33,
                    35,
                    1,
                    273,
                    0
                ],
                "title": "LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration\n  and Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration\n  and Search"
                },
                "summary": "Autonomous exploration and object search in unknown indoor environments\nremain challenging for multi-robot systems (MRS). Traditional approaches often\nrely on greedy frontier assignment strategies with limited inter-robot\ncoordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot\nCoordinated Exploration and Search), a novel framework that leverages Large\nLanguage Models (LLMs) for intelligent coordination of both homogeneous and\nheterogeneous robot teams tasked with efficient exploration and target object\nsearch. Our approach combines real-time LiDAR scan processing for frontier\ncluster extraction and doorway detection with multimodal LLM reasoning (e.g.,\nGPT-4o) to generate coordinated waypoint assignments based on shared\nenvironment maps and robot states. LLM-MCoX demonstrates superior performance\ncompared to existing methods, including greedy and Voronoi-based planners,\nachieving 22.7% faster exploration times and 50% improved search efficiency in\nlarge environments with 6 robots. Notably, LLM-MCoX enables natural\nlanguage-based object search capabilities, allowing human operators to provide\nhigh-level semantic guidance that traditional algorithms cannot interpret.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous exploration and object search in unknown indoor environments\nremain challenging for multi-robot systems (MRS). Traditional approaches often\nrely on greedy frontier assignment strategies with limited inter-robot\ncoordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot\nCoordinated Exploration and Search), a novel framework that leverages Large\nLanguage Models (LLMs) for intelligent coordination of both homogeneous and\nheterogeneous robot teams tasked with efficient exploration and target object\nsearch. Our approach combines real-time LiDAR scan processing for frontier\ncluster extraction and doorway detection with multimodal LLM reasoning (e.g.,\nGPT-4o) to generate coordinated waypoint assignments based on shared\nenvironment maps and robot states. LLM-MCoX demonstrates superior performance\ncompared to existing methods, including greedy and Voronoi-based planners,\nachieving 22.7% faster exploration times and 50% improved search efficiency in\nlarge environments with 6 robots. Notably, LLM-MCoX enables natural\nlanguage-based object search capabilities, allowing human operators to provide\nhigh-level semantic guidance that traditional algorithms cannot interpret."
                },
                "authors": [
                    {
                        "name": "Ruiyang Wang"
                    },
                    {
                        "name": "Haolun Tsu"
                    },
                    {
                        "name": "David Hunt"
                    },
                    {
                        "name": "Shaocheng Luo"
                    },
                    {
                        "name": "Jiwoo Kim"
                    },
                    {
                        "name": "Miroslav Pajic"
                    }
                ],
                "author_detail": {
                    "name": "Miroslav Pajic"
                },
                "author": "Miroslav Pajic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26314v1",
                "updated": "2025-09-30T14:26:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    26,
                    36,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:26:36Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    26,
                    36,
                    1,
                    273,
                    0
                ],
                "title": "Latent Thinking Optimization: Your Latent Reasoning Language Model\n  Secretly Encodes Reward Signals in its Latent Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Thinking Optimization: Your Latent Reasoning Language Model\n  Secretly Encodes Reward Signals in its Latent Thoughts"
                },
                "summary": "Large Language Models (LLMs) excel at problem solving by generating chain of\nthoughts in natural language, but such verbal thinking is computationally\ncostly and prone to overthinking. Recent work instead proposes a latent\nthinking architecture Huggin-3.5B, which represents intermediate reasoning\nsteps as sequence of latent representations. However, latent thoughts lack\ninterpretability and are difficult to supervise, raising concerns about the\ncorrectness and reliability of its latent thinking processes. In this paper, we\nprovide a systematic study of how Huggin-3.5B thinks in the latent space and\nhow external supervision signals can improve its latent thinking processes. We\nshow that latent thoughts leading to correct versus incorrect answers exhibit\nhighly distinguishable patterns, and that a latent classifier can reliably\npredict answer correctness directly from latent thoughts. Leveraging these\ninsights, we propose Latent Thinking Optimization (LTO), a probabilistic\nalgorithm that employs the latent classifier as a Latent Reward Model (LRM) to\noptimize the latent thinking processes. Extensive experiments across diverse\nreasoning tasks demonstrate that LRM is highly effective in detecting incorrect\nlatent thinking patterns, and LTO can significantly improve the latent thinking\nprocesses. Furthermore, we show that LRM can generalize across diverse domains,\nand LTO can be seamlessly applied to general LLMs to improve their thinking\nprocesses. In contrast to verbal thinking, our method demonstrates that reward\nmodeling and scaling test-time thinking with supervision can be performed\ndirectly in the latent space, highlighting its potential as a general,\nefficient, and domain-agnostic approach to improving the thinking processes of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at problem solving by generating chain of\nthoughts in natural language, but such verbal thinking is computationally\ncostly and prone to overthinking. Recent work instead proposes a latent\nthinking architecture Huggin-3.5B, which represents intermediate reasoning\nsteps as sequence of latent representations. However, latent thoughts lack\ninterpretability and are difficult to supervise, raising concerns about the\ncorrectness and reliability of its latent thinking processes. In this paper, we\nprovide a systematic study of how Huggin-3.5B thinks in the latent space and\nhow external supervision signals can improve its latent thinking processes. We\nshow that latent thoughts leading to correct versus incorrect answers exhibit\nhighly distinguishable patterns, and that a latent classifier can reliably\npredict answer correctness directly from latent thoughts. Leveraging these\ninsights, we propose Latent Thinking Optimization (LTO), a probabilistic\nalgorithm that employs the latent classifier as a Latent Reward Model (LRM) to\noptimize the latent thinking processes. Extensive experiments across diverse\nreasoning tasks demonstrate that LRM is highly effective in detecting incorrect\nlatent thinking patterns, and LTO can significantly improve the latent thinking\nprocesses. Furthermore, we show that LRM can generalize across diverse domains,\nand LTO can be seamlessly applied to general LLMs to improve their thinking\nprocesses. In contrast to verbal thinking, our method demonstrates that reward\nmodeling and scaling test-time thinking with supervision can be performed\ndirectly in the latent space, highlighting its potential as a general,\nefficient, and domain-agnostic approach to improving the thinking processes of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Hanwen Du"
                    },
                    {
                        "name": "Yuxin Dong"
                    },
                    {
                        "name": "Xia Ning"
                    }
                ],
                "author_detail": {
                    "name": "Xia Ning"
                },
                "author": "Xia Ning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26313v1",
                "updated": "2025-09-30T14:25:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    25,
                    56,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:25:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    25,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy\n  Gradient",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy\n  Gradient"
                },
                "summary": "Supervised fine-tuning (SFT) is the predominant method for adapting large\nlanguage models (LLMs), yet it often struggles with generalization compared to\nreinforcement learning (RL). In this work, we posit that this performance\ndisparity stems not just from the loss function, but from a more fundamental\ndifference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes\non-policy data sampled from the current policy. Building on this hypothesis, we\nintroduce one-token rollout (OTR), a novel fine-tuning algorithm that guides\nSFT with the policy gradient method. OTR reframes the autoregressive learning\nprocess by treating each token generation as a single-step reinforcement\nlearning trajectory. At each step, it performs a Monte Carlo ``rollout'' by\nsampling multiple candidate tokens from the current policy's distribution. The\nground-truth token from the supervised data is then used to provide a reward\nsignal to these samples. Guided by policy gradient, our algorithm repurposes\nstatic, off-policy supervised data into a dynamic, on-policy signal at the\ntoken level, capturing the generalization benefits of on-policy learning while\nbypassing the costly overhead of full sentence generation. Through extensive\nexperiments on a diverse suite of challenging benchmarks spanning mathematical\nreasoning, code generation, and general domain reasoning, we demonstrate that\nOTR consistently outperforms standard SFT. Our findings establish OTR as a\npowerful and practical alternative for fine-tuning LLMs and provide compelling\nevidence that the on-policy nature of data is a critical driver of\ngeneralization, offering a promising new direction for fine-tuning LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) is the predominant method for adapting large\nlanguage models (LLMs), yet it often struggles with generalization compared to\nreinforcement learning (RL). In this work, we posit that this performance\ndisparity stems not just from the loss function, but from a more fundamental\ndifference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes\non-policy data sampled from the current policy. Building on this hypothesis, we\nintroduce one-token rollout (OTR), a novel fine-tuning algorithm that guides\nSFT with the policy gradient method. OTR reframes the autoregressive learning\nprocess by treating each token generation as a single-step reinforcement\nlearning trajectory. At each step, it performs a Monte Carlo ``rollout'' by\nsampling multiple candidate tokens from the current policy's distribution. The\nground-truth token from the supervised data is then used to provide a reward\nsignal to these samples. Guided by policy gradient, our algorithm repurposes\nstatic, off-policy supervised data into a dynamic, on-policy signal at the\ntoken level, capturing the generalization benefits of on-policy learning while\nbypassing the costly overhead of full sentence generation. Through extensive\nexperiments on a diverse suite of challenging benchmarks spanning mathematical\nreasoning, code generation, and general domain reasoning, we demonstrate that\nOTR consistently outperforms standard SFT. Our findings establish OTR as a\npowerful and practical alternative for fine-tuning LLMs and provide compelling\nevidence that the on-policy nature of data is a critical driver of\ngeneralization, offering a promising new direction for fine-tuning LLMs."
                },
                "authors": [
                    {
                        "name": "Rui Ming"
                    },
                    {
                        "name": "Haoyuan Wu"
                    },
                    {
                        "name": "Shoubo Hu"
                    },
                    {
                        "name": "Zhuolun He"
                    },
                    {
                        "name": "Bei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bei Yu"
                },
                "author": "Bei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00225v2",
                "updated": "2025-09-30T14:23:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    23,
                    41,
                    1,
                    273,
                    0
                ],
                "published": "2025-01-31T23:42:53Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    23,
                    42,
                    53,
                    4,
                    31,
                    0
                ],
                "title": "Should You Use Your Large Language Model to Explore or Exploit?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Should You Use Your Large Language Model to Explore or Exploit?"
                },
                "summary": "We evaluate the ability of the current generation of large language models\n(LLMs) to help a decision-making agent facing an exploration-exploitation\ntradeoff. We use LLMs to explore and exploit in silos in various (contextual)\nbandit tasks. We find that while the current LLMs often struggle to exploit,\nin-context mitigations may be used to substantially improve performance for\nsmall-scale tasks. However even then, LLMs perform worse than a simple linear\nregression. On the other hand, we find that LLMs do help at exploring large\naction spaces with inherent semantics, by suggesting suitable candidates to\nexplore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We evaluate the ability of the current generation of large language models\n(LLMs) to help a decision-making agent facing an exploration-exploitation\ntradeoff. We use LLMs to explore and exploit in silos in various (contextual)\nbandit tasks. We find that while the current LLMs often struggle to exploit,\nin-context mitigations may be used to substantially improve performance for\nsmall-scale tasks. However even then, LLMs perform worse than a simple linear\nregression. On the other hand, we find that LLMs do help at exploring large\naction spaces with inherent semantics, by suggesting suitable candidates to\nexplore."
                },
                "authors": [
                    {
                        "name": "Keegan Harris"
                    },
                    {
                        "name": "Aleksandrs Slivkins"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandrs Slivkins"
                },
                "author": "Aleksandrs Slivkins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26307v1",
                "updated": "2025-09-30T14:21:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    21,
                    40,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:21:40Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    21,
                    40,
                    1,
                    273,
                    0
                ],
                "title": "Attribution-Guided Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribution-Guided Decoding"
                },
                "summary": "The capacity of Large Language Models (LLMs) to follow complex instructions\nand generate factually accurate text is critical for their real-world\napplication. However, standard decoding methods often fail to robustly satisfy\nthese requirements, while existing control techniques frequently degrade\ngeneral output quality. In this work, we introduce Attribution-Guided Decoding\n(AGD), an interpretability-based decoding strategy. Instead of directly\nmanipulating model activations, AGD considers a set of high-probability output\ntoken candidates and selects the one that exhibits the highest attribution to a\nuser-defined Region of Interest (ROI). This ROI can be flexibly defined over\ndifferent parts of the model's input or internal components, allowing AGD to\nsteer generation towards various desirable behaviors. We demonstrate AGD's\nefficacy across three challenging domains. For instruction following, we show\nthat AGD significantly boosts adherence (e.g., improving the overall success\nrate on Llama 3.1 from 66.0% to 79.1%). For knowledge-intensive tasks, we show\nthat guiding generation towards usage of internal knowledge components or\ncontextual sources can reduce hallucinations and improve factual accuracy in\nboth closed-book and open-book settings. Furthermore, we propose an adaptive,\nentropy-based variant of AGD that mitigates quality degradation and reduces\ncomputational overhead by applying guidance only when the model is uncertain.\nOur work presents a versatile, more interpretable, and effective method for\nenhancing the reliability of modern LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capacity of Large Language Models (LLMs) to follow complex instructions\nand generate factually accurate text is critical for their real-world\napplication. However, standard decoding methods often fail to robustly satisfy\nthese requirements, while existing control techniques frequently degrade\ngeneral output quality. In this work, we introduce Attribution-Guided Decoding\n(AGD), an interpretability-based decoding strategy. Instead of directly\nmanipulating model activations, AGD considers a set of high-probability output\ntoken candidates and selects the one that exhibits the highest attribution to a\nuser-defined Region of Interest (ROI). This ROI can be flexibly defined over\ndifferent parts of the model's input or internal components, allowing AGD to\nsteer generation towards various desirable behaviors. We demonstrate AGD's\nefficacy across three challenging domains. For instruction following, we show\nthat AGD significantly boosts adherence (e.g., improving the overall success\nrate on Llama 3.1 from 66.0% to 79.1%). For knowledge-intensive tasks, we show\nthat guiding generation towards usage of internal knowledge components or\ncontextual sources can reduce hallucinations and improve factual accuracy in\nboth closed-book and open-book settings. Furthermore, we propose an adaptive,\nentropy-based variant of AGD that mitigates quality degradation and reduces\ncomputational overhead by applying guidance only when the model is uncertain.\nOur work presents a versatile, more interpretable, and effective method for\nenhancing the reliability of modern LLMs."
                },
                "authors": [
                    {
                        "name": "Piotr Komorowski"
                    },
                    {
                        "name": "Elena Golimblevskaia"
                    },
                    {
                        "name": "Reduan Achtibat"
                    },
                    {
                        "name": "Thomas Wiegand"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Wojciech Samek"
                    }
                ],
                "author_detail": {
                    "name": "Wojciech Samek"
                },
                "author": "Wojciech Samek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26306v2",
                "updated": "2025-10-01T01:14:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    14,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T14:21:31Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    21,
                    31,
                    1,
                    273,
                    0
                ],
                "title": "Interactive Learning for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Learning for LLM Reasoning"
                },
                "summary": "Existing multi-agent learning approaches have developed interactive training\nenvironments to explicitly promote collaboration among multiple Large Language\nModels (LLMs), thereby constructing stronger multi-agent systems (MAS).\nHowever, during inference, they require re-executing the MAS to obtain final\nsolutions, which diverges from human cognition that individuals can enhance\ntheir reasoning capabilities through interactions with others and resolve\nquestions independently in the future. To investigate whether multi-agent\ninteraction can enhance LLMs' independent problem-solving ability, we introduce\nILR, a novel co-learning framework for MAS that integrates two key components:\nDynamic Interaction and Perception Calibration. Specifically, Dynamic\nInteraction first adaptively selects either cooperative or competitive\nstrategies depending on question difficulty and model ability. LLMs then\nexchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea\nFusion), an innovative interaction paradigm designed to mimic human discussion,\nbefore deriving their respective final answers. In Perception Calibration, ILR\nemploys Group Relative Policy Optimization (GRPO) to train LLMs while\nintegrating one LLM's reward distribution characteristics into another's reward\nfunction, thereby enhancing the cohesion of multi-agent interactions. We\nvalidate ILR on three LLMs across two model families of varying scales,\nevaluating performance on five mathematical benchmarks and one coding\nbenchmark. Experimental results show that ILR consistently outperforms\nsingle-agent learning, yielding an improvement of up to 5% over the strongest\nbaseline. We further discover that Idea3 can enhance the robustness of stronger\nLLMs during multi-agent inference, and dynamic interaction types can boost\nmulti-agent learning compared to pure cooperative or competitive strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing multi-agent learning approaches have developed interactive training\nenvironments to explicitly promote collaboration among multiple Large Language\nModels (LLMs), thereby constructing stronger multi-agent systems (MAS).\nHowever, during inference, they require re-executing the MAS to obtain final\nsolutions, which diverges from human cognition that individuals can enhance\ntheir reasoning capabilities through interactions with others and resolve\nquestions independently in the future. To investigate whether multi-agent\ninteraction can enhance LLMs' independent problem-solving ability, we introduce\nILR, a novel co-learning framework for MAS that integrates two key components:\nDynamic Interaction and Perception Calibration. Specifically, Dynamic\nInteraction first adaptively selects either cooperative or competitive\nstrategies depending on question difficulty and model ability. LLMs then\nexchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea\nFusion), an innovative interaction paradigm designed to mimic human discussion,\nbefore deriving their respective final answers. In Perception Calibration, ILR\nemploys Group Relative Policy Optimization (GRPO) to train LLMs while\nintegrating one LLM's reward distribution characteristics into another's reward\nfunction, thereby enhancing the cohesion of multi-agent interactions. We\nvalidate ILR on three LLMs across two model families of varying scales,\nevaluating performance on five mathematical benchmarks and one coding\nbenchmark. Experimental results show that ILR consistently outperforms\nsingle-agent learning, yielding an improvement of up to 5% over the strongest\nbaseline. We further discover that Idea3 can enhance the robustness of stronger\nLLMs during multi-agent inference, and dynamic interaction types can boost\nmulti-agent learning compared to pure cooperative or competitive strategies."
                },
                "authors": [
                    {
                        "name": "Hehai Lin"
                    },
                    {
                        "name": "Shilei Cao"
                    },
                    {
                        "name": "Minzhi Li"
                    },
                    {
                        "name": "Sudong Wang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Juepeng Zheng"
                    },
                    {
                        "name": "Chengwei Qin"
                    }
                ],
                "author_detail": {
                    "name": "Chengwei Qin"
                },
                "author": "Chengwei Qin",
                "arxiv_comment": "The code will be released later",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14043v3",
                "updated": "2025-09-30T14:21:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    21,
                    13,
                    1,
                    273,
                    0
                ],
                "published": "2025-03-18T09:04:37Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    9,
                    4,
                    37,
                    1,
                    77,
                    0
                ],
                "title": "Beyond Next Token Probabilities: Learnable, Fast Detection of\n  Hallucinations and Data Contamination on LLM Output Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Next Token Probabilities: Learnable, Fast Detection of\n  Hallucinations and Data Contamination on LLM Output Distributions"
                },
                "summary": "The automated detection of hallucinations and training data contamination is\npivotal to the safe deployment of Large Language Models (LLMs). These tasks are\nparticularly challenging in settings where no access to model internals is\navailable. Current approaches in this setup typically leverage only the\nprobabilities of actual tokens in the text, relying on simple task-specific\nheuristics. Crucially, they overlook the information contained in the full\nsequence of next-token probability distributions. We propose to go beyond\nhand-crafted decision rules by learning directly from the complete observable\noutput of LLMs -- consisting not only of next-token probabilities, but also the\nfull sequence of next-token distributions. We refer to this as the LLM Output\nSignature (LOS), and treat it as a reference data type for detecting\nhallucinations and data contamination. To that end, we introduce LOS-Net, a\nlightweight attention-based architecture trained on an efficient encoding of\nthe LOS, which can provably approximate a broad class of existing techniques\nfor both tasks. Empirically, LOS-Net achieves superior performance across\ndiverse benchmarks and LLMs, while maintaining extremely low detection latency.\nFurthermore, it demonstrates promising transfer capabilities across datasets\nand LLMs. Full code is available at\nhttps://github.com/BarSGuy/Beyond-next-token-probabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automated detection of hallucinations and training data contamination is\npivotal to the safe deployment of Large Language Models (LLMs). These tasks are\nparticularly challenging in settings where no access to model internals is\navailable. Current approaches in this setup typically leverage only the\nprobabilities of actual tokens in the text, relying on simple task-specific\nheuristics. Crucially, they overlook the information contained in the full\nsequence of next-token probability distributions. We propose to go beyond\nhand-crafted decision rules by learning directly from the complete observable\noutput of LLMs -- consisting not only of next-token probabilities, but also the\nfull sequence of next-token distributions. We refer to this as the LLM Output\nSignature (LOS), and treat it as a reference data type for detecting\nhallucinations and data contamination. To that end, we introduce LOS-Net, a\nlightweight attention-based architecture trained on an efficient encoding of\nthe LOS, which can provably approximate a broad class of existing techniques\nfor both tasks. Empirically, LOS-Net achieves superior performance across\ndiverse benchmarks and LLMs, while maintaining extremely low detection latency.\nFurthermore, it demonstrates promising transfer capabilities across datasets\nand LLMs. Full code is available at\nhttps://github.com/BarSGuy/Beyond-next-token-probabilities."
                },
                "authors": [
                    {
                        "name": "Guy Bar-Shalom"
                    },
                    {
                        "name": "Fabrizio Frasca"
                    },
                    {
                        "name": "Derek Lim"
                    },
                    {
                        "name": "Yoav Gelberg"
                    },
                    {
                        "name": "Yftah Ziser"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.14043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26302v1",
                "updated": "2025-09-30T14:16:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    16,
                    8,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:16:08Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    16,
                    8,
                    1,
                    273,
                    0
                ],
                "title": "QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented\n  Dialogue Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented\n  Dialogue Summarization"
                },
                "summary": "Dialogue summarization aims to distill the core meaning of a conversation\ninto a concise text. This is crucial for reducing the complexity and noise\ninherent in dialogue-heavy applications. While recent approaches typically\ntrain language models to mimic human-written summaries, such supervision is\ncostly and often results in outputs that lack task-specific focus limiting\ntheir effectiveness in downstream applications, such as medical tasks. In this\npaper, we propose \\app, a framework for task-oriented utility-based dialogue\nsummarization. \\app starts by generating multiple summaries and task-oriented\nquestion-answer pairs from a dialogue in a zero-shot manner using a pool of\nlarge language models (LLMs). The quality of the generated summaries is\nevaluated by having LLMs answer task-related questions before \\textit{(i)}\nselecting the best candidate answers and \\textit{(ii)} identifying the most\ninformative summary based on these answers. Finally, we fine-tune the best LLM\non the selected summaries. When validated on multiple datasets, \\app\ndemonstrates its effectiveness by achieving competitive results in various\nzero-shot settings, rivaling fully-supervised State-of-the-Art (SotA) methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue summarization aims to distill the core meaning of a conversation\ninto a concise text. This is crucial for reducing the complexity and noise\ninherent in dialogue-heavy applications. While recent approaches typically\ntrain language models to mimic human-written summaries, such supervision is\ncostly and often results in outputs that lack task-specific focus limiting\ntheir effectiveness in downstream applications, such as medical tasks. In this\npaper, we propose \\app, a framework for task-oriented utility-based dialogue\nsummarization. \\app starts by generating multiple summaries and task-oriented\nquestion-answer pairs from a dialogue in a zero-shot manner using a pool of\nlarge language models (LLMs). The quality of the generated summaries is\nevaluated by having LLMs answer task-related questions before \\textit{(i)}\nselecting the best candidate answers and \\textit{(ii)} identifying the most\ninformative summary based on these answers. Finally, we fine-tune the best LLM\non the selected summaries. When validated on multiple datasets, \\app\ndemonstrates its effectiveness by achieving competitive results in various\nzero-shot settings, rivaling fully-supervised State-of-the-Art (SotA) methods."
                },
                "authors": [
                    {
                        "name": "Mohamed Imed Eddine Ghebriout"
                    },
                    {
                        "name": "Gaël Guibon"
                    },
                    {
                        "name": "Ivan Lerner"
                    },
                    {
                        "name": "Emmanuel Vincent"
                    }
                ],
                "author_detail": {
                    "name": "Emmanuel Vincent"
                },
                "arxiv_affiliation": "Universite de Lorraine, CNRS, Inria, LORIA, Nancy, France",
                "author": "Emmanuel Vincent",
                "arxiv_comment": "Accepted to Empirical Methods in Natural Language Processing (EMNLP\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26292v1",
                "updated": "2025-09-30T14:08:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    8,
                    14,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:08:14Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    8,
                    14,
                    1,
                    273,
                    0
                ],
                "title": "Solar Low Energy X-ray Spectrometer on board Aditya-L1: Ground\n  Calibration and In-flight Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solar Low Energy X-ray Spectrometer on board Aditya-L1: Ground\n  Calibration and In-flight Performance"
                },
                "summary": "The Solar Low-Energy X-ray Spectrometer (SoLEXS) on board India's Aditya-L1\nmission was launched on 2 September 2023 and commenced solar observations on 13\nDecember 2023 following successful aperture cover deployment. Operating from\nthe Sun-Earth L1 Lagrange point, SoLEXS has been providing continuous\nSun-as-a-star soft X-ray spectroscopy across 2-22 keV with 170 eV resolution at\n5.9 keV and 1-second temporal cadence since 6 January 2024. The instrument\nemploys two Silicon Drift Detectors with aperture areas of 7.1 mm$^2$ and 0.1\nmm$^2$ to accommodate the full dynamic range of solar activity from A-class to\nX-class flares. This paper presents comprehensive ground and on board\ncalibration procedures that establish SoLEXS's quantitative spectroscopic\ncapabilities. Ground calibration encompassed energy-channel relationships,\nspectral resolution characterization, instrument response functions, and\ncollimator angular response measurements, with thermo-vacuum testing validating\nperformance stability across operational temperature ranges. On board\ncalibration utilizing an internal $^{55}$Fe source demonstrated preserved\npost-launch spectral resolution (164.9-171.2 eV), while cross-calibration with\nGOES-XRS and Chandrayaan-2/XSM confirmed radiometric accuracy and flux\nagreement. The instrument's 100% observational duty cycle at L1 enables\nunprecedented continuous monitoring of solar flare evolution across all\nintensity classes, providing calibrated data for advancing coronal heating\nmechanisms, flare energetics, and flare-coronal mass ejection relationship\nstudies through soft X-ray spectroscopy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Solar Low-Energy X-ray Spectrometer (SoLEXS) on board India's Aditya-L1\nmission was launched on 2 September 2023 and commenced solar observations on 13\nDecember 2023 following successful aperture cover deployment. Operating from\nthe Sun-Earth L1 Lagrange point, SoLEXS has been providing continuous\nSun-as-a-star soft X-ray spectroscopy across 2-22 keV with 170 eV resolution at\n5.9 keV and 1-second temporal cadence since 6 January 2024. The instrument\nemploys two Silicon Drift Detectors with aperture areas of 7.1 mm$^2$ and 0.1\nmm$^2$ to accommodate the full dynamic range of solar activity from A-class to\nX-class flares. This paper presents comprehensive ground and on board\ncalibration procedures that establish SoLEXS's quantitative spectroscopic\ncapabilities. Ground calibration encompassed energy-channel relationships,\nspectral resolution characterization, instrument response functions, and\ncollimator angular response measurements, with thermo-vacuum testing validating\nperformance stability across operational temperature ranges. On board\ncalibration utilizing an internal $^{55}$Fe source demonstrated preserved\npost-launch spectral resolution (164.9-171.2 eV), while cross-calibration with\nGOES-XRS and Chandrayaan-2/XSM confirmed radiometric accuracy and flux\nagreement. The instrument's 100% observational duty cycle at L1 enables\nunprecedented continuous monitoring of solar flare evolution across all\nintensity classes, providing calibrated data for advancing coronal heating\nmechanisms, flare energetics, and flare-coronal mass ejection relationship\nstudies through soft X-ray spectroscopy."
                },
                "authors": [
                    {
                        "name": "Abhilash R. Sarwade"
                    },
                    {
                        "name": "Ankur Kushwaha"
                    },
                    {
                        "name": "M. C. Ramadevi"
                    },
                    {
                        "name": "Monoj Bug"
                    },
                    {
                        "name": "Kiran Lakshmipathaiah"
                    },
                    {
                        "name": "Smrati Verma"
                    },
                    {
                        "name": "Vaishali Sharan"
                    },
                    {
                        "name": "K. Sankarasubramanian"
                    }
                ],
                "author_detail": {
                    "name": "K. Sankarasubramanian"
                },
                "author": "K. Sankarasubramanian",
                "arxiv_comment": "29 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20712v2",
                "updated": "2025-09-30T14:07:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    7,
                    14,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-25T03:22:04Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    22,
                    4,
                    3,
                    268,
                    0
                ],
                "title": "CE-GPPO: Coordinating Entropy via Gradient-Preserving Clipping Policy\n  Optimization in Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CE-GPPO: Coordinating Entropy via Gradient-Preserving Clipping Policy\n  Optimization in Reinforcement Learning"
                },
                "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing\nlarge language models (LLMs) to handle complex reasoning tasks. A core\nchallenge in this process lies in managing policy entropy, which reflects the\nbalance between exploration and exploitation during training. Existing methods,\nsuch as proximal policy optimization (PPO) and its variants, discard valuable\ngradient signals from low-probability tokens due to the clipping mechanism. We\nsystematically analyze the entropy dynamics and reveal that these clipped\ntokens play a critical yet overlooked role in regulating entropy evolution. We\npropose \\textbf{C}oordinating \\textbf{E}ntropy via\n\\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization\n(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in\nnative PPO in a gentle and bounded manner. By controlling the magnitude of\ngradients from tokens outside the clipping interval, CE-GPPO is able to achieve\nan exploration-exploitation trade-off. We provide theoretical justification and\nempirical evidence showing that CE-GPPO effectively mitigates entropy\ninstability. Extensive experiments on mathematical reasoning benchmarks show\nthat CE-GPPO consistently outperforms strong baselines across different model\nscales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a powerful paradigm for optimizing\nlarge language models (LLMs) to handle complex reasoning tasks. A core\nchallenge in this process lies in managing policy entropy, which reflects the\nbalance between exploration and exploitation during training. Existing methods,\nsuch as proximal policy optimization (PPO) and its variants, discard valuable\ngradient signals from low-probability tokens due to the clipping mechanism. We\nsystematically analyze the entropy dynamics and reveal that these clipped\ntokens play a critical yet overlooked role in regulating entropy evolution. We\npropose \\textbf{C}oordinating \\textbf{E}ntropy via\n\\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization\n(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in\nnative PPO in a gentle and bounded manner. By controlling the magnitude of\ngradients from tokens outside the clipping interval, CE-GPPO is able to achieve\nan exploration-exploitation trade-off. We provide theoretical justification and\nempirical evidence showing that CE-GPPO effectively mitigates entropy\ninstability. Extensive experiments on mathematical reasoning benchmarks show\nthat CE-GPPO consistently outperforms strong baselines across different model\nscales."
                },
                "authors": [
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Minxuan Lv"
                    },
                    {
                        "name": "Yuntao Li"
                    },
                    {
                        "name": "Wenping Hu"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23340v2",
                "updated": "2025-09-30T13:57:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    57,
                    38,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-27T14:42:48Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    14,
                    42,
                    48,
                    5,
                    270,
                    0
                ],
                "title": "CrediBench: Building Web-Scale Network Datasets for Information\n  Integrity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrediBench: Building Web-Scale Network Datasets for Information\n  Integrity"
                },
                "summary": "Online misinformation poses an escalating threat, amplified by the Internet's\nopen nature and increasingly capable LLMs that generate persuasive yet\ndeceptive content. Existing misinformation detection methods typically focus on\neither textual content or network structure in isolation, failing to leverage\nthe rich, dynamic interplay between website content and hyperlink relationships\nthat characterizes real-world misinformation ecosystems. We introduce\nCrediBench: a large-scale data processing pipeline for constructing temporal\nweb graphs that jointly model textual content and hyperlink structure for\nmisinformation detection. Unlike prior work, our approach captures the dynamic\nevolution of general misinformation domains, including changes in both content\nand inter-site references over time. Our processed one-month snapshot extracted\nfrom the Common Crawl archive in December 2024 contains 45 million nodes and 1\nbillion edges, representing the largest web graph dataset made publicly\navailable for misinformation research to date. From our experiments on this\ngraph snapshot, we demonstrate the strength of both structural and webpage\ncontent signals for learning credibility scores, which measure source\nreliability. The pipeline and experimentation code are all available here, and\nthe dataset is in this folder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online misinformation poses an escalating threat, amplified by the Internet's\nopen nature and increasingly capable LLMs that generate persuasive yet\ndeceptive content. Existing misinformation detection methods typically focus on\neither textual content or network structure in isolation, failing to leverage\nthe rich, dynamic interplay between website content and hyperlink relationships\nthat characterizes real-world misinformation ecosystems. We introduce\nCrediBench: a large-scale data processing pipeline for constructing temporal\nweb graphs that jointly model textual content and hyperlink structure for\nmisinformation detection. Unlike prior work, our approach captures the dynamic\nevolution of general misinformation domains, including changes in both content\nand inter-site references over time. Our processed one-month snapshot extracted\nfrom the Common Crawl archive in December 2024 contains 45 million nodes and 1\nbillion edges, representing the largest web graph dataset made publicly\navailable for misinformation research to date. From our experiments on this\ngraph snapshot, we demonstrate the strength of both structural and webpage\ncontent signals for learning credibility scores, which measure source\nreliability. The pipeline and experimentation code are all available here, and\nthe dataset is in this folder."
                },
                "authors": [
                    {
                        "name": "Emma Kondrup"
                    },
                    {
                        "name": "Sebastian Sabry"
                    },
                    {
                        "name": "Hussein Abdallah"
                    },
                    {
                        "name": "Zachary Yang"
                    },
                    {
                        "name": "James Zhou"
                    },
                    {
                        "name": "Kellin Pelrine"
                    },
                    {
                        "name": "Jean-François Godbout"
                    },
                    {
                        "name": "Michael M. Bronstein"
                    },
                    {
                        "name": "Reihaneh Rabbany"
                    },
                    {
                        "name": "Shenyang Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shenyang Huang"
                },
                "author": "Shenyang Huang",
                "arxiv_comment": "16 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26272v1",
                "updated": "2025-09-30T13:56:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    56,
                    5,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T13:56:05Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    56,
                    5,
                    1,
                    273,
                    0
                ],
                "title": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake\n  Detection"
                },
                "summary": "The rapid rise of synthetic media has made deepfake detection a critical\nchallenge for online safety and trust. Progress remains constrained by the\nscarcity of large, high-quality datasets. Although multimodal large language\nmodels (LLMs) exhibit strong reasoning capabilities, their performance on\ndeepfake detection is poor, often producing explanations that are misaligned\nwith visual evidence or hallucinatory. To address this limitation, we introduce\na reasoning-annotated dataset for deepfake detection and propose\nParagraph-level Relative Policy Optimization (PRPO), a reinforcement learning\nalgorithm that aligns LLM reasoning with image content at the paragraph level.\nExperiments show that PRPO improves detection accuracy by a wide margin and\nachieves the highest reasoning score of 4.55/5.0. Ablation studies further\ndemonstrate that PRPO significantly outperforms GRPO under test-time\nconditions. These results underscore the importance of grounding multimodal\nreasoning in visual evidence to enable more reliable and interpretable deepfake\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of synthetic media has made deepfake detection a critical\nchallenge for online safety and trust. Progress remains constrained by the\nscarcity of large, high-quality datasets. Although multimodal large language\nmodels (LLMs) exhibit strong reasoning capabilities, their performance on\ndeepfake detection is poor, often producing explanations that are misaligned\nwith visual evidence or hallucinatory. To address this limitation, we introduce\na reasoning-annotated dataset for deepfake detection and propose\nParagraph-level Relative Policy Optimization (PRPO), a reinforcement learning\nalgorithm that aligns LLM reasoning with image content at the paragraph level.\nExperiments show that PRPO improves detection accuracy by a wide margin and\nachieves the highest reasoning score of 4.55/5.0. Ablation studies further\ndemonstrate that PRPO significantly outperforms GRPO under test-time\nconditions. These results underscore the importance of grounding multimodal\nreasoning in visual evidence to enable more reliable and interpretable deepfake\ndetection."
                },
                "authors": [
                    {
                        "name": "Tuan Nguyen"
                    },
                    {
                        "name": "Naseem Khan"
                    },
                    {
                        "name": "Khang Tran"
                    },
                    {
                        "name": "NhatHai Phan"
                    },
                    {
                        "name": "Issa Khalil"
                    }
                ],
                "author_detail": {
                    "name": "Issa Khalil"
                },
                "author": "Issa Khalil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21107v2",
                "updated": "2025-09-30T13:55:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    55,
                    43,
                    1,
                    273,
                    0
                ],
                "published": "2025-08-28T14:32:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    32,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Generate Unit Test via Adversarial Reinforcement Learning"
                },
                "summary": "Unit testing is a core practice in programming, enabling systematic\nevaluation of programs produced by human developers or large language models\n(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have\nbeen employed to automate test generation, yet methods for training LLMs to\nproduce high-quality tests remain underexplored. In this work, we propose UTRL,\na novel reinforcement learning framework that trains an LLM to generate\nhigh-quality unit tests given a programming instruction. Our key idea is to\niteratively train two LLMs, the unit test generator and the code generator, in\nan adversarial manner via reinforcement learning. The unit test generator is\ntrained to maximize a discrimination reward, which reflects its ability to\nproduce tests that expose faults in the code generator's solutions, and the\ncode generator is trained to maximize a code reward, which reflects its ability\nto produce solutions that pass the unit tests generated by the test generator.\nIn our experiments, we demonstrate that unit tests generated by Qwen3-4B\ntrained via UTRL show higher quality compared to unit tests generated by the\nsame model trained via supervised fine-tuning on human-written ground-truth\nunit tests, yielding code evaluations that more closely align with those\ninduced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL\noutperforms frontier models such as GPT-4.1 in generating high-quality unit\ntests, highlighting the effectiveness of UTRL in training LLMs for this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit testing is a core practice in programming, enabling systematic\nevaluation of programs produced by human developers or large language models\n(LLMs). Given the challenges in writing comprehensive unit tests, LLMs have\nbeen employed to automate test generation, yet methods for training LLMs to\nproduce high-quality tests remain underexplored. In this work, we propose UTRL,\na novel reinforcement learning framework that trains an LLM to generate\nhigh-quality unit tests given a programming instruction. Our key idea is to\niteratively train two LLMs, the unit test generator and the code generator, in\nan adversarial manner via reinforcement learning. The unit test generator is\ntrained to maximize a discrimination reward, which reflects its ability to\nproduce tests that expose faults in the code generator's solutions, and the\ncode generator is trained to maximize a code reward, which reflects its ability\nto produce solutions that pass the unit tests generated by the test generator.\nIn our experiments, we demonstrate that unit tests generated by Qwen3-4B\ntrained via UTRL show higher quality compared to unit tests generated by the\nsame model trained via supervised fine-tuning on human-written ground-truth\nunit tests, yielding code evaluations that more closely align with those\ninduced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL\noutperforms frontier models such as GPT-4.1 in generating high-quality unit\ntests, highlighting the effectiveness of UTRL in training LLMs for this task."
                },
                "authors": [
                    {
                        "name": "Dongjun Lee"
                    },
                    {
                        "name": "Changho Hwang"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "arxiv_comment": "Code is available at: https://github.com/dgjun32/UTRL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26255v2",
                "updated": "2025-10-01T01:58:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    58,
                    1,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T13:44:34Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    44,
                    34,
                    1,
                    273,
                    0
                ],
                "title": "ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot\n  Planning"
                },
                "summary": "Long-horizon embodied planning is challenging because the world does not only\nchange through an agent's actions: exogenous processes (e.g., water heating,\ndominoes cascading) unfold concurrently with the agent's actions. We propose a\nframework for abstract world models that jointly learns (i) symbolic state\nrepresentations and (ii) causal processes for both endogenous actions and\nexogenous mechanisms. Each causal process models the time course of a\nstochastic cause-effect relation. We learn these world models from limited data\nvia variational Bayesian inference combined with LLM proposals. Across five\nsimulated tabletop robotics environments, the learned models enable fast\nplanning that generalizes to held-out tasks with more objects and more complex\ngoals, outperforming a range of baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-horizon embodied planning is challenging because the world does not only\nchange through an agent's actions: exogenous processes (e.g., water heating,\ndominoes cascading) unfold concurrently with the agent's actions. We propose a\nframework for abstract world models that jointly learns (i) symbolic state\nrepresentations and (ii) causal processes for both endogenous actions and\nexogenous mechanisms. Each causal process models the time course of a\nstochastic cause-effect relation. We learn these world models from limited data\nvia variational Bayesian inference combined with LLM proposals. Across five\nsimulated tabletop robotics environments, the learned models enable fast\nplanning that generalizes to held-out tasks with more objects and more complex\ngoals, outperforming a range of baselines."
                },
                "authors": [
                    {
                        "name": "Yichao Liang"
                    },
                    {
                        "name": "Dat Nguyen"
                    },
                    {
                        "name": "Cambridge Yang"
                    },
                    {
                        "name": "Tianyang Li"
                    },
                    {
                        "name": "Joshua B. Tenenbaum"
                    },
                    {
                        "name": "Carl Edward Rasmussen"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Zenna Tavares"
                    },
                    {
                        "name": "Tom Silver"
                    },
                    {
                        "name": "Kevin Ellis"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Ellis"
                },
                "author": "Kevin Ellis",
                "arxiv_comment": "41 pages. The last two authors contributed equally in co-advising",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26246v1",
                "updated": "2025-09-30T13:37:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    37,
                    48,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T13:37:48Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    37,
                    48,
                    1,
                    273,
                    0
                ],
                "title": "SlimPack: Fine-Grained Asymmetric Packing for Balanced and Efficient\n  Variable-Length LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimPack: Fine-Grained Asymmetric Packing for Balanced and Efficient\n  Variable-Length LLM Training"
                },
                "summary": "The efficient distributed training of Large Language Models (LLMs) is\nseverely hampered by the extreme variance in context lengths. This data\nheterogeneity, amplified by conventional packing strategies and asymmetric\nforward-backward costs, leads to critical inefficiencies such as cascading\nworkload imbalances and severe hardware underutilization. Existing solutions\nattempt to mitigate these challenges, but often at the expense of memory or\ncommunication efficiency.\n  To address these challenges, we introduce SlimPack, a framework that\nfundamentally rethinks data packing and scheduling by decomposing samples into\nfine-grained slices. This slice-level decomposition immediately mitigates\ncritical memory and communication bottlenecks by transforming large, volatile\nworkloads into a stream of smaller, manageable units. This flexibility is then\nharnessed for our core innovation, Asymmetric Partitioning, which assembles\nbalanced scheduling units uniquely optimized for the different demands of the\nforward and backward passes. Orchestrated by a two-phase solver and a\nhigh-fidelity simulator, SlimPack holistically resolves imbalances across all\nparallel dimensions. Extensive experiments demonstrate that SlimPack achieves\nup to a $2.8\\times$ training throughput improvement over baselines, breaking\nthe conventional trade-off by delivering both superior balance and high\nresource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient distributed training of Large Language Models (LLMs) is\nseverely hampered by the extreme variance in context lengths. This data\nheterogeneity, amplified by conventional packing strategies and asymmetric\nforward-backward costs, leads to critical inefficiencies such as cascading\nworkload imbalances and severe hardware underutilization. Existing solutions\nattempt to mitigate these challenges, but often at the expense of memory or\ncommunication efficiency.\n  To address these challenges, we introduce SlimPack, a framework that\nfundamentally rethinks data packing and scheduling by decomposing samples into\nfine-grained slices. This slice-level decomposition immediately mitigates\ncritical memory and communication bottlenecks by transforming large, volatile\nworkloads into a stream of smaller, manageable units. This flexibility is then\nharnessed for our core innovation, Asymmetric Partitioning, which assembles\nbalanced scheduling units uniquely optimized for the different demands of the\nforward and backward passes. Orchestrated by a two-phase solver and a\nhigh-fidelity simulator, SlimPack holistically resolves imbalances across all\nparallel dimensions. Extensive experiments demonstrate that SlimPack achieves\nup to a $2.8\\times$ training throughput improvement over baselines, breaking\nthe conventional trade-off by delivering both superior balance and high\nresource efficiency."
                },
                "authors": [
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Guohao Wu"
                    },
                    {
                        "name": "Shenglong Zhang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Qianchao Zhu"
                    },
                    {
                        "name": "Zhouyang Li"
                    },
                    {
                        "name": "Chenyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenyu Wang"
                },
                "author": "Chenyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26242v1",
                "updated": "2025-09-30T13:36:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    36,
                    17,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T13:36:17Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    36,
                    17,
                    1,
                    273,
                    0
                ],
                "title": "Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted\n  Annealing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted\n  Annealing"
                },
                "summary": "Large language models (LLMs) fine-tuning shows excellent implications.\nHowever, vanilla fine-tuning methods often require intricate data mixture and\nrepeated experiments for optimal generalization. To address these challenges\nand streamline the training process, we propose an efficient and universal\nsolution, Dynamic Boosted Annealing (DBA). We obtain a global gradient through\nzero-learning-rate training on general data, which is subsequently employed for\ngradient boosting and dynamic training step correction during domain training.\nIn conjunction with annealing learning, we end up establishing a fine-tuning\npipeline that relies solely on domain data without collapse. By evaluating both\ngeneral and domain-specific performance across multiple tasks on several\npopular base models, DBA achieves an average improvement of 5.8% in joint\nperformance over vanilla fine-tuning. Furthermore, since general data is no\nlonger involved in annealing, repeated experiments led by data mixture are also\neliminated. According to our tests, the DBA method can reduce GPU hours by\n91.0% compared to the vanilla method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) fine-tuning shows excellent implications.\nHowever, vanilla fine-tuning methods often require intricate data mixture and\nrepeated experiments for optimal generalization. To address these challenges\nand streamline the training process, we propose an efficient and universal\nsolution, Dynamic Boosted Annealing (DBA). We obtain a global gradient through\nzero-learning-rate training on general data, which is subsequently employed for\ngradient boosting and dynamic training step correction during domain training.\nIn conjunction with annealing learning, we end up establishing a fine-tuning\npipeline that relies solely on domain data without collapse. By evaluating both\ngeneral and domain-specific performance across multiple tasks on several\npopular base models, DBA achieves an average improvement of 5.8% in joint\nperformance over vanilla fine-tuning. Furthermore, since general data is no\nlonger involved in annealing, repeated experiments led by data mixture are also\neliminated. According to our tests, the DBA method can reduce GPU hours by\n91.0% compared to the vanilla method."
                },
                "authors": [
                    {
                        "name": "Yang Tang"
                    },
                    {
                        "name": "Ruijie Liu"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11176v2",
                "updated": "2025-09-30T13:34:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    34,
                    13,
                    1,
                    273,
                    0
                ],
                "published": "2025-06-12T10:59:28Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    10,
                    59,
                    28,
                    3,
                    163,
                    0
                ],
                "title": "Model Discovery and Graph Simulation: A Lightweight Gateway to Chaos\n  Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Discovery and Graph Simulation: A Lightweight Gateway to Chaos\n  Engineering"
                },
                "summary": "Chaos engineering reveals resilience risks but is expensive and operationally\nrisky to run broadly and often. Model-based analyses can estimate\ndependability, yet in practice they are tricky to build and keep current\nbecause models are typically handcrafted. We claim that a simple\nconnectivity-only topological model - just the service-dependency graph plus\nreplica counts - can provide fast, low-risk availability estimates under\nfail-stop faults. To make this claim practical without hand-built models, we\nintroduce model discovery: an automated step that can run in CI/CD or as an\nobservability-platform capability, synthesizing an explicit, analyzable model\nfrom artifacts teams already have (e.g., distributed traces, service-mesh\ntelemetry, configs/manifests) - providing an accessible gateway for teams to\nbegin resilience testing. As a proof by instance on the DeathStarBench Social\nNetwork, we extract the dependency graph from Jaeger and estimate availability\nacross two deployment modes and five failure rates. The discovered model\nclosely tracks live fault-injection results; with replication, median error at\nmid-range failure rates is near zero, while no-replication shows signed biases\nconsistent with excluded mechanisms. These results create two opportunities:\nfirst, to triage and reduce the scope of expensive chaos experiments in\nadvance, and second, to generate real-time signals on the system's resilience\nposture as its topology evolves, preserving live validation for the most\ncritical or ambiguous scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chaos engineering reveals resilience risks but is expensive and operationally\nrisky to run broadly and often. Model-based analyses can estimate\ndependability, yet in practice they are tricky to build and keep current\nbecause models are typically handcrafted. We claim that a simple\nconnectivity-only topological model - just the service-dependency graph plus\nreplica counts - can provide fast, low-risk availability estimates under\nfail-stop faults. To make this claim practical without hand-built models, we\nintroduce model discovery: an automated step that can run in CI/CD or as an\nobservability-platform capability, synthesizing an explicit, analyzable model\nfrom artifacts teams already have (e.g., distributed traces, service-mesh\ntelemetry, configs/manifests) - providing an accessible gateway for teams to\nbegin resilience testing. As a proof by instance on the DeathStarBench Social\nNetwork, we extract the dependency graph from Jaeger and estimate availability\nacross two deployment modes and five failure rates. The discovered model\nclosely tracks live fault-injection results; with replication, median error at\nmid-range failure rates is near zero, while no-replication shows signed biases\nconsistent with excluded mechanisms. These results create two opportunities:\nfirst, to triage and reduce the scope of expensive chaos experiments in\nadvance, and second, to generate real-time signals on the system's resilience\nposture as its topology evolves, preserving live validation for the most\ncritical or ambiguous scenarios."
                },
                "authors": [
                    {
                        "name": "Anatoly A. Krasnovsky"
                    }
                ],
                "author_detail": {
                    "name": "Anatoly A. Krasnovsky"
                },
                "author": "Anatoly A. Krasnovsky",
                "arxiv_comment": "v2: Extended experiment, major revision. Includes a more rigorous\n  statistical analysis, a formal algorithm specification, and refined\n  positioning of the work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26239v1",
                "updated": "2025-09-30T13:33:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    33,
                    46,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T13:33:46Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    33,
                    46,
                    1,
                    273,
                    0
                ],
                "title": "Sandbagging in a Simple Survival Bandit Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sandbagging in a Simple Survival Bandit Problem"
                },
                "summary": "Evaluating the safety of frontier AI systems is an increasingly important\nconcern, helping to measure the capabilities of such models and identify risks\nbefore deployment. However, it has been recognised that if AI agents are aware\nthat they are being evaluated, such agents may deliberately hide dangerous\ncapabilities or intentionally demonstrate suboptimal performance in\nsafety-related tasks in order to be released and to avoid being deactivated or\nretrained. Such strategic deception - often known as \"sandbagging\" - threatens\nto undermine the integrity of safety evaluations. For this reason, it is of\nvalue to identify methods that enable us to distinguish behavioural patterns\nthat demonstrate a true lack of capability from behavioural patterns that are\nconsistent with sandbagging. In this paper, we develop a simple model of\nstrategic deception in sequential decision-making tasks, inspired by the\nrecently developed survival bandit framework. We demonstrate theoretically that\nthis problem induces sandbagging behaviour in optimal rational agents, and\nconstruct a statistical test to distinguish between sandbagging and\nincompetence from sequences of test scores. In simulation experiments, we\ninvestigate the reliability of this test in allowing us to distinguish between\nsuch behaviours in bandit models. This work aims to establish a potential\navenue for developing robust statistical procedures for use in the science of\nfrontier model evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the safety of frontier AI systems is an increasingly important\nconcern, helping to measure the capabilities of such models and identify risks\nbefore deployment. However, it has been recognised that if AI agents are aware\nthat they are being evaluated, such agents may deliberately hide dangerous\ncapabilities or intentionally demonstrate suboptimal performance in\nsafety-related tasks in order to be released and to avoid being deactivated or\nretrained. Such strategic deception - often known as \"sandbagging\" - threatens\nto undermine the integrity of safety evaluations. For this reason, it is of\nvalue to identify methods that enable us to distinguish behavioural patterns\nthat demonstrate a true lack of capability from behavioural patterns that are\nconsistent with sandbagging. In this paper, we develop a simple model of\nstrategic deception in sequential decision-making tasks, inspired by the\nrecently developed survival bandit framework. We demonstrate theoretically that\nthis problem induces sandbagging behaviour in optimal rational agents, and\nconstruct a statistical test to distinguish between sandbagging and\nincompetence from sequences of test scores. In simulation experiments, we\ninvestigate the reliability of this test in allowing us to distinguish between\nsuch behaviours in bandit models. This work aims to establish a potential\navenue for developing robust statistical procedures for use in the science of\nfrontier model evaluations."
                },
                "authors": [
                    {
                        "name": "Joel Dyer"
                    },
                    {
                        "name": "Daniel Jarne Ornia"
                    },
                    {
                        "name": "Nicholas Bishop"
                    },
                    {
                        "name": "Anisoara Calinescu"
                    },
                    {
                        "name": "Michael Wooldridge"
                    }
                ],
                "author_detail": {
                    "name": "Michael Wooldridge"
                },
                "author": "Michael Wooldridge",
                "arxiv_comment": "Forthcoming in the \"Reliable ML from Unreliable Data Workshop\" at\n  NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26238v1",
                "updated": "2025-09-30T13:32:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    32,
                    59,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T13:32:59Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    32,
                    59,
                    1,
                    273,
                    0
                ],
                "title": "Beyond Linear Probes: Dynamic Safety Monitoring for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Linear Probes: Dynamic Safety Monitoring for Language Models"
                },
                "summary": "Monitoring large language models' (LLMs) activations is an effective way to\ndetect harmful requests before they lead to unsafe outputs. However,\ntraditional safety monitors often require the same amount of compute for every\nquery. This creates a trade-off: expensive monitors waste resources on easy\ninputs, while cheap ones risk missing subtle cases. We argue that safety\nmonitors should be flexible--costs should rise only when inputs are difficult\nto assess, or when more compute is available. To achieve this, we introduce\nTruncated Polynomial Classifiers (TPCs), a natural extension of linear probes\nfor dynamic activation monitoring. Our key insight is that polynomials can be\ntrained and evaluated progressively, term-by-term. At test-time, one can\nearly-stop for lightweight monitoring, or use more terms for stronger\nguardrails when needed. TPCs provide two modes of use. First, as a safety dial:\nby evaluating more terms, developers and regulators can \"buy\" stronger\nguardrails from the same model. Second, as an adaptive cascade: clear cases\nexit early after low-order checks, and higher-order guardrails are evaluated\nonly for ambiguous inputs, reducing overall monitoring costs. On two\nlarge-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with\nup to 30B parameters, we show that TPCs compete with or outperform MLP-based\nprobe baselines of the same size, all the while being more interpretable than\ntheir black-box counterparts. Our code is available at\nhttp://github.com/james-oldfield/tpc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring large language models' (LLMs) activations is an effective way to\ndetect harmful requests before they lead to unsafe outputs. However,\ntraditional safety monitors often require the same amount of compute for every\nquery. This creates a trade-off: expensive monitors waste resources on easy\ninputs, while cheap ones risk missing subtle cases. We argue that safety\nmonitors should be flexible--costs should rise only when inputs are difficult\nto assess, or when more compute is available. To achieve this, we introduce\nTruncated Polynomial Classifiers (TPCs), a natural extension of linear probes\nfor dynamic activation monitoring. Our key insight is that polynomials can be\ntrained and evaluated progressively, term-by-term. At test-time, one can\nearly-stop for lightweight monitoring, or use more terms for stronger\nguardrails when needed. TPCs provide two modes of use. First, as a safety dial:\nby evaluating more terms, developers and regulators can \"buy\" stronger\nguardrails from the same model. Second, as an adaptive cascade: clear cases\nexit early after low-order checks, and higher-order guardrails are evaluated\nonly for ambiguous inputs, reducing overall monitoring costs. On two\nlarge-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with\nup to 30B parameters, we show that TPCs compete with or outperform MLP-based\nprobe baselines of the same size, all the while being more interpretable than\ntheir black-box counterparts. Our code is available at\nhttp://github.com/james-oldfield/tpc."
                },
                "authors": [
                    {
                        "name": "James Oldfield"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Ioannis Patras"
                    },
                    {
                        "name": "Adel Bibi"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "arxiv_comment": "Project page: http://james-oldfield.github.io/tpc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26235v1",
                "updated": "2025-09-30T13:31:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    31,
                    3,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T13:31:03Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    31,
                    3,
                    1,
                    273,
                    0
                ],
                "title": "Interpret, prune and distill Donut : towards lightweight VLMs for VQA on\n  document",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpret, prune and distill Donut : towards lightweight VLMs for VQA on\n  document"
                },
                "summary": "Recent advances in Visually-rich Document Understanding rely on large\nVision-Language Models like Donut, which perform document-level Visual Question\nAnswering without Optical Character Recognition. Despite their effectiveness,\nthese models are too costly for real-time or resource-constrained applications.\nWe investigate model compression through knowledge distillation, training\ncompact student models from a larger teacher. We leverage mechanistic\ninterpretability to drive student architecture design within this framework. By\nanalyzing internal computations, we identify essential subcomponents to retain,\nwhile having a clear view of which subcomponents should be approximated,\nskipped, or reparametrized based on their function. This approach yields\nDonut-MINT (Mechanistic Interpretability-based Network Trimming), a pruned\nDonut variant that reduces inference time and memory usage while maintaining\nstrong performance on DocVQA, a standard benchmark for document Visual Question\nAnswering. Our method reframes compression as circuit discovery, bridging\ninterpretability research and practical Vision-Language Model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Visually-rich Document Understanding rely on large\nVision-Language Models like Donut, which perform document-level Visual Question\nAnswering without Optical Character Recognition. Despite their effectiveness,\nthese models are too costly for real-time or resource-constrained applications.\nWe investigate model compression through knowledge distillation, training\ncompact student models from a larger teacher. We leverage mechanistic\ninterpretability to drive student architecture design within this framework. By\nanalyzing internal computations, we identify essential subcomponents to retain,\nwhile having a clear view of which subcomponents should be approximated,\nskipped, or reparametrized based on their function. This approach yields\nDonut-MINT (Mechanistic Interpretability-based Network Trimming), a pruned\nDonut variant that reduces inference time and memory usage while maintaining\nstrong performance on DocVQA, a standard benchmark for document Visual Question\nAnswering. Our method reframes compression as circuit discovery, bridging\ninterpretability research and practical Vision-Language Model deployment."
                },
                "authors": [
                    {
                        "name": "Adnan Ben Mansour"
                    },
                    {
                        "name": "Ayoub Karine"
                    },
                    {
                        "name": "David Naccache"
                    }
                ],
                "author_detail": {
                    "name": "David Naccache"
                },
                "author": "David Naccache",
                "arxiv_comment": "Accepted at Workshop on Machine Learning in Document Analysis and\n  Recognition (ICDAR WML 2025), Wuhan, China",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11027v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11027v2",
                "updated": "2025-09-30T13:25:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    25,
                    18,
                    1,
                    273,
                    0
                ],
                "published": "2024-11-17T10:17:01Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    10,
                    17,
                    1,
                    6,
                    322,
                    0
                ],
                "title": "BianCang: A Traditional Chinese Medicine Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BianCang: A Traditional Chinese Medicine Large Language Model"
                },
                "summary": "The surge of large language models (LLMs) has driven significant progress in\nmedical applications, including traditional Chinese medicine (TCM). However,\ncurrent medical LLMs struggle with TCM diagnosis and syndrome differentiation\ndue to substantial differences between TCM and modern medical theory, and the\nscarcity of specialized, high-quality corpora. To this end, in this paper we\npropose BianCang, a TCM-specific LLM, using a two-stage training process that\nfirst injects domain-specific knowledge and then aligns it through targeted\nstimulation to enhance diagnostic and differentiation capabilities.\nSpecifically, we constructed pre-training corpora, instruction-aligned datasets\nbased on real hospital records, and the ChP-TCM dataset derived from the\nPharmacopoeia of the People's Republic of China. We compiled extensive TCM and\nmedical corpora for continual pre-training and supervised fine-tuning, building\na comprehensive dataset to refine the model's understanding of TCM. Evaluations\nacross 11 test sets involving 31 models and 4 tasks demonstrate the\neffectiveness of BianCang, offering valuable insights for future research.\nCode, datasets, and models are available on\nhttps://github.com/QLU-NLP/BianCang.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge of large language models (LLMs) has driven significant progress in\nmedical applications, including traditional Chinese medicine (TCM). However,\ncurrent medical LLMs struggle with TCM diagnosis and syndrome differentiation\ndue to substantial differences between TCM and modern medical theory, and the\nscarcity of specialized, high-quality corpora. To this end, in this paper we\npropose BianCang, a TCM-specific LLM, using a two-stage training process that\nfirst injects domain-specific knowledge and then aligns it through targeted\nstimulation to enhance diagnostic and differentiation capabilities.\nSpecifically, we constructed pre-training corpora, instruction-aligned datasets\nbased on real hospital records, and the ChP-TCM dataset derived from the\nPharmacopoeia of the People's Republic of China. We compiled extensive TCM and\nmedical corpora for continual pre-training and supervised fine-tuning, building\na comprehensive dataset to refine the model's understanding of TCM. Evaluations\nacross 11 test sets involving 31 models and 4 tasks demonstrate the\neffectiveness of BianCang, offering valuable insights for future research.\nCode, datasets, and models are available on\nhttps://github.com/QLU-NLP/BianCang."
                },
                "authors": [
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Xueping Peng"
                    },
                    {
                        "name": "Yi-Fei Wang"
                    },
                    {
                        "name": "Tao Shen"
                    },
                    {
                        "name": "Jiasheng Si"
                    },
                    {
                        "name": "Weiyu Zhang"
                    },
                    {
                        "name": "Fa Zhu"
                    },
                    {
                        "name": "Athanasios V. Vasilakos"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Xiaoming Wu"
                    },
                    {
                        "name": "Yinglong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinglong Wang"
                },
                "author": "Yinglong Wang",
                "arxiv_doi": "10.1109/JBHI.2025.3612415",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JBHI.2025.3612415",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.11027v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11027v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Journal of Biomedical and Health Informatics (Early Access),\n  2025, 1-12",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26217v1",
                "updated": "2025-09-30T13:19:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    19,
                    0,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T13:19:00Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    19,
                    0,
                    1,
                    273,
                    0
                ],
                "title": "Benchmarking Deep Learning Convolutions on Energy-constrained CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Deep Learning Convolutions on Energy-constrained CPUs"
                },
                "summary": "This work evaluates state-of-the-art convolution algorithms for CPU-based\ndeep learning inference. While most prior studies focus on GPUs or NPUs, CPU\nimplementations remain relatively underoptimized. We benchmark direct,\nGEMM-based, and Winograd convolutions across modern CPUs from ARM __ , Intel __\n, AMD __ , Apple __ , and Nvidia __ , considering both latency and energy\nefficiency. Our results highlight the key architectural factors that govern CPU\nefficiency for convolution operations, providing practical guidance for\nenergy-aware embedded deployment. As a main results of this work, the Nvidia __\nAGX Orin combined with the GEMM algorithm achieves the best trade-off between\ninference latency and energy consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work evaluates state-of-the-art convolution algorithms for CPU-based\ndeep learning inference. While most prior studies focus on GPUs or NPUs, CPU\nimplementations remain relatively underoptimized. We benchmark direct,\nGEMM-based, and Winograd convolutions across modern CPUs from ARM __ , Intel __\n, AMD __ , Apple __ , and Nvidia __ , considering both latency and energy\nefficiency. Our results highlight the key architectural factors that govern CPU\nefficiency for convolution operations, providing practical guidance for\nenergy-aware embedded deployment. As a main results of this work, the Nvidia __\nAGX Orin combined with the GEMM algorithm achieves the best trade-off between\ninference latency and energy consumption."
                },
                "authors": [
                    {
                        "name": "Enrique Galvez"
                    },
                    {
                        "name": "Adrien Cassagne"
                    },
                    {
                        "name": "Alix Munier"
                    },
                    {
                        "name": "Manuel Bouyer"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Bouyer"
                },
                "arxiv_affiliation": "ALSOC",
                "author": "Manuel Bouyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]