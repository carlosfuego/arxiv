[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v1",
                "updated": "2024-08-13T13:14:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v1",
                "updated": "2024-08-11T18:40:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve \\(\\frac{1}{16}\\) token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\n\\url{https://github.com/andy-yang-1/DoubleSparse}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve \\(\\frac{1}{16}\\) token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\n\\url{https://github.com/andy-yang-1/DoubleSparse}."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adrià Armejach"
                    },
                    {
                        "name": "Miquel Moretó"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moretó"
                },
                "author": "Miquel Moretó",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v1",
                "updated": "2024-07-27T16:20:21Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v1",
                "updated": "2024-07-25T00:27:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.11055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.11055v5",
                "updated": "2024-07-22T10:02:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    2,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2022-12-21T14:59:23Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    14,
                    59,
                    23,
                    2,
                    355,
                    0
                ],
                "title": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coalgebraic Satisfiability Checking for Arithmetic $μ$-Calculi"
                },
                "summary": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities."
                },
                "authors": [
                    {
                        "name": "Daniel Hausmann"
                    },
                    {
                        "name": "Lutz Schröder"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Schröder"
                },
                "author": "Lutz Schröder",
                "arxiv_doi": "10.46298/lmcs-20(3:9)2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/lmcs-20(3:9)2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.11055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.11055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Logical Methods in Computer Science, Volume 20, Issue 3 (July 23,\n  2024) lmcs:10532",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B70, 03B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15360v1",
                "updated": "2024-07-22T04:07:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T04:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Multiplication in Transformers: Insights into LLMs"
                },
                "summary": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
                },
                "authors": [
                    {
                        "name": "Luyu Qiu"
                    },
                    {
                        "name": "Jianing Li"
                    },
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15891v1",
                "updated": "2024-07-22T01:12:23Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T01:12:23Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads"
                },
                "summary": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model."
                },
                "authors": [
                    {
                        "name": "Hanlin Tang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Qingsen Han"
                    },
                    {
                        "name": "Shikuan Hong"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Gongyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gongyi Wang"
                },
                "author": "Gongyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15264v1",
                "updated": "2024-07-21T20:41:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T20:41:39Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "title": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training"
                },
                "authors": [
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Zaid Quresh"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v1",
                "updated": "2024-07-21T14:23:37Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope"
                },
                "summary": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v2",
                "updated": "2024-07-21T14:08:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    8,
                    42,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nefficiency limitations due to the substantial Key-Value (KV) cache required for\nlong-sequence inference. Recent efforts try to evict non-critical cache\nelements during runtime, thereby reducing cache size within given memory\nbudgets while preserving generation quality. Our reexamination of foundational\nprinciples reveals that prevailing methods aim to minimize an upper bound of\neviction loss, quantified as the L1 distance between the pre- and post-eviction\noutputs of multi-head self-attention mechanisms. Moreover, our analysis\nindicates that the common practices of uniformly assigning budgets across\ndifferent attention heads during cache eviction hinder their budget\nutilization, negatively impacting generation quality. In light of these\nfindings, we propose a simple yet effective adaptive budget allocation\nalgorithm. This algorithm not only optimizes the loss upper bound in theory but\nalso reduces the eviction loss in practice by aligning with the intrinsic\npatterns of self-attention mechanisms. Integrating this algorithm into two\nadvanced methods, we develop Ada-SnapKV and Ada-Pyramid. Extensive evaluations\non 16 datasets and the Needle-in-a-Haystack test confirm that they both\nsignificantly boost performance across various tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nefficiency limitations due to the substantial Key-Value (KV) cache required for\nlong-sequence inference. Recent efforts try to evict non-critical cache\nelements during runtime, thereby reducing cache size within given memory\nbudgets while preserving generation quality. Our reexamination of foundational\nprinciples reveals that prevailing methods aim to minimize an upper bound of\neviction loss, quantified as the L1 distance between the pre- and post-eviction\noutputs of multi-head self-attention mechanisms. Moreover, our analysis\nindicates that the common practices of uniformly assigning budgets across\ndifferent attention heads during cache eviction hinder their budget\nutilization, negatively impacting generation quality. In light of these\nfindings, we propose a simple yet effective adaptive budget allocation\nalgorithm. This algorithm not only optimizes the loss upper bound in theory but\nalso reduces the eviction loss in practice by aligning with the intrinsic\npatterns of self-attention mechanisms. Integrating this algorithm into two\nadvanced methods, we develop Ada-SnapKV and Ada-Pyramid. Extensive evaluations\non 16 datasets and the Needle-in-a-Haystack test confirm that they both\nsignificantly boost performance across various tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.00250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.00250v3",
                "updated": "2024-07-21T11:47:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    11,
                    47,
                    4,
                    6,
                    203,
                    0
                ],
                "published": "2022-12-01T03:35:14Z",
                "published_parsed": [
                    2022,
                    12,
                    1,
                    3,
                    35,
                    14,
                    3,
                    335,
                    0
                ],
                "title": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy"
                },
                "summary": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients."
                },
                "authors": [
                    {
                        "name": "Ngoc Duy Pham"
                    },
                    {
                        "name": "Tran Khoa Phan"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Doan Nguyen"
                    },
                    {
                        "name": "Naveen Chilamkurti"
                    }
                ],
                "author_detail": {
                    "name": "Naveen Chilamkurti"
                },
                "author": "Naveen Chilamkurti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.00250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.00250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08454v2",
                "updated": "2024-07-21T02:37:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    2,
                    37,
                    11,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-11T12:50:42Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    50,
                    42,
                    3,
                    193,
                    0
                ],
                "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks"
                },
                "summary": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Boxiao Jin"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10516v2",
                "updated": "2024-07-20T22:14:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    22,
                    14,
                    42,
                    5,
                    202,
                    0
                ],
                "published": "2023-03-28T03:55:47Z",
                "published_parsed": [
                    2023,
                    3,
                    28,
                    3,
                    55,
                    47,
                    1,
                    87,
                    0
                ],
                "title": "Distributed Neural Representation for Reactive in situ Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Neural Representation for Reactive in situ Visualization"
                },
                "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations."
                },
                "authors": [
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Joseph A. Insley"
                    },
                    {
                        "name": "Victor A. Mateevitsi"
                    },
                    {
                        "name": "Silvio Rizzi"
                    },
                    {
                        "name": "Michael E. Papka"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14801v1",
                "updated": "2024-07-20T08:21:46Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "published": "2024-07-20T08:21:46Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "title": "SquareSort: a cache-oblivious sorting algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SquareSort: a cache-oblivious sorting algorithm"
                },
                "summary": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort."
                },
                "authors": [
                    {
                        "name": "Michal Koucký"
                    },
                    {
                        "name": "Josef Matějka"
                    }
                ],
                "author_detail": {
                    "name": "Josef Matějka"
                },
                "author": "Josef Matějka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07240v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07240v6",
                "updated": "2024-07-19T21:04:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    21,
                    4,
                    14,
                    4,
                    201,
                    0
                ],
                "published": "2023-10-11T07:08:20Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    8,
                    20,
                    2,
                    284,
                    0
                ],
                "title": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving"
                },
                "summary": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Michael Maire"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "arxiv_comment": "SIGCOMM'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07240v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07240v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v1",
                "updated": "2024-07-19T14:28:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "8 pages, 8 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v5",
                "updated": "2024-07-19T09:37:19Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    9,
                    37,
                    19,
                    4,
                    201,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14057v1",
                "updated": "2024-07-19T06:34:45Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T06:34:45Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference"
                },
                "summary": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Qichen Fu"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Thomas Merth"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Mahyar Najibi"
                    }
                ],
                "author_detail": {
                    "name": "Mahyar Najibi"
                },
                "author": "Mahyar Najibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v1",
                "updated": "2024-07-18T18:47:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Data-driven Forecasting of Deep Learning Performance on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Forecasting of Deep Learning Performance on GPUs"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03482v2",
                "updated": "2024-07-18T16:31:29Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    16,
                    31,
                    29,
                    3,
                    200,
                    0
                ],
                "published": "2024-06-05T17:42:05Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    17,
                    42,
                    5,
                    2,
                    157,
                    0
                ],
                "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead"
                },
                "summary": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Insu Han"
                    }
                ],
                "author_detail": {
                    "name": "Insu Han"
                },
                "author": "Insu Han",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.12925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.12925v2",
                "updated": "2024-07-18T09:06:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    6,
                    0,
                    3,
                    200,
                    0
                ],
                "published": "2023-09-22T15:23:57Z",
                "published_parsed": [
                    2023,
                    9,
                    22,
                    15,
                    23,
                    57,
                    4,
                    265,
                    0
                ],
                "title": "MCU-Wide Timing Side Channels and Their Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCU-Wide Timing Side Channels and Their Detection"
                },
                "summary": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels."
                },
                "authors": [
                    {
                        "name": "Johannes Müller"
                    },
                    {
                        "name": "Anna Lena Duque Antón"
                    },
                    {
                        "name": "Lucas Deutschmann"
                    },
                    {
                        "name": "Dino Mehmedagić"
                    },
                    {
                        "name": "Cristiano Rodrigues"
                    },
                    {
                        "name": "Daniel Oliveira"
                    },
                    {
                        "name": "Keerthikumara Devarajegowda"
                    },
                    {
                        "name": "Mohammad Rahmani Fadiheh"
                    },
                    {
                        "name": "Sandro Pinto"
                    },
                    {
                        "name": "Dominik Stoffel"
                    },
                    {
                        "name": "Wolfgang Kunz"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Kunz"
                },
                "author": "Wolfgang Kunz",
                "arxiv_doi": "10.1145/3649329.3656541",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3656541",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.12925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.12925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This version extends the work of the previous version and was\n  accepted and presented at DAC'24",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v3",
                "updated": "2024-07-18T06:18:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    6,
                    18,
                    4,
                    3,
                    200,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v2",
                "updated": "2024-07-17T23:09:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    23,
                    9,
                    10,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12850v2",
                "updated": "2024-07-17T16:56:18Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    16,
                    56,
                    18,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-19T12:39:11Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    12,
                    39,
                    11,
                    4,
                    110,
                    0
                ],
                "title": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance"
                },
                "summary": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements."
                },
                "authors": [
                    {
                        "name": "Zeke Xia"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Dengke Yan"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Junlong Zhou"
                    },
                    {
                        "name": "Mingsong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingsong Chen"
                },
                "author": "Mingsong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19626v2",
                "updated": "2024-07-17T03:02:49Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    3,
                    2,
                    49,
                    2,
                    199,
                    0
                ],
                "published": "2024-05-30T02:23:50Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    2,
                    23,
                    50,
                    3,
                    151,
                    0
                ],
                "title": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent"
                },
                "summary": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ziheng Liu"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12077v1",
                "updated": "2024-07-16T18:00:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T18:00:00Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "title": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression"
                },
                "summary": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use."
                },
                "authors": [
                    {
                        "name": "Daniel Goldstein"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Eric Alcaide"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Eugene Cheah"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Cheah"
                },
                "author": "Eugene Cheah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.04877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.04877v2",
                "updated": "2024-07-16T09:05:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    5,
                    39,
                    1,
                    198,
                    0
                ],
                "published": "2023-05-08T17:20:30Z",
                "published_parsed": [
                    2023,
                    5,
                    8,
                    17,
                    20,
                    30,
                    0,
                    128,
                    0
                ],
                "title": "Coherently amplified ultrafast imaging using a free-electron\n  interferometer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherently amplified ultrafast imaging using a free-electron\n  interferometer"
                },
                "summary": "Accessing the low-energy non-equilibrium dynamics of materials and their\npolaritons with simultaneous high spatial and temporal resolution has been a\nbold frontier of electron microscopy in recent years. One of the main\nchallenges lies in the ability to retrieve extremely weak signals while\nsimultaneously disentangling amplitude and phase information. Here, we present\nFree-Electron Ramsey Imaging (FERI), a microscopy approach based on\nlight-induced electron modulation that enables coherent amplification of\noptical near-fields in electron imaging. We provide simultaneous time-, space-,\nand phase-resolved measurements of a micro-drum made from a hexagonal boron\nnitride membrane visualizing the sub-cycle dynamics of 2D polariton wavepackets\ntherein. The phase-resolved measurements reveals vortex-anti-vortex\nsingularities on the polariton wavefronts, together with an intriguing\nphenomenon of a traveling wave mimicking the amplitude profile of a standing\nwave. Our experiments show a 20-fold coherent amplification of the near-field\nsignal compared to conventional electron near-field imaging, resolving peak\nfield intensities in the order of ~W/cm2, corresponding to field amplitudes of\na few kV/m. As a result, our work paves the way for spatio-temporal electron\nmicroscopy of biological specimens and quantum materials, exciting yet delicate\nsamples that are currently difficult to investigate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessing the low-energy non-equilibrium dynamics of materials and their\npolaritons with simultaneous high spatial and temporal resolution has been a\nbold frontier of electron microscopy in recent years. One of the main\nchallenges lies in the ability to retrieve extremely weak signals while\nsimultaneously disentangling amplitude and phase information. Here, we present\nFree-Electron Ramsey Imaging (FERI), a microscopy approach based on\nlight-induced electron modulation that enables coherent amplification of\noptical near-fields in electron imaging. We provide simultaneous time-, space-,\nand phase-resolved measurements of a micro-drum made from a hexagonal boron\nnitride membrane visualizing the sub-cycle dynamics of 2D polariton wavepackets\ntherein. The phase-resolved measurements reveals vortex-anti-vortex\nsingularities on the polariton wavefronts, together with an intriguing\nphenomenon of a traveling wave mimicking the amplitude profile of a standing\nwave. Our experiments show a 20-fold coherent amplification of the near-field\nsignal compared to conventional electron near-field imaging, resolving peak\nfield intensities in the order of ~W/cm2, corresponding to field amplitudes of\na few kV/m. As a result, our work paves the way for spatio-temporal electron\nmicroscopy of biological specimens and quantum materials, exciting yet delicate\nsamples that are currently difficult to investigate."
                },
                "authors": [
                    {
                        "name": "Tomer Bucher"
                    },
                    {
                        "name": "Harel Nahari"
                    },
                    {
                        "name": "Hanan Herzig Sheinfux"
                    },
                    {
                        "name": "Ron Ruimy"
                    },
                    {
                        "name": "Arthur Niedermayr"
                    },
                    {
                        "name": "Raphael Dahan"
                    },
                    {
                        "name": "Qinghui Yan"
                    },
                    {
                        "name": "Yuval Adiv"
                    },
                    {
                        "name": "Michael Yannai"
                    },
                    {
                        "name": "Jialin Chen"
                    },
                    {
                        "name": "Yaniv Kurman"
                    },
                    {
                        "name": "Sang Tae Park"
                    },
                    {
                        "name": "Daniel J. Masiel"
                    },
                    {
                        "name": "Eli Janzen"
                    },
                    {
                        "name": "James H. Edgar"
                    },
                    {
                        "name": "Fabrizio Carbone"
                    },
                    {
                        "name": "Guy Bartal"
                    },
                    {
                        "name": "Shai Tsesses"
                    },
                    {
                        "name": "Frank H. L. Koppens"
                    },
                    {
                        "name": "Giovanni Maria Vanacore"
                    },
                    {
                        "name": "Ido Kaminer"
                    }
                ],
                "author_detail": {
                    "name": "Ido Kaminer"
                },
                "author": "Ido Kaminer",
                "arxiv_doi": "10.1038/s41566-024-01451-w",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41566-024-01451-w",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.04877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.04877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11483v1",
                "updated": "2024-07-16T08:18:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    18,
                    41,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T08:18:41Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    18,
                    41,
                    1,
                    198,
                    0
                ],
                "title": "Performance Analysis of Internet of Vehicles Mesh Networks Based on\n  Actual Switch Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of Internet of Vehicles Mesh Networks Based on\n  Actual Switch Models"
                },
                "summary": "The rapid growth of the automotive industry has exacerbated the conflict\nbetween the complex traffic environment, increasing communication demands, and\nlimited resources. Given the imperative to mitigate traffic and network\ncongestion, analyzing the performance of Internet of Vehicles (IoV) mesh\nnetworks is of great practical significance. Most studies focus solely on\nindividual performance metrics and influencing factors, and the adopted\nsimulation tools, such as OPNET, cannot achieve the dynamic link generation of\nIoV mesh networks. To address these problems, a network performance analysis\nmodel based on actual switches is proposed. First, a typical IoV mesh network\narchitecture is constructed and abstracted into a mathematical model that\ndescribes how the link and topology changes over time. Then, the task\ngeneration model and the task forwarding model based on actual switches are\nproposed to obtain the real traffic distribution of the network. Finally, a\nscientific network performance indicator system is constructed. Simulation\nresults demonstrate that, with rising task traffic and decreasing node caching\ncapacity, the packet loss rate increases, and the task arrival rate decreases\nin the network. The proposed model can effectively evaluate the network\nperformance across various traffic states and provide valuable insights for\nnetwork construction and enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of the automotive industry has exacerbated the conflict\nbetween the complex traffic environment, increasing communication demands, and\nlimited resources. Given the imperative to mitigate traffic and network\ncongestion, analyzing the performance of Internet of Vehicles (IoV) mesh\nnetworks is of great practical significance. Most studies focus solely on\nindividual performance metrics and influencing factors, and the adopted\nsimulation tools, such as OPNET, cannot achieve the dynamic link generation of\nIoV mesh networks. To address these problems, a network performance analysis\nmodel based on actual switches is proposed. First, a typical IoV mesh network\narchitecture is constructed and abstracted into a mathematical model that\ndescribes how the link and topology changes over time. Then, the task\ngeneration model and the task forwarding model based on actual switches are\nproposed to obtain the real traffic distribution of the network. Finally, a\nscientific network performance indicator system is constructed. Simulation\nresults demonstrate that, with rising task traffic and decreasing node caching\ncapacity, the packet loss rate increases, and the task arrival rate decreases\nin the network. The proposed model can effectively evaluate the network\nperformance across various traffic states and provide valuable insights for\nnetwork construction and enhancement."
                },
                "authors": [
                    {
                        "name": "Jialin Hu"
                    },
                    {
                        "name": "Zhiyuan Ren"
                    },
                    {
                        "name": "Wenchi Cheng"
                    },
                    {
                        "name": "Zhiliang Shuai"
                    },
                    {
                        "name": "Zhao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Li"
                },
                "author": "Zhao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11338v1",
                "updated": "2024-07-16T03:08:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    3,
                    8,
                    41,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T03:08:41Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    3,
                    8,
                    41,
                    1,
                    198,
                    0
                ],
                "title": "Heterogeneous integration of high endurance ferroelectric and\n  piezoelectric epitaxial BaTiO$_3$ devices on Si",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous integration of high endurance ferroelectric and\n  piezoelectric epitaxial BaTiO$_3$ devices on Si"
                },
                "summary": "Integrating epitaxial BaTiO$_3$ (BTO) with Si is essential for leveraging its\nferroelectric, piezoelectric, and nonlinear optical properties in\nmicroelectronics. Recently, heterogeneous integration approaches that involve\ngrowth of BTO on ideal substrates followed by transfer to a desired substrate\nshow promise of achieving excellent device-quality films. However, beyond\nsimple demonstrations of the existence of ferroelectricity, robust devices with\nhigh endurance were not yet demonstrated on Si using the latter approach. Here,\nusing a novel two-step approach to synthesize epitaxial BTO using pulsed laser\ndeposition (PLD) on water soluble Sr3Al2O7 (SAO) (on SrTiO$_3$ (STO)\nsubstrates), we demonstrate successful integration of high-quality BTO\ncapacitors on Si, with Pr of 7 uC/cm2, Ec 150 kV/cm, ferroelectric and\nelectromechanical endurance of greater than $10^6$ cycles. We further address\nthe challenge of cracking and disintegration of thicker films by first\ntransferring a large area (5 mm x 5 mm) of the templated layer of BTO (~30 nm\nthick) on the desired substrate, followed by the growth of high-quality BTO on\nthis substrate, as revealed by HRXRD and HRSTEM measurements. These templated\nSi substrates offer a versatile platform for integrating any epitaxial complex\noxides with diverse functionalities onto any inorganic substrate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating epitaxial BaTiO$_3$ (BTO) with Si is essential for leveraging its\nferroelectric, piezoelectric, and nonlinear optical properties in\nmicroelectronics. Recently, heterogeneous integration approaches that involve\ngrowth of BTO on ideal substrates followed by transfer to a desired substrate\nshow promise of achieving excellent device-quality films. However, beyond\nsimple demonstrations of the existence of ferroelectricity, robust devices with\nhigh endurance were not yet demonstrated on Si using the latter approach. Here,\nusing a novel two-step approach to synthesize epitaxial BTO using pulsed laser\ndeposition (PLD) on water soluble Sr3Al2O7 (SAO) (on SrTiO$_3$ (STO)\nsubstrates), we demonstrate successful integration of high-quality BTO\ncapacitors on Si, with Pr of 7 uC/cm2, Ec 150 kV/cm, ferroelectric and\nelectromechanical endurance of greater than $10^6$ cycles. We further address\nthe challenge of cracking and disintegration of thicker films by first\ntransferring a large area (5 mm x 5 mm) of the templated layer of BTO (~30 nm\nthick) on the desired substrate, followed by the growth of high-quality BTO on\nthis substrate, as revealed by HRXRD and HRSTEM measurements. These templated\nSi substrates offer a versatile platform for integrating any epitaxial complex\noxides with diverse functionalities onto any inorganic substrate."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Harshal Jason D'Souza"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Rama Satya Sandilya"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "author": "Pavan Nukala",
                "arxiv_comment": "29 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v3",
                "updated": "2024-07-15T22:33:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    22,
                    33,
                    58,
                    0,
                    197,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Cache for Large Language Model Based\n  Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Cache for Large Language Model Based\n  Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "This study presents the first privacy aware semantic cache for LLMs\n  based on Federated Learning. MeanCache is the first cache that can handle\n  contextual queries efficiently. Total pages 14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.05740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.05740v2",
                "updated": "2024-07-15T18:38:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    18,
                    38,
                    54,
                    0,
                    197,
                    0
                ],
                "published": "2023-07-11T19:08:06Z",
                "published_parsed": [
                    2023,
                    7,
                    11,
                    19,
                    8,
                    6,
                    1,
                    192,
                    0
                ],
                "title": "Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor\n  Network"
                },
                "summary": "Sparse tensor decomposition and completion are common in numerous\napplications, ranging from machine learning to computational quantum chemistry.\nTypically, the main bottleneck in optimization of these models are contractions\nof a single large sparse tensor with a network of several dense matrices or\ntensors (SpTTN). Prior works on high-performance tensor decomposition and\ncompletion have focused on performance and scalability optimizations for\nspecific SpTTN kernels. We present algorithms and a runtime system for\nidentifying and executing the most efficient loop nest for any SpTTN kernel. We\nconsider both enumeration of such loop nests for autotuning and efficient\nalgorithms for finding the lowest cost loop-nest for simpler metrics, such as\nbuffer size or cache miss models. Our runtime system identifies the best choice\nof loop nest without user guidance, and also provides a distributed-memory\nparallelization of SpTTN kernels. We evaluate our framework using both\nreal-world and synthetic tensors. Our results demonstrate that our approach\noutperforms available generalized state-of-the-art libraries and matches the\nperformance of specialized codes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse tensor decomposition and completion are common in numerous\napplications, ranging from machine learning to computational quantum chemistry.\nTypically, the main bottleneck in optimization of these models are contractions\nof a single large sparse tensor with a network of several dense matrices or\ntensors (SpTTN). Prior works on high-performance tensor decomposition and\ncompletion have focused on performance and scalability optimizations for\nspecific SpTTN kernels. We present algorithms and a runtime system for\nidentifying and executing the most efficient loop nest for any SpTTN kernel. We\nconsider both enumeration of such loop nests for autotuning and efficient\nalgorithms for finding the lowest cost loop-nest for simpler metrics, such as\nbuffer size or cache miss models. Our runtime system identifies the best choice\nof loop nest without user guidance, and also provides a distributed-memory\nparallelization of SpTTN kernels. We evaluate our framework using both\nreal-world and synthetic tensors. Our results demonstrate that our approach\noutperforms available generalized state-of-the-art libraries and matches the\nperformance of specialized codes."
                },
                "authors": [
                    {
                        "name": "Raghavendra Kanakagiri"
                    },
                    {
                        "name": "Edgar Solomonik"
                    }
                ],
                "author_detail": {
                    "name": "Edgar Solomonik"
                },
                "author": "Edgar Solomonik",
                "arxiv_doi": "10.1145/3626183.3659985",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626183.3659985",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.05740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.05740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.3; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v1",
                "updated": "2024-07-15T17:25:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v1",
                "updated": "2024-07-15T14:09:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02350v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02350v3",
                "updated": "2024-07-15T14:00:24Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    0,
                    24,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-02T15:16:06Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    15,
                    16,
                    6,
                    1,
                    184,
                    0
                ],
                "title": "Conceptual Codebook Learning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conceptual Codebook Learning for Vision-Language Models"
                },
                "summary": "In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel\nfine-tuning method for vision-language models (VLMs) to address the challenge\nof improving the generalization capability of VLMs while fine-tuning them on\ndownstream tasks in a few-shot setting. We recognize that visual concepts, such\nas textures, shapes, and colors are naturally transferable across domains and\nplay a crucial role in generalization tasks. Motivated by this interesting\nfinding, we learn a conceptual codebook consisting of visual concepts as keys\nand conceptual prompts as values, which serves as a link between the image\nencoder's outputs and the text encoder's inputs. Specifically, for a given\nimage, we leverage the codebook to identify the most relevant conceptual\nprompts associated with the class embeddings to perform the classification.\nAdditionally, we incorporate a handcrafted concept cache as a regularization to\nalleviate the overfitting issues in low-shot scenarios. We observe that this\nconceptual codebook learning method is able to achieve enhanced alignment\nbetween visual and linguistic modalities. Extensive experimental results\ndemonstrate that our CoCoLe method remarkably outperforms the existing\nstate-of-the-art methods across various evaluation settings, including\nbase-to-new generalization, cross-dataset evaluation, and domain generalization\ntasks. Detailed ablation studies further confirm the efficacy of each component\nin CoCoLe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel\nfine-tuning method for vision-language models (VLMs) to address the challenge\nof improving the generalization capability of VLMs while fine-tuning them on\ndownstream tasks in a few-shot setting. We recognize that visual concepts, such\nas textures, shapes, and colors are naturally transferable across domains and\nplay a crucial role in generalization tasks. Motivated by this interesting\nfinding, we learn a conceptual codebook consisting of visual concepts as keys\nand conceptual prompts as values, which serves as a link between the image\nencoder's outputs and the text encoder's inputs. Specifically, for a given\nimage, we leverage the codebook to identify the most relevant conceptual\nprompts associated with the class embeddings to perform the classification.\nAdditionally, we incorporate a handcrafted concept cache as a regularization to\nalleviate the overfitting issues in low-shot scenarios. We observe that this\nconceptual codebook learning method is able to achieve enhanced alignment\nbetween visual and linguistic modalities. Extensive experimental results\ndemonstrate that our CoCoLe method remarkably outperforms the existing\nstate-of-the-art methods across various evaluation settings, including\nbase-to-new generalization, cross-dataset evaluation, and domain generalization\ntasks. Detailed ablation studies further confirm the efficacy of each component\nin CoCoLe."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Ke Yu"
                    },
                    {
                        "name": "Siqi Wu"
                    },
                    {
                        "name": "Zhihai He"
                    }
                ],
                "author_detail": {
                    "name": "Zhihai He"
                },
                "author": "Zhihai He",
                "arxiv_comment": "Accepted by ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02350v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02350v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.07702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07702v1",
                "updated": "2024-08-14T17:59:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    59,
                    4,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T17:59:04Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    59,
                    4,
                    2,
                    227,
                    0
                ],
                "title": "The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned\n  Language Models"
                },
                "summary": "Schema linking is a crucial step in Text-to-SQL pipelines, which translate\nnatural language queries into SQL. The goal of schema linking is to retrieve\nrelevant tables and columns (signal) while disregarding irrelevant ones\n(noise). However, imperfect schema linking can often exclude essential columns\nneeded for accurate query generation. In this work, we revisit the need for\nschema linking when using the latest generation of large language models\n(LLMs). We find empirically that newer models are adept at identifying relevant\nschema elements during generation, without the need for explicit schema\nlinking. This allows Text-to-SQL pipelines to bypass schema linking entirely\nand instead pass the full database schema to the LLM, eliminating the risk of\nexcluding necessary information. Furthermore, as alternatives to schema\nlinking, we propose techniques that improve Text-to-SQL accuracy without\ncompromising on essential schema information. Our approach achieves 71.83\\%\nexecution accuracy on the BIRD benchmark, ranking first at the time of\nsubmission.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema linking is a crucial step in Text-to-SQL pipelines, which translate\nnatural language queries into SQL. The goal of schema linking is to retrieve\nrelevant tables and columns (signal) while disregarding irrelevant ones\n(noise). However, imperfect schema linking can often exclude essential columns\nneeded for accurate query generation. In this work, we revisit the need for\nschema linking when using the latest generation of large language models\n(LLMs). We find empirically that newer models are adept at identifying relevant\nschema elements during generation, without the need for explicit schema\nlinking. This allows Text-to-SQL pipelines to bypass schema linking entirely\nand instead pass the full database schema to the LLM, eliminating the risk of\nexcluding necessary information. Furthermore, as alternatives to schema\nlinking, we propose techniques that improve Text-to-SQL accuracy without\ncompromising on essential schema information. Our approach achieves 71.83\\%\nexecution accuracy on the BIRD benchmark, ranking first at the time of\nsubmission."
                },
                "authors": [
                    {
                        "name": "Karime Maamari"
                    },
                    {
                        "name": "Fadhil Abubaker"
                    },
                    {
                        "name": "Daniel Jaroslawicz"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07700v1",
                "updated": "2024-08-14T17:57:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    57,
                    18,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T17:57:18Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    57,
                    18,
                    2,
                    227,
                    0
                ],
                "title": "Profile Likelihoods in Cosmology: When, Why and How illustrated with\n  $Λ$CDM, Massive Neutrinos and Dark Energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profile Likelihoods in Cosmology: When, Why and How illustrated with\n  $Λ$CDM, Massive Neutrinos and Dark Energy"
                },
                "summary": "Frequentist parameter inference using profile likelihoods has received\nincreased attention in the cosmology literature recently since it can give\nimportant complementary information to Bayesian credible intervals. Here, we\ngive a pedagogical review to frequentist parameter inference in cosmology with\nparticular focus on when the graphical profile likelihood construction gives\nmeaningful constraints, i.e.\\ confidence intervals with correct coverage. This\nconstruction rests on the assumption of the asymptotic limit of a large data\nset such as in \\textit{Wilks' theorem}. We assess the validity of this\nassumption in the context of three cosmological models with \\textit{Planck}\n2018 \\texttt{Plik\\_lite} data: While our tests for the $\\Lambda$CDM model\nindicate that the profile likelihood method gives correct coverage,\n$\\Lambda$CDM with the sum of neutrino masses as a free parameter appears\nconsistent with a Gaussian near a boundary motivating the use of the\nboundary-corrected or Feldman-Cousins graphical method; for $w_0$CDM with the\nequation of state of dark energy, $w_0$, as a free parameter, we find\nindication of a violation of the assumptions. Finally, we compare frequentist\nand Bayesian constraints of these models. Our results motivate care when using\nthe graphical profile likelihood method in cosmology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequentist parameter inference using profile likelihoods has received\nincreased attention in the cosmology literature recently since it can give\nimportant complementary information to Bayesian credible intervals. Here, we\ngive a pedagogical review to frequentist parameter inference in cosmology with\nparticular focus on when the graphical profile likelihood construction gives\nmeaningful constraints, i.e.\\ confidence intervals with correct coverage. This\nconstruction rests on the assumption of the asymptotic limit of a large data\nset such as in \\textit{Wilks' theorem}. We assess the validity of this\nassumption in the context of three cosmological models with \\textit{Planck}\n2018 \\texttt{Plik\\_lite} data: While our tests for the $\\Lambda$CDM model\nindicate that the profile likelihood method gives correct coverage,\n$\\Lambda$CDM with the sum of neutrino masses as a free parameter appears\nconsistent with a Gaussian near a boundary motivating the use of the\nboundary-corrected or Feldman-Cousins graphical method; for $w_0$CDM with the\nequation of state of dark energy, $w_0$, as a free parameter, we find\nindication of a violation of the assumptions. Finally, we compare frequentist\nand Bayesian constraints of these models. Our results motivate care when using\nthe graphical profile likelihood method in cosmology."
                },
                "authors": [
                    {
                        "name": "Laura Herold"
                    },
                    {
                        "name": "Elisa G. M. Ferreira"
                    },
                    {
                        "name": "Lukas Heinrich"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Heinrich"
                },
                "author": "Lukas Heinrich",
                "arxiv_comment": "25 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06947v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06947v2",
                "updated": "2024-08-14T17:53:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    53,
                    21,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T15:05:45Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    5,
                    45,
                    1,
                    226,
                    0
                ],
                "title": "Kilonova Light Curve Parameter Estimation Using Likelihood-Free\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilonova Light Curve Parameter Estimation Using Likelihood-Free\n  Inference"
                },
                "summary": "We present a parameter estimation algorithm on kilonova light curves using\nlikelihood-free inference. Our inference is optimized through a pre-trained\nembedding network that marginalizes the time of arrival and the luminosity\ndistance of the signal. We find that parameter inference utilizing a\npre-trained embedding outperforms the use of likelihood-free inference alone,\nreducing training time and offering the capability to marginalize over certain\nnuisance parameters. The model is capable of retrieving the intrinsic\nparameters of the kilonova light curves with a comparable accuracy and\nprecision to nested sampling methods while taking significantly less\ncomputational time. This framework has been integrated into the publicly\navailable Nuclear Multi-Messenger Astronomy codebase so users can leverage the\nmodel for their inference purposes. This algorithm is broadly applicable to\nparameterized or simulated light curves of other transient objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a parameter estimation algorithm on kilonova light curves using\nlikelihood-free inference. Our inference is optimized through a pre-trained\nembedding network that marginalizes the time of arrival and the luminosity\ndistance of the signal. We find that parameter inference utilizing a\npre-trained embedding outperforms the use of likelihood-free inference alone,\nreducing training time and offering the capability to marginalize over certain\nnuisance parameters. The model is capable of retrieving the intrinsic\nparameters of the kilonova light curves with a comparable accuracy and\nprecision to nested sampling methods while taking significantly less\ncomputational time. This framework has been integrated into the publicly\navailable Nuclear Multi-Messenger Astronomy codebase so users can leverage the\nmodel for their inference purposes. This algorithm is broadly applicable to\nparameterized or simulated light curves of other transient objects."
                },
                "authors": [
                    {
                        "name": "Malina Desai"
                    },
                    {
                        "name": "Deep Chatterjee"
                    },
                    {
                        "name": "Sahil Jhawar"
                    },
                    {
                        "name": "Philip Harris"
                    },
                    {
                        "name": "Erik Katsavounidis"
                    },
                    {
                        "name": "Michael Coughlin"
                    }
                ],
                "author_detail": {
                    "name": "Michael Coughlin"
                },
                "author": "Michael Coughlin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06947v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07686v1",
                "updated": "2024-08-14T17:39:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    39,
                    54,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T17:39:54Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    39,
                    54,
                    2,
                    227,
                    0
                ],
                "title": "Kilonova Emissions from Neutron Star Merger Remnants: Implications for\n  Nuclear Equation of State",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilonova Emissions from Neutron Star Merger Remnants: Implications for\n  Nuclear Equation of State"
                },
                "summary": "Multi-messenger observation of binary neutron-star mergers can provide\nvaluable information on the nuclear equation of state (EoS). Here, we\ninvestigate to which extent electromagnetic observations of the associated\nkilonovae allow us to place constraints on the EoS. For this, we use\nstate-of-the-art three-dimensional general-relativistic magneto-hydrodynamics\nsimulations and detailed nucleosynthesis modeling to connect properties of\nobserved light curves to properties of the accretion disk, and hence, the EoS.\nUsing our general approach, we use multi-messenger observations of\nGW170817/AT2017gfo to study the impact of various sources of uncertainty on\ninferences of the EoS. We constrain the radius of a $\\rm{1.4 M_\\odot}$ neutron\nstar to lie within $\\rm{10.19\\leq R_{1.4}\\leq 13.0}$~km and the maximum mass to\nbe $\\rm{M_{TOV}\\leq 3.06 M_\\odot}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-messenger observation of binary neutron-star mergers can provide\nvaluable information on the nuclear equation of state (EoS). Here, we\ninvestigate to which extent electromagnetic observations of the associated\nkilonovae allow us to place constraints on the EoS. For this, we use\nstate-of-the-art three-dimensional general-relativistic magneto-hydrodynamics\nsimulations and detailed nucleosynthesis modeling to connect properties of\nobserved light curves to properties of the accretion disk, and hence, the EoS.\nUsing our general approach, we use multi-messenger observations of\nGW170817/AT2017gfo to study the impact of various sources of uncertainty on\ninferences of the EoS. We constrain the radius of a $\\rm{1.4 M_\\odot}$ neutron\nstar to lie within $\\rm{10.19\\leq R_{1.4}\\leq 13.0}$~km and the maximum mass to\nbe $\\rm{M_{TOV}\\leq 3.06 M_\\odot}$."
                },
                "authors": [
                    {
                        "name": "Kelsey A. Lund"
                    },
                    {
                        "name": "Rahul Somasundaram"
                    },
                    {
                        "name": "Gail C. McLaughlin"
                    },
                    {
                        "name": "Jonah M. Miller"
                    },
                    {
                        "name": "Matthew R. Mumpower"
                    },
                    {
                        "name": "Ingo Tews"
                    }
                ],
                "author_detail": {
                    "name": "Ingo Tews"
                },
                "author": "Ingo Tews",
                "arxiv_comment": "12 pages, 4 figures. Comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21090v2",
                "updated": "2024-08-14T17:35:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    35,
                    21,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-30T16:56:21Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    16,
                    56,
                    21,
                    1,
                    212,
                    0
                ],
                "title": "Learning Optimal Signal Temporal Logic Decision Trees for\n  Classification: A Max-Flow MILP Formulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Optimal Signal Temporal Logic Decision Trees for\n  Classification: A Max-Flow MILP Formulation"
                },
                "summary": "This paper presents a novel framework for inferring timed temporal logic\nproperties from data. The dataset comprises pairs of finite-time system traces\nand corresponding labels, denoting whether the traces demonstrate specific\ndesired behaviors, e.g. whether the ship follows a safe route or not. Our\nproposed approach leverages decision-tree-based methods to infer Signal\nTemporal Logic classifiers using primitive formulae. We formulate the inference\nprocess as a mixed integer linear programming optimization problem, recursively\ngenerating constraints to determine both data classification and tree\nstructure. Applying a max-flow algorithm on the resultant tree transforms the\nproblem into a global optimization challenge, leading to improved\nclassification rates compared to prior methodologies. Moreover, we introduce a\ntechnique to reduce the number of constraints by exploiting the symmetry\ninherent in STL primitives, which enhances the algorithm's time performance and\ninterpretability. To assess our algorithm's effectiveness and classification\nperformance, we conduct three case studies involving two-class, multi-class,\nand complex formula classification scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel framework for inferring timed temporal logic\nproperties from data. The dataset comprises pairs of finite-time system traces\nand corresponding labels, denoting whether the traces demonstrate specific\ndesired behaviors, e.g. whether the ship follows a safe route or not. Our\nproposed approach leverages decision-tree-based methods to infer Signal\nTemporal Logic classifiers using primitive formulae. We formulate the inference\nprocess as a mixed integer linear programming optimization problem, recursively\ngenerating constraints to determine both data classification and tree\nstructure. Applying a max-flow algorithm on the resultant tree transforms the\nproblem into a global optimization challenge, leading to improved\nclassification rates compared to prior methodologies. Moreover, we introduce a\ntechnique to reduce the number of constraints by exploiting the symmetry\ninherent in STL primitives, which enhances the algorithm's time performance and\ninterpretability. To assess our algorithm's effectiveness and classification\nperformance, we conduct three case studies involving two-class, multi-class,\nand complex formula classification scenarios."
                },
                "authors": [
                    {
                        "name": "Kaier Liang"
                    },
                    {
                        "name": "Gustavo A. Cardona"
                    },
                    {
                        "name": "Disha Kamale"
                    },
                    {
                        "name": "Cristian-Ioan Vasile"
                    }
                ],
                "author_detail": {
                    "name": "Cristian-Ioan Vasile"
                },
                "author": "Cristian-Ioan Vasile",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07666v2",
                "updated": "2024-08-15T01:49:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    1,
                    49,
                    29,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-14T16:58:48Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    58,
                    48,
                    2,
                    227,
                    0
                ],
                "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities"
                },
                "summary": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}."
                },
                "authors": [
                    {
                        "name": "Enneng Yang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Xingwei Wang"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07665v1",
                "updated": "2024-08-14T16:55:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    55,
                    6,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T16:55:06Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    55,
                    6,
                    2,
                    227,
                    0
                ],
                "title": "Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech\n  Large Language Models"
                },
                "summary": "Warning: This paper may contain texts with uncomfortable content.\n  Large Language Models (LLMs) have achieved remarkable performance in various\ntasks, including those involving multimodal data like speech. However, these\nmodels often exhibit biases due to the nature of their training data. Recently,\nmore Speech Large Language Models (SLLMs) have emerged, underscoring the urgent\nneed to address these biases. This study introduces Spoken Stereoset, a dataset\nspecifically designed to evaluate social biases in SLLMs. By examining how\ndifferent models respond to speech from diverse demographic groups, we aim to\nidentify these biases. Our experiments reveal significant insights into their\nperformance and bias levels. The findings indicate that while most models show\nminimal bias, some still exhibit slightly stereotypical or anti-stereotypical\ntendencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warning: This paper may contain texts with uncomfortable content.\n  Large Language Models (LLMs) have achieved remarkable performance in various\ntasks, including those involving multimodal data like speech. However, these\nmodels often exhibit biases due to the nature of their training data. Recently,\nmore Speech Large Language Models (SLLMs) have emerged, underscoring the urgent\nneed to address these biases. This study introduces Spoken Stereoset, a dataset\nspecifically designed to evaluate social biases in SLLMs. By examining how\ndifferent models respond to speech from diverse demographic groups, we aim to\nidentify these biases. Our experiments reveal significant insights into their\nperformance and bias levels. The findings indicate that while most models show\nminimal bias, some still exhibit slightly stereotypical or anti-stereotypical\ntendencies."
                },
                "authors": [
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Wei-Chih Chen"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11738v2",
                "updated": "2024-08-14T16:45:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    45,
                    28,
                    2,
                    227,
                    0
                ],
                "published": "2024-04-17T20:45:23Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    20,
                    45,
                    23,
                    2,
                    108,
                    0
                ],
                "title": "Data-driven computation of adjoint sensitivities without adjoint\n  solvers: An application to thermoacoustics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven computation of adjoint sensitivities without adjoint\n  solvers: An application to thermoacoustics"
                },
                "summary": "Adjoint methods have been the pillar of gradient-based optimization for\ndecades. They enable the accurate computation of a gradient (sensitivity) of a\nquantity of interest with respect to all system's parameters in one\ncalculation. When the gradient is embedded in an optimization routine, the\nquantity of interest can be optimized for the system to have the desired\nbehaviour. Adjoint methods require the system's governing equations and their\nJacobian. We propose a computational strategy to infer the adjoint\nsensitivities from data when the governing equations might be unknown (or\npartly unknown), and noise might be present. The key component of this strategy\nis an echo state network, which learns the dynamics of nonlinear regimes with\nvarying parameters, and evolves dynamically via a hidden state. Although the\nframework is general, we focus on thermoacoustics governed by nonlinear and\ntime-delayed systems. First, we show that a parameter-aware Echo State Network\n(ESN) infers the parameterized dynamics. Second, we derive the adjoint of the\nESN to compute the sensitivity to parameters and initial conditions. Third, we\npropose the Thermoacoustic Echo State Network (T-ESN), which embeds the\nphysical knowledge in the network architecture. Fourth, we apply the framework\nto a variety of nonlinear thermoacoustic regimes of a prototypical system. We\nshow that the T-ESN accurately infers the correct adjoint sensitivities of the\ntime-averaged acoustic energy with respect to the flame parameters. The results\nare robust to noisy data, from periodic, through quasiperiodic, to chaotic\nregimes. A single network predicts the nonlinear bifurcations on unseen\nscenarios, and so the inferred adjoint sensitivities are employed to suppress\nan instability via steepest descent. This work opens new possibilities for\ngradient-based data-driven design optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adjoint methods have been the pillar of gradient-based optimization for\ndecades. They enable the accurate computation of a gradient (sensitivity) of a\nquantity of interest with respect to all system's parameters in one\ncalculation. When the gradient is embedded in an optimization routine, the\nquantity of interest can be optimized for the system to have the desired\nbehaviour. Adjoint methods require the system's governing equations and their\nJacobian. We propose a computational strategy to infer the adjoint\nsensitivities from data when the governing equations might be unknown (or\npartly unknown), and noise might be present. The key component of this strategy\nis an echo state network, which learns the dynamics of nonlinear regimes with\nvarying parameters, and evolves dynamically via a hidden state. Although the\nframework is general, we focus on thermoacoustics governed by nonlinear and\ntime-delayed systems. First, we show that a parameter-aware Echo State Network\n(ESN) infers the parameterized dynamics. Second, we derive the adjoint of the\nESN to compute the sensitivity to parameters and initial conditions. Third, we\npropose the Thermoacoustic Echo State Network (T-ESN), which embeds the\nphysical knowledge in the network architecture. Fourth, we apply the framework\nto a variety of nonlinear thermoacoustic regimes of a prototypical system. We\nshow that the T-ESN accurately infers the correct adjoint sensitivities of the\ntime-averaged acoustic energy with respect to the flame parameters. The results\nare robust to noisy data, from periodic, through quasiperiodic, to chaotic\nregimes. A single network predicts the nonlinear bifurcations on unseen\nscenarios, and so the inferred adjoint sensitivities are employed to suppress\nan instability via steepest descent. This work opens new possibilities for\ngradient-based data-driven design optimization."
                },
                "authors": [
                    {
                        "name": "Defne E. Ozan"
                    },
                    {
                        "name": "Luca Magri"
                    }
                ],
                "author_detail": {
                    "name": "Luca Magri"
                },
                "author": "Luca Magri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07645v1",
                "updated": "2024-08-14T16:17:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    17,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T16:17:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    17,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "Properties of sunspot light bridges on a geometric height scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Properties of sunspot light bridges on a geometric height scale"
                },
                "summary": "Investigating light bridges (LBs) helps us comprehend key aspects of\nsunspots. However, few studies have analyzed the properties of LBs in terms of\nthe geometric height, which is a more realistic perspective given the\ncorrugation of the solar atmosphere. We aim to shed light on LBs by studying\nthe variation in their physical properties with geometric height. We used the\nSICON code to infer the physical quantities in terms of the optical depth and\nthe Wilson depression values of three LBs hosted by a sunspot observed with\nHinode/SP in the Fe I 630 nm pair lines. We also used SIR inversions to\ncross-check the height variation of the field inclination in the LBs. In both\noutput sets, we performed linear interpolation to convert the physical\nparameters from optical depth into a geometric height scale in each pixel. We\nclassified each LB as filamentary, grainy, or umbral. They appear as ridges\nthat reach different maximum heights, with the umbral LB being the deepest.\nWhile the filamentary LB hosts a plasma inflow from the penumbra, the results\nfor the grainy LB are compatible with an injection of hot plasma through\nconvective cells of reduced field strength. Only a few positions reveal hints\nsuggesting a cusp-like magnetic canopy. Moreover, strong gradients in the\nmagnetic field strength and inclination usually exhibit enhanced electric\ncurrents, with the filamentary LB having remarkably strong currents that appear\nto be related to chromospheric events. The height stratification in filamentary\nand grainy LBs differ, indicating diverse mechanisms at work. Our results are\nin general incompatible with a magnetic canopy scenario, and further analysis\nis needed to confirm whether it exists along the entire LB or only at specific\nlocations. Furthermore, this work assesses the usefulness of SICON when\ndetermining the height stratification of solar structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating light bridges (LBs) helps us comprehend key aspects of\nsunspots. However, few studies have analyzed the properties of LBs in terms of\nthe geometric height, which is a more realistic perspective given the\ncorrugation of the solar atmosphere. We aim to shed light on LBs by studying\nthe variation in their physical properties with geometric height. We used the\nSICON code to infer the physical quantities in terms of the optical depth and\nthe Wilson depression values of three LBs hosted by a sunspot observed with\nHinode/SP in the Fe I 630 nm pair lines. We also used SIR inversions to\ncross-check the height variation of the field inclination in the LBs. In both\noutput sets, we performed linear interpolation to convert the physical\nparameters from optical depth into a geometric height scale in each pixel. We\nclassified each LB as filamentary, grainy, or umbral. They appear as ridges\nthat reach different maximum heights, with the umbral LB being the deepest.\nWhile the filamentary LB hosts a plasma inflow from the penumbra, the results\nfor the grainy LB are compatible with an injection of hot plasma through\nconvective cells of reduced field strength. Only a few positions reveal hints\nsuggesting a cusp-like magnetic canopy. Moreover, strong gradients in the\nmagnetic field strength and inclination usually exhibit enhanced electric\ncurrents, with the filamentary LB having remarkably strong currents that appear\nto be related to chromospheric events. The height stratification in filamentary\nand grainy LBs differ, indicating diverse mechanisms at work. Our results are\nin general incompatible with a magnetic canopy scenario, and further analysis\nis needed to confirm whether it exists along the entire LB or only at specific\nlocations. Furthermore, this work assesses the usefulness of SICON when\ndetermining the height stratification of solar structures."
                },
                "authors": [
                    {
                        "name": "S. Esteban Pozuelo"
                    },
                    {
                        "name": "A. Asensio Ramos"
                    },
                    {
                        "name": "C. J. Díaz Baso"
                    },
                    {
                        "name": "B. Ruiz Cobo"
                    }
                ],
                "author_detail": {
                    "name": "B. Ruiz Cobo"
                },
                "author": "B. Ruiz Cobo",
                "arxiv_comment": "13 pages, 12 figures. Accepted for publication in A&A. Abstract has\n  been abridged",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.01076v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.01076v3",
                "updated": "2024-08-14T16:14:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    14,
                    16,
                    2,
                    227,
                    0
                ],
                "published": "2022-06-02T14:50:05Z",
                "published_parsed": [
                    2022,
                    6,
                    2,
                    14,
                    50,
                    5,
                    3,
                    153,
                    0
                ],
                "title": "Likelihood-based Inference for Random Networks with Changepoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood-based Inference for Random Networks with Changepoints"
                },
                "summary": "Generative, temporal network models play an important role in analyzing the\ndependence structure and evolution patterns of complex networks. Due to the\ncomplicated nature of real network data, it is often naive to assume that the\nunderlying data-generative mechanism itself is invariant with time. Such\nobservation leads to the study of changepoints or sudden shifts in the\ndistributional structure of the evolving network. In this paper, we propose a\nlikelihood-based methodology to detect changepoints in undirected, affine\npreferential attachment networks, and establish a hypothesis testing framework\nto detect a single changepoint, together with a consistent estimator for the\nchangepoint. Such results require establishing consistency and asymptotic\nnormality of the MLE under the changepoint regime, which suffers from long\nrange dependence. The methodology is then extended to the multiple changepoint\nsetting via both a sliding window method and a more computationally efficient\nscore statistic. We also compare the proposed methodology with previously\ndeveloped non-parametric estimators of the changepoint via simulation, and the\nmethods developed herein are applied to modeling the popularity of a topic in a\nTwitter network over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative, temporal network models play an important role in analyzing the\ndependence structure and evolution patterns of complex networks. Due to the\ncomplicated nature of real network data, it is often naive to assume that the\nunderlying data-generative mechanism itself is invariant with time. Such\nobservation leads to the study of changepoints or sudden shifts in the\ndistributional structure of the evolving network. In this paper, we propose a\nlikelihood-based methodology to detect changepoints in undirected, affine\npreferential attachment networks, and establish a hypothesis testing framework\nto detect a single changepoint, together with a consistent estimator for the\nchangepoint. Such results require establishing consistency and asymptotic\nnormality of the MLE under the changepoint regime, which suffers from long\nrange dependence. The methodology is then extended to the multiple changepoint\nsetting via both a sliding window method and a more computationally efficient\nscore statistic. We also compare the proposed methodology with previously\ndeveloped non-parametric estimators of the changepoint via simulation, and the\nmethods developed herein are applied to modeling the popularity of a topic in a\nTwitter network over time."
                },
                "authors": [
                    {
                        "name": "Daniel Cirkovic"
                    },
                    {
                        "name": "Tiandong Wang"
                    },
                    {
                        "name": "Xianyang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xianyang Zhang"
                },
                "author": "Xianyang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2206.01076v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.01076v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "05C82, 60F05, 60G44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3; I.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01198v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01198v4",
                "updated": "2024-08-14T16:10:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    10,
                    38,
                    2,
                    227,
                    0
                ],
                "published": "2023-10-02T13:37:28Z",
                "published_parsed": [
                    2023,
                    10,
                    2,
                    13,
                    37,
                    28,
                    0,
                    275,
                    0
                ],
                "title": "Likelihood Based Inference for ARMA Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood Based Inference for ARMA Models"
                },
                "summary": "Autoregressive moving average (ARMA) models are frequently used to analyze\ntime series data. Despite the popularity of these models, likelihood-based\ninference for ARMA models has subtleties that have been previously identified\nbut continue to cause difficulties in widely used data analysis strategies. We\ndiscuss common pitfalls that may lead to sub-optimal maximum likelihood\nparameter estimates. We propose a random initialization algorithm for parameter\nestimation that frequently yields higher likelihoods than traditional maximum\nlikelihood estimation procedures. We then investigate the parameter uncertainty\nof maximum likelihood estimates, and propose the use of profile confidence\nintervals as a superior alternative to intervals derived from the Fisher\ninformation matrix. Through a data analysis example and a series of simulation\nstudies, we demonstrate the efficacy of our proposed algorithm and the improved\nnominal coverage of profile confidence intervals compared to the normal\napproximation based on Fisher information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive moving average (ARMA) models are frequently used to analyze\ntime series data. Despite the popularity of these models, likelihood-based\ninference for ARMA models has subtleties that have been previously identified\nbut continue to cause difficulties in widely used data analysis strategies. We\ndiscuss common pitfalls that may lead to sub-optimal maximum likelihood\nparameter estimates. We propose a random initialization algorithm for parameter\nestimation that frequently yields higher likelihoods than traditional maximum\nlikelihood estimation procedures. We then investigate the parameter uncertainty\nof maximum likelihood estimates, and propose the use of profile confidence\nintervals as a superior alternative to intervals derived from the Fisher\ninformation matrix. Through a data analysis example and a series of simulation\nstudies, we demonstrate the efficacy of our proposed algorithm and the improved\nnominal coverage of profile confidence intervals compared to the normal\napproximation based on Fisher information."
                },
                "authors": [
                    {
                        "name": "Jesse Wheeler"
                    },
                    {
                        "name": "Edward L. Ionides"
                    }
                ],
                "author_detail": {
                    "name": "Edward L. Ionides"
                },
                "author": "Edward L. Ionides",
                "arxiv_comment": "The developmental version of the R package used in this paper is\n  available at the following GitHub repository:\n  git@github.com:jeswheel/arima2.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01198v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01198v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17762v2",
                "updated": "2024-08-14T16:00:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    0,
                    49,
                    2,
                    227,
                    0
                ],
                "published": "2024-02-27T18:55:17Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    18,
                    55,
                    17,
                    1,
                    58,
                    0
                ],
                "title": "Massive Activations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Activations in Large Language Models"
                },
                "summary": "We observe an empirical phenomenon in Large Language Models (LLMs) -- very\nfew activations exhibit significantly larger values than others (e.g., 100,000\ntimes larger). We call them massive activations. First, we demonstrate the\nwidespread existence of massive activations across various LLMs and\ncharacterize their locations. Second, we find their values largely stay\nconstant regardless of the input, and they function as indispensable bias terms\nin LLMs. Third, these massive activations lead to the concentration of\nattention probabilities to their corresponding tokens, and further, implicit\nbias terms in the self-attention output. Last, we also study massive\nactivations in Vision Transformers. Code is available at\nhttps://github.com/locuslab/massive-activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We observe an empirical phenomenon in Large Language Models (LLMs) -- very\nfew activations exhibit significantly larger values than others (e.g., 100,000\ntimes larger). We call them massive activations. First, we demonstrate the\nwidespread existence of massive activations across various LLMs and\ncharacterize their locations. Second, we find their values largely stay\nconstant regardless of the input, and they function as indispensable bias terms\nin LLMs. Third, these massive activations lead to the concentration of\nattention probabilities to their corresponding tokens, and further, implicit\nbias terms in the self-attention output. Last, we also study massive\nactivations in Vision Transformers. Code is available at\nhttps://github.com/locuslab/massive-activations."
                },
                "authors": [
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Zhuang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Liu"
                },
                "author": "Zhuang Liu",
                "arxiv_comment": "First Conference on Language Modeling (COLM), 2024. Website at\n  https://eric-mingjie.github.io/massive-activations/index.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06583v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06583v3",
                "updated": "2024-08-15T15:24:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    24,
                    10,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-13T02:43:19Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    43,
                    19,
                    1,
                    226,
                    0
                ],
                "title": "An Event Structure-aware Generative Model for Biomedical Event\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Event Structure-aware Generative Model for Biomedical Event\n  Extraction"
                },
                "summary": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. BEE has traditionally been formulated as a classification problem. With\nthe recent technological advancements in large language models (LLMs),\ngeneration-based models that cast event extraction as a sequence generation\nproblem have attracted much attention from the NLP research communities.\nHowever, current generative models often overlook the importance of\ncross-instance information from complex event structures such as nested events\nand overlapping events, which contribute quite significantly in the benchmark\ndatasets. In this paper, we propose an event structure-aware generative model\ncalled GenBEE, which can capture complex event structures in biomedical text\nfor biomedical event extraction. In particular, GenBEE constructs event prompts\nthat distill knowledge from LLMs for incorporating both label semantics and\nargument dependency relationships into the proposed model. In addition, GenBEE\nalso generates prefixes with event structural prompts to incorporate structural\nfeatures for improving the model's overall performance. We have evaluated the\nproposed GenBEE model on three widely used biomedical event extraction\nbenchmark datasets, namely MLEE, GE11, and PHEE. Experimental results show that\nGenBEE has achieved state-of-the-art performance on the MLEE and GE11 datasets,\nand achieved competitive results when compared to the state-of-the-art\nclassification-based models on the PHEE dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. BEE has traditionally been formulated as a classification problem. With\nthe recent technological advancements in large language models (LLMs),\ngeneration-based models that cast event extraction as a sequence generation\nproblem have attracted much attention from the NLP research communities.\nHowever, current generative models often overlook the importance of\ncross-instance information from complex event structures such as nested events\nand overlapping events, which contribute quite significantly in the benchmark\ndatasets. In this paper, we propose an event structure-aware generative model\ncalled GenBEE, which can capture complex event structures in biomedical text\nfor biomedical event extraction. In particular, GenBEE constructs event prompts\nthat distill knowledge from LLMs for incorporating both label semantics and\nargument dependency relationships into the proposed model. In addition, GenBEE\nalso generates prefixes with event structural prompts to incorporate structural\nfeatures for improving the model's overall performance. We have evaluated the\nproposed GenBEE model on three widely used biomedical event extraction\nbenchmark datasets, namely MLEE, GE11, and PHEE. Experimental results show that\nGenBEE has achieved state-of-the-art performance on the MLEE and GE11 datasets,\nand achieved competitive results when compared to the state-of-the-art\nclassification-based models on the PHEE dataset."
                },
                "authors": [
                    {
                        "name": "Haohan Yuan"
                    },
                    {
                        "name": "Siu Cheung Hui"
                    },
                    {
                        "name": "Haopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haopeng Zhang"
                },
                "author": "Haopeng Zhang",
                "arxiv_comment": "8 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06583v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06583v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.00645v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.00645v2",
                "updated": "2024-08-14T15:35:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    35,
                    12,
                    2,
                    227,
                    0
                ],
                "published": "2023-05-01T03:35:43Z",
                "published_parsed": [
                    2023,
                    5,
                    1,
                    3,
                    35,
                    43,
                    0,
                    121,
                    0
                ],
                "title": "GTree: GPU-Friendly Privacy-preserving Decision Tree Training and\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTree: GPU-Friendly Privacy-preserving Decision Tree Training and\n  Inference"
                },
                "summary": "Decision tree (DT) is a widely used machine learning model due to its\nversatility, speed, and interpretability. However, for privacy-sensitive\napplications, outsourcing DT training and inference to cloud platforms raise\nconcerns about data privacy. Researchers have developed privacy-preserving\napproaches for DT training and inference using cryptographic primitives, such\nas Secure Multi-Party Computation (MPC). While these approaches have shown\nprogress, they still suffer from heavy computation and communication overheads.\nFew recent works employ Graphical Processing Units (GPU) to improve the\nperformance of MPC-protected deep learning. This raises a natural question:\n\\textit{can MPC-protected DT training and inference be accelerated by GPU?}\n  We present GTree, the first scheme that uses GPU to accelerate MPC-protected\nsecure DT training and inference. GTree is built across 3 parties who securely\nand jointly perform each step of DT training and inference with GPU. Each MPC\nprotocol in GTree is designed in a GPU-friendly version. The performance\nevaluation shows that GTree achieves ${\\thicksim}11{\\times}$ and\n${\\thicksim}21{\\times}$ improvements in training SPECT and Adult datasets,\ncompared to the prior most efficient CPU-based work. For inference, GTree shows\nits superior efficiency when the DT has less than 10 levels, which is\n$126\\times$ faster than the prior most efficient work when inferring $10^4$\ninstances with a tree of 7 levels. GTree also achieves a stronger security\nguarantee than prior solutions, which only leaks the tree depth and size of\ndata samples while prior solutions also leak the tree structure. With\n\\textit{oblivious array access}, the access pattern on GPU is also protected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision tree (DT) is a widely used machine learning model due to its\nversatility, speed, and interpretability. However, for privacy-sensitive\napplications, outsourcing DT training and inference to cloud platforms raise\nconcerns about data privacy. Researchers have developed privacy-preserving\napproaches for DT training and inference using cryptographic primitives, such\nas Secure Multi-Party Computation (MPC). While these approaches have shown\nprogress, they still suffer from heavy computation and communication overheads.\nFew recent works employ Graphical Processing Units (GPU) to improve the\nperformance of MPC-protected deep learning. This raises a natural question:\n\\textit{can MPC-protected DT training and inference be accelerated by GPU?}\n  We present GTree, the first scheme that uses GPU to accelerate MPC-protected\nsecure DT training and inference. GTree is built across 3 parties who securely\nand jointly perform each step of DT training and inference with GPU. Each MPC\nprotocol in GTree is designed in a GPU-friendly version. The performance\nevaluation shows that GTree achieves ${\\thicksim}11{\\times}$ and\n${\\thicksim}21{\\times}$ improvements in training SPECT and Adult datasets,\ncompared to the prior most efficient CPU-based work. For inference, GTree shows\nits superior efficiency when the DT has less than 10 levels, which is\n$126\\times$ faster than the prior most efficient work when inferring $10^4$\ninstances with a tree of 7 levels. GTree also achieves a stronger security\nguarantee than prior solutions, which only leaks the tree depth and size of\ndata samples while prior solutions also leak the tree structure. With\n\\textit{oblivious array access}, the access pattern on GPU is also protected."
                },
                "authors": [
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Shujie Cui"
                    },
                    {
                        "name": "Lei Zhou"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jianli Bai"
                    },
                    {
                        "name": "Yun Sing Koh"
                    },
                    {
                        "name": "Giovanni Russello"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Russello"
                },
                "author": "Giovanni Russello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.00645v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.00645v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15363v2",
                "updated": "2024-08-14T15:32:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    32,
                    25,
                    2,
                    227,
                    0
                ],
                "published": "2024-04-01T15:17:39Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    15,
                    17,
                    39,
                    0,
                    92,
                    0
                ],
                "title": "Exploring LLM Multi-Agents for ICD Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLM Multi-Agents for ICD Coding"
                },
                "summary": "To address the limitations of Large Language Models (LLMs) in the\nInternational Classification of Diseases (ICD) coding task, where they often\nproduce inaccurate and incomplete prediction results due to the\nhigh-dimensional and skewed distribution of the ICD codes, and often lack\ninterpretability and reliability as well. We introduce an innovative\nmulti-agent approach for ICD coding which mimics the ICD coding assignment\nprocedure in real-world settings, comprising five distinct agents: the patient,\nphysician, coder, reviewer, and adjuster. Each agent utilizes an LLM-based\nmodel tailored to their specific role within the coding process. We also\nintegrate the system with Electronic Health Record (HER)'s SOAP (subjective,\nobjective, assessment and plan) structure to boost the performances. We compare\nour method with a system of agents designed solely by LLMs and other strong\nbaselines and evaluate it using the Medical Information Mart for Intensive Care\nIII (MIMIC-III) dataset. Our multi-agent coding framework significantly\noutperforms Zero-shot Chain of Thought (CoT) prompting and self-consistency\nwith CoT (CoT-SC) in coding common and rare ICD codes. An ablation study\nvalidates the effectiveness of the designated agent roles. it also outperforms\nthe LLM-designed agent system. Moreover, our method achieves comparable results\nto state-of-the-art ICD coding methods that require extensive pre-training or\nfine-tuning, and outperforms them in rare code accuracy, and explainability.\nAdditionally, we demonstrate the method's practical applicability by presenting\nits performance in scenarios not limited by the common or rare ICD code\nconstraints.The proposed multi-agent method for ICD coding effectively mimics\nthe real-world coding process and improves performance on both common and rare\ncodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the limitations of Large Language Models (LLMs) in the\nInternational Classification of Diseases (ICD) coding task, where they often\nproduce inaccurate and incomplete prediction results due to the\nhigh-dimensional and skewed distribution of the ICD codes, and often lack\ninterpretability and reliability as well. We introduce an innovative\nmulti-agent approach for ICD coding which mimics the ICD coding assignment\nprocedure in real-world settings, comprising five distinct agents: the patient,\nphysician, coder, reviewer, and adjuster. Each agent utilizes an LLM-based\nmodel tailored to their specific role within the coding process. We also\nintegrate the system with Electronic Health Record (HER)'s SOAP (subjective,\nobjective, assessment and plan) structure to boost the performances. We compare\nour method with a system of agents designed solely by LLMs and other strong\nbaselines and evaluate it using the Medical Information Mart for Intensive Care\nIII (MIMIC-III) dataset. Our multi-agent coding framework significantly\noutperforms Zero-shot Chain of Thought (CoT) prompting and self-consistency\nwith CoT (CoT-SC) in coding common and rare ICD codes. An ablation study\nvalidates the effectiveness of the designated agent roles. it also outperforms\nthe LLM-designed agent system. Moreover, our method achieves comparable results\nto state-of-the-art ICD coding methods that require extensive pre-training or\nfine-tuning, and outperforms them in rare code accuracy, and explainability.\nAdditionally, we demonstrate the method's practical applicability by presenting\nits performance in scenarios not limited by the common or rare ICD code\nconstraints.The proposed multi-agent method for ICD coding effectively mimics\nthe real-world coding process and improves performance on both common and rare\ncodes."
                },
                "authors": [
                    {
                        "name": "Rumeng Li"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "12pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2204.05566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2204.05566v3",
                "updated": "2024-08-14T15:31:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    31,
                    26,
                    2,
                    227,
                    0
                ],
                "published": "2022-04-12T06:53:25Z",
                "published_parsed": [
                    2022,
                    4,
                    12,
                    6,
                    53,
                    25,
                    1,
                    102,
                    0
                ],
                "title": "Compact Model Training by Low-Rank Projection with Energy Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Model Training by Low-Rank Projection with Energy Transfer"
                },
                "summary": "Low-rankness plays an important role in traditional machine learning, but is\nnot so popular in deep learning. Most previous low-rank network compression\nmethods compress networks by approximating pre-trained models and re-training.\nHowever, the optimal solution in the Euclidean space may be quite different\nfrom the one with low-rank constraint. A well-pre-trained model is not a good\ninitialization for the model with low-rank constraints. Thus, the performance\nof a low-rank compressed network degrades significantly. Compared with other\nnetwork compression methods such as pruning, low-rank methods attract less\nattention in recent years. In this paper, we devise a new training method,\nlow-rank projection with energy transfer (LRPET), that trains low-rank\ncompressed networks from scratch and achieves competitive performance. We\npropose to alternately perform stochastic gradient descent training and\nprojection of each weight matrix onto the corresponding low-rank manifold.\nCompared to re-training on the compact model, this enables full utilization of\nmodel capacity since solution space is relaxed back to Euclidean space after\nprojection. The matrix energy (the sum of squares of singular values) reduction\ncaused by projection is compensated by energy transfer. We uniformly transfer\nthe energy of the pruned singular values to the remaining ones. We\ntheoretically show that energy transfer eases the trend of gradient vanishing\ncaused by projection. In modern networks, a batch normalization (BN) layer can\nbe merged into the previous convolution layer for inference, thereby\ninfluencing the optimal low-rank approximation of the previous layer. We\npropose BN rectification to cut off its effect on the optimal low-rank\napproximation, which further improves the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rankness plays an important role in traditional machine learning, but is\nnot so popular in deep learning. Most previous low-rank network compression\nmethods compress networks by approximating pre-trained models and re-training.\nHowever, the optimal solution in the Euclidean space may be quite different\nfrom the one with low-rank constraint. A well-pre-trained model is not a good\ninitialization for the model with low-rank constraints. Thus, the performance\nof a low-rank compressed network degrades significantly. Compared with other\nnetwork compression methods such as pruning, low-rank methods attract less\nattention in recent years. In this paper, we devise a new training method,\nlow-rank projection with energy transfer (LRPET), that trains low-rank\ncompressed networks from scratch and achieves competitive performance. We\npropose to alternately perform stochastic gradient descent training and\nprojection of each weight matrix onto the corresponding low-rank manifold.\nCompared to re-training on the compact model, this enables full utilization of\nmodel capacity since solution space is relaxed back to Euclidean space after\nprojection. The matrix energy (the sum of squares of singular values) reduction\ncaused by projection is compensated by energy transfer. We uniformly transfer\nthe energy of the pruned singular values to the remaining ones. We\ntheoretically show that energy transfer eases the trend of gradient vanishing\ncaused by projection. In modern networks, a batch normalization (BN) layer can\nbe merged into the previous convolution layer for inference, thereby\ninfluencing the optimal low-rank approximation of the previous layer. We\npropose BN rectification to cut off its effect on the optimal low-rank\napproximation, which further improves the performance."
                },
                "authors": [
                    {
                        "name": "Kailing Guo"
                    },
                    {
                        "name": "Zhenquan Lin"
                    },
                    {
                        "name": "Canyang Chen"
                    },
                    {
                        "name": "Xiaofen Xing"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiangmin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangmin Xu"
                },
                "author": "Xiangmin Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2204.05566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2204.05566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07611v1",
                "updated": "2024-08-14T15:19:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    19,
                    16,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T15:19:16Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    19,
                    16,
                    2,
                    227,
                    0
                ],
                "title": "WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation\n  Integrating Web Search and Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation\n  Integrating Web Search and Knowledge Graphs"
                },
                "summary": "Large Language Models (LLMs) have greatly contributed to the development of\nadaptive intelligent agents and are positioned as an important way to achieve\nArtificial General Intelligence (AGI). However, LLMs are prone to produce\nfactually incorrect information and often produce \"phantom\" content that\nundermines their reliability, which poses a serious challenge for their\ndeployment in real-world scenarios. Enhancing LLMs by combining external\ndatabases and information retrieval mechanisms is an effective path. To address\nthe above challenges, we propose a new approach called WeKnow-RAG, which\nintegrates Web search and Knowledge Graphs into a \"Retrieval-Augmented\nGeneration (RAG)\" system. First, the accuracy and reliability of LLM responses\nare improved by combining the structured representation of Knowledge Graphs\nwith the flexibility of dense vector retrieval. WeKnow-RAG then utilizes\ndomain-specific knowledge graphs to satisfy a variety of queries and domains,\nthereby improving performance on factual information and complex reasoning\ntasks by employing multi-stage web page retrieval techniques using both sparse\nand dense retrieval methods. Our approach effectively balances the efficiency\nand accuracy of information retrieval, thus improving the overall retrieval\nprocess. Finally, we also integrate a self-assessment mechanism for the LLM to\nevaluate the trustworthiness of the answers it generates. Our approach proves\nits outstanding effectiveness in a wide range of offline experiments and online\nsubmissions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have greatly contributed to the development of\nadaptive intelligent agents and are positioned as an important way to achieve\nArtificial General Intelligence (AGI). However, LLMs are prone to produce\nfactually incorrect information and often produce \"phantom\" content that\nundermines their reliability, which poses a serious challenge for their\ndeployment in real-world scenarios. Enhancing LLMs by combining external\ndatabases and information retrieval mechanisms is an effective path. To address\nthe above challenges, we propose a new approach called WeKnow-RAG, which\nintegrates Web search and Knowledge Graphs into a \"Retrieval-Augmented\nGeneration (RAG)\" system. First, the accuracy and reliability of LLM responses\nare improved by combining the structured representation of Knowledge Graphs\nwith the flexibility of dense vector retrieval. WeKnow-RAG then utilizes\ndomain-specific knowledge graphs to satisfy a variety of queries and domains,\nthereby improving performance on factual information and complex reasoning\ntasks by employing multi-stage web page retrieval techniques using both sparse\nand dense retrieval methods. Our approach effectively balances the efficiency\nand accuracy of information retrieval, thus improving the overall retrieval\nprocess. Finally, we also integrate a self-assessment mechanism for the LLM to\nevaluate the trustworthiness of the answers it generates. Our approach proves\nits outstanding effectiveness in a wide range of offline experiments and online\nsubmissions."
                },
                "authors": [
                    {
                        "name": "Weijian Xie"
                    },
                    {
                        "name": "Xuefeng Liang"
                    },
                    {
                        "name": "Yuhui Liu"
                    },
                    {
                        "name": "Kaihua Ni"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Zetian Hu"
                    }
                ],
                "author_detail": {
                    "name": "Zetian Hu"
                },
                "author": "Zetian Hu",
                "arxiv_comment": "8 pages, 2 figures, technical report for 3rd place in Task 3 of Meta\n  KDD Cup 2024 CRAG Challenge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07594v1",
                "updated": "2024-08-14T14:49:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    49,
                    25,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T14:49:25Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    49,
                    25,
                    2,
                    227,
                    0
                ],
                "title": "Crossover Designs in Software Engineering Experiments: Review of the\n  State of Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crossover Designs in Software Engineering Experiments: Review of the\n  State of Analysis"
                },
                "summary": "Experimentation is an essential method for causal inference in any empirical\ndiscipline. Crossover-design experiments are common in Software Engineering\n(SE) research. In these, subjects apply more than one treatment in different\norders. This design increases the amount of obtained data and deals with\nsubject variability but introduces threats to internal validity like the\nlearning and carryover effect. Vegas et al. reviewed the state of practice for\ncrossover designs in SE research and provided guidelines on how to address its\nthreats during data analysis while still harnessing its benefits. In this\npaper, we reflect on the impact of these guidelines and review the state of\nanalysis of crossover design experiments in SE publications between 2015 and\nMarch 2024. To this end, by conducting a forward snowballing of the guidelines,\nwe survey 136 publications reporting 67 crossover-design experiments and\nevaluate their data analysis against the provided guidelines. The results show\nthat the validity of data analyses has improved compared to the original state\nof analysis. Still, despite the explicit guidelines, only 29.5% of all threats\nto validity were addressed properly. While the maturation and the optimal\nsequence threats are properly addressed in 35.8% and 38.8% of all studies in\nour sample respectively, the carryover threat is only modeled in about 3% of\nthe observed cases. The lack of adherence to the analysis guidelines threatens\nthe validity of the conclusions drawn from crossover design experiments",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimentation is an essential method for causal inference in any empirical\ndiscipline. Crossover-design experiments are common in Software Engineering\n(SE) research. In these, subjects apply more than one treatment in different\norders. This design increases the amount of obtained data and deals with\nsubject variability but introduces threats to internal validity like the\nlearning and carryover effect. Vegas et al. reviewed the state of practice for\ncrossover designs in SE research and provided guidelines on how to address its\nthreats during data analysis while still harnessing its benefits. In this\npaper, we reflect on the impact of these guidelines and review the state of\nanalysis of crossover design experiments in SE publications between 2015 and\nMarch 2024. To this end, by conducting a forward snowballing of the guidelines,\nwe survey 136 publications reporting 67 crossover-design experiments and\nevaluate their data analysis against the provided guidelines. The results show\nthat the validity of data analyses has improved compared to the original state\nof analysis. Still, despite the explicit guidelines, only 29.5% of all threats\nto validity were addressed properly. While the maturation and the optimal\nsequence threats are properly addressed in 35.8% and 38.8% of all studies in\nour sample respectively, the carryover threat is only modeled in about 3% of\nthe observed cases. The lack of adherence to the analysis guidelines threatens\nthe validity of the conclusions drawn from crossover design experiments"
                },
                "authors": [
                    {
                        "name": "Julian Frattini"
                    },
                    {
                        "name": "Davide Fucci"
                    },
                    {
                        "name": "Sira Vegas"
                    }
                ],
                "author_detail": {
                    "name": "Sira Vegas"
                },
                "author": "Sira Vegas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07592v1",
                "updated": "2024-08-14T14:48:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    48,
                    13,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T14:48:13Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    48,
                    13,
                    2,
                    227,
                    0
                ],
                "title": "Multi-periodicity dependency Transformer based on spectrum offset for\n  radio frequency fingerprint identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-periodicity dependency Transformer based on spectrum offset for\n  radio frequency fingerprint identification"
                },
                "summary": "Radio Frequency Fingerprint Identification (RFFI) has emerged as a pivotal\ntask for reliable device authentication. Despite advancements in RFFI methods,\nbackground noise and intentional modulation features result in weak energy and\nsubtle differences in the RFF features. These challenges diminish the\ncapability of RFFI methods in feature representation, complicating the\neffective identification of device identities. This paper proposes a novel\nMulti-Periodicity Dependency Transformer (MPDFormer) to address these\nchallenges. The MPDFormer employs a spectrum offset-based periodic embedding\nrepresentation to augment the discrepency of intrinsic features. We delve into\nthe intricacies of the periodicity-dependency attention mechanism, integrating\nboth inter-period and intra-period attention mechanisms. This mechanism\nfacilitates the extraction of both long and short-range periodicity-dependency\nfeatures , accentuating the feature distinction whilst concurrently attenuating\nthe perturbations caused by background noise and weak-periodicity features.\nEmpirical results demonstrate MPDFormer's superiority over established baseline\nmethods, achieving a 0.07s inference time on NVIDIA Jetson Orin NX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio Frequency Fingerprint Identification (RFFI) has emerged as a pivotal\ntask for reliable device authentication. Despite advancements in RFFI methods,\nbackground noise and intentional modulation features result in weak energy and\nsubtle differences in the RFF features. These challenges diminish the\ncapability of RFFI methods in feature representation, complicating the\neffective identification of device identities. This paper proposes a novel\nMulti-Periodicity Dependency Transformer (MPDFormer) to address these\nchallenges. The MPDFormer employs a spectrum offset-based periodic embedding\nrepresentation to augment the discrepency of intrinsic features. We delve into\nthe intricacies of the periodicity-dependency attention mechanism, integrating\nboth inter-period and intra-period attention mechanisms. This mechanism\nfacilitates the extraction of both long and short-range periodicity-dependency\nfeatures , accentuating the feature distinction whilst concurrently attenuating\nthe perturbations caused by background noise and weak-periodicity features.\nEmpirical results demonstrate MPDFormer's superiority over established baseline\nmethods, achieving a 0.07s inference time on NVIDIA Jetson Orin NX."
                },
                "authors": [
                    {
                        "name": "Jing Xiao"
                    },
                    {
                        "name": "Wenrui Ding"
                    },
                    {
                        "name": "Zeqi Shao"
                    },
                    {
                        "name": "Duona Zhang"
                    },
                    {
                        "name": "Yanan Ma"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01419v2",
                "updated": "2024-08-14T14:42:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    42,
                    32,
                    2,
                    227,
                    0
                ],
                "published": "2024-05-02T16:08:08Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    8,
                    3,
                    123,
                    0
                ],
                "title": "Natural Language to Verilog: Design of a Recurrent Spiking Neural\n  Network using Large Language Models and ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language to Verilog: Design of a Recurrent Spiking Neural\n  Network using Large Language Models and ChatGPT"
                },
                "summary": "This paper investigates the use of Large Language Models (LLMs) for\nautomating the generation of hardware description code, aiming to explore their\npotential in supporting and enhancing the development of efficient neuromorphic\ncomputing architectures. Building on our prior work, we employ OpenAI's\nChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a\nprogrammable recurrent spiking neural network, while also generating test\nbenches to assess the system's correctness. The resultant design was validated\nin three case studies, the exclusive OR,the IRIS flower classification and the\nMNIST hand-written digit classification, achieving accuracies of up to 96.6%.\nTo verify its synthesizability and implementability, the design was prototyped\non a field-programmable gate array and implemented on SkyWater 130 nm\ntechnology by using an open-source electronic design automation flow.\nAdditionally, we have submitted it to Tiny Tapeout 6 chip fabrication program\nto further evaluate the system on-chip performance in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the use of Large Language Models (LLMs) for\nautomating the generation of hardware description code, aiming to explore their\npotential in supporting and enhancing the development of efficient neuromorphic\ncomputing architectures. Building on our prior work, we employ OpenAI's\nChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a\nprogrammable recurrent spiking neural network, while also generating test\nbenches to assess the system's correctness. The resultant design was validated\nin three case studies, the exclusive OR,the IRIS flower classification and the\nMNIST hand-written digit classification, achieving accuracies of up to 96.6%.\nTo verify its synthesizability and implementability, the design was prototyped\non a field-programmable gate array and implemented on SkyWater 130 nm\ntechnology by using an open-source electronic design automation flow.\nAdditionally, we have submitted it to Tiny Tapeout 6 chip fabrication program\nto further evaluate the system on-chip performance in the future."
                },
                "authors": [
                    {
                        "name": "Paola Vitolo"
                    },
                    {
                        "name": "George Psaltakis"
                    },
                    {
                        "name": "Michael Tomlinson"
                    },
                    {
                        "name": "Gian Domenico Licciardo"
                    },
                    {
                        "name": "Andreas G. Andreou"
                    }
                ],
                "author_detail": {
                    "name": "Andreas G. Andreou"
                },
                "author": "Andreas G. Andreou",
                "arxiv_comment": "This paper was presented at the IEEE/ACM International Conference on\n  Neuromorphic Systems (ICONS), July 30-Aug 2, 2024, Arlington, VA, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07583v1",
                "updated": "2024-08-14T14:28:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    28,
                    11,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T14:28:11Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    28,
                    11,
                    2,
                    227,
                    0
                ],
                "title": "Transformers and Large Language Models for Efficient Intrusion Detection\n  Systems: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers and Large Language Models for Efficient Intrusion Detection\n  Systems: A Comprehensive Survey"
                },
                "summary": "With significant advancements in Transformers LLMs, NLP has extended its\nreach into many research fields due to its enhanced capabilities in text\ngeneration and user interaction. One field benefiting greatly from these\nadvancements is cybersecurity. In cybersecurity, many parameters that need to\nbe protected and exchanged between senders and receivers are in the form of\ntext and tabular data, making NLP a valuable tool in enhancing the security\nmeasures of communication protocols. This survey paper provides a comprehensive\nanalysis of the utilization of Transformers and LLMs in cyber-threat detection\nsystems. The methodology of paper selection and bibliometric analysis is\noutlined to establish a rigorous framework for evaluating existing research.\nThe fundamentals of Transformers are discussed, including background\ninformation on various cyber-attacks and datasets commonly used in this field.\nThe survey explores the application of Transformers in IDSs, focusing on\ndifferent architectures such as Attention-based models, LLMs like BERT and GPT,\nCNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.\nFurthermore, it explores the diverse environments and applications where\nTransformers and LLMs-based IDS have been implemented, including computer\nnetworks, IoT devices, critical infrastructure protection, cloud computing,\nSDN, as well as in autonomous vehicles. The paper also addresses research\nchallenges and future directions in this area, identifying key issues such as\ninterpretability, scalability, and adaptability to evolving threats, and more.\nFinally, the conclusion summarizes the findings and highlights the significance\nof Transformers and LLMs in enhancing cyber-threat detection capabilities,\nwhile also outlining potential avenues for further research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With significant advancements in Transformers LLMs, NLP has extended its\nreach into many research fields due to its enhanced capabilities in text\ngeneration and user interaction. One field benefiting greatly from these\nadvancements is cybersecurity. In cybersecurity, many parameters that need to\nbe protected and exchanged between senders and receivers are in the form of\ntext and tabular data, making NLP a valuable tool in enhancing the security\nmeasures of communication protocols. This survey paper provides a comprehensive\nanalysis of the utilization of Transformers and LLMs in cyber-threat detection\nsystems. The methodology of paper selection and bibliometric analysis is\noutlined to establish a rigorous framework for evaluating existing research.\nThe fundamentals of Transformers are discussed, including background\ninformation on various cyber-attacks and datasets commonly used in this field.\nThe survey explores the application of Transformers in IDSs, focusing on\ndifferent architectures such as Attention-based models, LLMs like BERT and GPT,\nCNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.\nFurthermore, it explores the diverse environments and applications where\nTransformers and LLMs-based IDS have been implemented, including computer\nnetworks, IoT devices, critical infrastructure protection, cloud computing,\nSDN, as well as in autonomous vehicles. The paper also addresses research\nchallenges and future directions in this area, identifying key issues such as\ninterpretability, scalability, and adaptability to evolving threats, and more.\nFinally, the conclusion summarizes the findings and highlights the significance\nof Transformers and LLMs in enhancing cyber-threat detection capabilities,\nwhile also outlining potential avenues for further research and development."
                },
                "authors": [
                    {
                        "name": "Hamza Kheddar"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Kheddar"
                },
                "author": "Hamza Kheddar",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2405.04760 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01686v2",
                "updated": "2024-08-14T14:11:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    11,
                    25,
                    2,
                    227,
                    0
                ],
                "published": "2023-11-03T03:18:40Z",
                "published_parsed": [
                    2023,
                    11,
                    3,
                    3,
                    18,
                    40,
                    4,
                    307,
                    0
                ],
                "title": "Disentangled Representation Learning with Transmitted Information\n  Bottleneck",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangled Representation Learning with Transmitted Information\n  Bottleneck"
                },
                "summary": "Encoding only the task-related information from the raw data, \\ie,\ndisentangled representation learning, can greatly contribute to the robustness\nand generalizability of models. Although significant advances have been made by\nregularizing the information in representations with information theory, two\nmajor challenges remain: 1) the representation compression inevitably leads to\nperformance drop; 2) the disentanglement constraints on representations are in\ncomplicated optimization. To these issues, we introduce Bayesian networks with\ntransmitted information to formulate the interaction among input and\nrepresentations during disentanglement. Building upon this framework, we\npropose \\textbf{DisTIB} (\\textbf{T}ransmitted \\textbf{I}nformation\n\\textbf{B}ottleneck for \\textbf{Dis}entangled representation learning), a novel\nobjective that navigates the balance between information compression and\npreservation. We employ variational inference to derive a tractable estimation\nfor DisTIB. This estimation can be simply optimized via standard gradient\ndescent with a reparameterization trick. Moreover, we theoretically prove that\nDisTIB can achieve optimal disentanglement, underscoring its superior efficacy.\nTo solidify our claims, we conduct extensive experiments on various downstream\ntasks to demonstrate the appealing efficacy of DisTIB and validate our\ntheoretical analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoding only the task-related information from the raw data, \\ie,\ndisentangled representation learning, can greatly contribute to the robustness\nand generalizability of models. Although significant advances have been made by\nregularizing the information in representations with information theory, two\nmajor challenges remain: 1) the representation compression inevitably leads to\nperformance drop; 2) the disentanglement constraints on representations are in\ncomplicated optimization. To these issues, we introduce Bayesian networks with\ntransmitted information to formulate the interaction among input and\nrepresentations during disentanglement. Building upon this framework, we\npropose \\textbf{DisTIB} (\\textbf{T}ransmitted \\textbf{I}nformation\n\\textbf{B}ottleneck for \\textbf{Dis}entangled representation learning), a novel\nobjective that navigates the balance between information compression and\npreservation. We employ variational inference to derive a tractable estimation\nfor DisTIB. This estimation can be simply optimized via standard gradient\ndescent with a reparameterization trick. Moreover, we theoretically prove that\nDisTIB can achieve optimal disentanglement, underscoring its superior efficacy.\nTo solidify our claims, we conduct extensive experiments on various downstream\ntasks to demonstrate the appealing efficacy of DisTIB and validate our\ntheoretical analyses."
                },
                "authors": [
                    {
                        "name": "Zhuohang Dang"
                    },
                    {
                        "name": "Minnan Luo"
                    },
                    {
                        "name": "Chengyou Jia"
                    },
                    {
                        "name": "Guang Dai"
                    },
                    {
                        "name": "Jihong Wang"
                    },
                    {
                        "name": "Xiaojun Chang"
                    },
                    {
                        "name": "Jingdong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingdong Wang"
                },
                "author": "Jingdong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07569v1",
                "updated": "2024-08-14T14:06:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    6,
                    13,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T14:06:13Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    6,
                    13,
                    2,
                    227,
                    0
                ],
                "title": "Multi-task Heterogeneous Graph Learning on Electronic Health Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task Heterogeneous Graph Learning on Electronic Health Records"
                },
                "summary": "Learning electronic health records (EHRs) has received emerging attention\nbecause of its capability to facilitate accurate medical diagnosis. Since the\nEHRs contain enriched information specifying complex interactions between\nentities, modeling EHRs with graphs is shown to be effective in practice. The\nEHRs, however, present a great degree of heterogeneity, sparsity, and\ncomplexity, which hamper the performance of most of the models applied to them.\nMoreover, existing approaches modeling EHRs often focus on learning the\nrepresentations for a single task, overlooking the multi-task nature of EHR\nanalysis problems and resulting in limited generalizability across different\ntasks. In view of these limitations, we propose a novel framework for EHR\nmodeling, namely MulT-EHR (Multi-Task EHR), which leverages a heterogeneous\ngraph to mine the complex relations and model the heterogeneity in the EHRs. To\nmitigate the large degree of noise, we introduce a denoising module based on\nthe causal inference framework to adjust for severe confounding effects and\nreduce noise in the EHR data. Additionally, since our model adopts a single\ngraph neural network for simultaneous multi-task prediction, we design a\nmulti-task learning module to leverage the inter-task knowledge to regularize\nthe training process. Extensive empirical studies on MIMIC-III and MIMIC-IV\ndatasets validate that the proposed method consistently outperforms the\nstate-of-the-art designs in four popular EHR analysis tasks -- drug\nrecommendation, and predictions of the length of stay, mortality, and\nreadmission. Thorough ablation studies demonstrate the robustness of our method\nupon variations to key components and hyperparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning electronic health records (EHRs) has received emerging attention\nbecause of its capability to facilitate accurate medical diagnosis. Since the\nEHRs contain enriched information specifying complex interactions between\nentities, modeling EHRs with graphs is shown to be effective in practice. The\nEHRs, however, present a great degree of heterogeneity, sparsity, and\ncomplexity, which hamper the performance of most of the models applied to them.\nMoreover, existing approaches modeling EHRs often focus on learning the\nrepresentations for a single task, overlooking the multi-task nature of EHR\nanalysis problems and resulting in limited generalizability across different\ntasks. In view of these limitations, we propose a novel framework for EHR\nmodeling, namely MulT-EHR (Multi-Task EHR), which leverages a heterogeneous\ngraph to mine the complex relations and model the heterogeneity in the EHRs. To\nmitigate the large degree of noise, we introduce a denoising module based on\nthe causal inference framework to adjust for severe confounding effects and\nreduce noise in the EHR data. Additionally, since our model adopts a single\ngraph neural network for simultaneous multi-task prediction, we design a\nmulti-task learning module to leverage the inter-task knowledge to regularize\nthe training process. Extensive empirical studies on MIMIC-III and MIMIC-IV\ndatasets validate that the proposed method consistently outperforms the\nstate-of-the-art designs in four popular EHR analysis tasks -- drug\nrecommendation, and predictions of the length of stay, mortality, and\nreadmission. Thorough ablation studies demonstrate the robustness of our method\nupon variations to key components and hyperparameters."
                },
                "authors": [
                    {
                        "name": "Tsai Hor Chan"
                    },
                    {
                        "name": "Guosheng Yin"
                    },
                    {
                        "name": "Kyongtae Bae"
                    },
                    {
                        "name": "Lequan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lequan Yu"
                },
                "author": "Lequan Yu",
                "arxiv_comment": "Accepted by Neural Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.11090v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.11090v3",
                "updated": "2024-08-14T14:03:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    3,
                    43,
                    2,
                    227,
                    0
                ],
                "published": "2023-07-20T17:59:59Z",
                "published_parsed": [
                    2023,
                    7,
                    20,
                    17,
                    59,
                    59,
                    3,
                    201,
                    0
                ],
                "title": "Testing the assumptions of the Effective Field Theory of Large-Scale\n  Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing the assumptions of the Effective Field Theory of Large-Scale\n  Structure"
                },
                "summary": "The Effective Field Theory of Large-Scale Structure (EFTofLSS) attempts to\namend some of the shortcomings of the traditional perturbative methods used in\ncosmology. It models the evolution of long-wavelength perturbations above a\ncutoff scale without the need for a detailed description of the\nshort-wavelength ones. Short-scale physics is encoded in the coefficients of a\nseries of operators composed of the long-wavelength fields, and ordered in a\nsystematic expansion. As applied in the literature, the EFTofLSS corrects a\nsummary statistic (such as the power spectrum) calculated from standard\nperturbation theory by matching it to $N$-body simulations or observations.\nThis `bottom-up' construction is remarkably successful in extending the range\nof validity of perturbation theory. In this work, we compare this framework to\na `top-down' approach, which estimates the EFT coefficients from the stress\ntensor of an $N$-body simulation, and propagates the corrections to the summary\nstatistic. We consider simple initial conditions, viz. two sinusoidal,\nplane-parallel density perturbations with substantially different frequencies\nand amplitudes. We find that the leading EFT correction to the power spectrum\nin the top-down model is in excellent agreement with that inferred from the\nbottom-up approach which, by construction, provides an exact match to the\nnumerical data. This result is robust to changes in the wavelength separation\nbetween the two linear perturbations. However, in our setup, the leading EFT\ncoefficient does not always grow linearly with the cosmic expansion factor as\nassumed in the literature based on perturbative considerations. Instead, it\ndecreases after orbit crossing takes place.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Effective Field Theory of Large-Scale Structure (EFTofLSS) attempts to\namend some of the shortcomings of the traditional perturbative methods used in\ncosmology. It models the evolution of long-wavelength perturbations above a\ncutoff scale without the need for a detailed description of the\nshort-wavelength ones. Short-scale physics is encoded in the coefficients of a\nseries of operators composed of the long-wavelength fields, and ordered in a\nsystematic expansion. As applied in the literature, the EFTofLSS corrects a\nsummary statistic (such as the power spectrum) calculated from standard\nperturbation theory by matching it to $N$-body simulations or observations.\nThis `bottom-up' construction is remarkably successful in extending the range\nof validity of perturbation theory. In this work, we compare this framework to\na `top-down' approach, which estimates the EFT coefficients from the stress\ntensor of an $N$-body simulation, and propagates the corrections to the summary\nstatistic. We consider simple initial conditions, viz. two sinusoidal,\nplane-parallel density perturbations with substantially different frequencies\nand amplitudes. We find that the leading EFT correction to the power spectrum\nin the top-down model is in excellent agreement with that inferred from the\nbottom-up approach which, by construction, provides an exact match to the\nnumerical data. This result is robust to changes in the wavelength separation\nbetween the two linear perturbations. However, in our setup, the leading EFT\ncoefficient does not always grow linearly with the cosmic expansion factor as\nassumed in the literature based on perturbative considerations. Instead, it\ndecreases after orbit crossing takes place."
                },
                "authors": [
                    {
                        "name": "Mandar Karandikar"
                    },
                    {
                        "name": "Cristiano Porciani"
                    },
                    {
                        "name": "Oliver Hahn"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Hahn"
                },
                "author": "Oliver Hahn",
                "arxiv_doi": "10.1088/1475-7516/2024/01/051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1475-7516/2024/01/051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.11090v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.11090v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "34 pages, 19 figures; updated to match published version",
                "arxiv_journal_ref": "JCAP01(2024)051",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07560v1",
                "updated": "2024-08-14T13:52:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    52,
                    29,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:52:29Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    52,
                    29,
                    2,
                    227,
                    0
                ],
                "title": "Variant Specific Treatment Effects with Applications in Vaccine Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variant Specific Treatment Effects with Applications in Vaccine Studies"
                },
                "summary": "Pathogens usually exist in heterogeneous variants, like subtypes and strains.\nQuantifying treatment effects on the different variants is important for\nguiding prevention policies and treatment development. Here we ground analyses\nof variant-specific effects on a formal framework for causal inference. This\nallows us to clarify the interpretation of existing methods and define new\nestimands. Unlike most of the existing literature, we explicitly consider the\n(realistic) setting with interference in the target population: even if\nindividuals can be sensibly perceived as iid in randomized trial data, there\nwill often be interference in the target population where treatments, like\nvaccines, are rolled out. Thus, one of our contributions is to derive explicit\nconditions guaranteeing that commonly reported vaccine efficacy parameters\nquantify well-defined causal effects, also in the presence of interference.\nFurthermore, our results give alternative justifications for reporting\nestimands on the relative, rather than absolute, scale. We illustrate the\nfindings with an analysis of a large HIV1 vaccine trial, where there is\ninterest in distinguishing vaccine effects on viruses with different genome\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathogens usually exist in heterogeneous variants, like subtypes and strains.\nQuantifying treatment effects on the different variants is important for\nguiding prevention policies and treatment development. Here we ground analyses\nof variant-specific effects on a formal framework for causal inference. This\nallows us to clarify the interpretation of existing methods and define new\nestimands. Unlike most of the existing literature, we explicitly consider the\n(realistic) setting with interference in the target population: even if\nindividuals can be sensibly perceived as iid in randomized trial data, there\nwill often be interference in the target population where treatments, like\nvaccines, are rolled out. Thus, one of our contributions is to derive explicit\nconditions guaranteeing that commonly reported vaccine efficacy parameters\nquantify well-defined causal effects, also in the presence of interference.\nFurthermore, our results give alternative justifications for reporting\nestimands on the relative, rather than absolute, scale. We illustrate the\nfindings with an analysis of a large HIV1 vaccine trial, where there is\ninterest in distinguishing vaccine effects on viruses with different genome\nsequences."
                },
                "authors": [
                    {
                        "name": "Gellert Perenyi"
                    },
                    {
                        "name": "Mats J. Stensrud"
                    }
                ],
                "author_detail": {
                    "name": "Mats J. Stensrud"
                },
                "author": "Mats J. Stensrud",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10683v2",
                "updated": "2024-08-14T13:43:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    43,
                    28,
                    2,
                    227,
                    0
                ],
                "published": "2024-03-15T21:06:14Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    21,
                    6,
                    14,
                    4,
                    75,
                    0
                ],
                "title": "GS-Pose: Generalizable Segmentation-based 6D Object Pose Estimation with\n  3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-Pose: Generalizable Segmentation-based 6D Object Pose Estimation with\n  3D Gaussian Splatting"
                },
                "summary": "This paper introduces GS-Pose, a unified framework for localizing and\nestimating the 6D pose of novel objects. GS-Pose begins with a set of posed RGB\nimages of a previously unseen object and builds three distinct representations\nstored in a database. At inference, GS-Pose operates sequentially by locating\nthe object in the input image, estimating its initial 6D pose using a retrieval\napproach, and refining the pose with a render-and-compare method. The key\ninsight is the application of the appropriate object representation at each\nstage of the process. In particular, for the refinement step, we leverage 3D\nGaussian splatting, a novel differentiable rendering technique that offers high\nrendering speed and relatively low optimization time. Off-the-shelf toolchains\nand commodity hardware, such as mobile phones, can be used to capture new\nobjects to be added to the database. Extensive evaluations on the LINEMOD and\nOnePose-LowTexture datasets demonstrate excellent performance, establishing the\nnew state-of-the-art. Project page: https://dingdingcai.github.io/gs-pose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GS-Pose, a unified framework for localizing and\nestimating the 6D pose of novel objects. GS-Pose begins with a set of posed RGB\nimages of a previously unseen object and builds three distinct representations\nstored in a database. At inference, GS-Pose operates sequentially by locating\nthe object in the input image, estimating its initial 6D pose using a retrieval\napproach, and refining the pose with a render-and-compare method. The key\ninsight is the application of the appropriate object representation at each\nstage of the process. In particular, for the refinement step, we leverage 3D\nGaussian splatting, a novel differentiable rendering technique that offers high\nrendering speed and relatively low optimization time. Off-the-shelf toolchains\nand commodity hardware, such as mobile phones, can be used to capture new\nobjects to be added to the database. Extensive evaluations on the LINEMOD and\nOnePose-LowTexture datasets demonstrate excellent performance, establishing the\nnew state-of-the-art. Project page: https://dingdingcai.github.io/gs-pose."
                },
                "authors": [
                    {
                        "name": "Dingding Cai"
                    },
                    {
                        "name": "Janne Heikkilä"
                    },
                    {
                        "name": "Esa Rahtu"
                    }
                ],
                "author_detail": {
                    "name": "Esa Rahtu"
                },
                "author": "Esa Rahtu",
                "arxiv_comment": "Project Page: https://dingdingcai.github.io/gs-pose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07555v1",
                "updated": "2024-08-14T13:41:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    41,
                    57,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:41:57Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    41,
                    57,
                    2,
                    227,
                    0
                ],
                "title": "Inverse Rendering of Fusion Plasmas: Inferring Plasma Composition from\n  Imaging Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Rendering of Fusion Plasmas: Inferring Plasma Composition from\n  Imaging Systems"
                },
                "summary": "In this work, we develop a differentiable rendering pipeline for visualising\nplasma emission within tokamaks, and estimating the gradients of the emission\nand estimating other physical quantities. Unlike prior work, we are able to\nleverage arbitrary representations of plasma quantities and easily incorporate\nthem into a non-linear optimisation framework. The efficiency of our method\nenables not only estimation of a physically plausible image of plasma, but also\nrecovery of the neutral Deuterium distribution from imaging and midplane\nmeasurements alone. We demonstrate our method with three different levels of\ncomplexity showing first that a poloidal neutrals density distribution can be\nrecovered from imaging alone, second that the distributions of neutral\nDeuterium, electron density and electron temperature can be recovered jointly,\nand finally, that this can be done in the presence of realistic imaging systems\nthat incorporate sensor cropping and quantisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we develop a differentiable rendering pipeline for visualising\nplasma emission within tokamaks, and estimating the gradients of the emission\nand estimating other physical quantities. Unlike prior work, we are able to\nleverage arbitrary representations of plasma quantities and easily incorporate\nthem into a non-linear optimisation framework. The efficiency of our method\nenables not only estimation of a physically plausible image of plasma, but also\nrecovery of the neutral Deuterium distribution from imaging and midplane\nmeasurements alone. We demonstrate our method with three different levels of\ncomplexity showing first that a poloidal neutrals density distribution can be\nrecovered from imaging alone, second that the distributions of neutral\nDeuterium, electron density and electron temperature can be recovered jointly,\nand finally, that this can be done in the presence of realistic imaging systems\nthat incorporate sensor cropping and quantisation."
                },
                "authors": [
                    {
                        "name": "Ekin Öztürk"
                    },
                    {
                        "name": "Rob Akers"
                    },
                    {
                        "name": "Stanislas Pamela"
                    },
                    {
                        "name": "The MAST Team"
                    },
                    {
                        "name": "Pieter Peers"
                    },
                    {
                        "name": "Abhijeet Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Abhijeet Ghosh"
                },
                "author": "Abhijeet Ghosh",
                "arxiv_comment": "22 pages, 8 figures, 3 tables, submitted to Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05826v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05826v4",
                "updated": "2024-08-14T13:41:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    41,
                    41,
                    2,
                    227,
                    0
                ],
                "published": "2023-12-10T08:59:43Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    59,
                    43,
                    6,
                    344,
                    0
                ],
                "title": "R2Human: Real-Time 3D Human Appearance Rendering from a Single Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R2Human: Real-Time 3D Human Appearance Rendering from a Single Image"
                },
                "summary": "Rendering 3D human appearance from a single image in real-time is crucial for\nachieving holographic communication and immersive VR/AR. Existing methods\neither rely on multi-camera setups or are constrained to offline operations. In\nthis paper, we propose R2Human, the first approach for real-time inference and\nrendering of photorealistic 3D human appearance from a single image. The core\nof our approach is to combine the strengths of implicit texture fields and\nexplicit neural rendering with our novel representation, namely Z-map. Based on\nthis, we present an end-to-end network that performs high-fidelity color\nreconstruction of visible areas and provides reliable color inference for\noccluded regions. To further enhance the 3D perception ability of our network,\nwe leverage the Fourier occupancy field as a prior for generating the texture\nfield and providing a sampling surface in the rendering stage. We also propose\na consistency loss and a spatial fusion strategy to ensure the multi-view\ncoherence. Experimental results show that our method outperforms the\nstate-of-the-art methods on both synthetic data and challenging real-world\nimages, in real-time. The project page can be found at\nhttp://cic.tju.edu.cn/faculty/likun/projects/R2Human.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rendering 3D human appearance from a single image in real-time is crucial for\nachieving holographic communication and immersive VR/AR. Existing methods\neither rely on multi-camera setups or are constrained to offline operations. In\nthis paper, we propose R2Human, the first approach for real-time inference and\nrendering of photorealistic 3D human appearance from a single image. The core\nof our approach is to combine the strengths of implicit texture fields and\nexplicit neural rendering with our novel representation, namely Z-map. Based on\nthis, we present an end-to-end network that performs high-fidelity color\nreconstruction of visible areas and provides reliable color inference for\noccluded regions. To further enhance the 3D perception ability of our network,\nwe leverage the Fourier occupancy field as a prior for generating the texture\nfield and providing a sampling surface in the rendering stage. We also propose\na consistency loss and a spatial fusion strategy to ensure the multi-view\ncoherence. Experimental results show that our method outperforms the\nstate-of-the-art methods on both synthetic data and challenging real-world\nimages, in real-time. The project page can be found at\nhttp://cic.tju.edu.cn/faculty/likun/projects/R2Human."
                },
                "authors": [
                    {
                        "name": "Yuanwang Yang"
                    },
                    {
                        "name": "Qiao Feng"
                    },
                    {
                        "name": "Yu-Kun Lai"
                    },
                    {
                        "name": "Kun Li"
                    }
                ],
                "author_detail": {
                    "name": "Kun Li"
                },
                "author": "Kun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05826v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05826v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01766v3",
                "updated": "2024-08-14T13:41:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    41,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-01-31T14:52:02Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    14,
                    52,
                    2,
                    2,
                    31,
                    0
                ],
                "title": "LLM Voting: Human Choices and AI Collective Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Voting: Human Choices and AI Collective Decision Making"
                },
                "summary": "This paper investigates the voting behaviors of Large Language Models (LLMs),\nspecifically GPT-4 and LLaMA-2, their biases, and how they align with human\nvoting patterns. Our methodology involved using a dataset from a human voting\nexperiment to establish a baseline for human preferences and conducting a\ncorresponding experiment with LLM agents. We observed that the choice of voting\nmethods and the presentation order influenced LLM voting outcomes. We found\nthat varying the persona can reduce some of these biases and enhance alignment\nwith human choices. While the Chain-of-Thought approach did not improve\nprediction accuracy, it has potential for AI explainability in the voting\nprocess. We also identified a trade-off between preference diversity and\nalignment accuracy in LLMs, influenced by different temperature settings. Our\nfindings indicate that LLMs may lead to less diverse collective outcomes and\nbiased assumptions when used in voting scenarios, emphasizing the need for\ncautious integration of LLMs into democratic processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the voting behaviors of Large Language Models (LLMs),\nspecifically GPT-4 and LLaMA-2, their biases, and how they align with human\nvoting patterns. Our methodology involved using a dataset from a human voting\nexperiment to establish a baseline for human preferences and conducting a\ncorresponding experiment with LLM agents. We observed that the choice of voting\nmethods and the presentation order influenced LLM voting outcomes. We found\nthat varying the persona can reduce some of these biases and enhance alignment\nwith human choices. While the Chain-of-Thought approach did not improve\nprediction accuracy, it has potential for AI explainability in the voting\nprocess. We also identified a trade-off between preference diversity and\nalignment accuracy in LLMs, influenced by different temperature settings. Our\nfindings indicate that LLMs may lead to less diverse collective outcomes and\nbiased assumptions when used in voting scenarios, emphasizing the need for\ncautious integration of LLMs into democratic processes."
                },
                "authors": [
                    {
                        "name": "Joshua C. Yang"
                    },
                    {
                        "name": "Damian Dailisan"
                    },
                    {
                        "name": "Marcin Korecki"
                    },
                    {
                        "name": "Carina I. Hausladen"
                    },
                    {
                        "name": "Dirk Helbing"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Helbing"
                },
                "author": "Dirk Helbing",
                "arxiv_comment": "Accepted in AAAI Conference on AI, Ethics, and Society (AIES)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 91B14, 91C20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07547v1",
                "updated": "2024-08-14T13:36:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    36,
                    17,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:36:17Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    36,
                    17,
                    2,
                    227,
                    0
                ],
                "title": "PeriodWave: Multi-Period Flow Matching for High-Fidelity Waveform\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeriodWave: Multi-Period Flow Matching for High-Fidelity Waveform\n  Generation"
                },
                "summary": "Recently, universal waveform generation tasks have been investigated\nconditioned on various out-of-distribution scenarios. Although GAN-based\nmethods have shown their strength in fast waveform generation, they are\nvulnerable to train-inference mismatch scenarios such as two-stage\ntext-to-speech. Meanwhile, diffusion-based models have shown their powerful\ngenerative performance in other domains; however, they stay out of the\nlimelight due to slow inference speed in waveform generation tasks. Above all,\nthere is no generator architecture that can explicitly disentangle the natural\nperiodic features of high-resolution waveform signals. In this paper, we\npropose PeriodWave, a novel universal waveform generation model. First, we\nintroduce a period-aware flow matching estimator that can capture the periodic\nfeatures of the waveform signal when estimating the vector fields.\nAdditionally, we utilize a multi-period estimator that avoids overlaps to\ncapture different periodic features of waveform signals. Although increasing\nthe number of periods can improve the performance significantly, this requires\nmore computational costs. To reduce this issue, we also propose a single\nperiod-conditional universal estimator that can feed-forward parallel by\nperiod-wise batch inference. Additionally, we utilize discrete wavelet\ntransform to losslessly disentangle the frequency information of waveform\nsignals for high-frequency modeling, and introduce FreeU to reduce the\nhigh-frequency noise for waveform generation. The experimental results\ndemonstrated that our model outperforms the previous models both in\nMel-spectrogram reconstruction and text-to-speech tasks. All source code will\nbe available at \\url{https://github.com/sh-lee-prml/PeriodWave}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, universal waveform generation tasks have been investigated\nconditioned on various out-of-distribution scenarios. Although GAN-based\nmethods have shown their strength in fast waveform generation, they are\nvulnerable to train-inference mismatch scenarios such as two-stage\ntext-to-speech. Meanwhile, diffusion-based models have shown their powerful\ngenerative performance in other domains; however, they stay out of the\nlimelight due to slow inference speed in waveform generation tasks. Above all,\nthere is no generator architecture that can explicitly disentangle the natural\nperiodic features of high-resolution waveform signals. In this paper, we\npropose PeriodWave, a novel universal waveform generation model. First, we\nintroduce a period-aware flow matching estimator that can capture the periodic\nfeatures of the waveform signal when estimating the vector fields.\nAdditionally, we utilize a multi-period estimator that avoids overlaps to\ncapture different periodic features of waveform signals. Although increasing\nthe number of periods can improve the performance significantly, this requires\nmore computational costs. To reduce this issue, we also propose a single\nperiod-conditional universal estimator that can feed-forward parallel by\nperiod-wise batch inference. Additionally, we utilize discrete wavelet\ntransform to losslessly disentangle the frequency information of waveform\nsignals for high-frequency modeling, and introduce FreeU to reduce the\nhigh-frequency noise for waveform generation. The experimental results\ndemonstrated that our model outperforms the previous models both in\nMel-spectrogram reconstruction and text-to-speech tasks. All source code will\nbe available at \\url{https://github.com/sh-lee-prml/PeriodWave}."
                },
                "authors": [
                    {
                        "name": "Sang-Hoon Lee"
                    },
                    {
                        "name": "Ha-Yeong Choi"
                    },
                    {
                        "name": "Seong-Whan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Whan Lee"
                },
                "author": "Seong-Whan Lee",
                "arxiv_comment": "24 pages, 16 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07545v1",
                "updated": "2024-08-14T13:31:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    31,
                    32,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:31:32Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    31,
                    32,
                    2,
                    227,
                    0
                ],
                "title": "$χ$SPN: Characteristic Interventional Sum-Product Networks for Causal\n  Inference in Hybrid Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$χ$SPN: Characteristic Interventional Sum-Product Networks for Causal\n  Inference in Hybrid Domains"
                },
                "summary": "Causal inference in hybrid domains, characterized by a mixture of discrete\nand continuous variables, presents a formidable challenge. We take a step\ntowards this direction and propose Characteristic Interventional Sum-Product\nNetwork ($\\chi$SPN) that is capable of estimating interventional distributions\nin presence of random variables drawn from mixed distributions. $\\chi$SPN uses\ncharacteristic functions in the leaves of an interventional SPN (iSPN) thereby\nproviding a unified view for discrete and continuous random variables through\nthe Fourier-Stieltjes transform of the probability measures. A neural network\nis used to estimate the parameters of the learned iSPN using the intervened\ndata. Our experiments on 3 synthetic heterogeneous datasets suggest that\n$\\chi$SPN can effectively capture the interventional distributions for both\ndiscrete and continuous variables while being expressive and causally adequate.\nWe also show that $\\chi$SPN generalize to multiple interventions while being\ntrained only on a single intervention data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference in hybrid domains, characterized by a mixture of discrete\nand continuous variables, presents a formidable challenge. We take a step\ntowards this direction and propose Characteristic Interventional Sum-Product\nNetwork ($\\chi$SPN) that is capable of estimating interventional distributions\nin presence of random variables drawn from mixed distributions. $\\chi$SPN uses\ncharacteristic functions in the leaves of an interventional SPN (iSPN) thereby\nproviding a unified view for discrete and continuous random variables through\nthe Fourier-Stieltjes transform of the probability measures. A neural network\nis used to estimate the parameters of the learned iSPN using the intervened\ndata. Our experiments on 3 synthetic heterogeneous datasets suggest that\n$\\chi$SPN can effectively capture the interventional distributions for both\ndiscrete and continuous variables while being expressive and causally adequate.\nWe also show that $\\chi$SPN generalize to multiple interventions while being\ntrained only on a single intervention data."
                },
                "authors": [
                    {
                        "name": "Harsh Poonia"
                    },
                    {
                        "name": "Moritz Willig"
                    },
                    {
                        "name": "Zhongjie Yu"
                    },
                    {
                        "name": "Matej Zečević"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Devendra Singh Dhami"
                    }
                ],
                "author_detail": {
                    "name": "Devendra Singh Dhami"
                },
                "author": "Devendra Singh Dhami",
                "arxiv_comment": "17 pages, 11 figures. Accepted as poster at UAI (Uncertainty in\n  Artificial Intelligence) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07542v1",
                "updated": "2024-08-14T13:22:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    22,
                    14,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:22:14Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    22,
                    14,
                    2,
                    227,
                    0
                ],
                "title": "New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson\n  Planning in Ugandan Secondary Schools. Prototype Quality Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson\n  Planning in Ugandan Secondary Schools. Prototype Quality Evaluation"
                },
                "summary": "Introduction: Poor educational quality in Secondary Schools is still regarded\nas one of the major struggles in 21st century Uganda - especially in rural\nareas. Research identifies several problems, including low quality or absent\nteacher lesson planning. As the government pushes towards the implementation of\na new curriculum, exiting lesson plans become obsolete and the problem is\nworsened. Using a Retrieval Augmented Generation approach, we developed a\nprototype that generates customized lesson plans based on the\ngovernment-accredited textbooks. This helps teachers create lesson plans more\nefficiently and with better quality, ensuring they are fully aligned the new\ncurriculum and the competence-based learning approach.\n  Methods: The prototype was created using Cohere LLM and Sentence Embeddings,\nand LangChain Framework - and thereafter made available on a public website.\nVector stores were trained for three new curriculum textbooks (ICT,\nMathematics, History), all at Secondary 1 Level. Twenty-four lessons plans were\ngenerated following a pseudo-random generation protocol, based on the suggested\nperiods in the textbooks. The lesson plans were analyzed regarding their\ntechnical quality by three independent raters following the Lesson Plan\nAnalysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically\ndesigned for East Africa and competence-based curriculums.\n  Results: Evaluation of 24 lesson plans using the LPAP resulted in an average\nquality of between 75 and 80%, corresponding to \"very good lesson plan\". None\nof the lesson plans scored below 65%, although one lesson plan could be argued\nto have been missing the topic. In conclusion, the quality of the generated\nlesson plans is at least comparable, if not better, than those created by\nhumans, as demonstrated in a study in Rwanda, whereby no lesson plan even\nreached the benchmark of 50%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: Poor educational quality in Secondary Schools is still regarded\nas one of the major struggles in 21st century Uganda - especially in rural\nareas. Research identifies several problems, including low quality or absent\nteacher lesson planning. As the government pushes towards the implementation of\na new curriculum, exiting lesson plans become obsolete and the problem is\nworsened. Using a Retrieval Augmented Generation approach, we developed a\nprototype that generates customized lesson plans based on the\ngovernment-accredited textbooks. This helps teachers create lesson plans more\nefficiently and with better quality, ensuring they are fully aligned the new\ncurriculum and the competence-based learning approach.\n  Methods: The prototype was created using Cohere LLM and Sentence Embeddings,\nand LangChain Framework - and thereafter made available on a public website.\nVector stores were trained for three new curriculum textbooks (ICT,\nMathematics, History), all at Secondary 1 Level. Twenty-four lessons plans were\ngenerated following a pseudo-random generation protocol, based on the suggested\nperiods in the textbooks. The lesson plans were analyzed regarding their\ntechnical quality by three independent raters following the Lesson Plan\nAnalysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically\ndesigned for East Africa and competence-based curriculums.\n  Results: Evaluation of 24 lesson plans using the LPAP resulted in an average\nquality of between 75 and 80%, corresponding to \"very good lesson plan\". None\nof the lesson plans scored below 65%, although one lesson plan could be argued\nto have been missing the topic. In conclusion, the quality of the generated\nlesson plans is at least comparable, if not better, than those created by\nhumans, as demonstrated in a study in Rwanda, whereby no lesson plan even\nreached the benchmark of 50%."
                },
                "authors": [
                    {
                        "name": "Simon Kloker"
                    },
                    {
                        "name": "Herbertson Bukoli"
                    },
                    {
                        "name": "Twaha Kateete"
                    }
                ],
                "author_detail": {
                    "name": "Twaha Kateete"
                },
                "author": "Twaha Kateete",
                "arxiv_comment": "Presented at Ndejje University Second Annual Research Dissemination\n  Symposium 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10020v2",
                "updated": "2024-08-14T13:15:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    15,
                    0,
                    2,
                    227,
                    0
                ],
                "published": "2024-03-15T05:06:21Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    5,
                    6,
                    21,
                    4,
                    75,
                    0
                ],
                "title": "Lost in Overlap: Exploring Watermark Collision in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Overlap: Exploring Watermark Collision in LLMs"
                },
                "summary": "The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread usage of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks, such as paraphrasing or translation. In this paper, we introduce\nwatermark collision as a novel and general philosophy for watermark attacks,\naimed at enhancing attack performance on top of any other attacking methods. We\nalso provide a comprehensive demonstration that watermark collision poses a\nthreat to all logit-based watermark algorithms, impacting not only specific\nattack scenarios but also downstream applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread usage of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks, such as paraphrasing or translation. In this paper, we introduce\nwatermark collision as a novel and general philosophy for watermark attacks,\naimed at enhancing attack performance on top of any other attacking methods. We\nalso provide a comprehensive demonstration that watermark collision poses a\nthreat to all logit-based watermark algorithms, impacting not only specific\nattack scenarios but also downstream applications."
                },
                "authors": [
                    {
                        "name": "Yiyang Luo"
                    },
                    {
                        "name": "Ke Lin"
                    },
                    {
                        "name": "Chao Gu"
                    }
                ],
                "author_detail": {
                    "name": "Chao Gu"
                },
                "author": "Chao Gu",
                "arxiv_comment": "Long Paper, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07537v2",
                "updated": "2024-08-15T09:30:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    30,
                    35,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-14T13:14:27Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    14,
                    27,
                    2,
                    227,
                    0
                ],
                "title": "Usefulness of data flow diagrams and large language models for security\n  threat validation: a registered report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usefulness of data flow diagrams and large language models for security\n  threat validation: a registered report"
                },
                "summary": "The arrival of recent cybersecurity standards has raised the bar for security\nassessments in organizations, but existing techniques don't always scale well.\nThreat analysis and risk assessment are used to identify security threats for\nnew or refactored systems. Still, there is a lack of definition-of-done, so\nidentified threats have to be validated which slows down the analysis. Existing\nliterature has focused on the overall performance of threat analysis, but no\nprevious work has investigated how deep must the analysts dig into the material\nbefore they can effectively validate the identified security threats. We\npropose a controlled experiment with practitioners to investigate whether some\nanalysis material (like LLM-generated advice) is better than none and whether\nmore material (the system's data flow diagram and LLM-generated advice) is\nbetter than some material. In addition, we present key findings from running a\npilot with 41 MSc students, which are used to improve the study design.\nFinally, we also provide an initial replication package, including experimental\nmaterial and data analysis scripts and a plan to extend it to include new\nmaterials based on the final data collection campaign with practitioners (e.g.,\npre-screening questions).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The arrival of recent cybersecurity standards has raised the bar for security\nassessments in organizations, but existing techniques don't always scale well.\nThreat analysis and risk assessment are used to identify security threats for\nnew or refactored systems. Still, there is a lack of definition-of-done, so\nidentified threats have to be validated which slows down the analysis. Existing\nliterature has focused on the overall performance of threat analysis, but no\nprevious work has investigated how deep must the analysts dig into the material\nbefore they can effectively validate the identified security threats. We\npropose a controlled experiment with practitioners to investigate whether some\nanalysis material (like LLM-generated advice) is better than none and whether\nmore material (the system's data flow diagram and LLM-generated advice) is\nbetter than some material. In addition, we present key findings from running a\npilot with 41 MSc students, which are used to improve the study design.\nFinally, we also provide an initial replication package, including experimental\nmaterial and data analysis scripts and a plan to extend it to include new\nmaterials based on the final data collection campaign with practitioners (e.g.,\npre-screening questions)."
                },
                "authors": [
                    {
                        "name": "Winnie Bahati Mbaka"
                    },
                    {
                        "name": "Katja Tuma"
                    }
                ],
                "author_detail": {
                    "name": "Katja Tuma"
                },
                "author": "Katja Tuma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2201.12577v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2201.12577v5",
                "updated": "2024-08-14T13:07:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    7,
                    13,
                    2,
                    227,
                    0
                ],
                "published": "2022-01-29T12:40:19Z",
                "published_parsed": [
                    2022,
                    1,
                    29,
                    12,
                    40,
                    19,
                    5,
                    29,
                    0
                ],
                "title": "Volley Revolver: A Novel Matrix-Encoding Method for Privacy-Preserving\n  Neural Networks (Inference)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Volley Revolver: A Novel Matrix-Encoding Method for Privacy-Preserving\n  Neural Networks (Inference)"
                },
                "summary": "In this work, we present a novel matrix-encoding method that is particularly\nconvenient for neural networks to make predictions in a privacy-preserving\nmanner using homomorphic encryption. Based on this encoding method, we\nimplement a convolutional neural network for handwritten image classification\nover encryption. For two matrices $A$ and $B$ to perform homomorphic\nmultiplication, the main idea behind it, in a simple version, is to encrypt\nmatrix $A$ and the transpose of matrix $B$ into two ciphertexts respectively.\nWith additional operations, the homomorphic matrix multiplication can be\ncalculated over encrypted matrices efficiently. For the convolution operation,\nwe in advance span each convolution kernel to a matrix space of the same size\nas the input image so as to generate several ciphertexts, each of which is\nlater used together with the ciphertext encrypting input images for calculating\nsome of the final convolution results. We accumulate all these intermediate\nresults and thus complete the convolution operation.\n  In a public cloud with 40 vCPUs, our convolutional neural network\nimplementation on the MNIST testing dataset takes $\\sim$ 287 seconds to compute\nten likelihoods of 32 encrypted images of size $28 \\times 28$ simultaneously.\nThe data owner only needs to upload one ciphertext ($\\sim 19.8$ MB) encrypting\nthese 32 images to the public cloud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present a novel matrix-encoding method that is particularly\nconvenient for neural networks to make predictions in a privacy-preserving\nmanner using homomorphic encryption. Based on this encoding method, we\nimplement a convolutional neural network for handwritten image classification\nover encryption. For two matrices $A$ and $B$ to perform homomorphic\nmultiplication, the main idea behind it, in a simple version, is to encrypt\nmatrix $A$ and the transpose of matrix $B$ into two ciphertexts respectively.\nWith additional operations, the homomorphic matrix multiplication can be\ncalculated over encrypted matrices efficiently. For the convolution operation,\nwe in advance span each convolution kernel to a matrix space of the same size\nas the input image so as to generate several ciphertexts, each of which is\nlater used together with the ciphertext encrypting input images for calculating\nsome of the final convolution results. We accumulate all these intermediate\nresults and thus complete the convolution operation.\n  In a public cloud with 40 vCPUs, our convolutional neural network\nimplementation on the MNIST testing dataset takes $\\sim$ 287 seconds to compute\nten likelihoods of 32 encrypted images of size $28 \\times 28$ simultaneously.\nThe data owner only needs to upload one ciphertext ($\\sim 19.8$ MB) encrypting\nthese 32 images to the public cloud."
                },
                "authors": [
                    {
                        "name": "John Chiang"
                    }
                ],
                "author_detail": {
                    "name": "John Chiang"
                },
                "author": "John Chiang",
                "arxiv_comment": "The encoding method we proposed in this work, $\\texttt{Volley\n  Revolver}$, is particularly tailored for privacy-preserving neural networks.\n  There is a great chance that it can be used to assist the private neural\n  networks training, in which case for the backpropagation algorithm of the\n  fully-connected layer the first matrix $A$ is revolved while the second\n  matrix $B$ is settled to be still",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2201.12577v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2201.12577v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07531v1",
                "updated": "2024-08-14T13:03:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    3,
                    41,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:03:41Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    3,
                    41,
                    2,
                    227,
                    0
                ],
                "title": "Development of a Multi-Agent Clinical Decision Support System for Korean\n  Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in\n  Emergency Departments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Multi-Agent Clinical Decision Support System for Korean\n  Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in\n  Emergency Departments"
                },
                "summary": "Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation."
                },
                "authors": [
                    {
                        "name": "Seungjun Han"
                    },
                    {
                        "name": "Wongyung Choi"
                    }
                ],
                "author_detail": {
                    "name": "Wongyung Choi"
                },
                "author": "Wongyung Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07530v1",
                "updated": "2024-08-14T13:03:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    3,
                    31,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:03:31Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    3,
                    31,
                    2,
                    227,
                    0
                ],
                "title": "Towards Real-time Video Compressive Sensing on Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Real-time Video Compressive Sensing on Mobile Devices"
                },
                "summary": "Video Snapshot Compressive Imaging (SCI) uses a low-speed 2D camera to\ncapture high-speed scenes as snapshot compressed measurements, followed by a\nreconstruction algorithm to retrieve the high-speed video frames. The fast\nevolving mobile devices and existing high-performance video SCI reconstruction\nalgorithms motivate us to develop mobile reconstruction methods for real-world\napplications. Yet, it is still challenging to deploy previous reconstruction\nalgorithms on mobile devices due to the complex inference process, let alone\nreal-time mobile reconstruction. To the best of our knowledge, there is no\nvideo SCI reconstruction model designed to run on the mobile devices. Towards\nthis end, in this paper, we present an effective approach for video SCI\nreconstruction, dubbed MobileSCI, which can run at real-time speed on the\nmobile devices for the first time. Specifically, we first build a U-shaped 2D\nconvolution-based architecture, which is much more efficient and\nmobile-friendly than previous state-of-the-art reconstruction methods. Besides,\nan efficient feature mixing block, based on the channel splitting and shuffling\nmechanisms, is introduced as a novel bottleneck block of our proposed MobileSCI\nto alleviate the computational burden. Finally, a customized knowledge\ndistillation strategy is utilized to further improve the reconstruction\nquality. Extensive results on both simulated and real data show that our\nproposed MobileSCI can achieve superior reconstruction quality with high\nefficiency on the mobile devices. Particularly, we can reconstruct a 256 X 256\nX 8 snapshot compressed measurement with real-time performance (about 35 FPS)\non an iPhone 15. Code is available at https://github.com/mcao92/MobileSCI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Snapshot Compressive Imaging (SCI) uses a low-speed 2D camera to\ncapture high-speed scenes as snapshot compressed measurements, followed by a\nreconstruction algorithm to retrieve the high-speed video frames. The fast\nevolving mobile devices and existing high-performance video SCI reconstruction\nalgorithms motivate us to develop mobile reconstruction methods for real-world\napplications. Yet, it is still challenging to deploy previous reconstruction\nalgorithms on mobile devices due to the complex inference process, let alone\nreal-time mobile reconstruction. To the best of our knowledge, there is no\nvideo SCI reconstruction model designed to run on the mobile devices. Towards\nthis end, in this paper, we present an effective approach for video SCI\nreconstruction, dubbed MobileSCI, which can run at real-time speed on the\nmobile devices for the first time. Specifically, we first build a U-shaped 2D\nconvolution-based architecture, which is much more efficient and\nmobile-friendly than previous state-of-the-art reconstruction methods. Besides,\nan efficient feature mixing block, based on the channel splitting and shuffling\nmechanisms, is introduced as a novel bottleneck block of our proposed MobileSCI\nto alleviate the computational burden. Finally, a customized knowledge\ndistillation strategy is utilized to further improve the reconstruction\nquality. Extensive results on both simulated and real data show that our\nproposed MobileSCI can achieve superior reconstruction quality with high\nefficiency on the mobile devices. Particularly, we can reconstruct a 256 X 256\nX 8 snapshot compressed measurement with real-time performance (about 35 FPS)\non an iPhone 15. Code is available at https://github.com/mcao92/MobileSCI."
                },
                "authors": [
                    {
                        "name": "Miao Cao"
                    },
                    {
                        "name": "Lishun Wang"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Xin Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yuan"
                },
                "author": "Xin Yuan",
                "arxiv_comment": "9 pages, Accepted by ACM MM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17385v2",
                "updated": "2024-08-14T13:01:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    1,
                    52,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-24T16:07:57Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    16,
                    7,
                    57,
                    2,
                    206,
                    0
                ],
                "title": "Causal modelling without introducing counterfactuals or abstract\n  distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal modelling without introducing counterfactuals or abstract\n  distributions"
                },
                "summary": "The most common approach to causal modelling is the potential outcomes\nframework due to Neyman and Rubin. In this framework, outcomes of\ncounterfactual treatments are assumed to be well-defined. This metaphysical\nassumption is often thought to be problematic yet indispensable. The\nconventional approach relies not only on counterfactuals but also on abstract\nnotions of distributions and assumptions of independence that are not directly\ntestable. In this paper, we construe causal inference as treatment-wise\npredictions for finite populations where all assumptions are testable; this\nmeans that one can not only test predictions themselves (without any\nfundamental problem) but also investigate sources of error when they fail. The\nnew framework highlights the model-dependence of causal claims as well as the\ndifference between statistical and scientific inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most common approach to causal modelling is the potential outcomes\nframework due to Neyman and Rubin. In this framework, outcomes of\ncounterfactual treatments are assumed to be well-defined. This metaphysical\nassumption is often thought to be problematic yet indispensable. The\nconventional approach relies not only on counterfactuals but also on abstract\nnotions of distributions and assumptions of independence that are not directly\ntestable. In this paper, we construe causal inference as treatment-wise\npredictions for finite populations where all assumptions are testable; this\nmeans that one can not only test predictions themselves (without any\nfundamental problem) but also investigate sources of error when they fail. The\nnew framework highlights the model-dependence of causal claims as well as the\ndifference between statistical and scientific inference."
                },
                "authors": [
                    {
                        "name": "Benedikt Höltgen"
                    },
                    {
                        "name": "Robert C. Williamson"
                    }
                ],
                "author_detail": {
                    "name": "Robert C. Williamson"
                },
                "author": "Robert C. Williamson",
                "arxiv_comment": "Presented at the Humans, Algorithmic Decision-Making and Society\n  Workshop at ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07526v1",
                "updated": "2024-08-14T13:01:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    1,
                    30,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:01:30Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    1,
                    30,
                    2,
                    227,
                    0
                ],
                "title": "Learning-based Models for Vulnerability Detection: An Extensive Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based Models for Vulnerability Detection: An Extensive Study"
                },
                "summary": "Though many deep learning-based models have made great progress in\nvulnerability detection, we have no good understanding of these models, which\nlimits the further advancement of model capability, understanding of the\nmechanism of model detection, and efficiency and safety of practical\napplication of models. In this paper, we extensively and comprehensively\ninvestigate two types of state-of-the-art learning-based approaches\n(sequence-based and graph-based) by conducting experiments on a recently built\nlarge-scale dataset. We investigate seven research questions from five\ndimensions, namely model capabilities, model interpretation, model stability,\nease of use of model, and model economy. We experimentally demonstrate the\npriority of sequence-based models and the limited abilities of both LLM\n(ChatGPT) and graph-based models. We explore the types of vulnerability that\nlearning-based models skilled in and reveal the instability of the models\nthough the input is subtlely semantical-equivalently changed. We empirically\nexplain what the models have learned. We summarize the pre-processing as well\nas requirements for easily using the models. Finally, we initially induce the\nvital information for economically and safely practical usage of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though many deep learning-based models have made great progress in\nvulnerability detection, we have no good understanding of these models, which\nlimits the further advancement of model capability, understanding of the\nmechanism of model detection, and efficiency and safety of practical\napplication of models. In this paper, we extensively and comprehensively\ninvestigate two types of state-of-the-art learning-based approaches\n(sequence-based and graph-based) by conducting experiments on a recently built\nlarge-scale dataset. We investigate seven research questions from five\ndimensions, namely model capabilities, model interpretation, model stability,\nease of use of model, and model economy. We experimentally demonstrate the\npriority of sequence-based models and the limited abilities of both LLM\n(ChatGPT) and graph-based models. We explore the types of vulnerability that\nlearning-based models skilled in and reveal the instability of the models\nthough the input is subtlely semantical-equivalently changed. We empirically\nexplain what the models have learned. We summarize the pre-processing as well\nas requirements for easily using the models. Finally, we initially induce the\nvital information for economically and safely practical usage of these models."
                },
                "authors": [
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Liyu Shen"
                    },
                    {
                        "name": "Xiaodan Xu"
                    },
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Shaohua Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shaohua Wang"
                },
                "author": "Shaohua Wang",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07524v1",
                "updated": "2024-08-14T12:58:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    58,
                    22,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T12:58:22Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    58,
                    22,
                    2,
                    227,
                    0
                ],
                "title": "Fast Inference for Probabilistic Answer Set Programs via the Residual\n  Program",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Probabilistic Answer Set Programs via the Residual\n  Program"
                },
                "summary": "When we want to compute the probability of a query from a Probabilistic\nAnswer Set Program, some parts of a program may not influence the probability\nof a query, but they impact on the size of the grounding. Identifying and\nremoving them is crucial to speed up the computation. Algorithms for SLG\nresolution offer the possibility of returning the residual program which can be\nused for computing answer sets for normal programs that do have a total\nwell-founded model. The residual program does not contain the parts of the\nprogram that do not influence the probability. In this paper, we propose to\nexploit the residual program for performing inference. Empirical results on\ngraph datasets show that the approach leads to significantly faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When we want to compute the probability of a query from a Probabilistic\nAnswer Set Program, some parts of a program may not influence the probability\nof a query, but they impact on the size of the grounding. Identifying and\nremoving them is crucial to speed up the computation. Algorithms for SLG\nresolution offer the possibility of returning the residual program which can be\nused for computing answer sets for normal programs that do have a total\nwell-founded model. The residual program does not contain the parts of the\nprogram that do not influence the probability. In this paper, we propose to\nexploit the residual program for performing inference. Empirical results on\ngraph datasets show that the approach leads to significantly faster inference."
                },
                "authors": [
                    {
                        "name": "Damiano Azzolini"
                    },
                    {
                        "name": "Fabrizio Riguzzi"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Riguzzi"
                },
                "author": "Fabrizio Riguzzi",
                "arxiv_comment": "The paper has been accepted at the ICLP2024 conference and under\n  consideration in Theory and Practice of Logic Programming (TPLP)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02646v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02646v4",
                "updated": "2024-08-14T12:54:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    54,
                    13,
                    2,
                    227,
                    0
                ],
                "published": "2024-06-04T13:18:27Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    13,
                    18,
                    27,
                    1,
                    156,
                    0
                ],
                "title": "Learning Coefficients in Semi-Regular Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Coefficients in Semi-Regular Models"
                },
                "summary": "Recent advances have clarified theoretical learning accuracy in Bayesian\ninference, revealing that the asymptotic behavior of metrics such as\ngeneralization loss and free energy, assessing predictive accuracy, is dictated\nby a rational number unique to each statistical model, termed the learning\ncoefficient (real log canonical threshold) . For models meeting regularity\nconditions, their learning coefficients are known. However, for singular models\nnot meeting these conditions, exact values of learning coefficients are\nprovided for specific models like reduced-rank regression, but a broadly\napplicable calculation method for these learning coefficients in singular\nmodels remains elusive. The problem of determining learning coefficients\nrelates to finding normal crossings of Kullback-Leibler divergence in algebraic\ngeometry. In this context, it is crucial to perform appropriate coordinate\ntransformations and blow-ups. This paper introduces an approach that utilizes\nproperties of the log-likelihood ratio function for constructing specific\nvariable transformations and blow-ups to uniformly calculate the real log\ncanonical threshold. It was found that linear independence in the differential\nstructure of the log-likelihood ratio function significantly influences the\nreal log canonical threshold. This approach has not been considered in previous\nresearch. In this approach, the paper presents cases and methods for\ncalculating the exact values of learning coefficients in statistical models\nthat satisfy a simple condition next to the regularity conditions (semi-regular\nmodels), offering examples of learning coefficients for two-parameter\nsemi-regular models and mixture distribution models with a constant mixing\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have clarified theoretical learning accuracy in Bayesian\ninference, revealing that the asymptotic behavior of metrics such as\ngeneralization loss and free energy, assessing predictive accuracy, is dictated\nby a rational number unique to each statistical model, termed the learning\ncoefficient (real log canonical threshold) . For models meeting regularity\nconditions, their learning coefficients are known. However, for singular models\nnot meeting these conditions, exact values of learning coefficients are\nprovided for specific models like reduced-rank regression, but a broadly\napplicable calculation method for these learning coefficients in singular\nmodels remains elusive. The problem of determining learning coefficients\nrelates to finding normal crossings of Kullback-Leibler divergence in algebraic\ngeometry. In this context, it is crucial to perform appropriate coordinate\ntransformations and blow-ups. This paper introduces an approach that utilizes\nproperties of the log-likelihood ratio function for constructing specific\nvariable transformations and blow-ups to uniformly calculate the real log\ncanonical threshold. It was found that linear independence in the differential\nstructure of the log-likelihood ratio function significantly influences the\nreal log canonical threshold. This approach has not been considered in previous\nresearch. In this approach, the paper presents cases and methods for\ncalculating the exact values of learning coefficients in statistical models\nthat satisfy a simple condition next to the regularity conditions (semi-regular\nmodels), offering examples of learning coefficients for two-parameter\nsemi-regular models and mixture distribution models with a constant mixing\nratio."
                },
                "authors": [
                    {
                        "name": "Yuki Kurumadani"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Kurumadani"
                },
                "author": "Yuki Kurumadani",
                "arxiv_comment": "50 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02646v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02646v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00656v2",
                "updated": "2024-08-14T12:42:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    42,
                    44,
                    2,
                    227,
                    0
                ],
                "published": "2024-03-31T12:01:32Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    12,
                    1,
                    32,
                    6,
                    91,
                    0
                ],
                "title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WavLLM: Towards Robust and Adaptive Speech Large Language Model"
                },
                "summary": "The recent advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing, progressively broadening their scope\nto multimodal perception and generation. However, effectively integrating\nlistening capabilities into LLMs poses significant challenges, particularly\nwith respect to generalizing across varied contexts and executing complex\nauditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech\nlarge language model with dual encoders, and a prompt-aware LoRA weight\nadapter, optimized by a two-stage curriculum learning approach. Leveraging dual\nencoders, we decouple different types of speech information, utilizing a\nWhisper encoder to process the semantic content of speech, and a WavLM encoder\nto capture the unique characteristics of the speaker's identity. Within the\ncurriculum learning framework, WavLLM first builds its foundational\ncapabilities by optimizing on mixed elementary single tasks, followed by\nadvanced multi-task training on more complex tasks such as combinations of the\nelementary tasks. To enhance the flexibility and adherence to different tasks\nand instructions, a prompt-aware LoRA weight adapter is introduced in the\nsecond advanced multi-task training stage. We validate the proposed model on\nuniversal speech benchmarks including tasks such as ASR, ST, SV, ER, and also\napply it to specialized datasets like Gaokao English listening comprehension\nset for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments\ndemonstrate that the proposed model achieves state-of-the-art performance\nacross a range of speech tasks on the same model size, exhibiting robust\ngeneralization capabilities in executing complex tasks using CoT approach.\nFurthermore, our model successfully completes Gaokao tasks without specialized\ntraining. The codes, models, audio, and Gaokao evaluation set can be accessed\nat \\url{aka.ms/wavllm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing, progressively broadening their scope\nto multimodal perception and generation. However, effectively integrating\nlistening capabilities into LLMs poses significant challenges, particularly\nwith respect to generalizing across varied contexts and executing complex\nauditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech\nlarge language model with dual encoders, and a prompt-aware LoRA weight\nadapter, optimized by a two-stage curriculum learning approach. Leveraging dual\nencoders, we decouple different types of speech information, utilizing a\nWhisper encoder to process the semantic content of speech, and a WavLM encoder\nto capture the unique characteristics of the speaker's identity. Within the\ncurriculum learning framework, WavLLM first builds its foundational\ncapabilities by optimizing on mixed elementary single tasks, followed by\nadvanced multi-task training on more complex tasks such as combinations of the\nelementary tasks. To enhance the flexibility and adherence to different tasks\nand instructions, a prompt-aware LoRA weight adapter is introduced in the\nsecond advanced multi-task training stage. We validate the proposed model on\nuniversal speech benchmarks including tasks such as ASR, ST, SV, ER, and also\napply it to specialized datasets like Gaokao English listening comprehension\nset for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments\ndemonstrate that the proposed model achieves state-of-the-art performance\nacross a range of speech tasks on the same model size, exhibiting robust\ngeneralization capabilities in executing complex tasks using CoT approach.\nFurthermore, our model successfully completes Gaokao tasks without specialized\ntraining. The codes, models, audio, and Gaokao evaluation set can be accessed\nat \\url{aka.ms/wavllm}."
                },
                "authors": [
                    {
                        "name": "Shujie Hu"
                    },
                    {
                        "name": "Long Zhou"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sanyuan Chen"
                    },
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Hongkun Hao"
                    },
                    {
                        "name": "Jing Pan"
                    },
                    {
                        "name": "Xunying Liu"
                    },
                    {
                        "name": "Jinyu Li"
                    },
                    {
                        "name": "Sunit Sivasankaran"
                    },
                    {
                        "name": "Linquan Liu"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07505v1",
                "updated": "2024-08-14T12:32:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    32,
                    41,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T12:32:41Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    32,
                    41,
                    2,
                    227,
                    0
                ],
                "title": "Large Language Models Know What Makes Exemplary Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Know What Makes Exemplary Contexts"
                },
                "summary": "In-context learning (ICL) has proven to be a significant capability with the\nadvancement of Large Language models (LLMs). By instructing LLMs using few-shot\ndemonstrative examples, ICL enables them to perform a wide range of tasks\nwithout needing to update millions of parameters. This paper presents a unified\nframework for LLMs that allows them to self-select influential in-context\nexamples to compose their contexts; self-rank candidates with different\ndemonstration compositions; self-optimize the demonstration selection and\nordering through reinforcement learning. Specifically, our method designs a\nparameter-efficient retrieval head that generates the optimized demonstration\nafter training with rewards from LLM's own preference. Experimental results\nvalidate the proposed method's effectiveness in enhancing ICL performance.\nAdditionally, our approach effectively identifies and selects the most\nrepresentative examples for the current task, and includes more diversity in\nretrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has proven to be a significant capability with the\nadvancement of Large Language models (LLMs). By instructing LLMs using few-shot\ndemonstrative examples, ICL enables them to perform a wide range of tasks\nwithout needing to update millions of parameters. This paper presents a unified\nframework for LLMs that allows them to self-select influential in-context\nexamples to compose their contexts; self-rank candidates with different\ndemonstration compositions; self-optimize the demonstration selection and\nordering through reinforcement learning. Specifically, our method designs a\nparameter-efficient retrieval head that generates the optimized demonstration\nafter training with rewards from LLM's own preference. Experimental results\nvalidate the proposed method's effectiveness in enhancing ICL performance.\nAdditionally, our approach effectively identifies and selects the most\nrepresentative examples for the current task, and includes more diversity in\nretrieval."
                },
                "authors": [
                    {
                        "name": "Quanyu Long"
                    },
                    {
                        "name": "Jianda Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianda Chen"
                },
                "author": "Jianda Chen",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06425v2",
                "updated": "2024-08-14T12:22:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    22,
                    4,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-12T18:04:59Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    4,
                    59,
                    0,
                    225,
                    0
                ],
                "title": "Bayesian Learning in a Nonlinear Multiscale State-Space Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Learning in a Nonlinear Multiscale State-Space Model"
                },
                "summary": "The ubiquity of multiscale interactions in complex systems is\nwell-recognized, with development and heredity serving as a prime example of\nhow processes at different temporal scales influence one another. This work\nintroduces a novel multiscale state-space model to explore the dynamic\ninterplay between systems interacting across different time scales, with\nfeedback between each scale. We propose a Bayesian learning framework to\nestimate unknown states by learning the unknown process noise covariances\nwithin this multiscale model. We develop a Particle Gibbs with Ancestor\nSampling (PGAS) algorithm for inference and demonstrate through simulations the\nefficacy of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ubiquity of multiscale interactions in complex systems is\nwell-recognized, with development and heredity serving as a prime example of\nhow processes at different temporal scales influence one another. This work\nintroduces a novel multiscale state-space model to explore the dynamic\ninterplay between systems interacting across different time scales, with\nfeedback between each scale. We propose a Bayesian learning framework to\nestimate unknown states by learning the unknown process noise covariances\nwithin this multiscale model. We develop a Particle Gibbs with Ancestor\nSampling (PGAS) algorithm for inference and demonstrate through simulations the\nefficacy of our approach."
                },
                "authors": [
                    {
                        "name": "Nayely Vélez-Cruz"
                    },
                    {
                        "name": "Manfred D. Laubichler"
                    }
                ],
                "author_detail": {
                    "name": "Manfred D. Laubichler"
                },
                "author": "Manfred D. Laubichler",
                "arxiv_comment": "Corrected a typo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07494v1",
                "updated": "2024-08-14T12:19:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    19,
                    25,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T12:19:25Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    19,
                    25,
                    2,
                    227,
                    0
                ],
                "title": "QirK: Question Answering via Intermediate Representation on Knowledge\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QirK: Question Answering via Intermediate Representation on Knowledge\n  Graphs"
                },
                "summary": "We demonstrate QirK, a system for answering natural language questions on\nKnowledge Graphs (KG). QirK can answer structurally complex questions that are\nstill beyond the reach of emerging Large Language Models (LLMs). It does so\nusing a unique combination of database technology, LLMs, and semantic search\nover vector embeddings. The glue for these components is an intermediate\nrepresentation (IR). The input question is mapped to IR using LLMs, which is\nthen repaired into a valid relational database query with the aid of a semantic\nsearch on vector embeddings. This allows a practical synthesis of LLM\ncapabilities and KG reliability.\n  A short video demonstrating QirK is available at\nhttps://youtu.be/6c81BLmOZ0U.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate QirK, a system for answering natural language questions on\nKnowledge Graphs (KG). QirK can answer structurally complex questions that are\nstill beyond the reach of emerging Large Language Models (LLMs). It does so\nusing a unique combination of database technology, LLMs, and semantic search\nover vector embeddings. The glue for these components is an intermediate\nrepresentation (IR). The input question is mapped to IR using LLMs, which is\nthen repaired into a valid relational database query with the aid of a semantic\nsearch on vector embeddings. This allows a practical synthesis of LLM\ncapabilities and KG reliability.\n  A short video demonstrating QirK is available at\nhttps://youtu.be/6c81BLmOZ0U."
                },
                "authors": [
                    {
                        "name": "Jan Luca Scheerer"
                    },
                    {
                        "name": "Anton Lykov"
                    },
                    {
                        "name": "Moe Kayali"
                    },
                    {
                        "name": "Ilias Fountalis"
                    },
                    {
                        "name": "Dan Olteanu"
                    },
                    {
                        "name": "Nikolaos Vasiloglou"
                    },
                    {
                        "name": "Dan Suciu"
                    }
                ],
                "author_detail": {
                    "name": "Dan Suciu"
                },
                "author": "Dan Suciu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07482v1",
                "updated": "2024-08-14T11:55:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    55,
                    28,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T11:55:28Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    55,
                    28,
                    2,
                    227,
                    0
                ],
                "title": "Training Overhead Ratio: A Practical Reliability Metric for Large\n  Language Model Training Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Overhead Ratio: A Practical Reliability Metric for Large\n  Language Model Training Systems"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the AI industry with their\nsuperior capabilities. Training these models requires large-scale GPU clusters\nand significant computing time, leading to frequent failures that significantly\nincrease training costs. Despite its significance, this field lacks a metric\nfor evaluating reliability. In this work, we introduce a novel reliability\nmetric called \\emph{Training Overhead Ratio} (TOR) to evaluate the reliability\nof fault-tolerant LLM training systems. TOR is defined as the ratio of optimal\ntraining time to the observed training time of a system, serving as a practical\ntool for users to estimate the actual time required to train an LLM on a given\nsystem. Furthermore, our investigation identifies the key factor for enhancing\nreliability and present TOR equations for various types of failures encountered\nin practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the AI industry with their\nsuperior capabilities. Training these models requires large-scale GPU clusters\nand significant computing time, leading to frequent failures that significantly\nincrease training costs. Despite its significance, this field lacks a metric\nfor evaluating reliability. In this work, we introduce a novel reliability\nmetric called \\emph{Training Overhead Ratio} (TOR) to evaluate the reliability\nof fault-tolerant LLM training systems. TOR is defined as the ratio of optimal\ntraining time to the observed training time of a system, serving as a practical\ntool for users to estimate the actual time required to train an LLM on a given\nsystem. Furthermore, our investigation identifies the key factor for enhancing\nreliability and present TOR equations for various types of failures encountered\nin practice."
                },
                "authors": [
                    {
                        "name": "Ning Lu"
                    },
                    {
                        "name": "Qian Xie"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Wenyi Fang"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Jiantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Ma"
                },
                "author": "Jiantao Ma",
                "arxiv_comment": "preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11811v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11811v3",
                "updated": "2024-08-14T11:47:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    47,
                    39,
                    2,
                    227,
                    0
                ],
                "published": "2024-02-19T03:56:44Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    3,
                    56,
                    44,
                    0,
                    50,
                    0
                ],
                "title": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference\n  Dataset and Modular Fine-tuning Schema",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference\n  Dataset and Modular Fine-tuning Schema"
                },
                "summary": "When the quality of naive prompts is carefully optimized by human experts,\nthe task performance of large language models (LLMs) can be significantly\nimproved. However, expert-based prompt optimizations are expensive. Herein,\nsome works have proposed Automatic Prompt Optimization (APO), to optimize naive\nprompts according to task outputs of given in-box testing models, with the help\nof advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing\nschemes suffer from poor generalization ability and privacy risk. To this end,\nwe collect the first large-scale Prompt Optimization Preference dataset (POP),\nfine-tune offline local LLM-based optimizers, then fairly test with various\ndownstream models. Our method allows accurate optimization of the core task\ninstruction part within the naive prompt in a model-agnostic manner, and thus\nis named Free-from Instruction-oriented Prompt Optimization (FIPO). In\nspecific, FIPO uses a modular APO template that dynamically integrate the naive\ntask instruction, optional instruction responses, and optional ground truth to\nproduce finely optimized prompts. The POP dataset is meticulously constructed\nusing advanced LLMs, undergoing rigorous cross-validation by human experts and\nanalytical models. Leveraging insights from the data with Tulu2 models and\ndiverse fine-tuning strategies, we validate the efficacy of FIPO framework\nacross five public benchmarks and six testing models. Check codes and data\nhere: https://github.com/LuJunru/FIPO_Project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When the quality of naive prompts is carefully optimized by human experts,\nthe task performance of large language models (LLMs) can be significantly\nimproved. However, expert-based prompt optimizations are expensive. Herein,\nsome works have proposed Automatic Prompt Optimization (APO), to optimize naive\nprompts according to task outputs of given in-box testing models, with the help\nof advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing\nschemes suffer from poor generalization ability and privacy risk. To this end,\nwe collect the first large-scale Prompt Optimization Preference dataset (POP),\nfine-tune offline local LLM-based optimizers, then fairly test with various\ndownstream models. Our method allows accurate optimization of the core task\ninstruction part within the naive prompt in a model-agnostic manner, and thus\nis named Free-from Instruction-oriented Prompt Optimization (FIPO). In\nspecific, FIPO uses a modular APO template that dynamically integrate the naive\ntask instruction, optional instruction responses, and optional ground truth to\nproduce finely optimized prompts. The POP dataset is meticulously constructed\nusing advanced LLMs, undergoing rigorous cross-validation by human experts and\nanalytical models. Leveraging insights from the data with Tulu2 models and\ndiverse fine-tuning strategies, we validate the efficacy of FIPO framework\nacross five public benchmarks and six testing models. Check codes and data\nhere: https://github.com/LuJunru/FIPO_Project."
                },
                "authors": [
                    {
                        "name": "Junru Lu"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11811v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11811v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19832v2",
                "updated": "2024-08-14T11:42:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    42,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-29T09:38:15Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    9,
                    38,
                    15,
                    0,
                    211,
                    0
                ],
                "title": "ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have attracted much attention for\ntheir multifunctionality. However, traditional Transformer architectures incur\nsignificant overhead due to their secondary computational complexity. To\naddress this issue, we introduce ML-Mamba, a multimodal language model, which\nutilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known\nfor its linear scalability and fast processing of long sequences. We replace\nthe Transformer-based backbone with a pre-trained Mamba-2 model and explore\nmethods for integrating 2D visual selective scanning mechanisms into multimodal\nlearning while also trying various visual encoders and Mamba-2 model variants.\nOur extensive experiments in various multimodal benchmark tests demonstrate the\ncompetitive performance of ML-Mamba and highlight the potential of state space\nmodels in multimodal tasks. The experimental results show that: (1) we\nempirically explore how to effectively apply the 2D vision selective scan\nmechanism for multimodal learning. We propose a novel multimodal connector\ncalled the Mamba-2 Scan Connector (MSC), which enhances representational\ncapabilities. (2) ML-Mamba achieves performance comparable to state-of-the-art\nmethods such as TinyLaVA and MobileVLM v2 through its linear sequential\nmodeling while faster inference speed; (3) Compared to multimodal models\nutilizing Mamba-1, the Mamba-2-based ML-Mamba exhibits superior inference\nperformance and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have attracted much attention for\ntheir multifunctionality. However, traditional Transformer architectures incur\nsignificant overhead due to their secondary computational complexity. To\naddress this issue, we introduce ML-Mamba, a multimodal language model, which\nutilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known\nfor its linear scalability and fast processing of long sequences. We replace\nthe Transformer-based backbone with a pre-trained Mamba-2 model and explore\nmethods for integrating 2D visual selective scanning mechanisms into multimodal\nlearning while also trying various visual encoders and Mamba-2 model variants.\nOur extensive experiments in various multimodal benchmark tests demonstrate the\ncompetitive performance of ML-Mamba and highlight the potential of state space\nmodels in multimodal tasks. The experimental results show that: (1) we\nempirically explore how to effectively apply the 2D vision selective scan\nmechanism for multimodal learning. We propose a novel multimodal connector\ncalled the Mamba-2 Scan Connector (MSC), which enhances representational\ncapabilities. (2) ML-Mamba achieves performance comparable to state-of-the-art\nmethods such as TinyLaVA and MobileVLM v2 through its linear sequential\nmodeling while faster inference speed; (3) Compared to multimodal models\nutilizing Mamba-1, the Mamba-2-based ML-Mamba exhibits superior inference\nperformance and effectiveness."
                },
                "authors": [
                    {
                        "name": "Wenjun Huang"
                    },
                    {
                        "name": "Jianguo Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Hu"
                },
                "author": "Jianguo Hu",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.13600,\n  arXiv:2406.07537 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07474v1",
                "updated": "2024-08-14T11:34:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    34,
                    48,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T11:34:48Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    34,
                    48,
                    2,
                    227,
                    0
                ],
                "title": "Debiasing astro-Photometric Observations with Corrections Using\n  Statistics (DePhOCUS)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiasing astro-Photometric Observations with Corrections Using\n  Statistics (DePhOCUS)"
                },
                "summary": "Photometric measurements allow the determination of an asteroid's absolute\nmagnitude, which often represents the sole means to infer its size. Photometric\nobservations can be obtained in a variety of filters that can be unique to a\nspecific observatory. Those observations are then calibrated into specific\nbands with respect to reference star catalogs. In order to combine all the\ndifferent measurements for evaluation, photometric observations need to be\nconverted to a common band, typically V-band. Current band-correction schemes\nin use by IAU's Minor Planet Center, JPL's Center for Near Earth Object Studies\nand ESA's NEO Coordination Centre use average correction values for the\napparent magnitude derived from photometry of asteroids as the corrections are\ndependent on the typically unknown spectrum of the object to be corrected. By\nstatistically analyzing the photometric residuals of asteroids, we develop a\nnew photometric correction scheme that does not only consider the band, but\nalso accounts for reference catalog and observatory. We describe a new\nstatistical photometry correction scheme for asteroid observations with\ndebiased corrections. Testing this scheme on a reference group of asteroids, we\nsee a 36% reduction in the photometric residuals. Moreover, the new scheme\nleads to a more accurate and debiased determination of the H-G magnitude system\nand, in turn, to more reliable inferred sizes. We discuss the significant shift\nin the corrections with this \"DePhOCUS\" debiasing system, its limitations, and\nthe impact for photometric and physical properties of all asteroids, especially\nNear-Earth Objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photometric measurements allow the determination of an asteroid's absolute\nmagnitude, which often represents the sole means to infer its size. Photometric\nobservations can be obtained in a variety of filters that can be unique to a\nspecific observatory. Those observations are then calibrated into specific\nbands with respect to reference star catalogs. In order to combine all the\ndifferent measurements for evaluation, photometric observations need to be\nconverted to a common band, typically V-band. Current band-correction schemes\nin use by IAU's Minor Planet Center, JPL's Center for Near Earth Object Studies\nand ESA's NEO Coordination Centre use average correction values for the\napparent magnitude derived from photometry of asteroids as the corrections are\ndependent on the typically unknown spectrum of the object to be corrected. By\nstatistically analyzing the photometric residuals of asteroids, we develop a\nnew photometric correction scheme that does not only consider the band, but\nalso accounts for reference catalog and observatory. We describe a new\nstatistical photometry correction scheme for asteroid observations with\ndebiased corrections. Testing this scheme on a reference group of asteroids, we\nsee a 36% reduction in the photometric residuals. Moreover, the new scheme\nleads to a more accurate and debiased determination of the H-G magnitude system\nand, in turn, to more reliable inferred sizes. We discuss the significant shift\nin the corrections with this \"DePhOCUS\" debiasing system, its limitations, and\nthe impact for photometric and physical properties of all asteroids, especially\nNear-Earth Objects."
                },
                "authors": [
                    {
                        "name": "Tobias Hoffmann"
                    },
                    {
                        "name": "Marco Micheli"
                    },
                    {
                        "name": "Juan Luis Cano"
                    },
                    {
                        "name": "Maxime Devogèle"
                    },
                    {
                        "name": "Davide Farnocchia"
                    },
                    {
                        "name": "Petr Pravec"
                    },
                    {
                        "name": "Peter Vereš"
                    },
                    {
                        "name": "Björn Poppe"
                    }
                ],
                "author_detail": {
                    "name": "Björn Poppe"
                },
                "author": "Björn Poppe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07471v1",
                "updated": "2024-08-14T11:29:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    29,
                    47,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T11:29:47Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    29,
                    47,
                    2,
                    227,
                    0
                ],
                "title": "Bridging and Modeling Correlations in Pairwise Data for Direct\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging and Modeling Correlations in Pairwise Data for Direct\n  Preference Optimization"
                },
                "summary": "Direct preference optimization (DPO), a widely adopted offline preference\noptimization algorithm, aims to align large language models (LLMs) with\nhuman-desired behaviors using pairwise preference data. However, the winning\nresponse and the losing response within pairwise data are generated isolatedly,\nleading to weak correlations between them as well as suboptimal alignment\nperformance. To address this issue, we propose an effective framework named\nBMC, for bridging and modeling correlations in pairwise data. Firstly, we\nincrease the consistency and informativeness of the pairwise preference signals\nby targeted modifications, synthesizing a pseudo winning response through\nimproving the losing response based on the winning response. Secondly, we\nidentify that DPO alone is insufficient to model these correlations and capture\nnuanced variations. Therefore, we propose learning token-level correlations by\ndynamically leveraging the policy model's confidence during training.\nComprehensive experiments on QA, math, and instruction-following tasks\ndemonstrate the effectiveness of our approach, significantly surpassing\ncompetitive baselines, including DPO. Additionally, our in-depth quantitative\nanalysis reveals the reasons behind our method's superior performance over DPO\nand showcases its versatility to other DPO variants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct preference optimization (DPO), a widely adopted offline preference\noptimization algorithm, aims to align large language models (LLMs) with\nhuman-desired behaviors using pairwise preference data. However, the winning\nresponse and the losing response within pairwise data are generated isolatedly,\nleading to weak correlations between them as well as suboptimal alignment\nperformance. To address this issue, we propose an effective framework named\nBMC, for bridging and modeling correlations in pairwise data. Firstly, we\nincrease the consistency and informativeness of the pairwise preference signals\nby targeted modifications, synthesizing a pseudo winning response through\nimproving the losing response based on the winning response. Secondly, we\nidentify that DPO alone is insufficient to model these correlations and capture\nnuanced variations. Therefore, we propose learning token-level correlations by\ndynamically leveraging the policy model's confidence during training.\nComprehensive experiments on QA, math, and instruction-following tasks\ndemonstrate the effectiveness of our approach, significantly surpassing\ncompetitive baselines, including DPO. Additionally, our in-depth quantitative\nanalysis reveals the reasons behind our method's superior performance over DPO\nand showcases its versatility to other DPO variants."
                },
                "authors": [
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Bo Huang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Liangyou Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "18 pages, 8 figures, 8 tables, working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07465v1",
                "updated": "2024-08-14T11:19:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    19,
                    28,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T11:19:28Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    19,
                    28,
                    2,
                    227,
                    0
                ],
                "title": "Large Language Models Prompting With Episodic Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Prompting With Episodic Memory"
                },
                "summary": "Prompt optimization is essential for enhancing the performance of Large\nLanguage Models (LLMs) in a range of Natural Language Processing (NLP) tasks,\nparticularly in scenarios of few-shot learning where training examples are\nincorporated directly into the prompt. Despite the growing interest in\noptimizing prompts with few-shot examples, existing methods for prompt\noptimization are often resource-intensive or perform inadequately. In this\nwork, we propose PrOmpting with Episodic Memory (POEM), a novel prompt\noptimization technique that is simple, efficient, and demonstrates strong\ngeneralization capabilities. We approach prompt optimization as a Reinforcement\nLearning (RL) challenge, using episodic memory to archive combinations of input\ndata, permutations of few-shot examples, and the rewards observed during\ntraining. In the testing phase, we optimize the sequence of examples for each\ntest query by selecting the sequence that yields the highest total rewards from\nthe top-k most similar training examples in the episodic memory. Our results\nshow that POEM outperforms recent techniques like TEMPERA and RLPrompt by over\n5.3% in various text classification tasks. Furthermore, our approach adapts\nwell to broader language understanding tasks, consistently outperforming\nconventional heuristic methods for ordering examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt optimization is essential for enhancing the performance of Large\nLanguage Models (LLMs) in a range of Natural Language Processing (NLP) tasks,\nparticularly in scenarios of few-shot learning where training examples are\nincorporated directly into the prompt. Despite the growing interest in\noptimizing prompts with few-shot examples, existing methods for prompt\noptimization are often resource-intensive or perform inadequately. In this\nwork, we propose PrOmpting with Episodic Memory (POEM), a novel prompt\noptimization technique that is simple, efficient, and demonstrates strong\ngeneralization capabilities. We approach prompt optimization as a Reinforcement\nLearning (RL) challenge, using episodic memory to archive combinations of input\ndata, permutations of few-shot examples, and the rewards observed during\ntraining. In the testing phase, we optimize the sequence of examples for each\ntest query by selecting the sequence that yields the highest total rewards from\nthe top-k most similar training examples in the episodic memory. Our results\nshow that POEM outperforms recent techniques like TEMPERA and RLPrompt by over\n5.3% in various text classification tasks. Furthermore, our approach adapts\nwell to broader language understanding tasks, consistently outperforming\nconventional heuristic methods for ordering examples."
                },
                "authors": [
                    {
                        "name": "Dai Do"
                    },
                    {
                        "name": "Quan Tran"
                    },
                    {
                        "name": "Svetha Venkatesh"
                    },
                    {
                        "name": "Hung Le"
                    }
                ],
                "author_detail": {
                    "name": "Hung Le"
                },
                "author": "Hung Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07429v1",
                "updated": "2024-08-14T10:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    6,
                    20,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T10:06:20Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    6,
                    20,
                    2,
                    227,
                    0
                ],
                "title": "Limit Theorems for Weakly Dependent Non-stationary Random Field Arrays\n  and Asymptotic Inference of Dynamic Spatio-temporal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limit Theorems for Weakly Dependent Non-stationary Random Field Arrays\n  and Asymptotic Inference of Dynamic Spatio-temporal Models"
                },
                "summary": "We obtain the law of large numbers (LLN) and the central limit theorem (CLT)\nfor weakly dependent non-stationary arrays of random fields with asymptotically\nunbounded moments. The weak dependence condition for arrays of random fields is\nproved to be inherited through transformation and infinite shift. This paves a\nway to prove the consistency and asymptotic normality of maximum likelihood\nestimation for dynamic spatio-temporal models (i.e. so-called ultra\nhigh-dimensional time series models) when the sample size and/or dimension go\nto infinity. Especially the asymptotic properties of estimation for network\nautoregression are obtained under reasonable regularity conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We obtain the law of large numbers (LLN) and the central limit theorem (CLT)\nfor weakly dependent non-stationary arrays of random fields with asymptotically\nunbounded moments. The weak dependence condition for arrays of random fields is\nproved to be inherited through transformation and infinite shift. This paves a\nway to prove the consistency and asymptotic normality of maximum likelihood\nestimation for dynamic spatio-temporal models (i.e. so-called ultra\nhigh-dimensional time series models) when the sample size and/or dimension go\nto infinity. Especially the asymptotic properties of estimation for network\nautoregression are obtained under reasonable regularity conditions."
                },
                "authors": [
                    {
                        "name": "Yue Pan"
                    },
                    {
                        "name": "Jiazhu Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jiazhu Pan"
                },
                "author": "Jiazhu Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07427v1",
                "updated": "2024-08-14T10:03:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    3,
                    40,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T10:03:40Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    3,
                    40,
                    2,
                    227,
                    0
                ],
                "title": "Beyond Inter-Item Relations: Dynamic Adaptive Mixture-of-Experts for\n  LLM-Based Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Inter-Item Relations: Dynamic Adaptive Mixture-of-Experts for\n  LLM-Based Sequential Recommendation"
                },
                "summary": "Sequential recommender system (SRS) predicts the next items that users may\nprefer based on user historical interaction sequences. Inspired by the rise of\nlarge language models (LLMs) in various AI applications, there is a surge of\nwork on LLM-based SRS. Despite their attractive performance, existing LLM-based\nSRS still exhibit some limitations, including neglecting intra-item relations,\nignoring long-term collaborative knowledge and using inflexible architecture\ndesigns for adaption. To alleviate these issues, we propose an LLM-based SRS\nnamed MixRec. Built on top of coarse-grained adaption for capturing inter-item\nrelations, MixRec is further enhanced with (1) context masking that models\nintra-item relations to help LLM better understand token and item semantics in\nthe context of SRS, (2) collaborative knowledge injection that helps LLM\nincorporate long-term collaborative knowledge, and (3) a dynamic adaptive\nmixture-of-experts design that can flexibly choose expert architectures based\non Bayesian optimization to better incorporate different sequential\ninformation. Extensive experiments demonstrate that MixRec can effectively\nhandle sequential recommendation in a dynamic and adaptive manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommender system (SRS) predicts the next items that users may\nprefer based on user historical interaction sequences. Inspired by the rise of\nlarge language models (LLMs) in various AI applications, there is a surge of\nwork on LLM-based SRS. Despite their attractive performance, existing LLM-based\nSRS still exhibit some limitations, including neglecting intra-item relations,\nignoring long-term collaborative knowledge and using inflexible architecture\ndesigns for adaption. To alleviate these issues, we propose an LLM-based SRS\nnamed MixRec. Built on top of coarse-grained adaption for capturing inter-item\nrelations, MixRec is further enhanced with (1) context masking that models\nintra-item relations to help LLM better understand token and item semantics in\nthe context of SRS, (2) collaborative knowledge injection that helps LLM\nincorporate long-term collaborative knowledge, and (3) a dynamic adaptive\nmixture-of-experts design that can flexibly choose expert architectures based\non Bayesian optimization to better incorporate different sequential\ninformation. Extensive experiments demonstrate that MixRec can effectively\nhandle sequential recommendation in a dynamic and adaptive manner."
                },
                "authors": [
                    {
                        "name": "CanYi Liu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Youchen"
                    },
                    {
                        "name": "Zhang"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "arxiv_affiliation": "Victor",
                "author": "Rongrong Ji",
                "arxiv_comment": "11 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07425v1",
                "updated": "2024-08-14T10:03:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    3,
                    28,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T10:03:28Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    3,
                    28,
                    2,
                    227,
                    0
                ],
                "title": "Exploring Retrieval Augmented Generation in Arabic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Retrieval Augmented Generation in Arabic"
                },
                "summary": "Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful\ntechnique in natural language processing, combining the strengths of\nretrieval-based and generation-based models to enhance text generation tasks.\nHowever, the application of RAG in Arabic, a language with unique\ncharacteristics and resource constraints, remains underexplored. This paper\npresents a comprehensive case study on the implementation and evaluation of RAG\nfor Arabic text. The work focuses on exploring various semantic embedding\nmodels in the retrieval stage and several LLMs in the generation stage, in\norder to investigate what works and what doesn't in the context of Arabic. The\nwork also touches upon the issue of variations between document dialect and\nquery dialect in the retrieval stage. Results show that existing semantic\nembedding models and LLMs can be effectively employed to build Arabic RAG\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful\ntechnique in natural language processing, combining the strengths of\nretrieval-based and generation-based models to enhance text generation tasks.\nHowever, the application of RAG in Arabic, a language with unique\ncharacteristics and resource constraints, remains underexplored. This paper\npresents a comprehensive case study on the implementation and evaluation of RAG\nfor Arabic text. The work focuses on exploring various semantic embedding\nmodels in the retrieval stage and several LLMs in the generation stage, in\norder to investigate what works and what doesn't in the context of Arabic. The\nwork also touches upon the issue of variations between document dialect and\nquery dialect in the retrieval stage. Results show that existing semantic\nembedding models and LLMs can be effectively employed to build Arabic RAG\npipelines."
                },
                "authors": [
                    {
                        "name": "Samhaa R. El-Beltagy"
                    },
                    {
                        "name": "Mohamed A. Abdallah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed A. Abdallah"
                },
                "author": "Mohamed A. Abdallah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07422v1",
                "updated": "2024-08-14T10:00:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    0,
                    16,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T10:00:16Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    0,
                    16,
                    2,
                    227,
                    0
                ],
                "title": "LLMI3D: Empowering LLM with 3D Perception from a Single 2D Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMI3D: Empowering LLM with 3D Perception from a Single 2D Image"
                },
                "summary": "Recent advancements in autonomous driving, augmented reality, robotics, and\nembodied intelligence have necessitated 3D perception algorithms. However,\ncurrent 3D perception methods, particularly small models, struggle with\nprocessing logical reasoning, question-answering, and handling open scenario\ncategories. On the other hand, generative multimodal large language models\n(MLLMs) excel in general capacity but underperform in 3D tasks, due to weak\nspatial and local object perception, poor text-based geometric numerical\noutput, and inability to handle camera focal variations. To address these\nchallenges, we propose the following solutions: Spatial-Enhanced Local Feature\nMining for better spatial feature extraction, 3D Query Token-Derived Info\nDecoding for precise geometric regression, and Geometry Projection-Based 3D\nReasoning for handling camera focal length variations. We employ\nparameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a\npowerful 3D perception MLLM. Additionally, we have constructed the IG3D\ndataset, which provides fine-grained descriptions and question-answer\nannotations. Extensive experiments demonstrate that our LLMI3D achieves\nstate-of-the-art performance, significantly outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in autonomous driving, augmented reality, robotics, and\nembodied intelligence have necessitated 3D perception algorithms. However,\ncurrent 3D perception methods, particularly small models, struggle with\nprocessing logical reasoning, question-answering, and handling open scenario\ncategories. On the other hand, generative multimodal large language models\n(MLLMs) excel in general capacity but underperform in 3D tasks, due to weak\nspatial and local object perception, poor text-based geometric numerical\noutput, and inability to handle camera focal variations. To address these\nchallenges, we propose the following solutions: Spatial-Enhanced Local Feature\nMining for better spatial feature extraction, 3D Query Token-Derived Info\nDecoding for precise geometric regression, and Geometry Projection-Based 3D\nReasoning for handling camera focal length variations. We employ\nparameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a\npowerful 3D perception MLLM. Additionally, we have constructed the IG3D\ndataset, which provides fine-grained descriptions and question-answer\nannotations. Extensive experiments demonstrate that our LLMI3D achieves\nstate-of-the-art performance, significantly outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haoxiang Chen"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Wenbo Tang"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Pengfei Xu"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01107v2",
                "updated": "2024-08-14T09:54:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    54,
                    24,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-02T08:37:03Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    8,
                    37,
                    3,
                    4,
                    215,
                    0
                ],
                "title": "BioRAG: A RAG-LLM Framework for Biological Question Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioRAG: A RAG-LLM Framework for Biological Question Reasoning"
                },
                "summary": "The question-answering system for Life science research, which is\ncharacterized by the rapid pace of discovery, evolving insights, and complex\ninteractions among knowledge entities, presents unique challenges in\nmaintaining a comprehensive knowledge warehouse and accurate information\nretrieval. To address these issues, we introduce BioRAG, a novel\nRetrieval-Augmented Generation (RAG) with the Large Language Models (LLMs)\nframework. Our approach starts with parsing, indexing, and segmenting an\nextensive collection of 22 million scientific papers as the basic knowledge,\nfollowed by training a specialized embedding model tailored to this domain.\nAdditionally, we enhance the vector retrieval process by incorporating a\ndomain-specific knowledge hierarchy, which aids in modeling the intricate\ninterrelationships among each query and context. For queries requiring the most\ncurrent information, BioRAG deconstructs the question and employs an iterative\nretrieval process incorporated with the search engine for step-by-step\nreasoning. Rigorous experiments have demonstrated that our model outperforms\nfine-tuned LLM, LLM with search engines, and other scientific RAG frameworks\nacross multiple life science question-answering tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The question-answering system for Life science research, which is\ncharacterized by the rapid pace of discovery, evolving insights, and complex\ninteractions among knowledge entities, presents unique challenges in\nmaintaining a comprehensive knowledge warehouse and accurate information\nretrieval. To address these issues, we introduce BioRAG, a novel\nRetrieval-Augmented Generation (RAG) with the Large Language Models (LLMs)\nframework. Our approach starts with parsing, indexing, and segmenting an\nextensive collection of 22 million scientific papers as the basic knowledge,\nfollowed by training a specialized embedding model tailored to this domain.\nAdditionally, we enhance the vector retrieval process by incorporating a\ndomain-specific knowledge hierarchy, which aids in modeling the intricate\ninterrelationships among each query and context. For queries requiring the most\ncurrent information, BioRAG deconstructs the question and employs an iterative\nretrieval process incorporated with the search engine for step-by-step\nreasoning. Rigorous experiments have demonstrated that our model outperforms\nfine-tuned LLM, LLM with search engines, and other scientific RAG frameworks\nacross multiple life science question-answering tasks."
                },
                "authors": [
                    {
                        "name": "Chengrui Wang"
                    },
                    {
                        "name": "Qingqing Long"
                    },
                    {
                        "name": "Meng Xiao"
                    },
                    {
                        "name": "Xunxin Cai"
                    },
                    {
                        "name": "Chengjun Wu"
                    },
                    {
                        "name": "Zhen Meng"
                    },
                    {
                        "name": "Xuezhi Wang"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Zhou"
                },
                "author": "Yuanchun Zhou",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07413v1",
                "updated": "2024-08-14T09:43:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    43,
                    32,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T09:43:32Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    43,
                    32,
                    2,
                    227,
                    0
                ],
                "title": "Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge\n  Editing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge\n  Editing for Large Language Models"
                },
                "summary": "Knowledge editing aims to update outdated or incorrect knowledge in large\nlanguage models (LLMs). However, current knowledge editing methods have limited\nscalability for lifelong editing. This study explores the fundamental reason\nwhy knowledge editing fails in lifelong editing. We begin with the closed-form\nsolution derived from linear associative memory, which underpins\nstate-of-the-art knowledge editing methods. We extend the solution from single\nediting to lifelong editing, and through rigorous mathematical derivation,\nidentify an interference term in the final solution, suggesting that editing\nknowledge may impact irrelevant knowledge. Further analysis of the interference\nterm reveals a close relationship with superposition between knowledge\nrepresentations. When knowledge superposition does not exist in language\nmodels, the interference term vanishes, allowing for lossless knowledge\nediting. Experiments across numerous language models reveal that knowledge\nsuperposition is universal, exhibiting high kurtosis, zero mean, and\nheavy-tailed distributions with clear scaling laws. Ultimately, by combining\ntheory and experiments, we demonstrate that knowledge superposition is the\nfundamental reason for the failure of lifelong editing. Moreover, this is the\nfirst study to investigate knowledge editing from the perspective of\nsuperposition and provides a comprehensive observation of superposition across\nnumerous real-world language models. Code available at\nhttps://github.com/ChenhuiHu/knowledge_in_superposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing aims to update outdated or incorrect knowledge in large\nlanguage models (LLMs). However, current knowledge editing methods have limited\nscalability for lifelong editing. This study explores the fundamental reason\nwhy knowledge editing fails in lifelong editing. We begin with the closed-form\nsolution derived from linear associative memory, which underpins\nstate-of-the-art knowledge editing methods. We extend the solution from single\nediting to lifelong editing, and through rigorous mathematical derivation,\nidentify an interference term in the final solution, suggesting that editing\nknowledge may impact irrelevant knowledge. Further analysis of the interference\nterm reveals a close relationship with superposition between knowledge\nrepresentations. When knowledge superposition does not exist in language\nmodels, the interference term vanishes, allowing for lossless knowledge\nediting. Experiments across numerous language models reveal that knowledge\nsuperposition is universal, exhibiting high kurtosis, zero mean, and\nheavy-tailed distributions with clear scaling laws. Ultimately, by combining\ntheory and experiments, we demonstrate that knowledge superposition is the\nfundamental reason for the failure of lifelong editing. Moreover, this is the\nfirst study to investigate knowledge editing from the perspective of\nsuperposition and provides a comprehensive observation of superposition across\nnumerous real-world language models. Code available at\nhttps://github.com/ChenhuiHu/knowledge_in_superposition."
                },
                "authors": [
                    {
                        "name": "Chenhui Hu"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07405v1",
                "updated": "2024-08-14T09:24:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    24,
                    12,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T09:24:12Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    24,
                    12,
                    2,
                    227,
                    0
                ],
                "title": "Modeling of Measurement Error in Financial Returns Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling of Measurement Error in Financial Returns Data"
                },
                "summary": "In this paper we consider the modeling of measurement error for fund returns\ndata. In particular, given access to a time-series of discretely observed\nlog-returns and the associated maximum over the observation period, we develop\na stochastic model which models the true log-returns and maximum via a L\\'evy\nprocess and the data as a measurement error there-of. The main technical\ndifficulty of trying to infer this model, for instance Bayesian parameter\nestimation, is that the joint transition density of the return and maximum is\nseldom known, nor can it be simulated exactly. Based upon the novel stick\nbreaking representation of [12] we provide an approximation of the model. We\ndevelop a Markov chain Monte Carlo (MCMC) algorithm to sample from the Bayesian\nposterior of the approximated posterior and then extend this to a multilevel\nMCMC method which can reduce the computational cost to approximate posterior\nexpectations, relative to ordinary MCMC. We implement our methodology on\nseveral applications including for real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider the modeling of measurement error for fund returns\ndata. In particular, given access to a time-series of discretely observed\nlog-returns and the associated maximum over the observation period, we develop\na stochastic model which models the true log-returns and maximum via a L\\'evy\nprocess and the data as a measurement error there-of. The main technical\ndifficulty of trying to infer this model, for instance Bayesian parameter\nestimation, is that the joint transition density of the return and maximum is\nseldom known, nor can it be simulated exactly. Based upon the novel stick\nbreaking representation of [12] we provide an approximation of the model. We\ndevelop a Markov chain Monte Carlo (MCMC) algorithm to sample from the Bayesian\nposterior of the approximated posterior and then extend this to a multilevel\nMCMC method which can reduce the computational cost to approximate posterior\nexpectations, relative to ordinary MCMC. We implement our methodology on\nseveral applications including for real data."
                },
                "authors": [
                    {
                        "name": "Ajay Jasra"
                    },
                    {
                        "name": "Mohamed Maama"
                    },
                    {
                        "name": "Aleksandar Mijatović"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandar Mijatović"
                },
                "author": "Aleksandar Mijatović",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07394v1",
                "updated": "2024-08-14T09:13:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    13,
                    27,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T09:13:27Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    13,
                    27,
                    2,
                    227,
                    0
                ],
                "title": "Sum-Product-Set Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sum-Product-Set Networks"
                },
                "summary": "Daily internet communication relies heavily on tree-structured graphs,\nembodied by popular data formats such as XML and JSON. However, many recent\ngenerative (probabilistic) models utilize neural networks to learn a\nprobability distribution over undirected cyclic graphs. This assumption of a\ngeneric graph structure brings various computational challenges, and, more\nimportantly, the presence of non-linearities in neural networks does not permit\ntractable probabilistic inference. We address these problems by proposing\nsum-product-set networks, an extension of probabilistic circuits from\nunstructured tensor data to tree-structured graph data. To this end, we use\nrandom finite sets to reflect a variable number of nodes and edges in the graph\nand to allow for exact and efficient inference. We demonstrate that our\ntractable model performs comparably to various intractable models based on\nneural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Daily internet communication relies heavily on tree-structured graphs,\nembodied by popular data formats such as XML and JSON. However, many recent\ngenerative (probabilistic) models utilize neural networks to learn a\nprobability distribution over undirected cyclic graphs. This assumption of a\ngeneric graph structure brings various computational challenges, and, more\nimportantly, the presence of non-linearities in neural networks does not permit\ntractable probabilistic inference. We address these problems by proposing\nsum-product-set networks, an extension of probabilistic circuits from\nunstructured tensor data to tree-structured graph data. To this end, we use\nrandom finite sets to reflect a variable number of nodes and edges in the graph\nand to allow for exact and efficient inference. We demonstrate that our\ntractable model performs comparably to various intractable models based on\nneural networks."
                },
                "authors": [
                    {
                        "name": "Milan Papež"
                    },
                    {
                        "name": "Martin Rektoris"
                    },
                    {
                        "name": "Tomáš Pevný"
                    },
                    {
                        "name": "Václav Šmídl"
                    }
                ],
                "author_detail": {
                    "name": "Václav Šmídl"
                },
                "author": "Václav Šmídl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.16414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.16414v3",
                "updated": "2024-08-14T09:06:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    6,
                    0,
                    2,
                    227,
                    0
                ],
                "published": "2023-09-28T13:08:08Z",
                "published_parsed": [
                    2023,
                    9,
                    28,
                    13,
                    8,
                    8,
                    3,
                    271,
                    0
                ],
                "title": "AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models"
                },
                "summary": "Classifiers built upon vision-language models such as CLIP have shown\nremarkable zero-shot performance across a broad range of image classification\ntasks. Prior work has studied different ways of automatically creating\ndescriptor sets for every class based on prompt templates, ranging from\nmanually engineered templates over templates obtained from a large language\nmodel to templates built from random words and characters. Up until now,\nderiving zero-shot classifiers from the respective encoded class descriptors\nhas remained nearly unchanged, i.e., classify to the class that maximizes\ncosine similarity between its averaged encoded class descriptors and the image\nencoding. However, weighing all class descriptors equally can be suboptimal\nwhen certain descriptors match visual clues on a given image better than\nothers. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot\nclassifiers. AutoCLIP tunes per-image weights to each prompt template at\ninference time, based on statistics of class descriptor-image similarities.\nAutoCLIP is fully unsupervised, has only a minor additional computation\noverhead, and can be easily implemented in few lines of code. We show that\nAutoCLIP outperforms baselines across a broad range of vision-language models,\ndatasets, and prompt templates consistently and by up to 3 percent point\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classifiers built upon vision-language models such as CLIP have shown\nremarkable zero-shot performance across a broad range of image classification\ntasks. Prior work has studied different ways of automatically creating\ndescriptor sets for every class based on prompt templates, ranging from\nmanually engineered templates over templates obtained from a large language\nmodel to templates built from random words and characters. Up until now,\nderiving zero-shot classifiers from the respective encoded class descriptors\nhas remained nearly unchanged, i.e., classify to the class that maximizes\ncosine similarity between its averaged encoded class descriptors and the image\nencoding. However, weighing all class descriptors equally can be suboptimal\nwhen certain descriptors match visual clues on a given image better than\nothers. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot\nclassifiers. AutoCLIP tunes per-image weights to each prompt template at\ninference time, based on statistics of class descriptor-image similarities.\nAutoCLIP is fully unsupervised, has only a minor additional computation\noverhead, and can be easily implemented in few lines of code. We show that\nAutoCLIP outperforms baselines across a broad range of vision-language models,\ndatasets, and prompt templates consistently and by up to 3 percent point\naccuracy."
                },
                "authors": [
                    {
                        "name": "Jan Hendrik Metzen"
                    },
                    {
                        "name": "Piyapat Saranrittichai"
                    },
                    {
                        "name": "Chaithanya Kumar Mummadi"
                    }
                ],
                "author_detail": {
                    "name": "Chaithanya Kumar Mummadi"
                },
                "author": "Chaithanya Kumar Mummadi",
                "arxiv_comment": "accepted at TMLR, Camera Ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.16414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.16414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07369v1",
                "updated": "2024-08-14T08:34:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    8,
                    34,
                    0,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T08:34:00Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    8,
                    34,
                    0,
                    2,
                    227,
                    0
                ],
                "title": "ProCom: A Few-shot Targeted Community Detection Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProCom: A Few-shot Targeted Community Detection Algorithm"
                },
                "summary": "Targeted community detection aims to distinguish a particular type of\ncommunity in the network. This is an important task with a lot of real-world\napplications, e.g., identifying fraud groups in transaction networks.\nTraditional community detection methods fail to capture the specific features\nof the targeted community and detect all types of communities indiscriminately.\nSemi-supervised community detection algorithms, emerged as a feasible\nalternative, are inherently constrained by their limited adaptability and\nsubstantial reliance on a large amount of labeled data, which demands extensive\ndomain knowledge and manual effort.\n  In this paper, we address the aforementioned weaknesses in targeted community\ndetection by focusing on few-shot scenarios. We propose ProCom, a novel\nframework that extends the ``pre-train, prompt'' paradigm, offering a\nlow-resource, high-efficiency, and transferable solution. Within the framework,\nwe devise a dual-level context-aware pre-training method that fosters a deep\nunderstanding of latent communities in the network, establishing a rich\nknowledge foundation for downstream task. In the prompt learning stage, we\nreformulate the targeted community detection task into pre-training objectives,\nallowing the extraction of specific knowledge relevant to the targeted\ncommunity to facilitate effective and efficient inference. By leveraging both\nthe general community knowledge acquired during pre-training and the specific\ninsights gained from the prompt communities, ProCom exhibits remarkable\nadaptability across different datasets. We conduct extensive experiments on\nfive benchmarks to evaluate the ProCom framework, demonstrating its SOTA\nperformance under few-shot scenarios, strong efficiency, and transferability\nacross diverse datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted community detection aims to distinguish a particular type of\ncommunity in the network. This is an important task with a lot of real-world\napplications, e.g., identifying fraud groups in transaction networks.\nTraditional community detection methods fail to capture the specific features\nof the targeted community and detect all types of communities indiscriminately.\nSemi-supervised community detection algorithms, emerged as a feasible\nalternative, are inherently constrained by their limited adaptability and\nsubstantial reliance on a large amount of labeled data, which demands extensive\ndomain knowledge and manual effort.\n  In this paper, we address the aforementioned weaknesses in targeted community\ndetection by focusing on few-shot scenarios. We propose ProCom, a novel\nframework that extends the ``pre-train, prompt'' paradigm, offering a\nlow-resource, high-efficiency, and transferable solution. Within the framework,\nwe devise a dual-level context-aware pre-training method that fosters a deep\nunderstanding of latent communities in the network, establishing a rich\nknowledge foundation for downstream task. In the prompt learning stage, we\nreformulate the targeted community detection task into pre-training objectives,\nallowing the extraction of specific knowledge relevant to the targeted\ncommunity to facilitate effective and efficient inference. By leveraging both\nthe general community knowledge acquired during pre-training and the specific\ninsights gained from the prompt communities, ProCom exhibits remarkable\nadaptability across different datasets. We conduct extensive experiments on\nfive benchmarks to evaluate the ProCom framework, demonstrating its SOTA\nperformance under few-shot scenarios, strong efficiency, and transferability\nacross diverse datasets."
                },
                "authors": [
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Kaiyu Xiong"
                    },
                    {
                        "name": "Yun Xiong"
                    },
                    {
                        "name": "Xiaoxin He"
                    },
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Yizhu Jiao"
                    },
                    {
                        "name": "Jiawei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhang"
                },
                "author": "Jiawei Zhang",
                "arxiv_comment": "Accepted by SIGKDD'2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07365v1",
                "updated": "2024-08-14T08:23:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    8,
                    23,
                    55,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T08:23:55Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    8,
                    23,
                    55,
                    2,
                    227,
                    0
                ],
                "title": "Fast Bayesian inference in a class of sparse linear mixed effects models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Bayesian inference in a class of sparse linear mixed effects models"
                },
                "summary": "Linear mixed effects models are widely used in statistical modelling. We\nconsider a mixed effects model with Bayesian variable selection in the random\neffects using spike-and-slab priors and developed a variational Bayes inference\nscheme that can be applied to large data sets. An EM algorithm is proposed for\nthe model with normal errors where the posterior distribution of the variable\ninclusion parameters is approximated using an Occam's window approach. Placing\nthis approach within a variational Bayes scheme also the algorithm to be\nextended to the model with skew-t errors. The performance of the algorithm is\nevaluated in a simulation study and applied to a longitudinal model for elite\nathlete performance in the 100 metre sprint and weightlifting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear mixed effects models are widely used in statistical modelling. We\nconsider a mixed effects model with Bayesian variable selection in the random\neffects using spike-and-slab priors and developed a variational Bayes inference\nscheme that can be applied to large data sets. An EM algorithm is proposed for\nthe model with normal errors where the posterior distribution of the variable\ninclusion parameters is approximated using an Occam's window approach. Placing\nthis approach within a variational Bayes scheme also the algorithm to be\nextended to the model with skew-t errors. The performance of the algorithm is\nevaluated in a simulation study and applied to a longitudinal model for elite\nathlete performance in the 100 metre sprint and weightlifting."
                },
                "authors": [
                    {
                        "name": "M-Z. Spyropoulou"
                    },
                    {
                        "name": "J. Hopker"
                    },
                    {
                        "name": "J. E. Griffin"
                    }
                ],
                "author_detail": {
                    "name": "J. E. Griffin"
                },
                "author": "J. E. Griffin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21264v2",
                "updated": "2024-08-14T08:10:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    8,
                    10,
                    43,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-31T00:56:09Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    0,
                    56,
                    9,
                    2,
                    213,
                    0
                ],
                "title": "Model Attribution in LLM-Generated Disinformation: A Domain\n  Generalization Approach with Supervised Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Attribution in LLM-Generated Disinformation: A Domain\n  Generalization Approach with Supervised Contrastive Learning"
                },
                "summary": "Model attribution for LLM-generated disinformation poses a significant\nchallenge in understanding its origins and mitigating its spread. This task is\nespecially challenging because modern large language models (LLMs) produce\ndisinformation with human-like quality. Additionally, the diversity in\nprompting methods used to generate disinformation complicates accurate source\nattribution. These methods introduce domain-specific features that can mask the\nfundamental characteristics of the models. In this paper, we introduce the\nconcept of model attribution as a domain generalization problem, where each\nprompting method represents a unique domain. We argue that an effective\nattribution model must be invariant to these domain-specific features. It\nshould also be proficient in identifying the originating models across all\nscenarios, reflecting real-world detection challenges. To address this, we\nintroduce a novel approach based on Supervised Contrastive Learning. This\nmethod is designed to enhance the model's robustness to variations in prompts\nand focuses on distinguishing between different source LLMs. We evaluate our\nmodel through rigorous experiments involving three common prompting methods:\n``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:\n``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the\neffectiveness of our approach in model attribution tasks, achieving\nstate-of-the-art performance across diverse and unseen datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model attribution for LLM-generated disinformation poses a significant\nchallenge in understanding its origins and mitigating its spread. This task is\nespecially challenging because modern large language models (LLMs) produce\ndisinformation with human-like quality. Additionally, the diversity in\nprompting methods used to generate disinformation complicates accurate source\nattribution. These methods introduce domain-specific features that can mask the\nfundamental characteristics of the models. In this paper, we introduce the\nconcept of model attribution as a domain generalization problem, where each\nprompting method represents a unique domain. We argue that an effective\nattribution model must be invariant to these domain-specific features. It\nshould also be proficient in identifying the originating models across all\nscenarios, reflecting real-world detection challenges. To address this, we\nintroduce a novel approach based on Supervised Contrastive Learning. This\nmethod is designed to enhance the model's robustness to variations in prompts\nand focuses on distinguishing between different source LLMs. We evaluate our\nmodel through rigorous experiments involving three common prompting methods:\n``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:\n``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the\neffectiveness of our approach in model attribution tasks, achieving\nstate-of-the-art performance across diverse and unseen datasets."
                },
                "authors": [
                    {
                        "name": "Alimohammad Beigi"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Nivedh Mudiam"
                    },
                    {
                        "name": "Canyu Chen"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "10 pages, 2 figures, accepted at DSAA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08796v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08796v3",
                "updated": "2024-08-14T08:06:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    8,
                    6,
                    28,
                    2,
                    227,
                    0
                ],
                "published": "2024-04-12T20:03:06Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    20,
                    3,
                    6,
                    4,
                    103,
                    0
                ],
                "title": "The Elephant in the Room: Rethinking the Usage of Pre-trained Language\n  Model in Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Elephant in the Room: Rethinking the Usage of Pre-trained Language\n  Model in Sequential Recommendation"
                },
                "summary": "Sequential recommendation (SR) has seen significant advancements with the\nhelp of Pre-trained Language Models (PLMs). Some PLM-based SR models directly\nuse PLM to encode user historical behavior's text sequences to learn user\nrepresentations, while there is seldom an in-depth exploration of the\ncapability and suitability of PLM in behavior sequence modeling. In this work,\nwe first conduct extensive model analyses between PLMs and PLM-based SR models,\ndiscovering great underutilization and parameter redundancy of PLMs in behavior\nsequence modeling. Inspired by this, we explore different lightweight usages of\nPLMs in SR, aiming to maximally stimulate the ability of PLMs for SR while\nsatisfying the efficiency and usability demands of practical systems. We\ndiscover that adopting behavior-tuned PLMs for item initializations of\nconventional ID-based SR models is the most economical framework of PLM-based\nSR, which would not bring in any additional inference cost but could achieve a\ndramatic performance boost compared with the original version. Extensive\nexperiments on five datasets show that our simple and universal framework leads\nto significant improvement compared to classical SR and SOTA PLM-based SR\nmodels without additional inference costs. Our code can be found in\nhttps://github.com/777pomingzi/Rethinking-PLM-in-RS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation (SR) has seen significant advancements with the\nhelp of Pre-trained Language Models (PLMs). Some PLM-based SR models directly\nuse PLM to encode user historical behavior's text sequences to learn user\nrepresentations, while there is seldom an in-depth exploration of the\ncapability and suitability of PLM in behavior sequence modeling. In this work,\nwe first conduct extensive model analyses between PLMs and PLM-based SR models,\ndiscovering great underutilization and parameter redundancy of PLMs in behavior\nsequence modeling. Inspired by this, we explore different lightweight usages of\nPLMs in SR, aiming to maximally stimulate the ability of PLMs for SR while\nsatisfying the efficiency and usability demands of practical systems. We\ndiscover that adopting behavior-tuned PLMs for item initializations of\nconventional ID-based SR models is the most economical framework of PLM-based\nSR, which would not bring in any additional inference cost but could achieve a\ndramatic performance boost compared with the original version. Extensive\nexperiments on five datasets show that our simple and universal framework leads\nto significant improvement compared to classical SR and SOTA PLM-based SR\nmodels without additional inference costs. Our code can be found in\nhttps://github.com/777pomingzi/Rethinking-PLM-in-RS."
                },
                "authors": [
                    {
                        "name": "Zekai Qu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "arxiv_comment": "Accepted at RecSys 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08796v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08796v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07353v1",
                "updated": "2024-08-14T07:57:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    57,
                    51,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T07:57:51Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    57,
                    51,
                    2,
                    227,
                    0
                ],
                "title": "Only One Relation Possible? Modeling the Ambiguity in Event Temporal\n  Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Only One Relation Possible? Modeling the Ambiguity in Event Temporal\n  Relation Extraction"
                },
                "summary": "Event Temporal Relation Extraction (ETRE) aims to identify the temporal\nrelationship between two events, which plays an important role in natural\nlanguage understanding. Most previous works follow a single-label\nclassification style, classifying an event pair into either a specific temporal\nrelation (e.g., \\textit{Before}, \\textit{After}), or a special label\n\\textit{Vague} when there may be multiple possible temporal relations between\nthe pair. In our work, instead of directly making predictions on\n\\textit{Vague}, we propose a multi-label classification solution for ETRE\n(METRE) to infer the possibility of each temporal relation independently, where\nwe treat \\textit{Vague} as the cases when there is more than one possible\nrelation between two events. We design a speculation mechanism to explore the\npossible relations hidden behind \\textit{Vague}, which enables the latent\ninformation to be used efficiently. Experiments on TB-Dense, MATRES and UDS-T\nshow that our method can effectively utilize the \\textit{Vague} instances to\nimprove the recognition for specific temporal relations and outperforms most\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event Temporal Relation Extraction (ETRE) aims to identify the temporal\nrelationship between two events, which plays an important role in natural\nlanguage understanding. Most previous works follow a single-label\nclassification style, classifying an event pair into either a specific temporal\nrelation (e.g., \\textit{Before}, \\textit{After}), or a special label\n\\textit{Vague} when there may be multiple possible temporal relations between\nthe pair. In our work, instead of directly making predictions on\n\\textit{Vague}, we propose a multi-label classification solution for ETRE\n(METRE) to infer the possibility of each temporal relation independently, where\nwe treat \\textit{Vague} as the cases when there is more than one possible\nrelation between two events. We design a speculation mechanism to explore the\npossible relations hidden behind \\textit{Vague}, which enables the latent\ninformation to be used efficiently. Experiments on TB-Dense, MATRES and UDS-T\nshow that our method can effectively utilize the \\textit{Vague} instances to\nimprove the recognition for specific temporal relations and outperforms most\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yutong Hu"
                    },
                    {
                        "name": "Quzhe Huang"
                    },
                    {
                        "name": "Yansong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Feng"
                },
                "author": "Yansong Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2201.10208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2201.10208v2",
                "updated": "2024-08-14T07:45:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    45,
                    10,
                    2,
                    227,
                    0
                ],
                "published": "2022-01-25T10:02:23Z",
                "published_parsed": [
                    2022,
                    1,
                    25,
                    10,
                    2,
                    23,
                    1,
                    25,
                    0
                ],
                "title": "Semi-Supervised Quantile Estimation: Robust and Efficient Inference in\n  High Dimensional Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Quantile Estimation: Robust and Efficient Inference in\n  High Dimensional Settings"
                },
                "summary": "We consider quantile estimation in a semi-supervised setting, characterized\nby two available data sets: (i) a small or moderate sized labeled data set\ncontaining observations for a response and a set of possibly high dimensional\ncovariates, and (ii) a much larger unlabeled data set where only the covariates\nare observed. We propose a family of semi-supervised estimators for the\nresponse quantile(s) based on the two data sets, to improve the estimation\naccuracy compared to the supervised estimator, i.e., the sample quantile from\nthe labeled data. These estimators use a flexible imputation strategy applied\nto the estimating equation along with a debiasing step that allows for full\nrobustness against misspecification of the imputation model. Further, a\none-step update strategy is adopted to enable easy implementation of our method\nand handle the complexity from the non-linear nature of the quantile estimating\nequation. Under mild assumptions, our estimators are fully robust to the choice\nof the nuisance imputation model, in the sense of always maintaining root-n\nconsistency and asymptotic normality, while having improved efficiency relative\nto the supervised estimator. They also attain semi-parametric optimality if the\nrelation between the response and the covariates is correctly specified via the\nimputation model. As an illustration of estimating the nuisance imputation\nfunction, we consider kernel smoothing type estimators on lower dimensional and\npossibly estimated transformations of the high dimensional covariates, and we\nestablish novel results on their uniform convergence rates in high dimensions,\ninvolving responses indexed by a function class and usage of dimension\nreduction techniques. These results may be of independent interest. Numerical\nresults on both simulated and real data confirm our semi-supervised approach's\nimproved performance, in terms of both estimation and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider quantile estimation in a semi-supervised setting, characterized\nby two available data sets: (i) a small or moderate sized labeled data set\ncontaining observations for a response and a set of possibly high dimensional\ncovariates, and (ii) a much larger unlabeled data set where only the covariates\nare observed. We propose a family of semi-supervised estimators for the\nresponse quantile(s) based on the two data sets, to improve the estimation\naccuracy compared to the supervised estimator, i.e., the sample quantile from\nthe labeled data. These estimators use a flexible imputation strategy applied\nto the estimating equation along with a debiasing step that allows for full\nrobustness against misspecification of the imputation model. Further, a\none-step update strategy is adopted to enable easy implementation of our method\nand handle the complexity from the non-linear nature of the quantile estimating\nequation. Under mild assumptions, our estimators are fully robust to the choice\nof the nuisance imputation model, in the sense of always maintaining root-n\nconsistency and asymptotic normality, while having improved efficiency relative\nto the supervised estimator. They also attain semi-parametric optimality if the\nrelation between the response and the covariates is correctly specified via the\nimputation model. As an illustration of estimating the nuisance imputation\nfunction, we consider kernel smoothing type estimators on lower dimensional and\npossibly estimated transformations of the high dimensional covariates, and we\nestablish novel results on their uniform convergence rates in high dimensions,\ninvolving responses indexed by a function class and usage of dimension\nreduction techniques. These results may be of independent interest. Numerical\nresults on both simulated and real data confirm our semi-supervised approach's\nimproved performance, in terms of both estimation and inference."
                },
                "authors": [
                    {
                        "name": "Abhishek Chakrabortty"
                    },
                    {
                        "name": "Guorong Dai"
                    },
                    {
                        "name": "Raymond J. Carroll"
                    }
                ],
                "author_detail": {
                    "name": "Raymond J. Carroll"
                },
                "author": "Raymond J. Carroll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2201.10208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2201.10208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.14483v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.14483v2",
                "updated": "2024-08-14T07:42:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    42,
                    30,
                    2,
                    227,
                    0
                ],
                "published": "2023-10-23T01:29:18Z",
                "published_parsed": [
                    2023,
                    10,
                    23,
                    1,
                    29,
                    18,
                    0,
                    296,
                    0
                ],
                "title": "Chain-of-Factors Paper-Reviewer Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Factors Paper-Reviewer Matching"
                },
                "summary": "With the rapid increase in paper submissions to academic conferences, the\nneed for automated and accurate paper-reviewer matching is more critical than\never. Previous efforts in this area have considered various factors to assess\nthe relevance of a reviewer's expertise to a paper, such as the semantic\nsimilarity, shared topics, and citation connections between the paper and the\nreviewer's previous works. However, most of these studies focus on only one\nfactor, resulting in an incomplete evaluation of the paper-reviewer relevance.\nTo address this issue, we propose a unified model for paper-reviewer matching\nthat jointly considers semantic, topic, and citation factors. To be specific,\nduring training, we instruction-tune a contextualized language model shared\nacross all factors to capture their commonalities and characteristics; during\ninference, we chain the three factors to enable step-by-step, coarse-to-fine\nsearch for qualified reviewers given a submission. Experiments on four datasets\n(one of which is newly contributed by us) spanning various fields such as\nmachine learning, computer vision, information retrieval, and data mining\nconsistently demonstrate the effectiveness of our proposed Chain-of-Factors\nmodel in comparison with state-of-the-art paper-reviewer matching methods and\nscientific pre-trained language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid increase in paper submissions to academic conferences, the\nneed for automated and accurate paper-reviewer matching is more critical than\never. Previous efforts in this area have considered various factors to assess\nthe relevance of a reviewer's expertise to a paper, such as the semantic\nsimilarity, shared topics, and citation connections between the paper and the\nreviewer's previous works. However, most of these studies focus on only one\nfactor, resulting in an incomplete evaluation of the paper-reviewer relevance.\nTo address this issue, we propose a unified model for paper-reviewer matching\nthat jointly considers semantic, topic, and citation factors. To be specific,\nduring training, we instruction-tune a contextualized language model shared\nacross all factors to capture their commonalities and characteristics; during\ninference, we chain the three factors to enable step-by-step, coarse-to-fine\nsearch for qualified reviewers given a submission. Experiments on four datasets\n(one of which is newly contributed by us) spanning various fields such as\nmachine learning, computer vision, information retrieval, and data mining\nconsistently demonstrate the effectiveness of our proposed Chain-of-Factors\nmodel in comparison with state-of-the-art paper-reviewer matching methods and\nscientific pre-trained language models."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Yanzhen Shen"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.14483v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.14483v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07343v1",
                "updated": "2024-08-14T07:37:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    37,
                    7,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T07:37:07Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    37,
                    7,
                    2,
                    227,
                    0
                ],
                "title": "Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation"
                },
                "summary": "Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels will be available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels will be available."
                },
                "authors": [
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Yiwen Ye"
                    },
                    {
                        "name": "Yongsheng Pan"
                    },
                    {
                        "name": "Yong Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xia"
                },
                "author": "Yong Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00655v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00655v5",
                "updated": "2024-08-14T07:34:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    34,
                    44,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-01T15:45:19Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    15,
                    45,
                    19,
                    3,
                    214,
                    0
                ],
                "title": "SentenceVAE: Enable Next-sentence Prediction for Large Language Models\n  with Faster Speed, Higher Accuracy and Longer Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceVAE: Enable Next-sentence Prediction for Large Language Models\n  with Faster Speed, Higher Accuracy and Longer Context"
                },
                "summary": "Current large language models (LLMs) primarily utilize next-token prediction\nmethod for inference, which significantly impedes their processing speed. In\nthis paper, we introduce a novel inference methodology termed next-sentence\nprediction, aiming at enhancing the inference efficiency of LLMs. We present\nSentence Variational Autoencoder (SentenceVAE), which includes a Sentence\nEncoder to compress multiple tokens in a sentence into a single token, and a\nSentence Decoder to reconstruct it. By integrating SentenceVAE into the input\nand output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a\nsentence-by-sentence inference method. In addition, the SentenceVAE module of\nSLLMs can maintain the integrity of the original semantic content by segmenting\nthe context into sentences, thereby improving accuracy while boosting inference\nspeed. Moreover, compared to previous LLMs, SLLMs process fewer tokens over\nequivalent context length, significantly reducing memory demands for\nself-attention computation and facilitating the handling of longer context.\nExtensive experiments on Wanjuan dataset have revealed that the proposed method\ncan accelerate inference speed by 204~365%, reduce perplexity (PPL) to 46~75%\nof its original metric, and decrease memory overhead by 86~91% for the\nequivalent context length, compared to previous token-by-token methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language models (LLMs) primarily utilize next-token prediction\nmethod for inference, which significantly impedes their processing speed. In\nthis paper, we introduce a novel inference methodology termed next-sentence\nprediction, aiming at enhancing the inference efficiency of LLMs. We present\nSentence Variational Autoencoder (SentenceVAE), which includes a Sentence\nEncoder to compress multiple tokens in a sentence into a single token, and a\nSentence Decoder to reconstruct it. By integrating SentenceVAE into the input\nand output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a\nsentence-by-sentence inference method. In addition, the SentenceVAE module of\nSLLMs can maintain the integrity of the original semantic content by segmenting\nthe context into sentences, thereby improving accuracy while boosting inference\nspeed. Moreover, compared to previous LLMs, SLLMs process fewer tokens over\nequivalent context length, significantly reducing memory demands for\nself-attention computation and facilitating the handling of longer context.\nExtensive experiments on Wanjuan dataset have revealed that the proposed method\ncan accelerate inference speed by 204~365%, reduce perplexity (PPL) to 46~75%\nof its original metric, and decrease memory overhead by 86~91% for the\nequivalent context length, compared to previous token-by-token methods."
                },
                "authors": [
                    {
                        "name": "Hongjun An"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Zhe Sun"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "arxiv_comment": "update the article",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00655v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00655v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16333v2",
                "updated": "2024-08-14T07:34:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    34,
                    18,
                    2,
                    227,
                    0
                ],
                "published": "2024-04-25T04:46:02Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    4,
                    46,
                    2,
                    3,
                    116,
                    0
                ],
                "title": "AI Coders Are Among Us: Rethinking Programming Language Grammar Towards\n  Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Coders Are Among Us: Rethinking Programming Language Grammar Towards\n  Efficient Code Generation"
                },
                "summary": "Artificial Intelligence (AI) models have emerged as another important\naudience for programming languages alongside humans and machines, as we enter\nthe era of large language models (LLMs). LLMs can now perform well in coding\ncompetitions and even write programs like developers to solve various tasks,\nincluding mathematical problems. However, the grammar and layout of current\nprograms are designed to cater the needs of human developers -- with many\ngrammar tokens and formatting tokens being used to make the code easier for\nhumans to read. While this is helpful, such a design adds unnecessary\ncomputational work for LLMs, as each token they either use or produce consumes\ncomputational resources. To improve inference efficiency and reduce\ncomputational costs, we propose the concept of AI-oriented grammar. This aims\nto represent code in a way that better suits the working mechanism of AI\nmodels. Code written with AI-oriented grammar discards formats and uses a\nminimum number of tokens to convey code semantics effectively. To demonstrate\nthe feasibility of this concept, we explore and implement the first AI-oriented\ngrammar for Python, named SimPy. SimPy is crafted by revising the original\nPython grammar through a series of heuristic rules. Programs written in SimPy\nmaintain identical AST structures to those in standard Python. This allows for\nnot only execution via a modified AST parser, but also seamless transformation\nbetween programs written in Python and SimPy, enabling human developers and\nLLMs to use Python and SimPy, respectively, when they need to collaborate. In\nthe experiments, compared with Python, SimPy enables a reduction in token usage\nby 13.5% and 10.4% for CodeLlama and GPT-4, respectively, when completing the\nsame set of code-related tasks. Additionally, these models can maintain or even\nimprove their performance when using SimPy instead of Python for these tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) models have emerged as another important\naudience for programming languages alongside humans and machines, as we enter\nthe era of large language models (LLMs). LLMs can now perform well in coding\ncompetitions and even write programs like developers to solve various tasks,\nincluding mathematical problems. However, the grammar and layout of current\nprograms are designed to cater the needs of human developers -- with many\ngrammar tokens and formatting tokens being used to make the code easier for\nhumans to read. While this is helpful, such a design adds unnecessary\ncomputational work for LLMs, as each token they either use or produce consumes\ncomputational resources. To improve inference efficiency and reduce\ncomputational costs, we propose the concept of AI-oriented grammar. This aims\nto represent code in a way that better suits the working mechanism of AI\nmodels. Code written with AI-oriented grammar discards formats and uses a\nminimum number of tokens to convey code semantics effectively. To demonstrate\nthe feasibility of this concept, we explore and implement the first AI-oriented\ngrammar for Python, named SimPy. SimPy is crafted by revising the original\nPython grammar through a series of heuristic rules. Programs written in SimPy\nmaintain identical AST structures to those in standard Python. This allows for\nnot only execution via a modified AST parser, but also seamless transformation\nbetween programs written in Python and SimPy, enabling human developers and\nLLMs to use Python and SimPy, respectively, when they need to collaborate. In\nthe experiments, compared with Python, SimPy enables a reduction in token usage\nby 13.5% and 10.4% for CodeLlama and GPT-4, respectively, when completing the\nsame set of code-related tasks. Additionally, these models can maintain or even\nimprove their performance when using SimPy instead of Python for these tasks."
                },
                "authors": [
                    {
                        "name": "Zhensu Sun"
                    },
                    {
                        "name": "Xiaoning Du"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "Accepted by ISSTA'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03337v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03337v3",
                "updated": "2024-08-15T06:58:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    58,
                    8,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-22T07:19:12Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    19,
                    12,
                    0,
                    204,
                    0
                ],
                "title": "PsyDI: Towards a Personalized and Progressively In-depth Chatbot for\n  Psychological Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyDI: Towards a Personalized and Progressively In-depth Chatbot for\n  Psychological Measurements"
                },
                "summary": "In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI"
                },
                "authors": [
                    {
                        "name": "Xueyan Li"
                    },
                    {
                        "name": "Xinyan Chen"
                    },
                    {
                        "name": "Yazhe Niu"
                    },
                    {
                        "name": "Shuai Hu"
                    },
                    {
                        "name": "Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Liu"
                },
                "author": "Yu Liu",
                "arxiv_comment": "29 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03337v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03337v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00816v3",
                "updated": "2024-08-14T07:26:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    26,
                    8,
                    2,
                    227,
                    0
                ],
                "published": "2024-02-26T01:17:50Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    1,
                    17,
                    50,
                    0,
                    57,
                    0
                ],
                "title": "Read and Think: An Efficient Step-wise Multimodal Language Model for\n  Document Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read and Think: An Efficient Step-wise Multimodal Language Model for\n  Document Understanding and Reasoning"
                },
                "summary": "Understanding the contents of multimodal documents is essential to accurately\nextract relevant evidence and use it for reasoning. Existing document\nunderstanding models tend to generate answers with a single word or phrase\ndirectly, ignoring the source document's evidence and lacking interpretability.\nIn this work, we address the lack of step-wise capabilities through data\naugmentation and extension. Specifically, We use Multi-modal Large Language\nModels (MLLMs), which have strong visual understanding and reasoning abilities,\nas data generators to generate step-wise question-and-answer pairs for document\nimages and use a high-performance LLM as the error detector to filter out noisy\ndata. This step-wise data generation pipeline is implemented using both\ntemplate-based and few-shot methods. We then use the generated high-quality\ndata to train a humanized document understanding and reasoning model,\nspecifically designed to solve complex questions that require reasoning or\nmulti-hop question answering, dubbed DocAssistant. Experimental results\ndemonstrate the effectiveness and application value of step-wise generation,\nshowing a 5 improvement on InfoVQA with complex layouts and a 7 improvement on\nChartQA with complex reasoning, compared to directly generated answers. We hope\nour work highlights the potential of synthetic data and encourages further\nexploration of multi-modal document reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the contents of multimodal documents is essential to accurately\nextract relevant evidence and use it for reasoning. Existing document\nunderstanding models tend to generate answers with a single word or phrase\ndirectly, ignoring the source document's evidence and lacking interpretability.\nIn this work, we address the lack of step-wise capabilities through data\naugmentation and extension. Specifically, We use Multi-modal Large Language\nModels (MLLMs), which have strong visual understanding and reasoning abilities,\nas data generators to generate step-wise question-and-answer pairs for document\nimages and use a high-performance LLM as the error detector to filter out noisy\ndata. This step-wise data generation pipeline is implemented using both\ntemplate-based and few-shot methods. We then use the generated high-quality\ndata to train a humanized document understanding and reasoning model,\nspecifically designed to solve complex questions that require reasoning or\nmulti-hop question answering, dubbed DocAssistant. Experimental results\ndemonstrate the effectiveness and application value of step-wise generation,\nshowing a 5 improvement on InfoVQA with complex layouts and a 7 improvement on\nChartQA with complex reasoning, compared to directly generated answers. We hope\nour work highlights the potential of synthetic data and encourages further\nexploration of multi-modal document reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Jinxu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jinxu Zhang"
                },
                "author": "Jinxu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2201.00468v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2201.00468v3",
                "updated": "2024-08-14T07:24:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    24,
                    17,
                    2,
                    227,
                    0
                ],
                "published": "2022-01-03T04:12:44Z",
                "published_parsed": [
                    2022,
                    1,
                    3,
                    4,
                    12,
                    44,
                    0,
                    3,
                    0
                ],
                "title": "A General Framework for Treatment Effect Estimation in Semi-Supervised\n  and High Dimensional Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Framework for Treatment Effect Estimation in Semi-Supervised\n  and High Dimensional Settings"
                },
                "summary": "In this article, we aim to provide a general and complete understanding of\nsemi-supervised (SS) causal inference for treatment effects. Specifically, we\nconsider two such estimands: (a) the average treatment effect and (b) the\nquantile treatment effect, as prototype cases, in an SS setting, characterized\nby two available data sets: (i) a labeled data set of size $n$, providing\nobservations for a response and a set of high dimensional covariates, as well\nas a binary treatment indicator; and (ii) an unlabeled data set of size $N$,\nmuch larger than $n$, but without the response observed. Using these two data\nsets, we develop a family of SS estimators which are ensured to be: (1) more\nrobust and (2) more efficient than their supervised counterparts based on the\nlabeled data set only. Beyond the 'standard' double robustness results (in\nterms of consistency) that can be achieved by supervised methods as well, we\nfurther establish root-n consistency and asymptotic normality of our SS\nestimators whenever the propensity score in the model is correctly specified,\nwithout requiring specific forms of the nuisance functions involved. Such an\nimprovement of robustness arises from the use of the massive unlabeled data, so\nit is generally not attainable in a purely supervised setting. In addition, our\nestimators are shown to be semi-parametrically efficient as long as all the\nnuisance functions are correctly specified. Moreover, as an illustration of the\nnuisance estimators, we consider inverse-probability-weighting type kernel\nsmoothing estimators involving unknown covariate transformation mechanisms, and\nestablish in high dimensional scenarios novel results on their uniform\nconvergence rates, which should be of independent interest. Numerical results\non both simulated and real data validate the advantage of our methods over\ntheir supervised counterparts with respect to both robustness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we aim to provide a general and complete understanding of\nsemi-supervised (SS) causal inference for treatment effects. Specifically, we\nconsider two such estimands: (a) the average treatment effect and (b) the\nquantile treatment effect, as prototype cases, in an SS setting, characterized\nby two available data sets: (i) a labeled data set of size $n$, providing\nobservations for a response and a set of high dimensional covariates, as well\nas a binary treatment indicator; and (ii) an unlabeled data set of size $N$,\nmuch larger than $n$, but without the response observed. Using these two data\nsets, we develop a family of SS estimators which are ensured to be: (1) more\nrobust and (2) more efficient than their supervised counterparts based on the\nlabeled data set only. Beyond the 'standard' double robustness results (in\nterms of consistency) that can be achieved by supervised methods as well, we\nfurther establish root-n consistency and asymptotic normality of our SS\nestimators whenever the propensity score in the model is correctly specified,\nwithout requiring specific forms of the nuisance functions involved. Such an\nimprovement of robustness arises from the use of the massive unlabeled data, so\nit is generally not attainable in a purely supervised setting. In addition, our\nestimators are shown to be semi-parametrically efficient as long as all the\nnuisance functions are correctly specified. Moreover, as an illustration of the\nnuisance estimators, we consider inverse-probability-weighting type kernel\nsmoothing estimators involving unknown covariate transformation mechanisms, and\nestablish in high dimensional scenarios novel results on their uniform\nconvergence rates, which should be of independent interest. Numerical results\non both simulated and real data validate the advantage of our methods over\ntheir supervised counterparts with respect to both robustness and efficiency."
                },
                "authors": [
                    {
                        "name": "Abhishek Chakrabortty"
                    },
                    {
                        "name": "Guorong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guorong Dai"
                },
                "author": "Guorong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2201.00468v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2201.00468v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07326v1",
                "updated": "2024-08-14T06:56:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    56,
                    20,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T06:56:20Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    56,
                    20,
                    2,
                    227,
                    0
                ],
                "title": "LPU: A Latency-Optimized and Highly Scalable Processor for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LPU: A Latency-Optimized and Highly Scalable Processor for Large\n  Language Model Inference"
                },
                "summary": "The explosive arrival of OpenAI's ChatGPT has fueled the globalization of\nlarge language model (LLM), which consists of billions of pretrained parameters\nthat embodies the aspects of syntax and semantics. HyperAccel introduces\nlatency processing unit (LPU), a latency-optimized and highly scalable\nprocessor architecture for the acceleration of LLM inference. LPU perfectly\nbalances the memory bandwidth and compute logic with streamlined dataflow to\nmaximize performance and efficiency. LPU is equipped with expandable\nsynchronization link (ESL) that hides data synchronization latency between\nmultiple LPUs. HyperDex complements LPU as an intuitive software framework to\nrun LLM applications. LPU achieves 1.25 ms/token and 20.9 ms/token for 1.3B and\n66B model, respectively, which is 2.09x and 1.37x faster than the GPU. LPU,\nsynthesized using Samsung 4nm process, has total area of 0.824 mm2 and power\nconsumption of 284.31 mW. LPU-based servers achieve 1.33x and 1.32x energy\nefficiency over NVIDIA H100 and L4 servers, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosive arrival of OpenAI's ChatGPT has fueled the globalization of\nlarge language model (LLM), which consists of billions of pretrained parameters\nthat embodies the aspects of syntax and semantics. HyperAccel introduces\nlatency processing unit (LPU), a latency-optimized and highly scalable\nprocessor architecture for the acceleration of LLM inference. LPU perfectly\nbalances the memory bandwidth and compute logic with streamlined dataflow to\nmaximize performance and efficiency. LPU is equipped with expandable\nsynchronization link (ESL) that hides data synchronization latency between\nmultiple LPUs. HyperDex complements LPU as an intuitive software framework to\nrun LLM applications. LPU achieves 1.25 ms/token and 20.9 ms/token for 1.3B and\n66B model, respectively, which is 2.09x and 1.37x faster than the GPU. LPU,\nsynthesized using Samsung 4nm process, has total area of 0.824 mm2 and power\nconsumption of 284.31 mW. LPU-based servers achieve 1.33x and 1.32x energy\nefficiency over NVIDIA H100 and L4 servers, respectively."
                },
                "authors": [
                    {
                        "name": "Seungjae Moon"
                    },
                    {
                        "name": "Jung-Hoon Kim"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "Junseo Cha"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Sukbin Lim"
                    },
                    {
                        "name": "Gyubin Choi"
                    },
                    {
                        "name": "Dongjin Seo"
                    },
                    {
                        "name": "Jongho Kim"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Hyunjun Park"
                    },
                    {
                        "name": "Ryeowook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Jongse Park"
                    },
                    {
                        "name": "Jinwon Lee"
                    },
                    {
                        "name": "Joo-Young Kim"
                    }
                ],
                "author_detail": {
                    "name": "Joo-Young Kim"
                },
                "author": "Joo-Young Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07321v1",
                "updated": "2024-08-14T06:43:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    43,
                    6,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T06:43:06Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    43,
                    6,
                    2,
                    227,
                    0
                ],
                "title": "LLM-Enhanced Static Analysis for Precise Identification of Vulnerable\n  OSS Versions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Static Analysis for Precise Identification of Vulnerable\n  OSS Versions"
                },
                "summary": "Open-source software (OSS) has experienced a surge in popularity, attributed\nto its collaborative development model and cost-effective nature. However, the\nadoption of specific software versions in development projects may introduce\nsecurity risks when these versions bring along vulnerabilities. Current methods\nof identifying vulnerable versions typically analyze and trace the code\ninvolved in vulnerability patches using static analysis with pre-defined rules.\nThey then use syntactic-level code clone detection to identify the vulnerable\nversions. These methods are hindered by imprecisions due to (1) the inclusion\nof vulnerability-irrelevant code in the analysis and (2) the inadequacy of\nsyntactic-level code clone detection. This paper presents Vercation, an\napproach designed to identify vulnerable versions of OSS written in C/C++.\nVercation combines program slicing with a Large Language Model (LLM) to\nidentify vulnerability-relevant code from vulnerability patches. It then\nbacktraces historical commits to gather previous modifications of identified\nvulnerability-relevant code. We propose semantic-level code clone detection to\ncompare the differences between pre-modification and post-modification code,\nthereby locating the vulnerability-introducing commit (vic) and enabling to\nidentify the vulnerable versions between the patch commit and the vic. We\ncurate a dataset linking 74 OSS vulnerabilities and 1013 versions to evaluate\nVercation. On this dataset, our approach achieves the F1 score of 92.4%,\noutperforming current state-of-the-art methods. More importantly, Vercation\ndetected 134 incorrect vulnerable OSS versions in NVD reports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source software (OSS) has experienced a surge in popularity, attributed\nto its collaborative development model and cost-effective nature. However, the\nadoption of specific software versions in development projects may introduce\nsecurity risks when these versions bring along vulnerabilities. Current methods\nof identifying vulnerable versions typically analyze and trace the code\ninvolved in vulnerability patches using static analysis with pre-defined rules.\nThey then use syntactic-level code clone detection to identify the vulnerable\nversions. These methods are hindered by imprecisions due to (1) the inclusion\nof vulnerability-irrelevant code in the analysis and (2) the inadequacy of\nsyntactic-level code clone detection. This paper presents Vercation, an\napproach designed to identify vulnerable versions of OSS written in C/C++.\nVercation combines program slicing with a Large Language Model (LLM) to\nidentify vulnerability-relevant code from vulnerability patches. It then\nbacktraces historical commits to gather previous modifications of identified\nvulnerability-relevant code. We propose semantic-level code clone detection to\ncompare the differences between pre-modification and post-modification code,\nthereby locating the vulnerability-introducing commit (vic) and enabling to\nidentify the vulnerable versions between the patch commit and the vic. We\ncurate a dataset linking 74 OSS vulnerabilities and 1013 versions to evaluate\nVercation. On this dataset, our approach achieves the F1 score of 92.4%,\noutperforming current state-of-the-art methods. More importantly, Vercation\ndetected 134 incorrect vulnerable OSS versions in NVD reports."
                },
                "authors": [
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Shouguo Yang"
                    },
                    {
                        "name": "Chaopeng Dong"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Shichao Lv"
                    },
                    {
                        "name": "Zhiqiang Shi"
                    },
                    {
                        "name": "Limin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Limin Sun"
                },
                "author": "Limin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06643v2",
                "updated": "2024-08-14T06:18:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    18,
                    3,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T05:27:22Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    5,
                    27,
                    22,
                    1,
                    226,
                    0
                ],
                "title": "BMX: Entropy-weighted Similarity and Semantic-enhanced Lexical Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BMX: Entropy-weighted Similarity and Semantic-enhanced Lexical Search"
                },
                "summary": "BM25, a widely-used lexical search algorithm, remains crucial in information\nretrieval despite the rise of pre-trained and large language models\n(PLMs/LLMs). However, it neglects query-document similarity and lacks semantic\nunderstanding, limiting its performance. We revisit BM25 and introduce BMX, a\nnovel extension of BM25 incorporating entropy-weighted similarity and semantic\nenhancement techniques. Extensive experiments demonstrate that BMX consistently\noutperforms traditional BM25 and surpasses PLM/LLM-based dense retrieval in\nlong-context and real-world retrieval benchmarks. This study bridges the gap\nbetween classical lexical search and modern semantic approaches, offering a\npromising direction for future information retrieval research. The reference\nimplementation of BMX can be found in Baguetter, which was created in the\ncontext of this work. The code can be found here:\nhttps://github.com/mixedbread-ai/baguetter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BM25, a widely-used lexical search algorithm, remains crucial in information\nretrieval despite the rise of pre-trained and large language models\n(PLMs/LLMs). However, it neglects query-document similarity and lacks semantic\nunderstanding, limiting its performance. We revisit BM25 and introduce BMX, a\nnovel extension of BM25 incorporating entropy-weighted similarity and semantic\nenhancement techniques. Extensive experiments demonstrate that BMX consistently\noutperforms traditional BM25 and surpasses PLM/LLM-based dense retrieval in\nlong-context and real-world retrieval benchmarks. This study bridges the gap\nbetween classical lexical search and modern semantic approaches, offering a\npromising direction for future information retrieval research. The reference\nimplementation of BMX can be found in Baguetter, which was created in the\ncontext of this work. The code can be found here:\nhttps://github.com/mixedbread-ai/baguetter."
                },
                "authors": [
                    {
                        "name": "Xianming Li"
                    },
                    {
                        "name": "Julius Lipp"
                    },
                    {
                        "name": "Aamir Shakir"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "arxiv_comment": "correct the affiliation order",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07313v1",
                "updated": "2024-08-14T06:14:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    14,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T06:14:02Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    14,
                    2,
                    2,
                    227,
                    0
                ],
                "title": "Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal\n  Data for Mental Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal\n  Data for Mental Health"
                },
                "summary": "Integrating physiological signals such as electroencephalogram (EEG), with\nother data such as interview audio, may offer valuable multimodal insights into\npsychological states or neurological disorders. Recent advancements with Large\nLanguage Models (LLMs) position them as prospective ``health agents'' for\nmental health assessment. However, current research predominantly focus on\nsingle data modalities, presenting an opportunity to advance understanding\nthrough multimodal data. Our study aims to advance this approach by\ninvestigating multimodal data using LLMs for mental health assessment,\nspecifically through zero-shot and few-shot prompting. Three datasets are\nadopted for depression and emotion classifications incorporating EEG, facial\nexpressions, and audio (text). The results indicate that multimodal information\nconfers substantial advantages over single modality approaches in mental health\nassessment. Notably, integrating EEG alongside commonly used LLM modalities\nsuch as audio and images demonstrates promising potential. Moreover, our\nfindings reveal that 1-shot learning offers greater benefits compared to\nzero-shot learning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating physiological signals such as electroencephalogram (EEG), with\nother data such as interview audio, may offer valuable multimodal insights into\npsychological states or neurological disorders. Recent advancements with Large\nLanguage Models (LLMs) position them as prospective ``health agents'' for\nmental health assessment. However, current research predominantly focus on\nsingle data modalities, presenting an opportunity to advance understanding\nthrough multimodal data. Our study aims to advance this approach by\ninvestigating multimodal data using LLMs for mental health assessment,\nspecifically through zero-shot and few-shot prompting. Three datasets are\nadopted for depression and emotion classifications incorporating EEG, facial\nexpressions, and audio (text). The results indicate that multimodal information\nconfers substantial advantages over single modality approaches in mental health\nassessment. Notably, integrating EEG alongside commonly used LLM modalities\nsuch as audio and images demonstrates promising potential. Moreover, our\nfindings reveal that 1-shot learning offers greater benefits compared to\nzero-shot learning methods."
                },
                "authors": [
                    {
                        "name": "Yongquan Hu"
                    },
                    {
                        "name": "Shuning Zhang"
                    },
                    {
                        "name": "Ting Dang"
                    },
                    {
                        "name": "Hong Jia"
                    },
                    {
                        "name": "Flora D. Salim"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Aaron J. Quigley"
                    }
                ],
                "author_detail": {
                    "name": "Aaron J. Quigley"
                },
                "author": "Aaron J. Quigley",
                "arxiv_doi": "10.1145/3675094.3678494",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3675094.3678494",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.07313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages; UbiComp Companion '24, Companion of the 2024 ACM\n  International Joint Conference on Pervasive and Ubiquitous Computing, October\n  5--9, 2024}{Melbourne, VIC, Australia",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07311v1",
                "updated": "2024-08-14T06:07:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    7,
                    1,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T06:07:01Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    7,
                    1,
                    2,
                    227,
                    0
                ],
                "title": "MultiSurf-GPT: Facilitating Context-Aware Reasoning with Large-Scale\n  Language Models for Multimodal Surface Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiSurf-GPT: Facilitating Context-Aware Reasoning with Large-Scale\n  Language Models for Multimodal Surface Sensing"
                },
                "summary": "Surface sensing is widely employed in health diagnostics, manufacturing and\nsafety monitoring. Advances in mobile sensing affords this potential for\ncontext awareness in mobile computing, typically with a single sensing\nmodality. Emerging multimodal large-scale language models offer new\nopportunities. We propose MultiSurf-GPT, which utilizes the advanced\ncapabilities of GPT-4o to process and interpret diverse modalities (radar,\nmicroscope and multispectral data) uniformly based on prompting strategies\n(zero-shot and few-shot prompting). We preliminarily validated our framework by\nusing MultiSurf-GPT to identify low-level information, and to infer high-level\ncontext-aware analytics, demonstrating the capability of augmenting\ncontext-aware insights. This framework shows promise as a tool to expedite the\ndevelopment of more complex context-aware applications in the future, providing\na faster, more cost-effective, and integrated solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface sensing is widely employed in health diagnostics, manufacturing and\nsafety monitoring. Advances in mobile sensing affords this potential for\ncontext awareness in mobile computing, typically with a single sensing\nmodality. Emerging multimodal large-scale language models offer new\nopportunities. We propose MultiSurf-GPT, which utilizes the advanced\ncapabilities of GPT-4o to process and interpret diverse modalities (radar,\nmicroscope and multispectral data) uniformly based on prompting strategies\n(zero-shot and few-shot prompting). We preliminarily validated our framework by\nusing MultiSurf-GPT to identify low-level information, and to infer high-level\ncontext-aware analytics, demonstrating the capability of augmenting\ncontext-aware insights. This framework shows promise as a tool to expedite the\ndevelopment of more complex context-aware applications in the future, providing\na faster, more cost-effective, and integrated solution."
                },
                "authors": [
                    {
                        "name": "Yongquan Hu"
                    },
                    {
                        "name": "Black Sun"
                    },
                    {
                        "name": "Pengcheng An"
                    },
                    {
                        "name": "Zhuying Li"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Aaron J. Quigley"
                    }
                ],
                "author_detail": {
                    "name": "Aaron J. Quigley"
                },
                "author": "Aaron J. Quigley",
                "arxiv_doi": "10.1145/3640471.3680450",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3640471.3680450",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.07311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages. MOBILEHCI Adjunct '24, 26th International Conference on\n  Mobile Human-Computer Interaction, September 30-October 3, 2024, Melbourne,\n  VIC, Australia",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07303v1",
                "updated": "2024-08-14T05:18:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    18,
                    43,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:18:43Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    18,
                    43,
                    2,
                    227,
                    0
                ],
                "title": "Enhancing Visual Question Answering through Ranking-Based Hybrid\n  Training and Multimodal Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Visual Question Answering through Ranking-Based Hybrid\n  Training and Multimodal Fusion"
                },
                "summary": "Visual Question Answering (VQA) is a challenging task that requires systems\nto provide accurate answers to questions based on image content. Current VQA\nmodels struggle with complex questions due to limitations in capturing and\nintegrating multimodal information effectively. To address these challenges, we\npropose the Rank VQA model, which leverages a ranking-inspired hybrid training\nstrategy to enhance VQA performance. The Rank VQA model integrates high-quality\nvisual features extracted using the Faster R-CNN model and rich semantic text\nfeatures obtained from a pre-trained BERT model. These features are fused\nthrough a sophisticated multimodal fusion technique employing multi-head\nself-attention mechanisms. Additionally, a ranking learning module is\nincorporated to optimize the relative ranking of answers, thus improving answer\naccuracy. The hybrid training strategy combines classification and ranking\nlosses, enhancing the model's generalization ability and robustness across\ndiverse datasets. Experimental results demonstrate the effectiveness of the\nRank VQA model. Our model significantly outperforms existing state-of-the-art\nmodels on standard VQA datasets, including VQA v2.0 and COCO-QA, in terms of\nboth accuracy and Mean Reciprocal Rank (MRR). The superior performance of Rank\nVQA is evident in its ability to handle complex questions that require\nunderstanding nuanced details and making sophisticated inferences from the\nimage and text. This work highlights the effectiveness of a ranking-based\nhybrid training strategy in improving VQA performance and lays the groundwork\nfor further research in multimodal learning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Question Answering (VQA) is a challenging task that requires systems\nto provide accurate answers to questions based on image content. Current VQA\nmodels struggle with complex questions due to limitations in capturing and\nintegrating multimodal information effectively. To address these challenges, we\npropose the Rank VQA model, which leverages a ranking-inspired hybrid training\nstrategy to enhance VQA performance. The Rank VQA model integrates high-quality\nvisual features extracted using the Faster R-CNN model and rich semantic text\nfeatures obtained from a pre-trained BERT model. These features are fused\nthrough a sophisticated multimodal fusion technique employing multi-head\nself-attention mechanisms. Additionally, a ranking learning module is\nincorporated to optimize the relative ranking of answers, thus improving answer\naccuracy. The hybrid training strategy combines classification and ranking\nlosses, enhancing the model's generalization ability and robustness across\ndiverse datasets. Experimental results demonstrate the effectiveness of the\nRank VQA model. Our model significantly outperforms existing state-of-the-art\nmodels on standard VQA datasets, including VQA v2.0 and COCO-QA, in terms of\nboth accuracy and Mean Reciprocal Rank (MRR). The superior performance of Rank\nVQA is evident in its ability to handle complex questions that require\nunderstanding nuanced details and making sophisticated inferences from the\nimage and text. This work highlights the effectiveness of a ranking-based\nhybrid training strategy in improving VQA performance and lays the groundwork\nfor further research in multimodal learning methods."
                },
                "authors": [
                    {
                        "name": "Peiyuan Chen"
                    },
                    {
                        "name": "Zecheng Zhang"
                    },
                    {
                        "name": "Yiping Dong"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Han Wang"
                    }
                ],
                "author_detail": {
                    "name": "Han Wang"
                },
                "author": "Han Wang",
                "arxiv_comment": "Visual Question Answering, Rank VQA, Faster R-CNN, BERT, Multimodal\n  Fusion, Ranking Learning, Hybrid Training Strategy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09021v2",
                "updated": "2024-08-14T04:52:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    4,
                    52,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-06-13T11:55:40Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    11,
                    55,
                    40,
                    3,
                    165,
                    0
                ],
                "title": "Contextual Distillation Model for Diversified Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Distillation Model for Diversified Recommendation"
                },
                "summary": "The diversity of recommendation is equally crucial as accuracy in improving\nuser experience. Existing studies, e.g., Determinantal Point Process (DPP) and\nMaximal Marginal Relevance (MMR), employ a greedy paradigm to iteratively\nselect items that optimize both accuracy and diversity. However, prior methods\ntypically exhibit quadratic complexity, limiting their applications to the\nre-ranking stage and are not applicable to other recommendation stages with a\nlarger pool of candidate items, such as the pre-ranking and ranking stages. In\nthis paper, we propose Contextual Distillation Model (CDM), an efficient\nrecommendation model that addresses diversification, suitable for the\ndeployment in all stages of industrial recommendation pipelines. Specifically,\nCDM utilizes the candidate items in the same user request as context to enhance\nthe diversification of the results. We propose a contrastive context encoder\nthat employs attention mechanisms to model both positive and negative contexts.\nFor the training of CDM, we compare each target item with its context embedding\nand utilize the knowledge distillation framework to learn the win probability\nof each target item under the MMR algorithm, where the teacher is derived from\nMMR outputs. During inference, ranking is performed through a linear\ncombination of the recommendation and student model scores, ensuring both\ndiversity and efficiency. We perform offline evaluations on two industrial\ndatasets and conduct online A/B test of CDM on the short-video platform\nKuaiShou. The considerable enhancements observed in both recommendation quality\nand diversity, as shown by metrics, provide strong superiority for the\neffectiveness of CDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diversity of recommendation is equally crucial as accuracy in improving\nuser experience. Existing studies, e.g., Determinantal Point Process (DPP) and\nMaximal Marginal Relevance (MMR), employ a greedy paradigm to iteratively\nselect items that optimize both accuracy and diversity. However, prior methods\ntypically exhibit quadratic complexity, limiting their applications to the\nre-ranking stage and are not applicable to other recommendation stages with a\nlarger pool of candidate items, such as the pre-ranking and ranking stages. In\nthis paper, we propose Contextual Distillation Model (CDM), an efficient\nrecommendation model that addresses diversification, suitable for the\ndeployment in all stages of industrial recommendation pipelines. Specifically,\nCDM utilizes the candidate items in the same user request as context to enhance\nthe diversification of the results. We propose a contrastive context encoder\nthat employs attention mechanisms to model both positive and negative contexts.\nFor the training of CDM, we compare each target item with its context embedding\nand utilize the knowledge distillation framework to learn the win probability\nof each target item under the MMR algorithm, where the teacher is derived from\nMMR outputs. During inference, ranking is performed through a linear\ncombination of the recommendation and student model scores, ensuring both\ndiversity and efficiency. We perform offline evaluations on two industrial\ndatasets and conduct online A/B test of CDM on the short-video platform\nKuaiShou. The considerable enhancements observed in both recommendation quality\nand diversity, as shown by metrics, provide strong superiority for the\neffectiveness of CDM."
                },
                "authors": [
                    {
                        "name": "Fan Li"
                    },
                    {
                        "name": "Xu Si"
                    },
                    {
                        "name": "Shisong Tang"
                    },
                    {
                        "name": "Dingmin Wang"
                    },
                    {
                        "name": "Kunyan Han"
                    },
                    {
                        "name": "Bing Han"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Hechang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hechang Chen"
                },
                "author": "Hechang Chen",
                "arxiv_doi": "10.1145/3637528.3671514",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3637528.3671514",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.09021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted by KDD 2024 v2",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07291v1",
                "updated": "2024-08-14T04:49:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    4,
                    49,
                    30,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T04:49:30Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    4,
                    49,
                    30,
                    2,
                    227,
                    0
                ],
                "title": "Evaluating Large Language Model based Personal Information Extraction\n  and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Model based Personal Information Extraction\n  and Countermeasures"
                },
                "summary": "Automatically extracting personal information--such as name, phone number,\nand email address--from publicly available profiles at a large scale is a\nstepstone to many other security attacks including spear phishing. Traditional\nmethods--such as regular expression, keyword search, and entity\ndetection--achieve limited success at such personal information extraction. In\nthis work, we perform a systematic measurement study to benchmark large\nlanguage model (LLM) based personal information extraction and countermeasures.\nTowards this goal, we present a framework for LLM-based extraction attacks;\ncollect three datasets including a synthetic dataset generated by GPT-4 and two\nreal-world datasets with manually labeled 8 categories of personal information;\nintroduce a novel mitigation strategy based on \\emph{prompt injection}; and\nsystematically benchmark LLM-based attacks and countermeasures using 10 LLMs\nand our 3 datasets. Our key findings include: LLM can be misused by attackers\nto accurately extract various personal information from personal profiles; LLM\noutperforms conventional methods at such extraction; and prompt injection can\nmitigate such risk to a large extent and outperforms conventional\ncountermeasures. Our code and data are available at:\n\\url{https://github.com/liu00222/LLM-Based-Personal-Profile-Extraction}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically extracting personal information--such as name, phone number,\nand email address--from publicly available profiles at a large scale is a\nstepstone to many other security attacks including spear phishing. Traditional\nmethods--such as regular expression, keyword search, and entity\ndetection--achieve limited success at such personal information extraction. In\nthis work, we perform a systematic measurement study to benchmark large\nlanguage model (LLM) based personal information extraction and countermeasures.\nTowards this goal, we present a framework for LLM-based extraction attacks;\ncollect three datasets including a synthetic dataset generated by GPT-4 and two\nreal-world datasets with manually labeled 8 categories of personal information;\nintroduce a novel mitigation strategy based on \\emph{prompt injection}; and\nsystematically benchmark LLM-based attacks and countermeasures using 10 LLMs\nand our 3 datasets. Our key findings include: LLM can be misused by attackers\nto accurately extract various personal information from personal profiles; LLM\noutperforms conventional methods at such extraction; and prompt injection can\nmitigate such risk to a large extent and outperforms conventional\ncountermeasures. Our code and data are available at:\n\\url{https://github.com/liu00222/LLM-Based-Personal-Profile-Extraction}."
                },
                "authors": [
                    {
                        "name": "Yupei Liu"
                    },
                    {
                        "name": "Yuqi Jia"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zhenqiang Gong"
                },
                "author": "Neil Zhenqiang Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03601v2",
                "updated": "2024-08-14T04:31:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    4,
                    31,
                    32,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-07T07:41:01Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    7,
                    41,
                    1,
                    2,
                    220,
                    0
                ],
                "title": "DRAMA: An Efficient End-to-end Motion Planner for Autonomous Driving\n  with Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRAMA: An Efficient End-to-end Motion Planner for Autonomous Driving\n  with Mamba"
                },
                "summary": "Motion planning is a challenging task to generate safe and feasible\ntrajectories in highly dynamic and complex environments, forming a core\ncapability for autonomous vehicles. In this paper, we propose DRAMA, the first\nMamba-based end-to-end motion planner for autonomous vehicles. DRAMA fuses\ncamera, LiDAR Bird's Eye View images in the feature space, as well as ego\nstatus information, to generate a series of future ego trajectories. Unlike\ntraditional transformer-based methods with quadratic attention complexity for\nsequence length, DRAMA is able to achieve a less computationally intensive\nattention complexity, demonstrating potential to deal with increasingly complex\nscenarios. Leveraging our Mamba fusion module, DRAMA efficiently and\neffectively fuses the features of the camera and LiDAR modalities. In addition,\nwe introduce a Mamba-Transformer decoder that enhances the overall planning\nperformance. This module is universally adaptable to any Transformer-based\nmodel, especially for tasks with long sequence inputs. We further introduce a\nnovel feature state dropout which improves the planner's robustness without\nincreasing training and inference times. Extensive experimental results show\nthat DRAMA achieves higher accuracy on the NAVSIM dataset compared to the\nbaseline Transfuser, with fewer parameters and lower computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion planning is a challenging task to generate safe and feasible\ntrajectories in highly dynamic and complex environments, forming a core\ncapability for autonomous vehicles. In this paper, we propose DRAMA, the first\nMamba-based end-to-end motion planner for autonomous vehicles. DRAMA fuses\ncamera, LiDAR Bird's Eye View images in the feature space, as well as ego\nstatus information, to generate a series of future ego trajectories. Unlike\ntraditional transformer-based methods with quadratic attention complexity for\nsequence length, DRAMA is able to achieve a less computationally intensive\nattention complexity, demonstrating potential to deal with increasingly complex\nscenarios. Leveraging our Mamba fusion module, DRAMA efficiently and\neffectively fuses the features of the camera and LiDAR modalities. In addition,\nwe introduce a Mamba-Transformer decoder that enhances the overall planning\nperformance. This module is universally adaptable to any Transformer-based\nmodel, especially for tasks with long sequence inputs. We further introduce a\nnovel feature state dropout which improves the planner's robustness without\nincreasing training and inference times. Extensive experimental results show\nthat DRAMA achieves higher accuracy on the NAVSIM dataset compared to the\nbaseline Transfuser, with fewer parameters and lower computational costs."
                },
                "authors": [
                    {
                        "name": "Chengran Yuan"
                    },
                    {
                        "name": "Zhanqi Zhang"
                    },
                    {
                        "name": "Jiawei Sun"
                    },
                    {
                        "name": "Shuo Sun"
                    },
                    {
                        "name": "Zefan Huang"
                    },
                    {
                        "name": "Christina Dao Wen Lee"
                    },
                    {
                        "name": "Dongen Li"
                    },
                    {
                        "name": "Yuhang Han"
                    },
                    {
                        "name": "Anthony Wong"
                    },
                    {
                        "name": "Keng Peng Tee"
                    },
                    {
                        "name": "Marcelo H. Ang Jr"
                    }
                ],
                "author_detail": {
                    "name": "Marcelo H. Ang Jr"
                },
                "author": "Marcelo H. Ang Jr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06909v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06909v3",
                "updated": "2024-08-14T04:26:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    4,
                    26,
                    3,
                    2,
                    227,
                    0
                ],
                "published": "2023-06-12T07:27:31Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    7,
                    27,
                    31,
                    0,
                    163,
                    0
                ],
                "title": "Graph Agent Network: Empowering Nodes with Inference Capabilities for\n  Adversarial Resilience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Agent Network: Empowering Nodes with Inference Capabilities for\n  Adversarial Resilience"
                },
                "summary": "End-to-end training with global optimization have popularized graph neural\nnetworks (GNNs) for node classification, yet inadvertently introduced\nvulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploit\nthe inherent opened interfaces of GNNs' input and output, perturbing critical\nedges and thus manipulating the classification results. Current defenses, due\nto their persistent utilization of global-optimization-based end-to-end\ntraining schemes, inherently encapsulate the vulnerabilities of GNNs. This is\nspecifically evidenced in their inability to defend against targeted secondary\nattacks. In this paper, we propose the Graph Agent Network (GAgN) to address\nthe aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agent\nnetwork in which each node is designed as an 1-hop-view agent. Through the\ndecentralized interactions between agents, they can learn to infer global\nperceptions to perform tasks including inferring embeddings, degrees and\nneighbor relationships for given nodes. This empowers nodes to filtering\nadversarial edges while carrying out classification tasks. Furthermore, agents'\nlimited view prevents malicious messages from propagating globally in GAgN,\nthereby resisting global-optimization-based secondary attacks. We prove that\nsingle-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficient\nto achieve these functionalities. Experimental results show that GAgN\neffectively implements all its intended capabilities and, compared to\nstate-of-the-art defenses, achieves optimal classification accuracy on the\nperturbed datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end training with global optimization have popularized graph neural\nnetworks (GNNs) for node classification, yet inadvertently introduced\nvulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploit\nthe inherent opened interfaces of GNNs' input and output, perturbing critical\nedges and thus manipulating the classification results. Current defenses, due\nto their persistent utilization of global-optimization-based end-to-end\ntraining schemes, inherently encapsulate the vulnerabilities of GNNs. This is\nspecifically evidenced in their inability to defend against targeted secondary\nattacks. In this paper, we propose the Graph Agent Network (GAgN) to address\nthe aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agent\nnetwork in which each node is designed as an 1-hop-view agent. Through the\ndecentralized interactions between agents, they can learn to infer global\nperceptions to perform tasks including inferring embeddings, degrees and\nneighbor relationships for given nodes. This empowers nodes to filtering\nadversarial edges while carrying out classification tasks. Furthermore, agents'\nlimited view prevents malicious messages from propagating globally in GAgN,\nthereby resisting global-optimization-based secondary attacks. We prove that\nsingle-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficient\nto achieve these functionalities. Experimental results show that GAgN\neffectively implements all its intended capabilities and, compared to\nstate-of-the-art defenses, achieves optimal classification accuracy on the\nperturbed datasets."
                },
                "authors": [
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Wenshan Li"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Beibei Li"
                    },
                    {
                        "name": "Guangquan Xu"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Wengang Ma"
                    },
                    {
                        "name": "Hanyuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hanyuan Huang"
                },
                "author": "Hanyuan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06909v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06909v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.13764v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.13764v3",
                "updated": "2024-08-15T10:03:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    3,
                    37,
                    3,
                    228,
                    0
                ],
                "published": "2023-12-21T11:43:41Z",
                "published_parsed": [
                    2023,
                    12,
                    21,
                    11,
                    43,
                    41,
                    3,
                    355,
                    0
                ],
                "title": "A Semantic Space is Worth 256 Language Descriptions: Make Stronger\n  Segmentation Models with Descriptive Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Semantic Space is Worth 256 Language Descriptions: Make Stronger\n  Segmentation Models with Descriptive Properties"
                },
                "summary": "This paper introduces ProLab, a novel approach using property-level label\nspace for creating strong interpretable segmentation models. Instead of relying\nsolely on category-specific annotations, ProLab uses descriptive properties\ngrounded in common sense knowledge for supervising segmentation models. It is\nbased on two core designs. First, we employ Large Language Models (LLMs) and\ncarefully crafted prompts to generate descriptions of all involved categories\nthat carry meaningful common sense knowledge and follow a structured format.\nSecond, we introduce a description embedding model preserving semantic\ncorrelation across descriptions and then cluster them into a set of descriptive\nproperties (e.g., 256) using K-Means. These properties are based on\ninterpretable common sense knowledge consistent with theories of human\nrecognition. We empirically show that our approach makes segmentation models\nperform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal\nContext, Cityscapes, and BDD). Our method also shows better scalability with\nextended training steps than category-level supervision. Our interpretable\nsegmentation framework also emerges with the generalization ability to segment\nout-of-domain or unknown categories using only in-domain descriptive\nproperties. Code is available at https://github.com/lambert-x/ProLab.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ProLab, a novel approach using property-level label\nspace for creating strong interpretable segmentation models. Instead of relying\nsolely on category-specific annotations, ProLab uses descriptive properties\ngrounded in common sense knowledge for supervising segmentation models. It is\nbased on two core designs. First, we employ Large Language Models (LLMs) and\ncarefully crafted prompts to generate descriptions of all involved categories\nthat carry meaningful common sense knowledge and follow a structured format.\nSecond, we introduce a description embedding model preserving semantic\ncorrelation across descriptions and then cluster them into a set of descriptive\nproperties (e.g., 256) using K-Means. These properties are based on\ninterpretable common sense knowledge consistent with theories of human\nrecognition. We empirically show that our approach makes segmentation models\nperform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal\nContext, Cityscapes, and BDD). Our method also shows better scalability with\nextended training steps than category-level supervision. Our interpretable\nsegmentation framework also emerges with the generalization ability to segment\nout-of-domain or unknown categories using only in-domain descriptive\nproperties. Code is available at https://github.com/lambert-x/ProLab."
                },
                "authors": [
                    {
                        "name": "Junfei Xiao"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Wenxuan Li"
                    },
                    {
                        "name": "Shiyi Lan"
                    },
                    {
                        "name": "Jieru Mei"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Cihang Xie"
                    }
                ],
                "author_detail": {
                    "name": "Cihang Xie"
                },
                "author": "Cihang Xie",
                "arxiv_comment": "Accepted to ECCV 2024. Code is available at\n  https://github.com/lambert-x/ProLab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.13764v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.13764v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08651v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08651v5",
                "updated": "2024-08-14T03:56:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    3,
                    56,
                    1,
                    2,
                    227,
                    0
                ],
                "published": "2024-03-13T16:06:07Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    16,
                    6,
                    7,
                    2,
                    73,
                    0
                ],
                "title": "HAIFIT: Human-to-AI Fashion Image Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAIFIT: Human-to-AI Fashion Image Translation"
                },
                "summary": "In the realm of fashion design, sketches serve as the canvas for expressing\nan artist's distinctive drawing style and creative vision, capturing intricate\ndetails like stroke variations and texture nuances. The advent of\nsketch-to-image cross-modal translation technology has notably aided designers.\nHowever, existing methods often compromise these sketch details during image\ngeneration, resulting in images that deviate from the designer's intended\nconcept. This limitation hampers the ability to offer designers a precise\npreview of the final output. To overcome this challenge, we introduce HAIFIT, a\nnovel approach that transforms sketches into high-fidelity, lifelike clothing\nimages by integrating multi-scale features and capturing extensive feature map\ndependencies from diverse perspectives. Through extensive qualitative and\nquantitative evaluations conducted on our self-collected dataset, our method\ndemonstrates superior performance compared to existing methods in generating\nphotorealistic clothing images. Our method excels in preserving the distinctive\nstyle and intricate details essential for fashion design applications. In\naddition, our method also has obvious advantages in model training and\ninference speed, contributing to reducing designers' time costs and improving\ndesign efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of fashion design, sketches serve as the canvas for expressing\nan artist's distinctive drawing style and creative vision, capturing intricate\ndetails like stroke variations and texture nuances. The advent of\nsketch-to-image cross-modal translation technology has notably aided designers.\nHowever, existing methods often compromise these sketch details during image\ngeneration, resulting in images that deviate from the designer's intended\nconcept. This limitation hampers the ability to offer designers a precise\npreview of the final output. To overcome this challenge, we introduce HAIFIT, a\nnovel approach that transforms sketches into high-fidelity, lifelike clothing\nimages by integrating multi-scale features and capturing extensive feature map\ndependencies from diverse perspectives. Through extensive qualitative and\nquantitative evaluations conducted on our self-collected dataset, our method\ndemonstrates superior performance compared to existing methods in generating\nphotorealistic clothing images. Our method excels in preserving the distinctive\nstyle and intricate details essential for fashion design applications. In\naddition, our method also has obvious advantages in model training and\ninference speed, contributing to reducing designers' time costs and improving\ndesign efficiency."
                },
                "authors": [
                    {
                        "name": "Jianan Jiang"
                    },
                    {
                        "name": "Xinglin Li"
                    },
                    {
                        "name": "Weiren Yu"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "arxiv_comment": "10 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08651v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08651v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.09949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.09949v2",
                "updated": "2024-08-14T03:45:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    3,
                    45,
                    37,
                    2,
                    227,
                    0
                ],
                "published": "2024-01-18T12:51:38Z",
                "published_parsed": [
                    2024,
                    1,
                    18,
                    12,
                    51,
                    38,
                    3,
                    18,
                    0
                ],
                "title": "SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning"
                },
                "summary": "Contrary to genetic programming, the neural network approach to symbolic\nregression can efficiently handle high-dimensional inputs and leverage gradient\nmethods for faster equation searching. Common ways of constraining expression\ncomplexity often involve multistage pruning with fine-tuning, which can result\nin significant performance loss. In this work, we propose $\\tt{SymbolNet}$, a\nneural network approach to symbolic regression in a novel framework that allows\ndynamic pruning of model weights, input features, and mathematical operators in\na single training process, where both training loss and expression complexity\nare optimized simultaneously. We introduce a sparsity regularization term for\neach pruning type, which can adaptively adjust its strength, leading to\nconvergence at a target sparsity ratio. Unlike most existing symbolic\nregression methods that struggle with datasets containing more than\n$\\mathcal{O}(10)$ inputs, we demonstrate the effectiveness of our model on the\nLHC jet tagging task (16 inputs), MNIST (784 inputs), and SVHN (3072 inputs).\nOur approach enables symbolic regression to achieve fast inference with\nnanosecond-scale latency on FPGAs for high-dimensional datasets in environments\nwith stringent computational resource constraints, such as the high-energy\nphysics experiments at the LHC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrary to genetic programming, the neural network approach to symbolic\nregression can efficiently handle high-dimensional inputs and leverage gradient\nmethods for faster equation searching. Common ways of constraining expression\ncomplexity often involve multistage pruning with fine-tuning, which can result\nin significant performance loss. In this work, we propose $\\tt{SymbolNet}$, a\nneural network approach to symbolic regression in a novel framework that allows\ndynamic pruning of model weights, input features, and mathematical operators in\na single training process, where both training loss and expression complexity\nare optimized simultaneously. We introduce a sparsity regularization term for\neach pruning type, which can adaptively adjust its strength, leading to\nconvergence at a target sparsity ratio. Unlike most existing symbolic\nregression methods that struggle with datasets containing more than\n$\\mathcal{O}(10)$ inputs, we demonstrate the effectiveness of our model on the\nLHC jet tagging task (16 inputs), MNIST (784 inputs), and SVHN (3072 inputs).\nOur approach enables symbolic regression to achieve fast inference with\nnanosecond-scale latency on FPGAs for high-dimensional datasets in environments\nwith stringent computational resource constraints, such as the high-energy\nphysics experiments at the LHC."
                },
                "authors": [
                    {
                        "name": "Ho Fung Tsoi"
                    },
                    {
                        "name": "Vladimir Loncar"
                    },
                    {
                        "name": "Sridhara Dasu"
                    },
                    {
                        "name": "Philip Harris"
                    }
                ],
                "author_detail": {
                    "name": "Philip Harris"
                },
                "author": "Philip Harris",
                "arxiv_comment": "24 pages. Minor fixes and formatting, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.09949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.09949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07272v1",
                "updated": "2024-08-14T03:42:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    3,
                    42,
                    53,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T03:42:53Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    3,
                    42,
                    53,
                    2,
                    227,
                    0
                ],
                "title": "NL2OR: Solve Complex Operations Research Problems Using Natural Language\n  Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL2OR: Solve Complex Operations Research Problems Using Natural Language\n  Inputs"
                },
                "summary": "Operations research (OR) uses mathematical models to enhance decision-making,\nbut developing these models requires expert knowledge and can be\ntime-consuming. Automated mathematical programming (AMP) has emerged to\nsimplify this process, but existing systems have limitations. This paper\nintroduces a novel methodology that uses recent advances in Large Language\nModel (LLM) to create and edit OR solutions from non-expert user queries\nexpressed using Natural Language. This reduces the need for domain expertise\nand the time to formulate a problem. The paper presents an end-to-end pipeline,\nnamed NL2OR, that generates solutions to OR problems from natural language\ninput, and shares experimental results on several important OR problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operations research (OR) uses mathematical models to enhance decision-making,\nbut developing these models requires expert knowledge and can be\ntime-consuming. Automated mathematical programming (AMP) has emerged to\nsimplify this process, but existing systems have limitations. This paper\nintroduces a novel methodology that uses recent advances in Large Language\nModel (LLM) to create and edit OR solutions from non-expert user queries\nexpressed using Natural Language. This reduces the need for domain expertise\nand the time to formulate a problem. The paper presents an end-to-end pipeline,\nnamed NL2OR, that generates solutions to OR problems from natural language\ninput, and shares experimental results on several important OR problems."
                },
                "authors": [
                    {
                        "name": "Junxuan Li"
                    },
                    {
                        "name": "Ryan Wickman"
                    },
                    {
                        "name": "Sahil Bhatnagar"
                    },
                    {
                        "name": "Raj Kumar Maity"
                    },
                    {
                        "name": "Arko Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Arko Mukherjee"
                },
                "author": "Arko Mukherjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07263v1",
                "updated": "2024-08-14T03:03:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    3,
                    3,
                    5,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T03:03:05Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    3,
                    3,
                    5,
                    2,
                    227,
                    0
                ],
                "title": "Eavesdropping Mobile Apps and Actions through Wireless Traffic in the\n  Open World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eavesdropping Mobile Apps and Actions through Wireless Traffic in the\n  Open World"
                },
                "summary": "While smartphones and WiFi networks are bringing many positive changes to\npeople's lives, they are susceptible to traffic analysis attacks, which infer\nuser's private information from encrypted traffic. Existing traffic analysis\nattacks mainly target TCP/IP layers or are limited to the closed-world\nassumption, where all possible apps and actions have been involved in the model\ntraining. To overcome these limitations, we propose MACPrint, a novel system\nthat infers mobile apps and in-app actions based on WiFi MAC layer traffic in\nthe open-world setting. MACPrint first extracts rich statistical and contextual\nfeatures of encrypted wireless traffic. Then, we develop Label Recorder, an\nautomatic traffic labeling app, to improve labeling accuracy in the training\nphase. Finally, TCN models with OpenMax functions are used to recognize mobile\napps and actions in the open world accurately. To evaluate our system, we\ncollect MAC layer traffic data over 125 hours from more than 40 apps. The\nexperimental results show that MAC-Print can achieve an accuracy of over 96%\nfor recognizing apps and actions in the closed-world setting, and obtains an\naccuracy of over 86% in the open-world setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While smartphones and WiFi networks are bringing many positive changes to\npeople's lives, they are susceptible to traffic analysis attacks, which infer\nuser's private information from encrypted traffic. Existing traffic analysis\nattacks mainly target TCP/IP layers or are limited to the closed-world\nassumption, where all possible apps and actions have been involved in the model\ntraining. To overcome these limitations, we propose MACPrint, a novel system\nthat infers mobile apps and in-app actions based on WiFi MAC layer traffic in\nthe open-world setting. MACPrint first extracts rich statistical and contextual\nfeatures of encrypted wireless traffic. Then, we develop Label Recorder, an\nautomatic traffic labeling app, to improve labeling accuracy in the training\nphase. Finally, TCN models with OpenMax functions are used to recognize mobile\napps and actions in the open world accurately. To evaluate our system, we\ncollect MAC layer traffic data over 125 hours from more than 40 apps. The\nexperimental results show that MAC-Print can achieve an accuracy of over 96%\nfor recognizing apps and actions in the closed-world setting, and obtains an\naccuracy of over 86% in the open-world setting."
                },
                "authors": [
                    {
                        "name": "Xiaoguang Yang"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Junli Guo"
                    },
                    {
                        "name": "Dalong Zhang"
                    },
                    {
                        "name": "Qingxian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qingxian Wang"
                },
                "author": "Qingxian Wang",
                "arxiv_comment": "Accepted by International Conference on Intelligent Computing 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06457v2",
                "updated": "2024-08-14T02:41:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    2,
                    41,
                    48,
                    2,
                    227,
                    0
                ],
                "published": "2024-02-09T15:02:56Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    15,
                    2,
                    56,
                    4,
                    40,
                    0
                ],
                "title": "V-STaR: Training Verifiers for Self-Taught Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-STaR: Training Verifiers for Self-Taught Reasoners"
                },
                "summary": "Common self-improvement approaches for large language models (LLMs), such as\nSTaR, iteratively fine-tune LLMs on self-generated solutions to improve their\nproblem-solving ability. However, these approaches discard the large amounts of\nincorrect solutions generated during this process, potentially neglecting\nvaluable information in such solutions. To address this shortcoming, we propose\nV-STaR that utilizes both the correct and incorrect solutions generated during\nthe self-improvement process to train a verifier using DPO that judges\ncorrectness of model-generated solutions. This verifier is used at inference\ntime to select one solution among many candidate solutions. Running V-STaR for\nmultiple iterations results in progressively better reasoners and verifiers,\ndelivering a 4% to 17% test accuracy improvement over existing self-improvement\nand verification approaches on common code generation and math reasoning\nbenchmarks with LLaMA2 models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Common self-improvement approaches for large language models (LLMs), such as\nSTaR, iteratively fine-tune LLMs on self-generated solutions to improve their\nproblem-solving ability. However, these approaches discard the large amounts of\nincorrect solutions generated during this process, potentially neglecting\nvaluable information in such solutions. To address this shortcoming, we propose\nV-STaR that utilizes both the correct and incorrect solutions generated during\nthe self-improvement process to train a verifier using DPO that judges\ncorrectness of model-generated solutions. This verifier is used at inference\ntime to select one solution among many candidate solutions. Running V-STaR for\nmultiple iterations results in progressively better reasoners and verifiers,\ndelivering a 4% to 17% test accuracy improvement over existing self-improvement\nand verification approaches on common code generation and math reasoning\nbenchmarks with LLaMA2 models."
                },
                "authors": [
                    {
                        "name": "Arian Hosseini"
                    },
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Nikolay Malkin"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Agarwal"
                },
                "author": "Rishabh Agarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06578v2",
                "updated": "2024-08-14T01:37:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    37,
                    39,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T02:35:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    35,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "OpenEP: Open-Ended Future Event Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenEP: Open-Ended Future Event Prediction"
                },
                "summary": "Future event prediction (FEP) is a long-standing and crucial task in the\nworld, as understanding the evolution of events enables early risk\nidentification, informed decision-making, and strategic planning. Existing work\ntypically treats event prediction as classification tasks and confines the\noutcomes of future events to a fixed scope, such as yes/no questions, candidate\nset, and taxonomy, which is difficult to include all possible outcomes of\nfuture events. In this paper, we introduce OpenEP (an Open-Ended Future Event\nPrediction task), which generates flexible and diverse predictions aligned with\nreal-world scenarios. This is mainly reflected in two aspects: firstly, the\npredictive questions are diverse, covering different stages of event\ndevelopment and perspectives; secondly, the outcomes are flexible, without\nconstraints on scope or format. To facilitate the study of this task, we\nconstruct OpenEPBench, an open-ended future event prediction dataset. For\nquestion construction, we pose questions from seven perspectives, including\nlocation, time, event development, event outcome, event impact, event response,\nand other, to facilitate an in-depth analysis and understanding of the\ncomprehensive evolution of events. For outcome construction, we collect\nfree-form text containing the outcomes as ground truth to provide semantically\ncomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, a\nstakeholder-enhanced future event prediction framework, that incorporates event\ncharacteristics for open-ended settings. Our method extracts stakeholders\ninvolved in events to extend questions to gather diverse information. We also\ncollect historically events that are relevant and similar to the question to\nreveal potential evolutionary patterns. Experiment results indicate that\naccurately predicting future events in open-ended settings is challenging for\nexisting LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future event prediction (FEP) is a long-standing and crucial task in the\nworld, as understanding the evolution of events enables early risk\nidentification, informed decision-making, and strategic planning. Existing work\ntypically treats event prediction as classification tasks and confines the\noutcomes of future events to a fixed scope, such as yes/no questions, candidate\nset, and taxonomy, which is difficult to include all possible outcomes of\nfuture events. In this paper, we introduce OpenEP (an Open-Ended Future Event\nPrediction task), which generates flexible and diverse predictions aligned with\nreal-world scenarios. This is mainly reflected in two aspects: firstly, the\npredictive questions are diverse, covering different stages of event\ndevelopment and perspectives; secondly, the outcomes are flexible, without\nconstraints on scope or format. To facilitate the study of this task, we\nconstruct OpenEPBench, an open-ended future event prediction dataset. For\nquestion construction, we pose questions from seven perspectives, including\nlocation, time, event development, event outcome, event impact, event response,\nand other, to facilitate an in-depth analysis and understanding of the\ncomprehensive evolution of events. For outcome construction, we collect\nfree-form text containing the outcomes as ground truth to provide semantically\ncomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, a\nstakeholder-enhanced future event prediction framework, that incorporates event\ncharacteristics for open-ended settings. Our method extracts stakeholders\ninvolved in events to extend questions to gather diverse information. We also\ncollect historically events that are relevant and similar to the question to\nreveal potential evolutionary patterns. Experiment results indicate that\naccurately predicting future events in open-ended settings is challenging for\nexisting LLMs."
                },
                "authors": [
                    {
                        "name": "Yong Guan"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07249v1",
                "updated": "2024-08-14T01:24:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    24,
                    9,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T01:24:09Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    24,
                    9,
                    2,
                    227,
                    0
                ],
                "title": "GQE: Generalized Query Expansion for Enhanced Text-Video Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GQE: Generalized Query Expansion for Enhanced Text-Video Retrieval"
                },
                "summary": "In the rapidly expanding domain of web video content, the task of text-video\nretrieval has become increasingly critical, bridging the semantic gap between\ntextual queries and video data. This paper introduces a novel data-centric\napproach, Generalized Query Expansion (GQE), to address the inherent\ninformation imbalance between text and video, enhancing the effectiveness of\ntext-video retrieval systems. Unlike traditional model-centric methods that\nfocus on designing intricate cross-modal interaction mechanisms, GQE aims to\nexpand the text queries associated with videos both during training and testing\nphases. By adaptively segmenting videos into short clips and employing\nzero-shot captioning, GQE enriches the training dataset with comprehensive\nscene descriptions, effectively bridging the data imbalance gap. Furthermore,\nduring retrieval, GQE utilizes Large Language Models (LLM) to generate a\ndiverse set of queries and a query selection module to filter these queries\nbased on relevance and diversity, thus optimizing retrieval performance while\nreducing computational overhead. Our contributions include a detailed\nexamination of the information imbalance challenge, a novel approach to query\nexpansion in video-text datasets, and the introduction of a query selection\nstrategy that enhances retrieval accuracy without increasing computational\ncosts. GQE achieves state-of-the-art performance on several benchmarks,\nincluding MSR-VTT, MSVD, LSMDC, and VATEX, demonstrating the effectiveness of\naddressing text-video retrieval from a data-centric perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly expanding domain of web video content, the task of text-video\nretrieval has become increasingly critical, bridging the semantic gap between\ntextual queries and video data. This paper introduces a novel data-centric\napproach, Generalized Query Expansion (GQE), to address the inherent\ninformation imbalance between text and video, enhancing the effectiveness of\ntext-video retrieval systems. Unlike traditional model-centric methods that\nfocus on designing intricate cross-modal interaction mechanisms, GQE aims to\nexpand the text queries associated with videos both during training and testing\nphases. By adaptively segmenting videos into short clips and employing\nzero-shot captioning, GQE enriches the training dataset with comprehensive\nscene descriptions, effectively bridging the data imbalance gap. Furthermore,\nduring retrieval, GQE utilizes Large Language Models (LLM) to generate a\ndiverse set of queries and a query selection module to filter these queries\nbased on relevance and diversity, thus optimizing retrieval performance while\nreducing computational overhead. Our contributions include a detailed\nexamination of the information imbalance challenge, a novel approach to query\nexpansion in video-text datasets, and the introduction of a query selection\nstrategy that enhances retrieval accuracy without increasing computational\ncosts. GQE achieves state-of-the-art performance on several benchmarks,\nincluding MSR-VTT, MSVD, LSMDC, and VATEX, demonstrating the effectiveness of\naddressing text-video retrieval from a data-centric perspective."
                },
                "authors": [
                    {
                        "name": "Zechen Bai"
                    },
                    {
                        "name": "Tianjun Xiao"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Thomas Brox"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "18 pages including appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07246v1",
                "updated": "2024-08-14T01:16:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    16,
                    40,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T01:16:40Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    16,
                    40,
                    2,
                    227,
                    0
                ],
                "title": "Seeing and Understanding: Bridging Vision with Chemical Knowledge Via\n  ChemVLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing and Understanding: Bridging Vision with Chemical Knowledge Via\n  ChemVLM"
                },
                "summary": "In this technical report, we propose ChemVLM, the first open-source\nmultimodal large language model dedicated to the fields of chemistry, designed\nto address the incompatibility between chemical image understanding and text\nanalysis. Built upon the VIT-MLP-LLM architecture, we leverage ChemLLM-20B as\nthe foundational large model, endowing our model with robust capabilities in\nunderstanding and utilizing chemical text knowledge. Additionally, we employ\nInternVIT-6B as a powerful image encoder. We have curated high-quality data\nfrom the chemical domain, including molecules, reaction formulas, and chemistry\nexamination data, and compiled these into a bilingual multimodal\nquestion-answering dataset. We test the performance of our model on multiple\nopen-source benchmarks and three custom evaluation sets. Experimental results\ndemonstrate that our model achieves excellent performance, securing\nstate-of-the-art results in five out of six involved tasks. Our model can be\nfound at https://huggingface.co/AI4Chem/ChemVLM-26B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this technical report, we propose ChemVLM, the first open-source\nmultimodal large language model dedicated to the fields of chemistry, designed\nto address the incompatibility between chemical image understanding and text\nanalysis. Built upon the VIT-MLP-LLM architecture, we leverage ChemLLM-20B as\nthe foundational large model, endowing our model with robust capabilities in\nunderstanding and utilizing chemical text knowledge. Additionally, we employ\nInternVIT-6B as a powerful image encoder. We have curated high-quality data\nfrom the chemical domain, including molecules, reaction formulas, and chemistry\nexamination data, and compiled these into a bilingual multimodal\nquestion-answering dataset. We test the performance of our model on multiple\nopen-source benchmarks and three custom evaluation sets. Experimental results\ndemonstrate that our model achieves excellent performance, securing\nstate-of-the-art results in five out of six involved tasks. Our model can be\nfound at https://huggingface.co/AI4Chem/ChemVLM-26B."
                },
                "authors": [
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Xunzhi Wang"
                    },
                    {
                        "name": "Zeying Hao"
                    },
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Qian Tan"
                    },
                    {
                        "name": "Cai Zhou"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "arxiv_comment": "Techical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07244v1",
                "updated": "2024-08-14T00:56:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    0,
                    56,
                    51,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T00:56:51Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    0,
                    56,
                    51,
                    2,
                    227,
                    0
                ],
                "title": "Sign language recognition based on deep learning and low-cost\n  handcrafted descriptors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign language recognition based on deep learning and low-cost\n  handcrafted descriptors"
                },
                "summary": "In recent years, deep learning techniques have been used to develop sign\nlanguage recognition systems, potentially serving as a communication tool for\nmillions of hearing-impaired individuals worldwide. However, there are inherent\nchallenges in creating such systems. Firstly, it is important to consider as\nmany linguistic parameters as possible in gesture execution to avoid ambiguity\nbetween words. Moreover, to facilitate the real-world adoption of the created\nsolution, it is essential to ensure that the chosen technology is realistic,\navoiding expensive, intrusive, or low-mobility sensors, as well as very complex\ndeep learning architectures that impose high computational requirements. Based\non this, our work aims to propose an efficient sign language recognition system\nthat utilizes low-cost sensors and techniques. To this end, an object detection\nmodel was trained specifically for detecting the interpreter's face and hands,\nensuring focus on the most relevant regions of the image and generating inputs\nwith higher semantic value for the classifier. Additionally, we introduced a\nnovel approach to obtain features representing hand location and movement by\nleveraging spatial information derived from centroid positions of bounding\nboxes, thereby enhancing sign discrimination. The results demonstrate the\nefficiency of our handcrafted features, increasing accuracy by 7.96% on the\nAUTSL dataset, while adding fewer than 700 thousand parameters and incurring\nless than 10 milliseconds of additional inference time. These findings\nhighlight the potential of our technique to strike a favorable balance between\ncomputational cost and accuracy, making it a promising approach for practical\nsign language recognition applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, deep learning techniques have been used to develop sign\nlanguage recognition systems, potentially serving as a communication tool for\nmillions of hearing-impaired individuals worldwide. However, there are inherent\nchallenges in creating such systems. Firstly, it is important to consider as\nmany linguistic parameters as possible in gesture execution to avoid ambiguity\nbetween words. Moreover, to facilitate the real-world adoption of the created\nsolution, it is essential to ensure that the chosen technology is realistic,\navoiding expensive, intrusive, or low-mobility sensors, as well as very complex\ndeep learning architectures that impose high computational requirements. Based\non this, our work aims to propose an efficient sign language recognition system\nthat utilizes low-cost sensors and techniques. To this end, an object detection\nmodel was trained specifically for detecting the interpreter's face and hands,\nensuring focus on the most relevant regions of the image and generating inputs\nwith higher semantic value for the classifier. Additionally, we introduced a\nnovel approach to obtain features representing hand location and movement by\nleveraging spatial information derived from centroid positions of bounding\nboxes, thereby enhancing sign discrimination. The results demonstrate the\nefficiency of our handcrafted features, increasing accuracy by 7.96% on the\nAUTSL dataset, while adding fewer than 700 thousand parameters and incurring\nless than 10 milliseconds of additional inference time. These findings\nhighlight the potential of our technique to strike a favorable balance between\ncomputational cost and accuracy, making it a promising approach for practical\nsign language recognition applications."
                },
                "authors": [
                    {
                        "name": "Alvaro Leandro Cavalcante Carneiro"
                    },
                    {
                        "name": "Denis Henrique Pinheiro Salvadeo"
                    },
                    {
                        "name": "Lucas de Brito Silva"
                    }
                ],
                "author_detail": {
                    "name": "Lucas de Brito Silva"
                },
                "author": "Lucas de Brito Silva",
                "arxiv_comment": "28 pages, 12 figures, submitted to Image and Vision Computing Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8; I.4.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14268v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14268v3",
                "updated": "2024-08-14T00:48:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    0,
                    48,
                    43,
                    2,
                    227,
                    0
                ],
                "published": "2024-01-25T16:02:56Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    16,
                    2,
                    56,
                    3,
                    25,
                    0
                ],
                "title": "GPTVoiceTasker: Advancing Multi-step Mobile Task Efficiency Through\n  Dynamic Interface Exploration and Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTVoiceTasker: Advancing Multi-step Mobile Task Efficiency Through\n  Dynamic Interface Exploration and Learning"
                },
                "summary": "Virtual assistants have the potential to play an important role in helping\nusers achieves different tasks. However, these systems face challenges in their\nreal-world usability, characterized by inefficiency and struggles in grasping\nuser intentions. Leveraging recent advances in Large Language Models (LLMs), we\nintroduce GptVoiceTasker, a virtual assistant poised to enhance user\nexperiences and task efficiency on mobile devices. GptVoiceTasker excels at\nintelligently deciphering user commands and executing relevant device\ninteractions to streamline task completion. The system continually learns from\nhistorical user commands to automate subsequent usages, further enhancing\nexecution efficiency. Our experiments affirm GptVoiceTasker's exceptional\ncommand interpretation abilities and the precision of its task automation\nmodule. In our user study, GptVoiceTasker boosted task efficiency in real-world\nscenarios by 34.85%, accompanied by positive participant feedback. We made\nGptVoiceTasker open-source, inviting further research into LLMs utilization for\ndiverse tasks through prompt engineering and leveraging user usage data to\nimprove efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual assistants have the potential to play an important role in helping\nusers achieves different tasks. However, these systems face challenges in their\nreal-world usability, characterized by inefficiency and struggles in grasping\nuser intentions. Leveraging recent advances in Large Language Models (LLMs), we\nintroduce GptVoiceTasker, a virtual assistant poised to enhance user\nexperiences and task efficiency on mobile devices. GptVoiceTasker excels at\nintelligently deciphering user commands and executing relevant device\ninteractions to streamline task completion. The system continually learns from\nhistorical user commands to automate subsequent usages, further enhancing\nexecution efficiency. Our experiments affirm GptVoiceTasker's exceptional\ncommand interpretation abilities and the precision of its task automation\nmodule. In our user study, GptVoiceTasker boosted task efficiency in real-world\nscenarios by 34.85%, accompanied by positive participant feedback. We made\nGptVoiceTasker open-source, inviting further research into LLMs utilization for\ndiverse tasks through prompt engineering and leveraging user usage data to\nimprove efficiency."
                },
                "authors": [
                    {
                        "name": "Minh Duc Vu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Zhuang Li"
                    },
                    {
                        "name": "Jieshan Chen"
                    },
                    {
                        "name": "Shengdong Zhao"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Chunyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chunyang Chen"
                },
                "author": "Chunyang Chen",
                "arxiv_doi": "10.1145/3654777.3676356",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3654777.3676356",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14268v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14268v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by UIST 2024",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07238v1",
                "updated": "2024-08-13T23:59:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    23,
                    59,
                    36,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T23:59:36Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    23,
                    59,
                    36,
                    1,
                    226,
                    0
                ],
                "title": "Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge\n  Distillation Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge\n  Distillation Approach"
                },
                "summary": "Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior\nperformance in complex human-like interactions. But they are costly, or too\nlarge for edge devices such as smartphones and harder to self-host, leading to\nsecurity and privacy concerns. This paper introduces a novel interpretable\nknowledge distillation approach to enhance the performance of smaller, more\neconomical LLMs that firms can self-host. We study this problem in the context\nof building a customer service agent aimed at achieving high customer\nsatisfaction through goal-oriented dialogues. Unlike traditional knowledge\ndistillation, where the \"student\" model learns directly from the \"teacher\"\nmodel's responses via fine-tuning, our interpretable \"strategy\" teaching\napproach involves the teacher providing strategies to improve the student's\nperformance in various scenarios. This method alternates between a \"scenario\ngeneration\" step and a \"strategies for improvement\" step, creating a customized\nlibrary of scenarios and optimized strategies for automated prompting. The\nmethod requires only black-box access to both student and teacher models; hence\nit can be used without manipulating model parameters. In our customer service\napplication, the method improves performance, and the learned strategies are\ntransferable to other LLMs and scenarios beyond the training set. The method's\ninterpretabilty helps safeguard against potential harms through human audit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior\nperformance in complex human-like interactions. But they are costly, or too\nlarge for edge devices such as smartphones and harder to self-host, leading to\nsecurity and privacy concerns. This paper introduces a novel interpretable\nknowledge distillation approach to enhance the performance of smaller, more\neconomical LLMs that firms can self-host. We study this problem in the context\nof building a customer service agent aimed at achieving high customer\nsatisfaction through goal-oriented dialogues. Unlike traditional knowledge\ndistillation, where the \"student\" model learns directly from the \"teacher\"\nmodel's responses via fine-tuning, our interpretable \"strategy\" teaching\napproach involves the teacher providing strategies to improve the student's\nperformance in various scenarios. This method alternates between a \"scenario\ngeneration\" step and a \"strategies for improvement\" step, creating a customized\nlibrary of scenarios and optimized strategies for automated prompting. The\nmethod requires only black-box access to both student and teacher models; hence\nit can be used without manipulating model parameters. In our customer service\napplication, the method improves performance, and the learned strategies are\ntransferable to other LLMs and scenarios beyond the training set. The method's\ninterpretabilty helps safeguard against potential harms through human audit."
                },
                "authors": [
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "K. Sudhir"
                    },
                    {
                        "name": "Dat Hong"
                    }
                ],
                "author_detail": {
                    "name": "Dat Hong"
                },
                "author": "Dat Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07237v1",
                "updated": "2024-08-13T23:58:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    23,
                    58,
                    45,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T23:58:45Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    23,
                    58,
                    45,
                    1,
                    226,
                    0
                ],
                "title": "Neural embedding of beliefs reveals the role of relative dissonance in\n  human decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural embedding of beliefs reveals the role of relative dissonance in\n  human decision-making"
                },
                "summary": "Beliefs serve as the foundation for human cognition and decision-making. They\nguide individuals in deriving meaning from their lives, shaping their\nbehaviors, and forming social connections. Therefore, a model that encapsulates\nbeliefs and their interrelationships is crucial for quantitatively studying the\ninfluence of beliefs on our actions. Despite its importance, research on the\ninterplay between human beliefs has often been limited to a small set of\nbeliefs pertaining to specific issues, with a heavy reliance on surveys or\nexperiments. Here, we propose a method for extracting nuanced relations between\nthousands of beliefs by leveraging large-scale user participation data from an\nonline debate platform and mapping these beliefs to an embedding space using a\nfine-tuned large language model (LLM). This belief embedding space effectively\nencapsulates the interconnectedness of diverse beliefs as well as polarization\nacross various social issues. We discover that the positions within this belief\nspace predict new beliefs of individuals. Furthermore, we find that the\nrelative distance between one's existing beliefs and new beliefs can serve as a\nquantitative estimate of cognitive dissonance, allowing us to predict new\nbeliefs. Our study highlights how modern LLMs, when combined with collective\nonline records of human beliefs, can offer insights into the fundamental\nprinciples that govern human belief formation and decision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beliefs serve as the foundation for human cognition and decision-making. They\nguide individuals in deriving meaning from their lives, shaping their\nbehaviors, and forming social connections. Therefore, a model that encapsulates\nbeliefs and their interrelationships is crucial for quantitatively studying the\ninfluence of beliefs on our actions. Despite its importance, research on the\ninterplay between human beliefs has often been limited to a small set of\nbeliefs pertaining to specific issues, with a heavy reliance on surveys or\nexperiments. Here, we propose a method for extracting nuanced relations between\nthousands of beliefs by leveraging large-scale user participation data from an\nonline debate platform and mapping these beliefs to an embedding space using a\nfine-tuned large language model (LLM). This belief embedding space effectively\nencapsulates the interconnectedness of diverse beliefs as well as polarization\nacross various social issues. We discover that the positions within this belief\nspace predict new beliefs of individuals. Furthermore, we find that the\nrelative distance between one's existing beliefs and new beliefs can serve as a\nquantitative estimate of cognitive dissonance, allowing us to predict new\nbeliefs. Our study highlights how modern LLMs, when combined with collective\nonline records of human beliefs, can offer insights into the fundamental\nprinciples that govern human belief formation and decision-making processes."
                },
                "authors": [
                    {
                        "name": "Byunghwee Lee"
                    },
                    {
                        "name": "Rachith Aiyappa"
                    },
                    {
                        "name": "Yong-Yeol Ahn"
                    },
                    {
                        "name": "Haewoon Kwak"
                    },
                    {
                        "name": "Jisun An"
                    }
                ],
                "author_detail": {
                    "name": "Jisun An"
                },
                "author": "Jisun An",
                "arxiv_comment": "26 pages, 6 figures, SI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08850v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08850v3",
                "updated": "2024-08-13T23:41:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    23,
                    41,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-11T20:18:19Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    20,
                    18,
                    19,
                    3,
                    193,
                    0
                ],
                "title": "UICrit: Enhancing Automated Design Evaluation with a UICritique Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UICrit: Enhancing Automated Design Evaluation with a UICritique Dataset"
                },
                "summary": "Automated UI evaluation can be beneficial for the design process; for\nexample, to compare different UI designs, or conduct automated heuristic\nevaluation. LLM-based UI evaluation, in particular, holds the promise of\ngeneralizability to a wide variety of UI types and evaluation tasks. However,\ncurrent LLM-based techniques do not yet match the performance of human\nevaluators. We hypothesize that automatic evaluation can be improved by\ncollecting a targeted UI feedback dataset and then using this dataset to\nenhance the performance of general-purpose LLMs. We present a targeted dataset\nof 3,059 design critiques and quality ratings for 983 mobile UIs, collected\nfrom seven experienced designers. We carried out an in-depth analysis to\ncharacterize the dataset's features. We then applied this dataset to achieve a\n55% performance gain in LLM-generated UI feedback via various few-shot and\nvisual prompting techniques. We also discuss future applications of this\ndataset, including training a reward model for generative UI techniques, and\nfine-tuning a tool-agnostic multi-modal LLM that automates UI evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated UI evaluation can be beneficial for the design process; for\nexample, to compare different UI designs, or conduct automated heuristic\nevaluation. LLM-based UI evaluation, in particular, holds the promise of\ngeneralizability to a wide variety of UI types and evaluation tasks. However,\ncurrent LLM-based techniques do not yet match the performance of human\nevaluators. We hypothesize that automatic evaluation can be improved by\ncollecting a targeted UI feedback dataset and then using this dataset to\nenhance the performance of general-purpose LLMs. We present a targeted dataset\nof 3,059 design critiques and quality ratings for 983 mobile UIs, collected\nfrom seven experienced designers. We carried out an in-depth analysis to\ncharacterize the dataset's features. We then applied this dataset to achieve a\n55% performance gain in LLM-generated UI feedback via various few-shot and\nvisual prompting techniques. We also discuss future applications of this\ndataset, including training a reward model for generative UI techniques, and\nfine-tuning a tool-agnostic multi-modal LLM that automates UI evaluation."
                },
                "authors": [
                    {
                        "name": "Peitong Duan"
                    },
                    {
                        "name": "Chin-yi Chen"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Bjoern Hartmann"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "Accepted to ACM UIST 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08850v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03626v2",
                "updated": "2024-08-13T22:01:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    22,
                    1,
                    42,
                    1,
                    226,
                    0
                ],
                "published": "2024-04-04T17:48:28Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    17,
                    48,
                    28,
                    3,
                    95,
                    0
                ],
                "title": "Training LLMs over Neurally Compressed Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs over Neurally Compressed Text"
                },
                "summary": "In this paper, we explore the idea of training large language models (LLMs)\nover highly compressed text. While standard subword tokenizers compress text by\na small factor, neural text compressors can achieve much higher rates of\ncompression. If it were possible to train LLMs directly over neurally\ncompressed text, this would confer advantages in training and serving\nefficiency, as well as easier handling of long text spans. The main obstacle to\nthis goal is that strong compression tends to produce opaque outputs that are\nnot well-suited for learning. In particular, we find that text na\\\"ively\ncompressed via Arithmetic Coding is not readily learnable by LLMs. To overcome\nthis, we propose Equal-Info Windows, a novel compression technique whereby text\nis segmented into blocks that each compress to the same bit length. Using this\nmethod, we demonstrate effective learning over neurally compressed text that\nimproves with scale, and outperforms byte-level baselines by a wide margin on\nperplexity and inference speed benchmarks. While our method delivers worse\nperplexity than subword tokenizers for models trained with the same parameter\ncount, it has the benefit of shorter sequence lengths. Shorter sequence lengths\nrequire fewer autoregressive generation steps, and reduce latency. Finally, we\nprovide extensive analysis of the properties that contribute to learnability,\nand offer concrete suggestions for how to further improve the performance of\nhigh-compression tokenizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the idea of training large language models (LLMs)\nover highly compressed text. While standard subword tokenizers compress text by\na small factor, neural text compressors can achieve much higher rates of\ncompression. If it were possible to train LLMs directly over neurally\ncompressed text, this would confer advantages in training and serving\nefficiency, as well as easier handling of long text spans. The main obstacle to\nthis goal is that strong compression tends to produce opaque outputs that are\nnot well-suited for learning. In particular, we find that text na\\\"ively\ncompressed via Arithmetic Coding is not readily learnable by LLMs. To overcome\nthis, we propose Equal-Info Windows, a novel compression technique whereby text\nis segmented into blocks that each compress to the same bit length. Using this\nmethod, we demonstrate effective learning over neurally compressed text that\nimproves with scale, and outperforms byte-level baselines by a wide margin on\nperplexity and inference speed benchmarks. While our method delivers worse\nperplexity than subword tokenizers for models trained with the same parameter\ncount, it has the benefit of shorter sequence lengths. Shorter sequence lengths\nrequire fewer autoregressive generation steps, and reduce latency. Finally, we\nprovide extensive analysis of the properties that contribute to learnability,\nand offer concrete suggestions for how to further improve the performance of\nhigh-compression tokenizers."
                },
                "authors": [
                    {
                        "name": "Brian Lester"
                    },
                    {
                        "name": "Jaehoon Lee"
                    },
                    {
                        "name": "Alex Alemi"
                    },
                    {
                        "name": "Jeffrey Pennington"
                    },
                    {
                        "name": "Adam Roberts"
                    },
                    {
                        "name": "Jascha Sohl-Dickstein"
                    },
                    {
                        "name": "Noah Constant"
                    }
                ],
                "author_detail": {
                    "name": "Noah Constant"
                },
                "author": "Noah Constant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11097v2",
                "updated": "2024-08-13T21:57:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    21,
                    57,
                    20,
                    1,
                    226,
                    0
                ],
                "published": "2024-02-16T22:03:37Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    22,
                    3,
                    37,
                    4,
                    47,
                    0
                ],
                "title": "Interpolating many-body wave functions for accelerated molecular\n  dynamics on the near-exact electronic surface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpolating many-body wave functions for accelerated molecular\n  dynamics on the near-exact electronic surface"
                },
                "summary": "While there have been many developments in computational probes of both\nstrongly-correlated molecular systems and machine-learning accelerated\nmolecular dynamics, there remains a significant gap in capabilities in\nsimulating accurate non-local electronic structure over timescales on which\natoms move. We develop an approach to bridge these fields with a practical\ninterpolation scheme for the correlated many-electron state through the space\nof atomic configurations, whilst avoiding the exponential complexity of these\nunderlying electronic states. With a small number of accurate correlated wave\nfunctions as a training set, we demonstrate provable convergence to near-exact\npotential energy surfaces for subsequent dynamics with propagation of a valid\nmany-body wave function and inference of its variational energy whilst\nretaining a mean-field computational scaling. This represents a profoundly\ndifferent paradigm to the direct interpolation of potential energy surfaces in\nestablished machine-learning approaches. We combine this with modern electronic\nstructure approaches to systematically resolve molecular dynamics trajectories\nand converge thermodynamic quantities with a high-throughput of several million\ninterpolated wave functions with explicit validation of their accuracy from\nonly a few numerically exact quantum chemical calculations. We also highlight\nthe comparison to traditional machine-learned potentials or dynamics on\nmean-field surfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there have been many developments in computational probes of both\nstrongly-correlated molecular systems and machine-learning accelerated\nmolecular dynamics, there remains a significant gap in capabilities in\nsimulating accurate non-local electronic structure over timescales on which\natoms move. We develop an approach to bridge these fields with a practical\ninterpolation scheme for the correlated many-electron state through the space\nof atomic configurations, whilst avoiding the exponential complexity of these\nunderlying electronic states. With a small number of accurate correlated wave\nfunctions as a training set, we demonstrate provable convergence to near-exact\npotential energy surfaces for subsequent dynamics with propagation of a valid\nmany-body wave function and inference of its variational energy whilst\nretaining a mean-field computational scaling. This represents a profoundly\ndifferent paradigm to the direct interpolation of potential energy surfaces in\nestablished machine-learning approaches. We combine this with modern electronic\nstructure approaches to systematically resolve molecular dynamics trajectories\nand converge thermodynamic quantities with a high-throughput of several million\ninterpolated wave functions with explicit validation of their accuracy from\nonly a few numerically exact quantum chemical calculations. We also highlight\nthe comparison to traditional machine-learned potentials or dynamics on\nmean-field surfaces."
                },
                "authors": [
                    {
                        "name": "Yannic Rath"
                    },
                    {
                        "name": "George H. Booth"
                    }
                ],
                "author_detail": {
                    "name": "George H. Booth"
                },
                "author": "George H. Booth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.07702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07702v1",
                "updated": "2024-08-14T17:59:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    59,
                    4,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T17:59:04Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    59,
                    4,
                    2,
                    227,
                    0
                ],
                "title": "The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned\n  Language Models"
                },
                "summary": "Schema linking is a crucial step in Text-to-SQL pipelines, which translate\nnatural language queries into SQL. The goal of schema linking is to retrieve\nrelevant tables and columns (signal) while disregarding irrelevant ones\n(noise). However, imperfect schema linking can often exclude essential columns\nneeded for accurate query generation. In this work, we revisit the need for\nschema linking when using the latest generation of large language models\n(LLMs). We find empirically that newer models are adept at identifying relevant\nschema elements during generation, without the need for explicit schema\nlinking. This allows Text-to-SQL pipelines to bypass schema linking entirely\nand instead pass the full database schema to the LLM, eliminating the risk of\nexcluding necessary information. Furthermore, as alternatives to schema\nlinking, we propose techniques that improve Text-to-SQL accuracy without\ncompromising on essential schema information. Our approach achieves 71.83\\%\nexecution accuracy on the BIRD benchmark, ranking first at the time of\nsubmission.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schema linking is a crucial step in Text-to-SQL pipelines, which translate\nnatural language queries into SQL. The goal of schema linking is to retrieve\nrelevant tables and columns (signal) while disregarding irrelevant ones\n(noise). However, imperfect schema linking can often exclude essential columns\nneeded for accurate query generation. In this work, we revisit the need for\nschema linking when using the latest generation of large language models\n(LLMs). We find empirically that newer models are adept at identifying relevant\nschema elements during generation, without the need for explicit schema\nlinking. This allows Text-to-SQL pipelines to bypass schema linking entirely\nand instead pass the full database schema to the LLM, eliminating the risk of\nexcluding necessary information. Furthermore, as alternatives to schema\nlinking, we propose techniques that improve Text-to-SQL accuracy without\ncompromising on essential schema information. Our approach achieves 71.83\\%\nexecution accuracy on the BIRD benchmark, ranking first at the time of\nsubmission."
                },
                "authors": [
                    {
                        "name": "Karime Maamari"
                    },
                    {
                        "name": "Fadhil Abubaker"
                    },
                    {
                        "name": "Daniel Jaroslawicz"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07692v1",
                "updated": "2024-08-14T17:50:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    50,
                    6,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T17:50:06Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    50,
                    6,
                    2,
                    227,
                    0
                ],
                "title": "On the Parameter Selection of Phase-transmittance Radial Basis Function\n  Neural Networks for Communication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Parameter Selection of Phase-transmittance Radial Basis Function\n  Neural Networks for Communication Systems"
                },
                "summary": "In the ever-evolving field of digital communication systems, complex-valued\nneural networks (CVNNs) have become a cornerstone, delivering exceptional\nperformance in tasks like equalization, channel estimation, beamforming, and\ndecoding. Among the myriad of CVNN architectures, the phase-transmittance\nradial basis function neural network (PT-RBF) stands out, especially when\noperating in noisy environments such as 5G MIMO systems. Despite its\ncapabilities, achieving convergence in multi-layered, multi-input, and\nmulti-output PT-RBFs remains a daunting challenge. Addressing this gap, this\npaper presents a novel Deep PT-RBF parameter initialization technique. Through\nrigorous simulations conforming to 3GPP TS 38 standards, our method not only\noutperforms conventional initialization strategies like random, $K$-means, and\nconstellation-based methods but is also the only approach to achieve successful\nconvergence in deep PT-RBF architectures. These findings pave the way to more\nrobust and efficient neural network deployments in complex digital\ncommunication systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the ever-evolving field of digital communication systems, complex-valued\nneural networks (CVNNs) have become a cornerstone, delivering exceptional\nperformance in tasks like equalization, channel estimation, beamforming, and\ndecoding. Among the myriad of CVNN architectures, the phase-transmittance\nradial basis function neural network (PT-RBF) stands out, especially when\noperating in noisy environments such as 5G MIMO systems. Despite its\ncapabilities, achieving convergence in multi-layered, multi-input, and\nmulti-output PT-RBFs remains a daunting challenge. Addressing this gap, this\npaper presents a novel Deep PT-RBF parameter initialization technique. Through\nrigorous simulations conforming to 3GPP TS 38 standards, our method not only\noutperforms conventional initialization strategies like random, $K$-means, and\nconstellation-based methods but is also the only approach to achieve successful\nconvergence in deep PT-RBF architectures. These findings pave the way to more\nrobust and efficient neural network deployments in complex digital\ncommunication systems."
                },
                "authors": [
                    {
                        "name": "Jonathan A. Soares"
                    },
                    {
                        "name": "Kayol S. Mayer"
                    },
                    {
                        "name": "Dalton S. Arantes"
                    }
                ],
                "author_detail": {
                    "name": "Dalton S. Arantes"
                },
                "author": "Dalton S. Arantes",
                "arxiv_comment": "IEEE International Conference on Machine Learning for Communication\n  and Networking (ICMLCN 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07666v2",
                "updated": "2024-08-15T01:49:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    1,
                    49,
                    29,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-14T16:58:48Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    58,
                    48,
                    2,
                    227,
                    0
                ],
                "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities"
                },
                "summary": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}."
                },
                "authors": [
                    {
                        "name": "Enneng Yang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Xingwei Wang"
                    },
                    {
                        "name": "Xiaochun Cao"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07665v1",
                "updated": "2024-08-14T16:55:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    55,
                    6,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T16:55:06Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    55,
                    6,
                    2,
                    227,
                    0
                ],
                "title": "Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech\n  Large Language Models"
                },
                "summary": "Warning: This paper may contain texts with uncomfortable content.\n  Large Language Models (LLMs) have achieved remarkable performance in various\ntasks, including those involving multimodal data like speech. However, these\nmodels often exhibit biases due to the nature of their training data. Recently,\nmore Speech Large Language Models (SLLMs) have emerged, underscoring the urgent\nneed to address these biases. This study introduces Spoken Stereoset, a dataset\nspecifically designed to evaluate social biases in SLLMs. By examining how\ndifferent models respond to speech from diverse demographic groups, we aim to\nidentify these biases. Our experiments reveal significant insights into their\nperformance and bias levels. The findings indicate that while most models show\nminimal bias, some still exhibit slightly stereotypical or anti-stereotypical\ntendencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warning: This paper may contain texts with uncomfortable content.\n  Large Language Models (LLMs) have achieved remarkable performance in various\ntasks, including those involving multimodal data like speech. However, these\nmodels often exhibit biases due to the nature of their training data. Recently,\nmore Speech Large Language Models (SLLMs) have emerged, underscoring the urgent\nneed to address these biases. This study introduces Spoken Stereoset, a dataset\nspecifically designed to evaluate social biases in SLLMs. By examining how\ndifferent models respond to speech from diverse demographic groups, we aim to\nidentify these biases. Our experiments reveal significant insights into their\nperformance and bias levels. The findings indicate that while most models show\nminimal bias, some still exhibit slightly stereotypical or anti-stereotypical\ntendencies."
                },
                "authors": [
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Wei-Chih Chen"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17762v2",
                "updated": "2024-08-14T16:00:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    16,
                    0,
                    49,
                    2,
                    227,
                    0
                ],
                "published": "2024-02-27T18:55:17Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    18,
                    55,
                    17,
                    1,
                    58,
                    0
                ],
                "title": "Massive Activations in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Activations in Large Language Models"
                },
                "summary": "We observe an empirical phenomenon in Large Language Models (LLMs) -- very\nfew activations exhibit significantly larger values than others (e.g., 100,000\ntimes larger). We call them massive activations. First, we demonstrate the\nwidespread existence of massive activations across various LLMs and\ncharacterize their locations. Second, we find their values largely stay\nconstant regardless of the input, and they function as indispensable bias terms\nin LLMs. Third, these massive activations lead to the concentration of\nattention probabilities to their corresponding tokens, and further, implicit\nbias terms in the self-attention output. Last, we also study massive\nactivations in Vision Transformers. Code is available at\nhttps://github.com/locuslab/massive-activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We observe an empirical phenomenon in Large Language Models (LLMs) -- very\nfew activations exhibit significantly larger values than others (e.g., 100,000\ntimes larger). We call them massive activations. First, we demonstrate the\nwidespread existence of massive activations across various LLMs and\ncharacterize their locations. Second, we find their values largely stay\nconstant regardless of the input, and they function as indispensable bias terms\nin LLMs. Third, these massive activations lead to the concentration of\nattention probabilities to their corresponding tokens, and further, implicit\nbias terms in the self-attention output. Last, we also study massive\nactivations in Vision Transformers. Code is available at\nhttps://github.com/locuslab/massive-activations."
                },
                "authors": [
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "J. Zico Kolter"
                    },
                    {
                        "name": "Zhuang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Liu"
                },
                "author": "Zhuang Liu",
                "arxiv_comment": "First Conference on Language Modeling (COLM), 2024. Website at\n  https://eric-mingjie.github.io/massive-activations/index.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06583v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06583v3",
                "updated": "2024-08-15T15:24:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    15,
                    24,
                    10,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-13T02:43:19Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    43,
                    19,
                    1,
                    226,
                    0
                ],
                "title": "An Event Structure-aware Generative Model for Biomedical Event\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Event Structure-aware Generative Model for Biomedical Event\n  Extraction"
                },
                "summary": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. BEE has traditionally been formulated as a classification problem. With\nthe recent technological advancements in large language models (LLMs),\ngeneration-based models that cast event extraction as a sequence generation\nproblem have attracted much attention from the NLP research communities.\nHowever, current generative models often overlook the importance of\ncross-instance information from complex event structures such as nested events\nand overlapping events, which contribute quite significantly in the benchmark\ndatasets. In this paper, we propose an event structure-aware generative model\ncalled GenBEE, which can capture complex event structures in biomedical text\nfor biomedical event extraction. In particular, GenBEE constructs event prompts\nthat distill knowledge from LLMs for incorporating both label semantics and\nargument dependency relationships into the proposed model. In addition, GenBEE\nalso generates prefixes with event structural prompts to incorporate structural\nfeatures for improving the model's overall performance. We have evaluated the\nproposed GenBEE model on three widely used biomedical event extraction\nbenchmark datasets, namely MLEE, GE11, and PHEE. Experimental results show that\nGenBEE has achieved state-of-the-art performance on the MLEE and GE11 datasets,\nand achieved competitive results when compared to the state-of-the-art\nclassification-based models on the PHEE dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. BEE has traditionally been formulated as a classification problem. With\nthe recent technological advancements in large language models (LLMs),\ngeneration-based models that cast event extraction as a sequence generation\nproblem have attracted much attention from the NLP research communities.\nHowever, current generative models often overlook the importance of\ncross-instance information from complex event structures such as nested events\nand overlapping events, which contribute quite significantly in the benchmark\ndatasets. In this paper, we propose an event structure-aware generative model\ncalled GenBEE, which can capture complex event structures in biomedical text\nfor biomedical event extraction. In particular, GenBEE constructs event prompts\nthat distill knowledge from LLMs for incorporating both label semantics and\nargument dependency relationships into the proposed model. In addition, GenBEE\nalso generates prefixes with event structural prompts to incorporate structural\nfeatures for improving the model's overall performance. We have evaluated the\nproposed GenBEE model on three widely used biomedical event extraction\nbenchmark datasets, namely MLEE, GE11, and PHEE. Experimental results show that\nGenBEE has achieved state-of-the-art performance on the MLEE and GE11 datasets,\nand achieved competitive results when compared to the state-of-the-art\nclassification-based models on the PHEE dataset."
                },
                "authors": [
                    {
                        "name": "Haohan Yuan"
                    },
                    {
                        "name": "Siu Cheung Hui"
                    },
                    {
                        "name": "Haopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haopeng Zhang"
                },
                "author": "Haopeng Zhang",
                "arxiv_comment": "8 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06583v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06583v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15363v2",
                "updated": "2024-08-14T15:32:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    32,
                    25,
                    2,
                    227,
                    0
                ],
                "published": "2024-04-01T15:17:39Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    15,
                    17,
                    39,
                    0,
                    92,
                    0
                ],
                "title": "Exploring LLM Multi-Agents for ICD Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLM Multi-Agents for ICD Coding"
                },
                "summary": "To address the limitations of Large Language Models (LLMs) in the\nInternational Classification of Diseases (ICD) coding task, where they often\nproduce inaccurate and incomplete prediction results due to the\nhigh-dimensional and skewed distribution of the ICD codes, and often lack\ninterpretability and reliability as well. We introduce an innovative\nmulti-agent approach for ICD coding which mimics the ICD coding assignment\nprocedure in real-world settings, comprising five distinct agents: the patient,\nphysician, coder, reviewer, and adjuster. Each agent utilizes an LLM-based\nmodel tailored to their specific role within the coding process. We also\nintegrate the system with Electronic Health Record (HER)'s SOAP (subjective,\nobjective, assessment and plan) structure to boost the performances. We compare\nour method with a system of agents designed solely by LLMs and other strong\nbaselines and evaluate it using the Medical Information Mart for Intensive Care\nIII (MIMIC-III) dataset. Our multi-agent coding framework significantly\noutperforms Zero-shot Chain of Thought (CoT) prompting and self-consistency\nwith CoT (CoT-SC) in coding common and rare ICD codes. An ablation study\nvalidates the effectiveness of the designated agent roles. it also outperforms\nthe LLM-designed agent system. Moreover, our method achieves comparable results\nto state-of-the-art ICD coding methods that require extensive pre-training or\nfine-tuning, and outperforms them in rare code accuracy, and explainability.\nAdditionally, we demonstrate the method's practical applicability by presenting\nits performance in scenarios not limited by the common or rare ICD code\nconstraints.The proposed multi-agent method for ICD coding effectively mimics\nthe real-world coding process and improves performance on both common and rare\ncodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the limitations of Large Language Models (LLMs) in the\nInternational Classification of Diseases (ICD) coding task, where they often\nproduce inaccurate and incomplete prediction results due to the\nhigh-dimensional and skewed distribution of the ICD codes, and often lack\ninterpretability and reliability as well. We introduce an innovative\nmulti-agent approach for ICD coding which mimics the ICD coding assignment\nprocedure in real-world settings, comprising five distinct agents: the patient,\nphysician, coder, reviewer, and adjuster. Each agent utilizes an LLM-based\nmodel tailored to their specific role within the coding process. We also\nintegrate the system with Electronic Health Record (HER)'s SOAP (subjective,\nobjective, assessment and plan) structure to boost the performances. We compare\nour method with a system of agents designed solely by LLMs and other strong\nbaselines and evaluate it using the Medical Information Mart for Intensive Care\nIII (MIMIC-III) dataset. Our multi-agent coding framework significantly\noutperforms Zero-shot Chain of Thought (CoT) prompting and self-consistency\nwith CoT (CoT-SC) in coding common and rare ICD codes. An ablation study\nvalidates the effectiveness of the designated agent roles. it also outperforms\nthe LLM-designed agent system. Moreover, our method achieves comparable results\nto state-of-the-art ICD coding methods that require extensive pre-training or\nfine-tuning, and outperforms them in rare code accuracy, and explainability.\nAdditionally, we demonstrate the method's practical applicability by presenting\nits performance in scenarios not limited by the common or rare ICD code\nconstraints.The proposed multi-agent method for ICD coding effectively mimics\nthe real-world coding process and improves performance on both common and rare\ncodes."
                },
                "authors": [
                    {
                        "name": "Rumeng Li"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "12pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07611v1",
                "updated": "2024-08-14T15:19:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    19,
                    16,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T15:19:16Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    15,
                    19,
                    16,
                    2,
                    227,
                    0
                ],
                "title": "WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation\n  Integrating Web Search and Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation\n  Integrating Web Search and Knowledge Graphs"
                },
                "summary": "Large Language Models (LLMs) have greatly contributed to the development of\nadaptive intelligent agents and are positioned as an important way to achieve\nArtificial General Intelligence (AGI). However, LLMs are prone to produce\nfactually incorrect information and often produce \"phantom\" content that\nundermines their reliability, which poses a serious challenge for their\ndeployment in real-world scenarios. Enhancing LLMs by combining external\ndatabases and information retrieval mechanisms is an effective path. To address\nthe above challenges, we propose a new approach called WeKnow-RAG, which\nintegrates Web search and Knowledge Graphs into a \"Retrieval-Augmented\nGeneration (RAG)\" system. First, the accuracy and reliability of LLM responses\nare improved by combining the structured representation of Knowledge Graphs\nwith the flexibility of dense vector retrieval. WeKnow-RAG then utilizes\ndomain-specific knowledge graphs to satisfy a variety of queries and domains,\nthereby improving performance on factual information and complex reasoning\ntasks by employing multi-stage web page retrieval techniques using both sparse\nand dense retrieval methods. Our approach effectively balances the efficiency\nand accuracy of information retrieval, thus improving the overall retrieval\nprocess. Finally, we also integrate a self-assessment mechanism for the LLM to\nevaluate the trustworthiness of the answers it generates. Our approach proves\nits outstanding effectiveness in a wide range of offline experiments and online\nsubmissions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have greatly contributed to the development of\nadaptive intelligent agents and are positioned as an important way to achieve\nArtificial General Intelligence (AGI). However, LLMs are prone to produce\nfactually incorrect information and often produce \"phantom\" content that\nundermines their reliability, which poses a serious challenge for their\ndeployment in real-world scenarios. Enhancing LLMs by combining external\ndatabases and information retrieval mechanisms is an effective path. To address\nthe above challenges, we propose a new approach called WeKnow-RAG, which\nintegrates Web search and Knowledge Graphs into a \"Retrieval-Augmented\nGeneration (RAG)\" system. First, the accuracy and reliability of LLM responses\nare improved by combining the structured representation of Knowledge Graphs\nwith the flexibility of dense vector retrieval. WeKnow-RAG then utilizes\ndomain-specific knowledge graphs to satisfy a variety of queries and domains,\nthereby improving performance on factual information and complex reasoning\ntasks by employing multi-stage web page retrieval techniques using both sparse\nand dense retrieval methods. Our approach effectively balances the efficiency\nand accuracy of information retrieval, thus improving the overall retrieval\nprocess. Finally, we also integrate a self-assessment mechanism for the LLM to\nevaluate the trustworthiness of the answers it generates. Our approach proves\nits outstanding effectiveness in a wide range of offline experiments and online\nsubmissions."
                },
                "authors": [
                    {
                        "name": "Weijian Xie"
                    },
                    {
                        "name": "Xuefeng Liang"
                    },
                    {
                        "name": "Yuhui Liu"
                    },
                    {
                        "name": "Kaihua Ni"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Zetian Hu"
                    }
                ],
                "author_detail": {
                    "name": "Zetian Hu"
                },
                "author": "Zetian Hu",
                "arxiv_comment": "8 pages, 2 figures, technical report for 3rd place in Task 3 of Meta\n  KDD Cup 2024 CRAG Challenge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01419v2",
                "updated": "2024-08-14T14:42:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    42,
                    32,
                    2,
                    227,
                    0
                ],
                "published": "2024-05-02T16:08:08Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    8,
                    3,
                    123,
                    0
                ],
                "title": "Natural Language to Verilog: Design of a Recurrent Spiking Neural\n  Network using Large Language Models and ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language to Verilog: Design of a Recurrent Spiking Neural\n  Network using Large Language Models and ChatGPT"
                },
                "summary": "This paper investigates the use of Large Language Models (LLMs) for\nautomating the generation of hardware description code, aiming to explore their\npotential in supporting and enhancing the development of efficient neuromorphic\ncomputing architectures. Building on our prior work, we employ OpenAI's\nChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a\nprogrammable recurrent spiking neural network, while also generating test\nbenches to assess the system's correctness. The resultant design was validated\nin three case studies, the exclusive OR,the IRIS flower classification and the\nMNIST hand-written digit classification, achieving accuracies of up to 96.6%.\nTo verify its synthesizability and implementability, the design was prototyped\non a field-programmable gate array and implemented on SkyWater 130 nm\ntechnology by using an open-source electronic design automation flow.\nAdditionally, we have submitted it to Tiny Tapeout 6 chip fabrication program\nto further evaluate the system on-chip performance in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the use of Large Language Models (LLMs) for\nautomating the generation of hardware description code, aiming to explore their\npotential in supporting and enhancing the development of efficient neuromorphic\ncomputing architectures. Building on our prior work, we employ OpenAI's\nChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a\nprogrammable recurrent spiking neural network, while also generating test\nbenches to assess the system's correctness. The resultant design was validated\nin three case studies, the exclusive OR,the IRIS flower classification and the\nMNIST hand-written digit classification, achieving accuracies of up to 96.6%.\nTo verify its synthesizability and implementability, the design was prototyped\non a field-programmable gate array and implemented on SkyWater 130 nm\ntechnology by using an open-source electronic design automation flow.\nAdditionally, we have submitted it to Tiny Tapeout 6 chip fabrication program\nto further evaluate the system on-chip performance in the future."
                },
                "authors": [
                    {
                        "name": "Paola Vitolo"
                    },
                    {
                        "name": "George Psaltakis"
                    },
                    {
                        "name": "Michael Tomlinson"
                    },
                    {
                        "name": "Gian Domenico Licciardo"
                    },
                    {
                        "name": "Andreas G. Andreou"
                    }
                ],
                "author_detail": {
                    "name": "Andreas G. Andreou"
                },
                "author": "Andreas G. Andreou",
                "arxiv_comment": "This paper was presented at the IEEE/ACM International Conference on\n  Neuromorphic Systems (ICONS), July 30-Aug 2, 2024, Arlington, VA, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07589v1",
                "updated": "2024-08-14T14:41:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    41,
                    13,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T14:41:13Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    41,
                    13,
                    2,
                    227,
                    0
                ],
                "title": "Optimizing UAV Trajectory for Emergency Response Operations under Real\n  3D Environments: Integrating Priority Levels and LoS Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing UAV Trajectory for Emergency Response Operations under Real\n  3D Environments: Integrating Priority Levels and LoS Constraints"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) have emerged as a critical component in\nnext-generation wireless networks, particularly for disaster recovery\nscenarios, due to their flexibility, mobility, and rapid deployment\ncapabilities. This paper focuses on optimizing UAV trajectories to ensure\neffective communication in disaster-stricken areas using terahertz (THz) links.\nWe address specific challenges such as energy consumption, user priority\nlevels, and navigating complex urban environments to maintain Line of Sight\n(LoS) connections amidst 3D obstacles. Our contributions include the\ndevelopment of a detailed modeling approach using online 3D map data, the\nformulation of an optimal trajectory optimization problem, and the proposal of\na Genetic Algorithm (GA)-based method alongside an enhanced heuristic algorithm\nfor faster convergence. Through 3D simulations, we demonstrate the trade-off\nbetween minimizing total service time and prioritizing higher-weight nodes,\nshowing the impact of different priority weight factors on the trajectory time.\nThe proposed algorithms are evaluated using real-world data from the West Bay\narea of Doha, Qatar, demonstrating their effectiveness in optimizing UAV\ntrajectories for emergency response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) have emerged as a critical component in\nnext-generation wireless networks, particularly for disaster recovery\nscenarios, due to their flexibility, mobility, and rapid deployment\ncapabilities. This paper focuses on optimizing UAV trajectories to ensure\neffective communication in disaster-stricken areas using terahertz (THz) links.\nWe address specific challenges such as energy consumption, user priority\nlevels, and navigating complex urban environments to maintain Line of Sight\n(LoS) connections amidst 3D obstacles. Our contributions include the\ndevelopment of a detailed modeling approach using online 3D map data, the\nformulation of an optimal trajectory optimization problem, and the proposal of\na Genetic Algorithm (GA)-based method alongside an enhanced heuristic algorithm\nfor faster convergence. Through 3D simulations, we demonstrate the trade-off\nbetween minimizing total service time and prioritizing higher-weight nodes,\nshowing the impact of different priority weight factors on the trajectory time.\nThe proposed algorithms are evaluated using real-world data from the West Bay\narea of Doha, Qatar, demonstrating their effectiveness in optimizing UAV\ntrajectories for emergency response."
                },
                "authors": [
                    {
                        "name": "Mohammad Taghi Dabiri"
                    },
                    {
                        "name": "Mazen Hasna"
                    },
                    {
                        "name": "Saud Althunibat"
                    },
                    {
                        "name": "Khalid Qaraqe"
                    }
                ],
                "author_detail": {
                    "name": "Khalid Qaraqe"
                },
                "author": "Khalid Qaraqe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07583v1",
                "updated": "2024-08-14T14:28:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    28,
                    11,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T14:28:11Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    14,
                    28,
                    11,
                    2,
                    227,
                    0
                ],
                "title": "Transformers and Large Language Models for Efficient Intrusion Detection\n  Systems: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers and Large Language Models for Efficient Intrusion Detection\n  Systems: A Comprehensive Survey"
                },
                "summary": "With significant advancements in Transformers LLMs, NLP has extended its\nreach into many research fields due to its enhanced capabilities in text\ngeneration and user interaction. One field benefiting greatly from these\nadvancements is cybersecurity. In cybersecurity, many parameters that need to\nbe protected and exchanged between senders and receivers are in the form of\ntext and tabular data, making NLP a valuable tool in enhancing the security\nmeasures of communication protocols. This survey paper provides a comprehensive\nanalysis of the utilization of Transformers and LLMs in cyber-threat detection\nsystems. The methodology of paper selection and bibliometric analysis is\noutlined to establish a rigorous framework for evaluating existing research.\nThe fundamentals of Transformers are discussed, including background\ninformation on various cyber-attacks and datasets commonly used in this field.\nThe survey explores the application of Transformers in IDSs, focusing on\ndifferent architectures such as Attention-based models, LLMs like BERT and GPT,\nCNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.\nFurthermore, it explores the diverse environments and applications where\nTransformers and LLMs-based IDS have been implemented, including computer\nnetworks, IoT devices, critical infrastructure protection, cloud computing,\nSDN, as well as in autonomous vehicles. The paper also addresses research\nchallenges and future directions in this area, identifying key issues such as\ninterpretability, scalability, and adaptability to evolving threats, and more.\nFinally, the conclusion summarizes the findings and highlights the significance\nof Transformers and LLMs in enhancing cyber-threat detection capabilities,\nwhile also outlining potential avenues for further research and development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With significant advancements in Transformers LLMs, NLP has extended its\nreach into many research fields due to its enhanced capabilities in text\ngeneration and user interaction. One field benefiting greatly from these\nadvancements is cybersecurity. In cybersecurity, many parameters that need to\nbe protected and exchanged between senders and receivers are in the form of\ntext and tabular data, making NLP a valuable tool in enhancing the security\nmeasures of communication protocols. This survey paper provides a comprehensive\nanalysis of the utilization of Transformers and LLMs in cyber-threat detection\nsystems. The methodology of paper selection and bibliometric analysis is\noutlined to establish a rigorous framework for evaluating existing research.\nThe fundamentals of Transformers are discussed, including background\ninformation on various cyber-attacks and datasets commonly used in this field.\nThe survey explores the application of Transformers in IDSs, focusing on\ndifferent architectures such as Attention-based models, LLMs like BERT and GPT,\nCNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.\nFurthermore, it explores the diverse environments and applications where\nTransformers and LLMs-based IDS have been implemented, including computer\nnetworks, IoT devices, critical infrastructure protection, cloud computing,\nSDN, as well as in autonomous vehicles. The paper also addresses research\nchallenges and future directions in this area, identifying key issues such as\ninterpretability, scalability, and adaptability to evolving threats, and more.\nFinally, the conclusion summarizes the findings and highlights the significance\nof Transformers and LLMs in enhancing cyber-threat detection capabilities,\nwhile also outlining potential avenues for further research and development."
                },
                "authors": [
                    {
                        "name": "Hamza Kheddar"
                    }
                ],
                "author_detail": {
                    "name": "Hamza Kheddar"
                },
                "author": "Hamza Kheddar",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2405.04760 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01766v3",
                "updated": "2024-08-14T13:41:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    41,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-01-31T14:52:02Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    14,
                    52,
                    2,
                    2,
                    31,
                    0
                ],
                "title": "LLM Voting: Human Choices and AI Collective Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Voting: Human Choices and AI Collective Decision Making"
                },
                "summary": "This paper investigates the voting behaviors of Large Language Models (LLMs),\nspecifically GPT-4 and LLaMA-2, their biases, and how they align with human\nvoting patterns. Our methodology involved using a dataset from a human voting\nexperiment to establish a baseline for human preferences and conducting a\ncorresponding experiment with LLM agents. We observed that the choice of voting\nmethods and the presentation order influenced LLM voting outcomes. We found\nthat varying the persona can reduce some of these biases and enhance alignment\nwith human choices. While the Chain-of-Thought approach did not improve\nprediction accuracy, it has potential for AI explainability in the voting\nprocess. We also identified a trade-off between preference diversity and\nalignment accuracy in LLMs, influenced by different temperature settings. Our\nfindings indicate that LLMs may lead to less diverse collective outcomes and\nbiased assumptions when used in voting scenarios, emphasizing the need for\ncautious integration of LLMs into democratic processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the voting behaviors of Large Language Models (LLMs),\nspecifically GPT-4 and LLaMA-2, their biases, and how they align with human\nvoting patterns. Our methodology involved using a dataset from a human voting\nexperiment to establish a baseline for human preferences and conducting a\ncorresponding experiment with LLM agents. We observed that the choice of voting\nmethods and the presentation order influenced LLM voting outcomes. We found\nthat varying the persona can reduce some of these biases and enhance alignment\nwith human choices. While the Chain-of-Thought approach did not improve\nprediction accuracy, it has potential for AI explainability in the voting\nprocess. We also identified a trade-off between preference diversity and\nalignment accuracy in LLMs, influenced by different temperature settings. Our\nfindings indicate that LLMs may lead to less diverse collective outcomes and\nbiased assumptions when used in voting scenarios, emphasizing the need for\ncautious integration of LLMs into democratic processes."
                },
                "authors": [
                    {
                        "name": "Joshua C. Yang"
                    },
                    {
                        "name": "Damian Dailisan"
                    },
                    {
                        "name": "Marcin Korecki"
                    },
                    {
                        "name": "Carina I. Hausladen"
                    },
                    {
                        "name": "Dirk Helbing"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Helbing"
                },
                "author": "Dirk Helbing",
                "arxiv_comment": "Accepted in AAAI Conference on AI, Ethics, and Society (AIES)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 91B14, 91C20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07542v1",
                "updated": "2024-08-14T13:22:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    22,
                    14,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:22:14Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    22,
                    14,
                    2,
                    227,
                    0
                ],
                "title": "New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson\n  Planning in Ugandan Secondary Schools. Prototype Quality Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson\n  Planning in Ugandan Secondary Schools. Prototype Quality Evaluation"
                },
                "summary": "Introduction: Poor educational quality in Secondary Schools is still regarded\nas one of the major struggles in 21st century Uganda - especially in rural\nareas. Research identifies several problems, including low quality or absent\nteacher lesson planning. As the government pushes towards the implementation of\na new curriculum, exiting lesson plans become obsolete and the problem is\nworsened. Using a Retrieval Augmented Generation approach, we developed a\nprototype that generates customized lesson plans based on the\ngovernment-accredited textbooks. This helps teachers create lesson plans more\nefficiently and with better quality, ensuring they are fully aligned the new\ncurriculum and the competence-based learning approach.\n  Methods: The prototype was created using Cohere LLM and Sentence Embeddings,\nand LangChain Framework - and thereafter made available on a public website.\nVector stores were trained for three new curriculum textbooks (ICT,\nMathematics, History), all at Secondary 1 Level. Twenty-four lessons plans were\ngenerated following a pseudo-random generation protocol, based on the suggested\nperiods in the textbooks. The lesson plans were analyzed regarding their\ntechnical quality by three independent raters following the Lesson Plan\nAnalysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically\ndesigned for East Africa and competence-based curriculums.\n  Results: Evaluation of 24 lesson plans using the LPAP resulted in an average\nquality of between 75 and 80%, corresponding to \"very good lesson plan\". None\nof the lesson plans scored below 65%, although one lesson plan could be argued\nto have been missing the topic. In conclusion, the quality of the generated\nlesson plans is at least comparable, if not better, than those created by\nhumans, as demonstrated in a study in Rwanda, whereby no lesson plan even\nreached the benchmark of 50%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: Poor educational quality in Secondary Schools is still regarded\nas one of the major struggles in 21st century Uganda - especially in rural\nareas. Research identifies several problems, including low quality or absent\nteacher lesson planning. As the government pushes towards the implementation of\na new curriculum, exiting lesson plans become obsolete and the problem is\nworsened. Using a Retrieval Augmented Generation approach, we developed a\nprototype that generates customized lesson plans based on the\ngovernment-accredited textbooks. This helps teachers create lesson plans more\nefficiently and with better quality, ensuring they are fully aligned the new\ncurriculum and the competence-based learning approach.\n  Methods: The prototype was created using Cohere LLM and Sentence Embeddings,\nand LangChain Framework - and thereafter made available on a public website.\nVector stores were trained for three new curriculum textbooks (ICT,\nMathematics, History), all at Secondary 1 Level. Twenty-four lessons plans were\ngenerated following a pseudo-random generation protocol, based on the suggested\nperiods in the textbooks. The lesson plans were analyzed regarding their\ntechnical quality by three independent raters following the Lesson Plan\nAnalysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically\ndesigned for East Africa and competence-based curriculums.\n  Results: Evaluation of 24 lesson plans using the LPAP resulted in an average\nquality of between 75 and 80%, corresponding to \"very good lesson plan\". None\nof the lesson plans scored below 65%, although one lesson plan could be argued\nto have been missing the topic. In conclusion, the quality of the generated\nlesson plans is at least comparable, if not better, than those created by\nhumans, as demonstrated in a study in Rwanda, whereby no lesson plan even\nreached the benchmark of 50%."
                },
                "authors": [
                    {
                        "name": "Simon Kloker"
                    },
                    {
                        "name": "Herbertson Bukoli"
                    },
                    {
                        "name": "Twaha Kateete"
                    }
                ],
                "author_detail": {
                    "name": "Twaha Kateete"
                },
                "author": "Twaha Kateete",
                "arxiv_comment": "Presented at Ndejje University Second Annual Research Dissemination\n  Symposium 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10020v2",
                "updated": "2024-08-14T13:15:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    15,
                    0,
                    2,
                    227,
                    0
                ],
                "published": "2024-03-15T05:06:21Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    5,
                    6,
                    21,
                    4,
                    75,
                    0
                ],
                "title": "Lost in Overlap: Exploring Watermark Collision in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Overlap: Exploring Watermark Collision in LLMs"
                },
                "summary": "The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread usage of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks, such as paraphrasing or translation. In this paper, we introduce\nwatermark collision as a novel and general philosophy for watermark attacks,\naimed at enhancing attack performance on top of any other attacking methods. We\nalso provide a comprehensive demonstration that watermark collision poses a\nthreat to all logit-based watermark algorithms, impacting not only specific\nattack scenarios but also downstream applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread usage of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks, such as paraphrasing or translation. In this paper, we introduce\nwatermark collision as a novel and general philosophy for watermark attacks,\naimed at enhancing attack performance on top of any other attacking methods. We\nalso provide a comprehensive demonstration that watermark collision poses a\nthreat to all logit-based watermark algorithms, impacting not only specific\nattack scenarios but also downstream applications."
                },
                "authors": [
                    {
                        "name": "Yiyang Luo"
                    },
                    {
                        "name": "Ke Lin"
                    },
                    {
                        "name": "Chao Gu"
                    }
                ],
                "author_detail": {
                    "name": "Chao Gu"
                },
                "author": "Chao Gu",
                "arxiv_comment": "Long Paper, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07537v2",
                "updated": "2024-08-15T09:30:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    9,
                    30,
                    35,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-14T13:14:27Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    14,
                    27,
                    2,
                    227,
                    0
                ],
                "title": "Usefulness of data flow diagrams and large language models for security\n  threat validation: a registered report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Usefulness of data flow diagrams and large language models for security\n  threat validation: a registered report"
                },
                "summary": "The arrival of recent cybersecurity standards has raised the bar for security\nassessments in organizations, but existing techniques don't always scale well.\nThreat analysis and risk assessment are used to identify security threats for\nnew or refactored systems. Still, there is a lack of definition-of-done, so\nidentified threats have to be validated which slows down the analysis. Existing\nliterature has focused on the overall performance of threat analysis, but no\nprevious work has investigated how deep must the analysts dig into the material\nbefore they can effectively validate the identified security threats. We\npropose a controlled experiment with practitioners to investigate whether some\nanalysis material (like LLM-generated advice) is better than none and whether\nmore material (the system's data flow diagram and LLM-generated advice) is\nbetter than some material. In addition, we present key findings from running a\npilot with 41 MSc students, which are used to improve the study design.\nFinally, we also provide an initial replication package, including experimental\nmaterial and data analysis scripts and a plan to extend it to include new\nmaterials based on the final data collection campaign with practitioners (e.g.,\npre-screening questions).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The arrival of recent cybersecurity standards has raised the bar for security\nassessments in organizations, but existing techniques don't always scale well.\nThreat analysis and risk assessment are used to identify security threats for\nnew or refactored systems. Still, there is a lack of definition-of-done, so\nidentified threats have to be validated which slows down the analysis. Existing\nliterature has focused on the overall performance of threat analysis, but no\nprevious work has investigated how deep must the analysts dig into the material\nbefore they can effectively validate the identified security threats. We\npropose a controlled experiment with practitioners to investigate whether some\nanalysis material (like LLM-generated advice) is better than none and whether\nmore material (the system's data flow diagram and LLM-generated advice) is\nbetter than some material. In addition, we present key findings from running a\npilot with 41 MSc students, which are used to improve the study design.\nFinally, we also provide an initial replication package, including experimental\nmaterial and data analysis scripts and a plan to extend it to include new\nmaterials based on the final data collection campaign with practitioners (e.g.,\npre-screening questions)."
                },
                "authors": [
                    {
                        "name": "Winnie Bahati Mbaka"
                    },
                    {
                        "name": "Katja Tuma"
                    }
                ],
                "author_detail": {
                    "name": "Katja Tuma"
                },
                "author": "Katja Tuma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07531v1",
                "updated": "2024-08-14T13:03:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    3,
                    41,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:03:41Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    3,
                    41,
                    2,
                    227,
                    0
                ],
                "title": "Development of a Multi-Agent Clinical Decision Support System for Korean\n  Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in\n  Emergency Departments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Multi-Agent Clinical Decision Support System for Korean\n  Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in\n  Emergency Departments"
                },
                "summary": "Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency department (ED) overcrowding and the complexity of rapid\ndecision-making in critical care settings pose significant challenges to\nhealthcare systems worldwide. While clinical decision support systems (CDSS)\nhave shown promise, the integration of large language models (LLMs) offers new\npossibilities for enhancing triage accuracy and clinical decision-making. This\nstudy presents an LLM-driven CDSS designed to assist ED physicians and nurses\nin patient triage, treatment planning, and overall emergency care management.\n  We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM,\norchestrated by CrewAI and Langchain. The system comprises four AI agents\nemulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED\nCoordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for\ntriage assessment and integrates with the RxNorm API for medication management.\n  The model was evaluated using the Asclepius dataset, with performance\nassessed by a clinical emergency medicine specialist. The CDSS demonstrated\nhigh accuracy in triage decision-making compared to the baseline of a\nsingle-agent system. Furthermore, the system exhibited strong performance in\ncritical areas, including primary diagnosis, critical findings identification,\ndisposition decision-making, treatment planning, and resource allocation.\n  Our multi-agent CDSS demonstrates significant potential for supporting\ncomprehensive emergency care management. By leveraging state-of-the-art AI\ntechnologies, this system offers a scalable and adaptable tool that could\nenhance emergency medical care delivery, potentially alleviating ED\novercrowding and improving patient outcomes. This work contributes to the\ngrowing field of AI applications in emergency medicine and offers a promising\ndirection for future research and clinical implementation."
                },
                "authors": [
                    {
                        "name": "Seungjun Han"
                    },
                    {
                        "name": "Wongyung Choi"
                    }
                ],
                "author_detail": {
                    "name": "Wongyung Choi"
                },
                "author": "Wongyung Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07526v1",
                "updated": "2024-08-14T13:01:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    1,
                    30,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T13:01:30Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    13,
                    1,
                    30,
                    2,
                    227,
                    0
                ],
                "title": "Learning-based Models for Vulnerability Detection: An Extensive Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based Models for Vulnerability Detection: An Extensive Study"
                },
                "summary": "Though many deep learning-based models have made great progress in\nvulnerability detection, we have no good understanding of these models, which\nlimits the further advancement of model capability, understanding of the\nmechanism of model detection, and efficiency and safety of practical\napplication of models. In this paper, we extensively and comprehensively\ninvestigate two types of state-of-the-art learning-based approaches\n(sequence-based and graph-based) by conducting experiments on a recently built\nlarge-scale dataset. We investigate seven research questions from five\ndimensions, namely model capabilities, model interpretation, model stability,\nease of use of model, and model economy. We experimentally demonstrate the\npriority of sequence-based models and the limited abilities of both LLM\n(ChatGPT) and graph-based models. We explore the types of vulnerability that\nlearning-based models skilled in and reveal the instability of the models\nthough the input is subtlely semantical-equivalently changed. We empirically\nexplain what the models have learned. We summarize the pre-processing as well\nas requirements for easily using the models. Finally, we initially induce the\nvital information for economically and safely practical usage of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though many deep learning-based models have made great progress in\nvulnerability detection, we have no good understanding of these models, which\nlimits the further advancement of model capability, understanding of the\nmechanism of model detection, and efficiency and safety of practical\napplication of models. In this paper, we extensively and comprehensively\ninvestigate two types of state-of-the-art learning-based approaches\n(sequence-based and graph-based) by conducting experiments on a recently built\nlarge-scale dataset. We investigate seven research questions from five\ndimensions, namely model capabilities, model interpretation, model stability,\nease of use of model, and model economy. We experimentally demonstrate the\npriority of sequence-based models and the limited abilities of both LLM\n(ChatGPT) and graph-based models. We explore the types of vulnerability that\nlearning-based models skilled in and reveal the instability of the models\nthough the input is subtlely semantical-equivalently changed. We empirically\nexplain what the models have learned. We summarize the pre-processing as well\nas requirements for easily using the models. Finally, we initially induce the\nvital information for economically and safely practical usage of these models."
                },
                "authors": [
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Liyu Shen"
                    },
                    {
                        "name": "Xiaodan Xu"
                    },
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Shaohua Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shaohua Wang"
                },
                "author": "Shaohua Wang",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00656v2",
                "updated": "2024-08-14T12:42:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    42,
                    44,
                    2,
                    227,
                    0
                ],
                "published": "2024-03-31T12:01:32Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    12,
                    1,
                    32,
                    6,
                    91,
                    0
                ],
                "title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WavLLM: Towards Robust and Adaptive Speech Large Language Model"
                },
                "summary": "The recent advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing, progressively broadening their scope\nto multimodal perception and generation. However, effectively integrating\nlistening capabilities into LLMs poses significant challenges, particularly\nwith respect to generalizing across varied contexts and executing complex\nauditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech\nlarge language model with dual encoders, and a prompt-aware LoRA weight\nadapter, optimized by a two-stage curriculum learning approach. Leveraging dual\nencoders, we decouple different types of speech information, utilizing a\nWhisper encoder to process the semantic content of speech, and a WavLM encoder\nto capture the unique characteristics of the speaker's identity. Within the\ncurriculum learning framework, WavLLM first builds its foundational\ncapabilities by optimizing on mixed elementary single tasks, followed by\nadvanced multi-task training on more complex tasks such as combinations of the\nelementary tasks. To enhance the flexibility and adherence to different tasks\nand instructions, a prompt-aware LoRA weight adapter is introduced in the\nsecond advanced multi-task training stage. We validate the proposed model on\nuniversal speech benchmarks including tasks such as ASR, ST, SV, ER, and also\napply it to specialized datasets like Gaokao English listening comprehension\nset for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments\ndemonstrate that the proposed model achieves state-of-the-art performance\nacross a range of speech tasks on the same model size, exhibiting robust\ngeneralization capabilities in executing complex tasks using CoT approach.\nFurthermore, our model successfully completes Gaokao tasks without specialized\ntraining. The codes, models, audio, and Gaokao evaluation set can be accessed\nat \\url{aka.ms/wavllm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing, progressively broadening their scope\nto multimodal perception and generation. However, effectively integrating\nlistening capabilities into LLMs poses significant challenges, particularly\nwith respect to generalizing across varied contexts and executing complex\nauditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech\nlarge language model with dual encoders, and a prompt-aware LoRA weight\nadapter, optimized by a two-stage curriculum learning approach. Leveraging dual\nencoders, we decouple different types of speech information, utilizing a\nWhisper encoder to process the semantic content of speech, and a WavLM encoder\nto capture the unique characteristics of the speaker's identity. Within the\ncurriculum learning framework, WavLLM first builds its foundational\ncapabilities by optimizing on mixed elementary single tasks, followed by\nadvanced multi-task training on more complex tasks such as combinations of the\nelementary tasks. To enhance the flexibility and adherence to different tasks\nand instructions, a prompt-aware LoRA weight adapter is introduced in the\nsecond advanced multi-task training stage. We validate the proposed model on\nuniversal speech benchmarks including tasks such as ASR, ST, SV, ER, and also\napply it to specialized datasets like Gaokao English listening comprehension\nset for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments\ndemonstrate that the proposed model achieves state-of-the-art performance\nacross a range of speech tasks on the same model size, exhibiting robust\ngeneralization capabilities in executing complex tasks using CoT approach.\nFurthermore, our model successfully completes Gaokao tasks without specialized\ntraining. The codes, models, audio, and Gaokao evaluation set can be accessed\nat \\url{aka.ms/wavllm}."
                },
                "authors": [
                    {
                        "name": "Shujie Hu"
                    },
                    {
                        "name": "Long Zhou"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sanyuan Chen"
                    },
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Hongkun Hao"
                    },
                    {
                        "name": "Jing Pan"
                    },
                    {
                        "name": "Xunying Liu"
                    },
                    {
                        "name": "Jinyu Li"
                    },
                    {
                        "name": "Sunit Sivasankaran"
                    },
                    {
                        "name": "Linquan Liu"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07505v1",
                "updated": "2024-08-14T12:32:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    32,
                    41,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T12:32:41Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    32,
                    41,
                    2,
                    227,
                    0
                ],
                "title": "Large Language Models Know What Makes Exemplary Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Know What Makes Exemplary Contexts"
                },
                "summary": "In-context learning (ICL) has proven to be a significant capability with the\nadvancement of Large Language models (LLMs). By instructing LLMs using few-shot\ndemonstrative examples, ICL enables them to perform a wide range of tasks\nwithout needing to update millions of parameters. This paper presents a unified\nframework for LLMs that allows them to self-select influential in-context\nexamples to compose their contexts; self-rank candidates with different\ndemonstration compositions; self-optimize the demonstration selection and\nordering through reinforcement learning. Specifically, our method designs a\nparameter-efficient retrieval head that generates the optimized demonstration\nafter training with rewards from LLM's own preference. Experimental results\nvalidate the proposed method's effectiveness in enhancing ICL performance.\nAdditionally, our approach effectively identifies and selects the most\nrepresentative examples for the current task, and includes more diversity in\nretrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has proven to be a significant capability with the\nadvancement of Large Language models (LLMs). By instructing LLMs using few-shot\ndemonstrative examples, ICL enables them to perform a wide range of tasks\nwithout needing to update millions of parameters. This paper presents a unified\nframework for LLMs that allows them to self-select influential in-context\nexamples to compose their contexts; self-rank candidates with different\ndemonstration compositions; self-optimize the demonstration selection and\nordering through reinforcement learning. Specifically, our method designs a\nparameter-efficient retrieval head that generates the optimized demonstration\nafter training with rewards from LLM's own preference. Experimental results\nvalidate the proposed method's effectiveness in enhancing ICL performance.\nAdditionally, our approach effectively identifies and selects the most\nrepresentative examples for the current task, and includes more diversity in\nretrieval."
                },
                "authors": [
                    {
                        "name": "Quanyu Long"
                    },
                    {
                        "name": "Jianda Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianda Chen"
                },
                "author": "Jianda Chen",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07494v1",
                "updated": "2024-08-14T12:19:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    19,
                    25,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T12:19:25Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    12,
                    19,
                    25,
                    2,
                    227,
                    0
                ],
                "title": "QirK: Question Answering via Intermediate Representation on Knowledge\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QirK: Question Answering via Intermediate Representation on Knowledge\n  Graphs"
                },
                "summary": "We demonstrate QirK, a system for answering natural language questions on\nKnowledge Graphs (KG). QirK can answer structurally complex questions that are\nstill beyond the reach of emerging Large Language Models (LLMs). It does so\nusing a unique combination of database technology, LLMs, and semantic search\nover vector embeddings. The glue for these components is an intermediate\nrepresentation (IR). The input question is mapped to IR using LLMs, which is\nthen repaired into a valid relational database query with the aid of a semantic\nsearch on vector embeddings. This allows a practical synthesis of LLM\ncapabilities and KG reliability.\n  A short video demonstrating QirK is available at\nhttps://youtu.be/6c81BLmOZ0U.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate QirK, a system for answering natural language questions on\nKnowledge Graphs (KG). QirK can answer structurally complex questions that are\nstill beyond the reach of emerging Large Language Models (LLMs). It does so\nusing a unique combination of database technology, LLMs, and semantic search\nover vector embeddings. The glue for these components is an intermediate\nrepresentation (IR). The input question is mapped to IR using LLMs, which is\nthen repaired into a valid relational database query with the aid of a semantic\nsearch on vector embeddings. This allows a practical synthesis of LLM\ncapabilities and KG reliability.\n  A short video demonstrating QirK is available at\nhttps://youtu.be/6c81BLmOZ0U."
                },
                "authors": [
                    {
                        "name": "Jan Luca Scheerer"
                    },
                    {
                        "name": "Anton Lykov"
                    },
                    {
                        "name": "Moe Kayali"
                    },
                    {
                        "name": "Ilias Fountalis"
                    },
                    {
                        "name": "Dan Olteanu"
                    },
                    {
                        "name": "Nikolaos Vasiloglou"
                    },
                    {
                        "name": "Dan Suciu"
                    }
                ],
                "author_detail": {
                    "name": "Dan Suciu"
                },
                "author": "Dan Suciu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07482v1",
                "updated": "2024-08-14T11:55:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    55,
                    28,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T11:55:28Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    55,
                    28,
                    2,
                    227,
                    0
                ],
                "title": "Training Overhead Ratio: A Practical Reliability Metric for Large\n  Language Model Training Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Overhead Ratio: A Practical Reliability Metric for Large\n  Language Model Training Systems"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the AI industry with their\nsuperior capabilities. Training these models requires large-scale GPU clusters\nand significant computing time, leading to frequent failures that significantly\nincrease training costs. Despite its significance, this field lacks a metric\nfor evaluating reliability. In this work, we introduce a novel reliability\nmetric called \\emph{Training Overhead Ratio} (TOR) to evaluate the reliability\nof fault-tolerant LLM training systems. TOR is defined as the ratio of optimal\ntraining time to the observed training time of a system, serving as a practical\ntool for users to estimate the actual time required to train an LLM on a given\nsystem. Furthermore, our investigation identifies the key factor for enhancing\nreliability and present TOR equations for various types of failures encountered\nin practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the AI industry with their\nsuperior capabilities. Training these models requires large-scale GPU clusters\nand significant computing time, leading to frequent failures that significantly\nincrease training costs. Despite its significance, this field lacks a metric\nfor evaluating reliability. In this work, we introduce a novel reliability\nmetric called \\emph{Training Overhead Ratio} (TOR) to evaluate the reliability\nof fault-tolerant LLM training systems. TOR is defined as the ratio of optimal\ntraining time to the observed training time of a system, serving as a practical\ntool for users to estimate the actual time required to train an LLM on a given\nsystem. Furthermore, our investigation identifies the key factor for enhancing\nreliability and present TOR equations for various types of failures encountered\nin practice."
                },
                "authors": [
                    {
                        "name": "Ning Lu"
                    },
                    {
                        "name": "Qian Xie"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Wenyi Fang"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Jiantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Ma"
                },
                "author": "Jiantao Ma",
                "arxiv_comment": "preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11811v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11811v3",
                "updated": "2024-08-14T11:47:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    47,
                    39,
                    2,
                    227,
                    0
                ],
                "published": "2024-02-19T03:56:44Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    3,
                    56,
                    44,
                    0,
                    50,
                    0
                ],
                "title": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference\n  Dataset and Modular Fine-tuning Schema",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIPO: Free-form Instruction-oriented Prompt Optimization with Preference\n  Dataset and Modular Fine-tuning Schema"
                },
                "summary": "When the quality of naive prompts is carefully optimized by human experts,\nthe task performance of large language models (LLMs) can be significantly\nimproved. However, expert-based prompt optimizations are expensive. Herein,\nsome works have proposed Automatic Prompt Optimization (APO), to optimize naive\nprompts according to task outputs of given in-box testing models, with the help\nof advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing\nschemes suffer from poor generalization ability and privacy risk. To this end,\nwe collect the first large-scale Prompt Optimization Preference dataset (POP),\nfine-tune offline local LLM-based optimizers, then fairly test with various\ndownstream models. Our method allows accurate optimization of the core task\ninstruction part within the naive prompt in a model-agnostic manner, and thus\nis named Free-from Instruction-oriented Prompt Optimization (FIPO). In\nspecific, FIPO uses a modular APO template that dynamically integrate the naive\ntask instruction, optional instruction responses, and optional ground truth to\nproduce finely optimized prompts. The POP dataset is meticulously constructed\nusing advanced LLMs, undergoing rigorous cross-validation by human experts and\nanalytical models. Leveraging insights from the data with Tulu2 models and\ndiverse fine-tuning strategies, we validate the efficacy of FIPO framework\nacross five public benchmarks and six testing models. Check codes and data\nhere: https://github.com/LuJunru/FIPO_Project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When the quality of naive prompts is carefully optimized by human experts,\nthe task performance of large language models (LLMs) can be significantly\nimproved. However, expert-based prompt optimizations are expensive. Herein,\nsome works have proposed Automatic Prompt Optimization (APO), to optimize naive\nprompts according to task outputs of given in-box testing models, with the help\nof advanced LLMs (e.g., GPT-4) in an ad-hoc way. Although effective, existing\nschemes suffer from poor generalization ability and privacy risk. To this end,\nwe collect the first large-scale Prompt Optimization Preference dataset (POP),\nfine-tune offline local LLM-based optimizers, then fairly test with various\ndownstream models. Our method allows accurate optimization of the core task\ninstruction part within the naive prompt in a model-agnostic manner, and thus\nis named Free-from Instruction-oriented Prompt Optimization (FIPO). In\nspecific, FIPO uses a modular APO template that dynamically integrate the naive\ntask instruction, optional instruction responses, and optional ground truth to\nproduce finely optimized prompts. The POP dataset is meticulously constructed\nusing advanced LLMs, undergoing rigorous cross-validation by human experts and\nanalytical models. Leveraging insights from the data with Tulu2 models and\ndiverse fine-tuning strategies, we validate the efficacy of FIPO framework\nacross five public benchmarks and six testing models. Check codes and data\nhere: https://github.com/LuJunru/FIPO_Project."
                },
                "authors": [
                    {
                        "name": "Junru Lu"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11811v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11811v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07471v1",
                "updated": "2024-08-14T11:29:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    29,
                    47,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T11:29:47Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    29,
                    47,
                    2,
                    227,
                    0
                ],
                "title": "Bridging and Modeling Correlations in Pairwise Data for Direct\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging and Modeling Correlations in Pairwise Data for Direct\n  Preference Optimization"
                },
                "summary": "Direct preference optimization (DPO), a widely adopted offline preference\noptimization algorithm, aims to align large language models (LLMs) with\nhuman-desired behaviors using pairwise preference data. However, the winning\nresponse and the losing response within pairwise data are generated isolatedly,\nleading to weak correlations between them as well as suboptimal alignment\nperformance. To address this issue, we propose an effective framework named\nBMC, for bridging and modeling correlations in pairwise data. Firstly, we\nincrease the consistency and informativeness of the pairwise preference signals\nby targeted modifications, synthesizing a pseudo winning response through\nimproving the losing response based on the winning response. Secondly, we\nidentify that DPO alone is insufficient to model these correlations and capture\nnuanced variations. Therefore, we propose learning token-level correlations by\ndynamically leveraging the policy model's confidence during training.\nComprehensive experiments on QA, math, and instruction-following tasks\ndemonstrate the effectiveness of our approach, significantly surpassing\ncompetitive baselines, including DPO. Additionally, our in-depth quantitative\nanalysis reveals the reasons behind our method's superior performance over DPO\nand showcases its versatility to other DPO variants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct preference optimization (DPO), a widely adopted offline preference\noptimization algorithm, aims to align large language models (LLMs) with\nhuman-desired behaviors using pairwise preference data. However, the winning\nresponse and the losing response within pairwise data are generated isolatedly,\nleading to weak correlations between them as well as suboptimal alignment\nperformance. To address this issue, we propose an effective framework named\nBMC, for bridging and modeling correlations in pairwise data. Firstly, we\nincrease the consistency and informativeness of the pairwise preference signals\nby targeted modifications, synthesizing a pseudo winning response through\nimproving the losing response based on the winning response. Secondly, we\nidentify that DPO alone is insufficient to model these correlations and capture\nnuanced variations. Therefore, we propose learning token-level correlations by\ndynamically leveraging the policy model's confidence during training.\nComprehensive experiments on QA, math, and instruction-following tasks\ndemonstrate the effectiveness of our approach, significantly surpassing\ncompetitive baselines, including DPO. Additionally, our in-depth quantitative\nanalysis reveals the reasons behind our method's superior performance over DPO\nand showcases its versatility to other DPO variants."
                },
                "authors": [
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Bo Huang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Liangyou Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "18 pages, 8 figures, 8 tables, working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07465v1",
                "updated": "2024-08-14T11:19:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    19,
                    28,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T11:19:28Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    19,
                    28,
                    2,
                    227,
                    0
                ],
                "title": "Large Language Models Prompting With Episodic Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Prompting With Episodic Memory"
                },
                "summary": "Prompt optimization is essential for enhancing the performance of Large\nLanguage Models (LLMs) in a range of Natural Language Processing (NLP) tasks,\nparticularly in scenarios of few-shot learning where training examples are\nincorporated directly into the prompt. Despite the growing interest in\noptimizing prompts with few-shot examples, existing methods for prompt\noptimization are often resource-intensive or perform inadequately. In this\nwork, we propose PrOmpting with Episodic Memory (POEM), a novel prompt\noptimization technique that is simple, efficient, and demonstrates strong\ngeneralization capabilities. We approach prompt optimization as a Reinforcement\nLearning (RL) challenge, using episodic memory to archive combinations of input\ndata, permutations of few-shot examples, and the rewards observed during\ntraining. In the testing phase, we optimize the sequence of examples for each\ntest query by selecting the sequence that yields the highest total rewards from\nthe top-k most similar training examples in the episodic memory. Our results\nshow that POEM outperforms recent techniques like TEMPERA and RLPrompt by over\n5.3% in various text classification tasks. Furthermore, our approach adapts\nwell to broader language understanding tasks, consistently outperforming\nconventional heuristic methods for ordering examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt optimization is essential for enhancing the performance of Large\nLanguage Models (LLMs) in a range of Natural Language Processing (NLP) tasks,\nparticularly in scenarios of few-shot learning where training examples are\nincorporated directly into the prompt. Despite the growing interest in\noptimizing prompts with few-shot examples, existing methods for prompt\noptimization are often resource-intensive or perform inadequately. In this\nwork, we propose PrOmpting with Episodic Memory (POEM), a novel prompt\noptimization technique that is simple, efficient, and demonstrates strong\ngeneralization capabilities. We approach prompt optimization as a Reinforcement\nLearning (RL) challenge, using episodic memory to archive combinations of input\ndata, permutations of few-shot examples, and the rewards observed during\ntraining. In the testing phase, we optimize the sequence of examples for each\ntest query by selecting the sequence that yields the highest total rewards from\nthe top-k most similar training examples in the episodic memory. Our results\nshow that POEM outperforms recent techniques like TEMPERA and RLPrompt by over\n5.3% in various text classification tasks. Furthermore, our approach adapts\nwell to broader language understanding tasks, consistently outperforming\nconventional heuristic methods for ordering examples."
                },
                "authors": [
                    {
                        "name": "Dai Do"
                    },
                    {
                        "name": "Quan Tran"
                    },
                    {
                        "name": "Svetha Venkatesh"
                    },
                    {
                        "name": "Hung Le"
                    }
                ],
                "author_detail": {
                    "name": "Hung Le"
                },
                "author": "Hung Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07440v2",
                "updated": "2024-08-15T10:45:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    45,
                    18,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-14T10:18:42Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    18,
                    42,
                    2,
                    227,
                    0
                ],
                "title": "BAPLe: Backdoor Attacks on Medical Foundational Models using Prompt\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BAPLe: Backdoor Attacks on Medical Foundational Models using Prompt\n  Learning"
                },
                "summary": "Medical foundation models are gaining prominence in the medical community for\ntheir ability to derive general representations from extensive collections of\nmedical image-text pairs. Recent research indicates that these models are\nsusceptible to backdoor attacks, which allow them to classify clean images\naccurately but fail when specific triggers are introduced. However, traditional\nbackdoor attacks necessitate a considerable amount of additional data to\nmaliciously pre-train a model. This requirement is often impractical in medical\nimaging applications due to the usual scarcity of data. Inspired by the latest\ndevelopments in learnable prompts, this work introduces a method to embed a\nbackdoor into the medical foundation model during the prompt learning phase. By\nincorporating learnable prompts within the text encoder and introducing\nimperceptible learnable noise trigger to the input images, we exploit the full\ncapabilities of the medical foundation models (Med-FM). Our method, BAPLe,\nrequires only a minimal subset of data to adjust the noise trigger and the text\nprompts for downstream tasks, enabling the creation of an effective backdoor\nattack. Through extensive experiments with four medical foundation models, each\npre-trained on different modalities and evaluated across six downstream\ndatasets, we demonstrate the efficacy of our approach. BAPLe achieves a high\nbackdoor success rate across all models and datasets, outperforming the\nbaseline backdoor attack methods. Our work highlights the vulnerability of\nMed-FMs towards backdoor attacks and strives to promote the safe adoption of\nMed-FMs before their deployment in real-world applications. Code is available\nat https://asif-hanif.github.io/baple/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical foundation models are gaining prominence in the medical community for\ntheir ability to derive general representations from extensive collections of\nmedical image-text pairs. Recent research indicates that these models are\nsusceptible to backdoor attacks, which allow them to classify clean images\naccurately but fail when specific triggers are introduced. However, traditional\nbackdoor attacks necessitate a considerable amount of additional data to\nmaliciously pre-train a model. This requirement is often impractical in medical\nimaging applications due to the usual scarcity of data. Inspired by the latest\ndevelopments in learnable prompts, this work introduces a method to embed a\nbackdoor into the medical foundation model during the prompt learning phase. By\nincorporating learnable prompts within the text encoder and introducing\nimperceptible learnable noise trigger to the input images, we exploit the full\ncapabilities of the medical foundation models (Med-FM). Our method, BAPLe,\nrequires only a minimal subset of data to adjust the noise trigger and the text\nprompts for downstream tasks, enabling the creation of an effective backdoor\nattack. Through extensive experiments with four medical foundation models, each\npre-trained on different modalities and evaluated across six downstream\ndatasets, we demonstrate the efficacy of our approach. BAPLe achieves a high\nbackdoor success rate across all models and datasets, outperforming the\nbaseline backdoor attack methods. Our work highlights the vulnerability of\nMed-FMs towards backdoor attacks and strives to promote the safe adoption of\nMed-FMs before their deployment in real-world applications. Code is available\nat https://asif-hanif.github.io/baple/."
                },
                "authors": [
                    {
                        "name": "Asif Hanif"
                    },
                    {
                        "name": "Fahad Shamshad"
                    },
                    {
                        "name": "Muhammad Awais"
                    },
                    {
                        "name": "Muzammal Naseer"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Karthik Nandakumar"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    }
                ],
                "author_detail": {
                    "name": "Rao Muhammad Anwer"
                },
                "author": "Rao Muhammad Anwer",
                "arxiv_comment": "MICCAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07427v1",
                "updated": "2024-08-14T10:03:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    3,
                    40,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T10:03:40Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    3,
                    40,
                    2,
                    227,
                    0
                ],
                "title": "Beyond Inter-Item Relations: Dynamic Adaptive Mixture-of-Experts for\n  LLM-Based Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Inter-Item Relations: Dynamic Adaptive Mixture-of-Experts for\n  LLM-Based Sequential Recommendation"
                },
                "summary": "Sequential recommender system (SRS) predicts the next items that users may\nprefer based on user historical interaction sequences. Inspired by the rise of\nlarge language models (LLMs) in various AI applications, there is a surge of\nwork on LLM-based SRS. Despite their attractive performance, existing LLM-based\nSRS still exhibit some limitations, including neglecting intra-item relations,\nignoring long-term collaborative knowledge and using inflexible architecture\ndesigns for adaption. To alleviate these issues, we propose an LLM-based SRS\nnamed MixRec. Built on top of coarse-grained adaption for capturing inter-item\nrelations, MixRec is further enhanced with (1) context masking that models\nintra-item relations to help LLM better understand token and item semantics in\nthe context of SRS, (2) collaborative knowledge injection that helps LLM\nincorporate long-term collaborative knowledge, and (3) a dynamic adaptive\nmixture-of-experts design that can flexibly choose expert architectures based\non Bayesian optimization to better incorporate different sequential\ninformation. Extensive experiments demonstrate that MixRec can effectively\nhandle sequential recommendation in a dynamic and adaptive manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommender system (SRS) predicts the next items that users may\nprefer based on user historical interaction sequences. Inspired by the rise of\nlarge language models (LLMs) in various AI applications, there is a surge of\nwork on LLM-based SRS. Despite their attractive performance, existing LLM-based\nSRS still exhibit some limitations, including neglecting intra-item relations,\nignoring long-term collaborative knowledge and using inflexible architecture\ndesigns for adaption. To alleviate these issues, we propose an LLM-based SRS\nnamed MixRec. Built on top of coarse-grained adaption for capturing inter-item\nrelations, MixRec is further enhanced with (1) context masking that models\nintra-item relations to help LLM better understand token and item semantics in\nthe context of SRS, (2) collaborative knowledge injection that helps LLM\nincorporate long-term collaborative knowledge, and (3) a dynamic adaptive\nmixture-of-experts design that can flexibly choose expert architectures based\non Bayesian optimization to better incorporate different sequential\ninformation. Extensive experiments demonstrate that MixRec can effectively\nhandle sequential recommendation in a dynamic and adaptive manner."
                },
                "authors": [
                    {
                        "name": "CanYi Liu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Youchen"
                    },
                    {
                        "name": "Zhang"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "arxiv_affiliation": "Victor",
                "author": "Rongrong Ji",
                "arxiv_comment": "11 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07425v1",
                "updated": "2024-08-14T10:03:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    3,
                    28,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T10:03:28Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    3,
                    28,
                    2,
                    227,
                    0
                ],
                "title": "Exploring Retrieval Augmented Generation in Arabic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Retrieval Augmented Generation in Arabic"
                },
                "summary": "Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful\ntechnique in natural language processing, combining the strengths of\nretrieval-based and generation-based models to enhance text generation tasks.\nHowever, the application of RAG in Arabic, a language with unique\ncharacteristics and resource constraints, remains underexplored. This paper\npresents a comprehensive case study on the implementation and evaluation of RAG\nfor Arabic text. The work focuses on exploring various semantic embedding\nmodels in the retrieval stage and several LLMs in the generation stage, in\norder to investigate what works and what doesn't in the context of Arabic. The\nwork also touches upon the issue of variations between document dialect and\nquery dialect in the retrieval stage. Results show that existing semantic\nembedding models and LLMs can be effectively employed to build Arabic RAG\npipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful\ntechnique in natural language processing, combining the strengths of\nretrieval-based and generation-based models to enhance text generation tasks.\nHowever, the application of RAG in Arabic, a language with unique\ncharacteristics and resource constraints, remains underexplored. This paper\npresents a comprehensive case study on the implementation and evaluation of RAG\nfor Arabic text. The work focuses on exploring various semantic embedding\nmodels in the retrieval stage and several LLMs in the generation stage, in\norder to investigate what works and what doesn't in the context of Arabic. The\nwork also touches upon the issue of variations between document dialect and\nquery dialect in the retrieval stage. Results show that existing semantic\nembedding models and LLMs can be effectively employed to build Arabic RAG\npipelines."
                },
                "authors": [
                    {
                        "name": "Samhaa R. El-Beltagy"
                    },
                    {
                        "name": "Mohamed A. Abdallah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed A. Abdallah"
                },
                "author": "Mohamed A. Abdallah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07422v1",
                "updated": "2024-08-14T10:00:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    0,
                    16,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T10:00:16Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    0,
                    16,
                    2,
                    227,
                    0
                ],
                "title": "LLMI3D: Empowering LLM with 3D Perception from a Single 2D Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMI3D: Empowering LLM with 3D Perception from a Single 2D Image"
                },
                "summary": "Recent advancements in autonomous driving, augmented reality, robotics, and\nembodied intelligence have necessitated 3D perception algorithms. However,\ncurrent 3D perception methods, particularly small models, struggle with\nprocessing logical reasoning, question-answering, and handling open scenario\ncategories. On the other hand, generative multimodal large language models\n(MLLMs) excel in general capacity but underperform in 3D tasks, due to weak\nspatial and local object perception, poor text-based geometric numerical\noutput, and inability to handle camera focal variations. To address these\nchallenges, we propose the following solutions: Spatial-Enhanced Local Feature\nMining for better spatial feature extraction, 3D Query Token-Derived Info\nDecoding for precise geometric regression, and Geometry Projection-Based 3D\nReasoning for handling camera focal length variations. We employ\nparameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a\npowerful 3D perception MLLM. Additionally, we have constructed the IG3D\ndataset, which provides fine-grained descriptions and question-answer\nannotations. Extensive experiments demonstrate that our LLMI3D achieves\nstate-of-the-art performance, significantly outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in autonomous driving, augmented reality, robotics, and\nembodied intelligence have necessitated 3D perception algorithms. However,\ncurrent 3D perception methods, particularly small models, struggle with\nprocessing logical reasoning, question-answering, and handling open scenario\ncategories. On the other hand, generative multimodal large language models\n(MLLMs) excel in general capacity but underperform in 3D tasks, due to weak\nspatial and local object perception, poor text-based geometric numerical\noutput, and inability to handle camera focal variations. To address these\nchallenges, we propose the following solutions: Spatial-Enhanced Local Feature\nMining for better spatial feature extraction, 3D Query Token-Derived Info\nDecoding for precise geometric regression, and Geometry Projection-Based 3D\nReasoning for handling camera focal length variations. We employ\nparameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a\npowerful 3D perception MLLM. Additionally, we have constructed the IG3D\ndataset, which provides fine-grained descriptions and question-answer\nannotations. Extensive experiments demonstrate that our LLMI3D achieves\nstate-of-the-art performance, significantly outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haoxiang Chen"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Wenbo Tang"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Pengfei Xu"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01107v2",
                "updated": "2024-08-14T09:54:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    54,
                    24,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-02T08:37:03Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    8,
                    37,
                    3,
                    4,
                    215,
                    0
                ],
                "title": "BioRAG: A RAG-LLM Framework for Biological Question Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioRAG: A RAG-LLM Framework for Biological Question Reasoning"
                },
                "summary": "The question-answering system for Life science research, which is\ncharacterized by the rapid pace of discovery, evolving insights, and complex\ninteractions among knowledge entities, presents unique challenges in\nmaintaining a comprehensive knowledge warehouse and accurate information\nretrieval. To address these issues, we introduce BioRAG, a novel\nRetrieval-Augmented Generation (RAG) with the Large Language Models (LLMs)\nframework. Our approach starts with parsing, indexing, and segmenting an\nextensive collection of 22 million scientific papers as the basic knowledge,\nfollowed by training a specialized embedding model tailored to this domain.\nAdditionally, we enhance the vector retrieval process by incorporating a\ndomain-specific knowledge hierarchy, which aids in modeling the intricate\ninterrelationships among each query and context. For queries requiring the most\ncurrent information, BioRAG deconstructs the question and employs an iterative\nretrieval process incorporated with the search engine for step-by-step\nreasoning. Rigorous experiments have demonstrated that our model outperforms\nfine-tuned LLM, LLM with search engines, and other scientific RAG frameworks\nacross multiple life science question-answering tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The question-answering system for Life science research, which is\ncharacterized by the rapid pace of discovery, evolving insights, and complex\ninteractions among knowledge entities, presents unique challenges in\nmaintaining a comprehensive knowledge warehouse and accurate information\nretrieval. To address these issues, we introduce BioRAG, a novel\nRetrieval-Augmented Generation (RAG) with the Large Language Models (LLMs)\nframework. Our approach starts with parsing, indexing, and segmenting an\nextensive collection of 22 million scientific papers as the basic knowledge,\nfollowed by training a specialized embedding model tailored to this domain.\nAdditionally, we enhance the vector retrieval process by incorporating a\ndomain-specific knowledge hierarchy, which aids in modeling the intricate\ninterrelationships among each query and context. For queries requiring the most\ncurrent information, BioRAG deconstructs the question and employs an iterative\nretrieval process incorporated with the search engine for step-by-step\nreasoning. Rigorous experiments have demonstrated that our model outperforms\nfine-tuned LLM, LLM with search engines, and other scientific RAG frameworks\nacross multiple life science question-answering tasks."
                },
                "authors": [
                    {
                        "name": "Chengrui Wang"
                    },
                    {
                        "name": "Qingqing Long"
                    },
                    {
                        "name": "Meng Xiao"
                    },
                    {
                        "name": "Xunxin Cai"
                    },
                    {
                        "name": "Chengjun Wu"
                    },
                    {
                        "name": "Zhen Meng"
                    },
                    {
                        "name": "Xuezhi Wang"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Zhou"
                },
                "author": "Yuanchun Zhou",
                "arxiv_comment": "12 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07413v1",
                "updated": "2024-08-14T09:43:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    43,
                    32,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T09:43:32Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    43,
                    32,
                    2,
                    227,
                    0
                ],
                "title": "Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge\n  Editing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge\n  Editing for Large Language Models"
                },
                "summary": "Knowledge editing aims to update outdated or incorrect knowledge in large\nlanguage models (LLMs). However, current knowledge editing methods have limited\nscalability for lifelong editing. This study explores the fundamental reason\nwhy knowledge editing fails in lifelong editing. We begin with the closed-form\nsolution derived from linear associative memory, which underpins\nstate-of-the-art knowledge editing methods. We extend the solution from single\nediting to lifelong editing, and through rigorous mathematical derivation,\nidentify an interference term in the final solution, suggesting that editing\nknowledge may impact irrelevant knowledge. Further analysis of the interference\nterm reveals a close relationship with superposition between knowledge\nrepresentations. When knowledge superposition does not exist in language\nmodels, the interference term vanishes, allowing for lossless knowledge\nediting. Experiments across numerous language models reveal that knowledge\nsuperposition is universal, exhibiting high kurtosis, zero mean, and\nheavy-tailed distributions with clear scaling laws. Ultimately, by combining\ntheory and experiments, we demonstrate that knowledge superposition is the\nfundamental reason for the failure of lifelong editing. Moreover, this is the\nfirst study to investigate knowledge editing from the perspective of\nsuperposition and provides a comprehensive observation of superposition across\nnumerous real-world language models. Code available at\nhttps://github.com/ChenhuiHu/knowledge_in_superposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing aims to update outdated or incorrect knowledge in large\nlanguage models (LLMs). However, current knowledge editing methods have limited\nscalability for lifelong editing. This study explores the fundamental reason\nwhy knowledge editing fails in lifelong editing. We begin with the closed-form\nsolution derived from linear associative memory, which underpins\nstate-of-the-art knowledge editing methods. We extend the solution from single\nediting to lifelong editing, and through rigorous mathematical derivation,\nidentify an interference term in the final solution, suggesting that editing\nknowledge may impact irrelevant knowledge. Further analysis of the interference\nterm reveals a close relationship with superposition between knowledge\nrepresentations. When knowledge superposition does not exist in language\nmodels, the interference term vanishes, allowing for lossless knowledge\nediting. Experiments across numerous language models reveal that knowledge\nsuperposition is universal, exhibiting high kurtosis, zero mean, and\nheavy-tailed distributions with clear scaling laws. Ultimately, by combining\ntheory and experiments, we demonstrate that knowledge superposition is the\nfundamental reason for the failure of lifelong editing. Moreover, this is the\nfirst study to investigate knowledge editing from the perspective of\nsuperposition and provides a comprehensive observation of superposition across\nnumerous real-world language models. Code available at\nhttps://github.com/ChenhuiHu/knowledge_in_superposition."
                },
                "authors": [
                    {
                        "name": "Chenhui Hu"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07404v1",
                "updated": "2024-08-14T09:24:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    24,
                    0,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T09:24:00Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    24,
                    0,
                    2,
                    227,
                    0
                ],
                "title": "Efficient Edge AI: Deploying Convolutional Neural Networks on FPGA with\n  the Gemmini Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Edge AI: Deploying Convolutional Neural Networks on FPGA with\n  the Gemmini Accelerator"
                },
                "summary": "The growing concerns regarding energy consumption and privacy have prompted\nthe development of AI solutions deployable on the edge, circumventing the\nsubstantial CO2 emissions associated with cloud servers and mitigating risks\nrelated to sharing sensitive data. But deploying Convolutional Neural Networks\n(CNNs) on non-off-the-shelf edge devices remains a complex and labor-intensive\ntask. In this paper, we present and end-to-end workflow for deployment of CNNs\non Field Programmable Gate Arrays (FPGAs) using the Gemmini accelerator, which\nwe modified for efficient implementation on FPGAs. We describe how we leverage\nthe use of open source software on each optimization step of the deployment\nprocess, the customizations we added to them and its impact on the final\nsystem's performance. We were able to achieve real-time performance by\ndeploying a YOLOv7 model on a Xilinx ZCU102 FPGA with an energy efficiency of\n36.5 GOP/s/W. Our FPGA-based solution demonstrates superior power efficiency\ncompared with other embedded hardware devices, and even outperforms other FPGA\nreference implementations. Finally, we present how this kind of solution can be\nintegrated into a wider system, by testing our proposed platform in a traffic\nmonitoring scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing concerns regarding energy consumption and privacy have prompted\nthe development of AI solutions deployable on the edge, circumventing the\nsubstantial CO2 emissions associated with cloud servers and mitigating risks\nrelated to sharing sensitive data. But deploying Convolutional Neural Networks\n(CNNs) on non-off-the-shelf edge devices remains a complex and labor-intensive\ntask. In this paper, we present and end-to-end workflow for deployment of CNNs\non Field Programmable Gate Arrays (FPGAs) using the Gemmini accelerator, which\nwe modified for efficient implementation on FPGAs. We describe how we leverage\nthe use of open source software on each optimization step of the deployment\nprocess, the customizations we added to them and its impact on the final\nsystem's performance. We were able to achieve real-time performance by\ndeploying a YOLOv7 model on a Xilinx ZCU102 FPGA with an energy efficiency of\n36.5 GOP/s/W. Our FPGA-based solution demonstrates superior power efficiency\ncompared with other embedded hardware devices, and even outperforms other FPGA\nreference implementations. Finally, we present how this kind of solution can be\nintegrated into a wider system, by testing our proposed platform in a traffic\nmonitoring scenario."
                },
                "authors": [
                    {
                        "name": "Federico Nicolas Peccia"
                    },
                    {
                        "name": "Svetlana Pavlitska"
                    },
                    {
                        "name": "Tobias Fleck"
                    },
                    {
                        "name": "Oliver Bringmann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Bringmann"
                },
                "author": "Oliver Bringmann",
                "arxiv_comment": "8 pages, 9 figures, accepted at the 27th Euromicro Conference Series\n  on Digital System Design (DSD) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21264v2",
                "updated": "2024-08-14T08:10:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    8,
                    10,
                    43,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-31T00:56:09Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    0,
                    56,
                    9,
                    2,
                    213,
                    0
                ],
                "title": "Model Attribution in LLM-Generated Disinformation: A Domain\n  Generalization Approach with Supervised Contrastive Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Attribution in LLM-Generated Disinformation: A Domain\n  Generalization Approach with Supervised Contrastive Learning"
                },
                "summary": "Model attribution for LLM-generated disinformation poses a significant\nchallenge in understanding its origins and mitigating its spread. This task is\nespecially challenging because modern large language models (LLMs) produce\ndisinformation with human-like quality. Additionally, the diversity in\nprompting methods used to generate disinformation complicates accurate source\nattribution. These methods introduce domain-specific features that can mask the\nfundamental characteristics of the models. In this paper, we introduce the\nconcept of model attribution as a domain generalization problem, where each\nprompting method represents a unique domain. We argue that an effective\nattribution model must be invariant to these domain-specific features. It\nshould also be proficient in identifying the originating models across all\nscenarios, reflecting real-world detection challenges. To address this, we\nintroduce a novel approach based on Supervised Contrastive Learning. This\nmethod is designed to enhance the model's robustness to variations in prompts\nand focuses on distinguishing between different source LLMs. We evaluate our\nmodel through rigorous experiments involving three common prompting methods:\n``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:\n``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the\neffectiveness of our approach in model attribution tasks, achieving\nstate-of-the-art performance across diverse and unseen datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model attribution for LLM-generated disinformation poses a significant\nchallenge in understanding its origins and mitigating its spread. This task is\nespecially challenging because modern large language models (LLMs) produce\ndisinformation with human-like quality. Additionally, the diversity in\nprompting methods used to generate disinformation complicates accurate source\nattribution. These methods introduce domain-specific features that can mask the\nfundamental characteristics of the models. In this paper, we introduce the\nconcept of model attribution as a domain generalization problem, where each\nprompting method represents a unique domain. We argue that an effective\nattribution model must be invariant to these domain-specific features. It\nshould also be proficient in identifying the originating models across all\nscenarios, reflecting real-world detection challenges. To address this, we\nintroduce a novel approach based on Supervised Contrastive Learning. This\nmethod is designed to enhance the model's robustness to variations in prompts\nand focuses on distinguishing between different source LLMs. We evaluate our\nmodel through rigorous experiments involving three common prompting methods:\n``open-ended'', ``rewriting'', and ``paraphrasing'', and three advanced LLMs:\n``llama 2'', ``chatgpt'', and ``vicuna''. Our results demonstrate the\neffectiveness of our approach in model attribution tasks, achieving\nstate-of-the-art performance across diverse and unseen datasets."
                },
                "authors": [
                    {
                        "name": "Alimohammad Beigi"
                    },
                    {
                        "name": "Zhen Tan"
                    },
                    {
                        "name": "Nivedh Mudiam"
                    },
                    {
                        "name": "Canyu Chen"
                    },
                    {
                        "name": "Kai Shu"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "10 pages, 2 figures, accepted at DSAA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07348v1",
                "updated": "2024-08-14T07:43:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    43,
                    12,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T07:43:12Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    43,
                    12,
                    2,
                    227,
                    0
                ],
                "title": "From photon momentum transfer to acceleration sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From photon momentum transfer to acceleration sensing"
                },
                "summary": "As a typical application of photon momentum transfer, optical levitation\nsystems are known for their ideal isolation from mechanical dissipation and\nthermal noise. These characters offer extraordinary potential for acceleration\nprecision sensing and have attracted extensive attention in both fundamental\nand applied physics. Although considerable improvements of optical levitation\naccelerometers has been reported, the dynamic testing of the sensing\nperformance remains a crucial challenge before the utilization in practical\napplication scenarios. In this work, we present a dual-beam optical levitation\naccelerometer and demonstrate the test with dynamic inputs for the first time.\nAn acceleration sensing sensitivity of $0.1\\mu g$ and a measurement range of $\n1g$ are achieved. These advancements solidify the potential of optical\nlevitation accelerometer for deployment in practical domains, including\nnavigation, intelligent driving, and industrial automation, building a bridge\nbetween the laboratory systems and real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a typical application of photon momentum transfer, optical levitation\nsystems are known for their ideal isolation from mechanical dissipation and\nthermal noise. These characters offer extraordinary potential for acceleration\nprecision sensing and have attracted extensive attention in both fundamental\nand applied physics. Although considerable improvements of optical levitation\naccelerometers has been reported, the dynamic testing of the sensing\nperformance remains a crucial challenge before the utilization in practical\napplication scenarios. In this work, we present a dual-beam optical levitation\naccelerometer and demonstrate the test with dynamic inputs for the first time.\nAn acceleration sensing sensitivity of $0.1\\mu g$ and a measurement range of $\n1g$ are achieved. These advancements solidify the potential of optical\nlevitation accelerometer for deployment in practical domains, including\nnavigation, intelligent driving, and industrial automation, building a bridge\nbetween the laboratory systems and real-world applications."
                },
                "authors": [
                    {
                        "name": "Jianyu Yang"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Yuyao Pan"
                    },
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Zhiming Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Yuliang Wang"
                    },
                    {
                        "name": "Chuankun Han"
                    },
                    {
                        "name": "Xingfan Chen"
                    },
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Huizhu Hu"
                    }
                ],
                "author_detail": {
                    "name": "Huizhu Hu"
                },
                "author": "Huizhu Hu",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04145v2",
                "updated": "2024-08-14T07:43:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    43,
                    6,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-08T01:12:21Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    12,
                    21,
                    3,
                    221,
                    0
                ],
                "title": "ComKD-CLIP: Comprehensive Knowledge Distillation for Contrastive\n  Language-Image Pre-traning Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComKD-CLIP: Comprehensive Knowledge Distillation for Contrastive\n  Language-Image Pre-traning Model"
                },
                "summary": "Contrastive Language-Image Pre-training (CLIP) model excels in integrating\nsemantic information between images and text through contrastive learning\ntechniques. It has achieved remarkable performance in various multimodal tasks.\nHowever, the deployment of large CLIP models is hindered in resource-limited\nenvironments, while smaller models frequently fail to meet the performance\nbenchmarks required for practical applications. In this paper, we propose a\nnovel approach, ComKD-CLIP: Comprehensive Knowledge Distillation for\nContrastive Language-Image Pre-traning Model, which aims to comprehensively\ndistill the knowledge from a large teacher CLIP model into a smaller student\nmodel, ensuring comparable performance with significantly reduced parameters.\nComKD-CLIP is composed of two key mechanisms: Image Feature Alignment (IFAlign)\nand Educational Attention (EduAttention). IFAlign makes the image features\nextracted by the student model closely match those extracted by the teacher\nmodel, enabling the student to learn teacher's knowledge of extracting image\nfeatures. EduAttention explores the cross-relationships between text features\nextracted by the teacher model and image features extracted by the student\nmodel, enabling the student model to learn how the teacher model integrates\ntext-image features. In addition, ComKD-CLIP can refine the knowledge distilled\nfrom IFAlign and EduAttention by leveraging the text-image feature fusion\nresults of the teacher model, ensuring the student model accurately absorbs the\nteacher's knowledge. Extensive experiments conducted on 11 datasets have\ndemonstrated the superiority of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Image Pre-training (CLIP) model excels in integrating\nsemantic information between images and text through contrastive learning\ntechniques. It has achieved remarkable performance in various multimodal tasks.\nHowever, the deployment of large CLIP models is hindered in resource-limited\nenvironments, while smaller models frequently fail to meet the performance\nbenchmarks required for practical applications. In this paper, we propose a\nnovel approach, ComKD-CLIP: Comprehensive Knowledge Distillation for\nContrastive Language-Image Pre-traning Model, which aims to comprehensively\ndistill the knowledge from a large teacher CLIP model into a smaller student\nmodel, ensuring comparable performance with significantly reduced parameters.\nComKD-CLIP is composed of two key mechanisms: Image Feature Alignment (IFAlign)\nand Educational Attention (EduAttention). IFAlign makes the image features\nextracted by the student model closely match those extracted by the teacher\nmodel, enabling the student to learn teacher's knowledge of extracting image\nfeatures. EduAttention explores the cross-relationships between text features\nextracted by the teacher model and image features extracted by the student\nmodel, enabling the student model to learn how the teacher model integrates\ntext-image features. In addition, ComKD-CLIP can refine the knowledge distilled\nfrom IFAlign and EduAttention by leveraging the text-image feature fusion\nresults of the teacher model, ensuring the student model accurately absorbs the\nteacher's knowledge. Extensive experiments conducted on 11 datasets have\ndemonstrated the superiority of the proposed method."
                },
                "authors": [
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Xiaozhen Qiao"
                    },
                    {
                        "name": "Zhe Sun"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "arxiv_comment": "update",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07343v1",
                "updated": "2024-08-14T07:37:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    37,
                    7,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T07:37:07Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    37,
                    7,
                    2,
                    227,
                    0
                ],
                "title": "Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation"
                },
                "summary": "Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels will be available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels will be available."
                },
                "authors": [
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Yiwen Ye"
                    },
                    {
                        "name": "Yongsheng Pan"
                    },
                    {
                        "name": "Yong Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xia"
                },
                "author": "Yong Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00655v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00655v5",
                "updated": "2024-08-14T07:34:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    34,
                    44,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-01T15:45:19Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    15,
                    45,
                    19,
                    3,
                    214,
                    0
                ],
                "title": "SentenceVAE: Enable Next-sentence Prediction for Large Language Models\n  with Faster Speed, Higher Accuracy and Longer Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceVAE: Enable Next-sentence Prediction for Large Language Models\n  with Faster Speed, Higher Accuracy and Longer Context"
                },
                "summary": "Current large language models (LLMs) primarily utilize next-token prediction\nmethod for inference, which significantly impedes their processing speed. In\nthis paper, we introduce a novel inference methodology termed next-sentence\nprediction, aiming at enhancing the inference efficiency of LLMs. We present\nSentence Variational Autoencoder (SentenceVAE), which includes a Sentence\nEncoder to compress multiple tokens in a sentence into a single token, and a\nSentence Decoder to reconstruct it. By integrating SentenceVAE into the input\nand output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a\nsentence-by-sentence inference method. In addition, the SentenceVAE module of\nSLLMs can maintain the integrity of the original semantic content by segmenting\nthe context into sentences, thereby improving accuracy while boosting inference\nspeed. Moreover, compared to previous LLMs, SLLMs process fewer tokens over\nequivalent context length, significantly reducing memory demands for\nself-attention computation and facilitating the handling of longer context.\nExtensive experiments on Wanjuan dataset have revealed that the proposed method\ncan accelerate inference speed by 204~365%, reduce perplexity (PPL) to 46~75%\nof its original metric, and decrease memory overhead by 86~91% for the\nequivalent context length, compared to previous token-by-token methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language models (LLMs) primarily utilize next-token prediction\nmethod for inference, which significantly impedes their processing speed. In\nthis paper, we introduce a novel inference methodology termed next-sentence\nprediction, aiming at enhancing the inference efficiency of LLMs. We present\nSentence Variational Autoencoder (SentenceVAE), which includes a Sentence\nEncoder to compress multiple tokens in a sentence into a single token, and a\nSentence Decoder to reconstruct it. By integrating SentenceVAE into the input\nand output layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a\nsentence-by-sentence inference method. In addition, the SentenceVAE module of\nSLLMs can maintain the integrity of the original semantic content by segmenting\nthe context into sentences, thereby improving accuracy while boosting inference\nspeed. Moreover, compared to previous LLMs, SLLMs process fewer tokens over\nequivalent context length, significantly reducing memory demands for\nself-attention computation and facilitating the handling of longer context.\nExtensive experiments on Wanjuan dataset have revealed that the proposed method\ncan accelerate inference speed by 204~365%, reduce perplexity (PPL) to 46~75%\nof its original metric, and decrease memory overhead by 86~91% for the\nequivalent context length, compared to previous token-by-token methods."
                },
                "authors": [
                    {
                        "name": "Hongjun An"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Zhe Sun"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "arxiv_comment": "update the article",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00655v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00655v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16333v2",
                "updated": "2024-08-14T07:34:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    34,
                    18,
                    2,
                    227,
                    0
                ],
                "published": "2024-04-25T04:46:02Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    4,
                    46,
                    2,
                    3,
                    116,
                    0
                ],
                "title": "AI Coders Are Among Us: Rethinking Programming Language Grammar Towards\n  Efficient Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Coders Are Among Us: Rethinking Programming Language Grammar Towards\n  Efficient Code Generation"
                },
                "summary": "Artificial Intelligence (AI) models have emerged as another important\naudience for programming languages alongside humans and machines, as we enter\nthe era of large language models (LLMs). LLMs can now perform well in coding\ncompetitions and even write programs like developers to solve various tasks,\nincluding mathematical problems. However, the grammar and layout of current\nprograms are designed to cater the needs of human developers -- with many\ngrammar tokens and formatting tokens being used to make the code easier for\nhumans to read. While this is helpful, such a design adds unnecessary\ncomputational work for LLMs, as each token they either use or produce consumes\ncomputational resources. To improve inference efficiency and reduce\ncomputational costs, we propose the concept of AI-oriented grammar. This aims\nto represent code in a way that better suits the working mechanism of AI\nmodels. Code written with AI-oriented grammar discards formats and uses a\nminimum number of tokens to convey code semantics effectively. To demonstrate\nthe feasibility of this concept, we explore and implement the first AI-oriented\ngrammar for Python, named SimPy. SimPy is crafted by revising the original\nPython grammar through a series of heuristic rules. Programs written in SimPy\nmaintain identical AST structures to those in standard Python. This allows for\nnot only execution via a modified AST parser, but also seamless transformation\nbetween programs written in Python and SimPy, enabling human developers and\nLLMs to use Python and SimPy, respectively, when they need to collaborate. In\nthe experiments, compared with Python, SimPy enables a reduction in token usage\nby 13.5% and 10.4% for CodeLlama and GPT-4, respectively, when completing the\nsame set of code-related tasks. Additionally, these models can maintain or even\nimprove their performance when using SimPy instead of Python for these tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) models have emerged as another important\naudience for programming languages alongside humans and machines, as we enter\nthe era of large language models (LLMs). LLMs can now perform well in coding\ncompetitions and even write programs like developers to solve various tasks,\nincluding mathematical problems. However, the grammar and layout of current\nprograms are designed to cater the needs of human developers -- with many\ngrammar tokens and formatting tokens being used to make the code easier for\nhumans to read. While this is helpful, such a design adds unnecessary\ncomputational work for LLMs, as each token they either use or produce consumes\ncomputational resources. To improve inference efficiency and reduce\ncomputational costs, we propose the concept of AI-oriented grammar. This aims\nto represent code in a way that better suits the working mechanism of AI\nmodels. Code written with AI-oriented grammar discards formats and uses a\nminimum number of tokens to convey code semantics effectively. To demonstrate\nthe feasibility of this concept, we explore and implement the first AI-oriented\ngrammar for Python, named SimPy. SimPy is crafted by revising the original\nPython grammar through a series of heuristic rules. Programs written in SimPy\nmaintain identical AST structures to those in standard Python. This allows for\nnot only execution via a modified AST parser, but also seamless transformation\nbetween programs written in Python and SimPy, enabling human developers and\nLLMs to use Python and SimPy, respectively, when they need to collaborate. In\nthe experiments, compared with Python, SimPy enables a reduction in token usage\nby 13.5% and 10.4% for CodeLlama and GPT-4, respectively, when completing the\nsame set of code-related tasks. Additionally, these models can maintain or even\nimprove their performance when using SimPy instead of Python for these tasks."
                },
                "authors": [
                    {
                        "name": "Zhensu Sun"
                    },
                    {
                        "name": "Xiaoning Du"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "Accepted by ISSTA'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03337v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03337v3",
                "updated": "2024-08-15T06:58:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    6,
                    58,
                    8,
                    3,
                    228,
                    0
                ],
                "published": "2024-07-22T07:19:12Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    19,
                    12,
                    0,
                    204,
                    0
                ],
                "title": "PsyDI: Towards a Personalized and Progressively In-depth Chatbot for\n  Psychological Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsyDI: Towards a Personalized and Progressively In-depth Chatbot for\n  Psychological Measurements"
                },
                "summary": "In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of psychology, traditional assessment methods, such as\nstandardized scales, are frequently critiqued for their static nature, lack of\npersonalization, and reduced participant engagement, while comprehensive\ncounseling evaluations are often inaccessible. The complexity of quantifying\npsychological traits further limits these methods. Despite advances with large\nlanguage models (LLMs), many still depend on single-round Question-and-Answer\ninteractions. To bridge this gap, we introduce PsyDI, a personalized and\nprogressively in-depth chatbot designed for psychological measurements,\nexemplified by its application in the Myers-Briggs Type Indicator (MBTI)\nframework. PsyDI leverages user-related multi-modal information and engages in\ncustomized, multi-turn interactions to provide personalized, easily accessible\nmeasurements, while ensuring precise MBTI type determination. To address the\nchallenge of unquantifiable psychological traits, we introduce a novel training\nparadigm that involves learning the ranking of proxy variables associated with\nthese traits, culminating in a robust score model for MBTI measurements. The\nscore model enables PsyDI to conduct comprehensive and precise measurements\nthrough multi-turn interactions within a unified estimation context. Through\nvarious experiments, we validate the efficacy of both the score model and the\nPsyDI pipeline, demonstrating its potential to serve as a general framework for\npsychological measurements. Furthermore, the online deployment of PsyDI has\ngarnered substantial user engagement, with over 3,000 visits, resulting in the\ncollection of numerous multi-turn dialogues annotated with MBTI types, which\nfacilitates further research. The source code for the training and web service\ncomponents is publicly available as a part of OpenDILab at:\nhttps://github.com/opendilab/PsyDI"
                },
                "authors": [
                    {
                        "name": "Xueyan Li"
                    },
                    {
                        "name": "Xinyan Chen"
                    },
                    {
                        "name": "Yazhe Niu"
                    },
                    {
                        "name": "Shuai Hu"
                    },
                    {
                        "name": "Yu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Liu"
                },
                "author": "Yu Liu",
                "arxiv_comment": "29 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03337v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03337v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00816v3",
                "updated": "2024-08-14T07:26:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    26,
                    8,
                    2,
                    227,
                    0
                ],
                "published": "2024-02-26T01:17:50Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    1,
                    17,
                    50,
                    0,
                    57,
                    0
                ],
                "title": "Read and Think: An Efficient Step-wise Multimodal Language Model for\n  Document Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read and Think: An Efficient Step-wise Multimodal Language Model for\n  Document Understanding and Reasoning"
                },
                "summary": "Understanding the contents of multimodal documents is essential to accurately\nextract relevant evidence and use it for reasoning. Existing document\nunderstanding models tend to generate answers with a single word or phrase\ndirectly, ignoring the source document's evidence and lacking interpretability.\nIn this work, we address the lack of step-wise capabilities through data\naugmentation and extension. Specifically, We use Multi-modal Large Language\nModels (MLLMs), which have strong visual understanding and reasoning abilities,\nas data generators to generate step-wise question-and-answer pairs for document\nimages and use a high-performance LLM as the error detector to filter out noisy\ndata. This step-wise data generation pipeline is implemented using both\ntemplate-based and few-shot methods. We then use the generated high-quality\ndata to train a humanized document understanding and reasoning model,\nspecifically designed to solve complex questions that require reasoning or\nmulti-hop question answering, dubbed DocAssistant. Experimental results\ndemonstrate the effectiveness and application value of step-wise generation,\nshowing a 5 improvement on InfoVQA with complex layouts and a 7 improvement on\nChartQA with complex reasoning, compared to directly generated answers. We hope\nour work highlights the potential of synthetic data and encourages further\nexploration of multi-modal document reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the contents of multimodal documents is essential to accurately\nextract relevant evidence and use it for reasoning. Existing document\nunderstanding models tend to generate answers with a single word or phrase\ndirectly, ignoring the source document's evidence and lacking interpretability.\nIn this work, we address the lack of step-wise capabilities through data\naugmentation and extension. Specifically, We use Multi-modal Large Language\nModels (MLLMs), which have strong visual understanding and reasoning abilities,\nas data generators to generate step-wise question-and-answer pairs for document\nimages and use a high-performance LLM as the error detector to filter out noisy\ndata. This step-wise data generation pipeline is implemented using both\ntemplate-based and few-shot methods. We then use the generated high-quality\ndata to train a humanized document understanding and reasoning model,\nspecifically designed to solve complex questions that require reasoning or\nmulti-hop question answering, dubbed DocAssistant. Experimental results\ndemonstrate the effectiveness and application value of step-wise generation,\nshowing a 5 improvement on InfoVQA with complex layouts and a 7 improvement on\nChartQA with complex reasoning, compared to directly generated answers. We hope\nour work highlights the potential of synthetic data and encourages further\nexploration of multi-modal document reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Jinxu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jinxu Zhang"
                },
                "author": "Jinxu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07327v1",
                "updated": "2024-08-14T06:57:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    57,
                    58,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T06:57:58Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    57,
                    58,
                    2,
                    227,
                    0
                ],
                "title": "An Offline Meta Black-box Optimization Framework for Adaptive Design of\n  Urban Traffic Light Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Offline Meta Black-box Optimization Framework for Adaptive Design of\n  Urban Traffic Light Management Systems"
                },
                "summary": "Complex urban road networks with high vehicle occupancy frequently face\nsevere traffic congestion. Designing an effective strategy for managing\nmultiple traffic lights plays a crucial role in managing congestion. However,\nmost current traffic light management systems rely on human-crafted decisions,\nwhich may not adapt well to diverse traffic patterns. In this paper, we delve\ninto two pivotal design components of the traffic light management system that\ncan be dynamically adjusted to various traffic conditions: phase combination\nand phase time allocation. While numerous studies have sought an efficient\nstrategy for managing traffic lights, most of these approaches consider a fixed\ntraffic pattern and are limited to relatively small road networks. To overcome\nthese limitations, we introduce a novel and practical framework to formulate\nthe optimization of such design components using an offline meta black-box\noptimization. We then present a simple yet effective method to efficiently find\na solution for the aforementioned problem. In our framework, we first collect\nan offline meta dataset consisting of pairs of design choices and corresponding\ncongestion measures from various traffic patterns. After collecting the\ndataset, we employ the Attentive Neural Process (ANP) to predict the impact of\nthe proposed design on congestion across various traffic patterns with\nwell-calibrated uncertainty. Finally, Bayesian optimization, with ANP as a\nsurrogate model, is utilized to find an optimal design for unseen traffic\npatterns through limited online simulations. Our experiment results show that\nour method outperforms state-of-the-art baselines on complex road networks in\nterms of the number of waiting vehicles. Surprisingly, the deployment of our\nmethod into a real-world traffic system was able to improve traffic throughput\nby 4.80\\% compared to the original strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex urban road networks with high vehicle occupancy frequently face\nsevere traffic congestion. Designing an effective strategy for managing\nmultiple traffic lights plays a crucial role in managing congestion. However,\nmost current traffic light management systems rely on human-crafted decisions,\nwhich may not adapt well to diverse traffic patterns. In this paper, we delve\ninto two pivotal design components of the traffic light management system that\ncan be dynamically adjusted to various traffic conditions: phase combination\nand phase time allocation. While numerous studies have sought an efficient\nstrategy for managing traffic lights, most of these approaches consider a fixed\ntraffic pattern and are limited to relatively small road networks. To overcome\nthese limitations, we introduce a novel and practical framework to formulate\nthe optimization of such design components using an offline meta black-box\noptimization. We then present a simple yet effective method to efficiently find\na solution for the aforementioned problem. In our framework, we first collect\nan offline meta dataset consisting of pairs of design choices and corresponding\ncongestion measures from various traffic patterns. After collecting the\ndataset, we employ the Attentive Neural Process (ANP) to predict the impact of\nthe proposed design on congestion across various traffic patterns with\nwell-calibrated uncertainty. Finally, Bayesian optimization, with ANP as a\nsurrogate model, is utilized to find an optimal design for unseen traffic\npatterns through limited online simulations. Our experiment results show that\nour method outperforms state-of-the-art baselines on complex road networks in\nterms of the number of waiting vehicles. Surprisingly, the deployment of our\nmethod into a real-world traffic system was able to improve traffic throughput\nby 4.80\\% compared to the original strategy."
                },
                "authors": [
                    {
                        "name": "Taeyoung Yun"
                    },
                    {
                        "name": "Kanghoon Lee"
                    },
                    {
                        "name": "Sujin Yun"
                    },
                    {
                        "name": "Ilmyung Kim"
                    },
                    {
                        "name": "Won-Woo Jung"
                    },
                    {
                        "name": "Min-Cheol Kwon"
                    },
                    {
                        "name": "Kyujin Choi"
                    },
                    {
                        "name": "Yoohyeon Lee"
                    },
                    {
                        "name": "Jinkyoo Park"
                    }
                ],
                "author_detail": {
                    "name": "Jinkyoo Park"
                },
                "author": "Jinkyoo Park",
                "arxiv_comment": "12 pages, 7 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07326v1",
                "updated": "2024-08-14T06:56:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    56,
                    20,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T06:56:20Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    56,
                    20,
                    2,
                    227,
                    0
                ],
                "title": "LPU: A Latency-Optimized and Highly Scalable Processor for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LPU: A Latency-Optimized and Highly Scalable Processor for Large\n  Language Model Inference"
                },
                "summary": "The explosive arrival of OpenAI's ChatGPT has fueled the globalization of\nlarge language model (LLM), which consists of billions of pretrained parameters\nthat embodies the aspects of syntax and semantics. HyperAccel introduces\nlatency processing unit (LPU), a latency-optimized and highly scalable\nprocessor architecture for the acceleration of LLM inference. LPU perfectly\nbalances the memory bandwidth and compute logic with streamlined dataflow to\nmaximize performance and efficiency. LPU is equipped with expandable\nsynchronization link (ESL) that hides data synchronization latency between\nmultiple LPUs. HyperDex complements LPU as an intuitive software framework to\nrun LLM applications. LPU achieves 1.25 ms/token and 20.9 ms/token for 1.3B and\n66B model, respectively, which is 2.09x and 1.37x faster than the GPU. LPU,\nsynthesized using Samsung 4nm process, has total area of 0.824 mm2 and power\nconsumption of 284.31 mW. LPU-based servers achieve 1.33x and 1.32x energy\nefficiency over NVIDIA H100 and L4 servers, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The explosive arrival of OpenAI's ChatGPT has fueled the globalization of\nlarge language model (LLM), which consists of billions of pretrained parameters\nthat embodies the aspects of syntax and semantics. HyperAccel introduces\nlatency processing unit (LPU), a latency-optimized and highly scalable\nprocessor architecture for the acceleration of LLM inference. LPU perfectly\nbalances the memory bandwidth and compute logic with streamlined dataflow to\nmaximize performance and efficiency. LPU is equipped with expandable\nsynchronization link (ESL) that hides data synchronization latency between\nmultiple LPUs. HyperDex complements LPU as an intuitive software framework to\nrun LLM applications. LPU achieves 1.25 ms/token and 20.9 ms/token for 1.3B and\n66B model, respectively, which is 2.09x and 1.37x faster than the GPU. LPU,\nsynthesized using Samsung 4nm process, has total area of 0.824 mm2 and power\nconsumption of 284.31 mW. LPU-based servers achieve 1.33x and 1.32x energy\nefficiency over NVIDIA H100 and L4 servers, respectively."
                },
                "authors": [
                    {
                        "name": "Seungjae Moon"
                    },
                    {
                        "name": "Jung-Hoon Kim"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "Junseo Cha"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Sukbin Lim"
                    },
                    {
                        "name": "Gyubin Choi"
                    },
                    {
                        "name": "Dongjin Seo"
                    },
                    {
                        "name": "Jongho Kim"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Hyunjun Park"
                    },
                    {
                        "name": "Ryeowook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Jongse Park"
                    },
                    {
                        "name": "Jinwon Lee"
                    },
                    {
                        "name": "Joo-Young Kim"
                    }
                ],
                "author_detail": {
                    "name": "Joo-Young Kim"
                },
                "author": "Joo-Young Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07321v1",
                "updated": "2024-08-14T06:43:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    43,
                    6,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T06:43:06Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    43,
                    6,
                    2,
                    227,
                    0
                ],
                "title": "LLM-Enhanced Static Analysis for Precise Identification of Vulnerable\n  OSS Versions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Static Analysis for Precise Identification of Vulnerable\n  OSS Versions"
                },
                "summary": "Open-source software (OSS) has experienced a surge in popularity, attributed\nto its collaborative development model and cost-effective nature. However, the\nadoption of specific software versions in development projects may introduce\nsecurity risks when these versions bring along vulnerabilities. Current methods\nof identifying vulnerable versions typically analyze and trace the code\ninvolved in vulnerability patches using static analysis with pre-defined rules.\nThey then use syntactic-level code clone detection to identify the vulnerable\nversions. These methods are hindered by imprecisions due to (1) the inclusion\nof vulnerability-irrelevant code in the analysis and (2) the inadequacy of\nsyntactic-level code clone detection. This paper presents Vercation, an\napproach designed to identify vulnerable versions of OSS written in C/C++.\nVercation combines program slicing with a Large Language Model (LLM) to\nidentify vulnerability-relevant code from vulnerability patches. It then\nbacktraces historical commits to gather previous modifications of identified\nvulnerability-relevant code. We propose semantic-level code clone detection to\ncompare the differences between pre-modification and post-modification code,\nthereby locating the vulnerability-introducing commit (vic) and enabling to\nidentify the vulnerable versions between the patch commit and the vic. We\ncurate a dataset linking 74 OSS vulnerabilities and 1013 versions to evaluate\nVercation. On this dataset, our approach achieves the F1 score of 92.4%,\noutperforming current state-of-the-art methods. More importantly, Vercation\ndetected 134 incorrect vulnerable OSS versions in NVD reports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source software (OSS) has experienced a surge in popularity, attributed\nto its collaborative development model and cost-effective nature. However, the\nadoption of specific software versions in development projects may introduce\nsecurity risks when these versions bring along vulnerabilities. Current methods\nof identifying vulnerable versions typically analyze and trace the code\ninvolved in vulnerability patches using static analysis with pre-defined rules.\nThey then use syntactic-level code clone detection to identify the vulnerable\nversions. These methods are hindered by imprecisions due to (1) the inclusion\nof vulnerability-irrelevant code in the analysis and (2) the inadequacy of\nsyntactic-level code clone detection. This paper presents Vercation, an\napproach designed to identify vulnerable versions of OSS written in C/C++.\nVercation combines program slicing with a Large Language Model (LLM) to\nidentify vulnerability-relevant code from vulnerability patches. It then\nbacktraces historical commits to gather previous modifications of identified\nvulnerability-relevant code. We propose semantic-level code clone detection to\ncompare the differences between pre-modification and post-modification code,\nthereby locating the vulnerability-introducing commit (vic) and enabling to\nidentify the vulnerable versions between the patch commit and the vic. We\ncurate a dataset linking 74 OSS vulnerabilities and 1013 versions to evaluate\nVercation. On this dataset, our approach achieves the F1 score of 92.4%,\noutperforming current state-of-the-art methods. More importantly, Vercation\ndetected 134 incorrect vulnerable OSS versions in NVD reports."
                },
                "authors": [
                    {
                        "name": "Yiran Cheng"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Shouguo Yang"
                    },
                    {
                        "name": "Chaopeng Dong"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Shichao Lv"
                    },
                    {
                        "name": "Zhiqiang Shi"
                    },
                    {
                        "name": "Limin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Limin Sun"
                },
                "author": "Limin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06643v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06643v2",
                "updated": "2024-08-14T06:18:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    18,
                    3,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T05:27:22Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    5,
                    27,
                    22,
                    1,
                    226,
                    0
                ],
                "title": "BMX: Entropy-weighted Similarity and Semantic-enhanced Lexical Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BMX: Entropy-weighted Similarity and Semantic-enhanced Lexical Search"
                },
                "summary": "BM25, a widely-used lexical search algorithm, remains crucial in information\nretrieval despite the rise of pre-trained and large language models\n(PLMs/LLMs). However, it neglects query-document similarity and lacks semantic\nunderstanding, limiting its performance. We revisit BM25 and introduce BMX, a\nnovel extension of BM25 incorporating entropy-weighted similarity and semantic\nenhancement techniques. Extensive experiments demonstrate that BMX consistently\noutperforms traditional BM25 and surpasses PLM/LLM-based dense retrieval in\nlong-context and real-world retrieval benchmarks. This study bridges the gap\nbetween classical lexical search and modern semantic approaches, offering a\npromising direction for future information retrieval research. The reference\nimplementation of BMX can be found in Baguetter, which was created in the\ncontext of this work. The code can be found here:\nhttps://github.com/mixedbread-ai/baguetter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BM25, a widely-used lexical search algorithm, remains crucial in information\nretrieval despite the rise of pre-trained and large language models\n(PLMs/LLMs). However, it neglects query-document similarity and lacks semantic\nunderstanding, limiting its performance. We revisit BM25 and introduce BMX, a\nnovel extension of BM25 incorporating entropy-weighted similarity and semantic\nenhancement techniques. Extensive experiments demonstrate that BMX consistently\noutperforms traditional BM25 and surpasses PLM/LLM-based dense retrieval in\nlong-context and real-world retrieval benchmarks. This study bridges the gap\nbetween classical lexical search and modern semantic approaches, offering a\npromising direction for future information retrieval research. The reference\nimplementation of BMX can be found in Baguetter, which was created in the\ncontext of this work. The code can be found here:\nhttps://github.com/mixedbread-ai/baguetter."
                },
                "authors": [
                    {
                        "name": "Xianming Li"
                    },
                    {
                        "name": "Julius Lipp"
                    },
                    {
                        "name": "Aamir Shakir"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "arxiv_comment": "correct the affiliation order",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06643v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06643v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07313v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07313v1",
                "updated": "2024-08-14T06:14:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    14,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T06:14:02Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    6,
                    14,
                    2,
                    2,
                    227,
                    0
                ],
                "title": "Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal\n  Data for Mental Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal\n  Data for Mental Health"
                },
                "summary": "Integrating physiological signals such as electroencephalogram (EEG), with\nother data such as interview audio, may offer valuable multimodal insights into\npsychological states or neurological disorders. Recent advancements with Large\nLanguage Models (LLMs) position them as prospective ``health agents'' for\nmental health assessment. However, current research predominantly focus on\nsingle data modalities, presenting an opportunity to advance understanding\nthrough multimodal data. Our study aims to advance this approach by\ninvestigating multimodal data using LLMs for mental health assessment,\nspecifically through zero-shot and few-shot prompting. Three datasets are\nadopted for depression and emotion classifications incorporating EEG, facial\nexpressions, and audio (text). The results indicate that multimodal information\nconfers substantial advantages over single modality approaches in mental health\nassessment. Notably, integrating EEG alongside commonly used LLM modalities\nsuch as audio and images demonstrates promising potential. Moreover, our\nfindings reveal that 1-shot learning offers greater benefits compared to\nzero-shot learning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating physiological signals such as electroencephalogram (EEG), with\nother data such as interview audio, may offer valuable multimodal insights into\npsychological states or neurological disorders. Recent advancements with Large\nLanguage Models (LLMs) position them as prospective ``health agents'' for\nmental health assessment. However, current research predominantly focus on\nsingle data modalities, presenting an opportunity to advance understanding\nthrough multimodal data. Our study aims to advance this approach by\ninvestigating multimodal data using LLMs for mental health assessment,\nspecifically through zero-shot and few-shot prompting. Three datasets are\nadopted for depression and emotion classifications incorporating EEG, facial\nexpressions, and audio (text). The results indicate that multimodal information\nconfers substantial advantages over single modality approaches in mental health\nassessment. Notably, integrating EEG alongside commonly used LLM modalities\nsuch as audio and images demonstrates promising potential. Moreover, our\nfindings reveal that 1-shot learning offers greater benefits compared to\nzero-shot learning methods."
                },
                "authors": [
                    {
                        "name": "Yongquan Hu"
                    },
                    {
                        "name": "Shuning Zhang"
                    },
                    {
                        "name": "Ting Dang"
                    },
                    {
                        "name": "Hong Jia"
                    },
                    {
                        "name": "Flora D. Salim"
                    },
                    {
                        "name": "Wen Hu"
                    },
                    {
                        "name": "Aaron J. Quigley"
                    }
                ],
                "author_detail": {
                    "name": "Aaron J. Quigley"
                },
                "author": "Aaron J. Quigley",
                "arxiv_doi": "10.1145/3675094.3678494",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3675094.3678494",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.07313v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07313v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages; UbiComp Companion '24, Companion of the 2024 ACM\n  International Joint Conference on Pervasive and Ubiquitous Computing, October\n  5--9, 2024}{Melbourne, VIC, Australia",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06799v2",
                "updated": "2024-08-14T05:44:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    44,
                    37,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T10:42:32Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    10,
                    42,
                    32,
                    1,
                    226,
                    0
                ],
                "title": "On a Scale-Invariant Approach to Bundle Recommendations in Candy Crush\n  Saga",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On a Scale-Invariant Approach to Bundle Recommendations in Candy Crush\n  Saga"
                },
                "summary": "A good understanding of player preferences is crucial for increasing content\nrelevancy, especially in mobile games. This paper illustrates the use of\nattentive models for producing item recommendations in a mobile game scenario.\nThe methodology comprises a combination of supervised and unsupervised\napproaches to create user-level recommendations while introducing a novel\nscale-invariant approach to the prediction. The methodology is subsequently\napplied to a bundle recommendation in Candy Crush Saga. The strategy of\ndeployment, maintenance, and monitoring of ML models that are scaled up to\nserve millions of users is presented, along with the best practices and design\npatterns adopted to minimize technical debt typical of ML systems. The\nrecommendation approach is evaluated both offline and online, with a focus on\nunderstanding the increase in engagement, click- and take rates, novelty\neffects, recommendation diversity, and the impact of degenerate feedback loops.\nWe have demonstrated that the recommendation enhances user engagement by 30%\nconcerning click rate and by more than 40% concerning take rate. In addition,\nwe empirically quantify the diminishing effects of recommendation accuracy on\nuser engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A good understanding of player preferences is crucial for increasing content\nrelevancy, especially in mobile games. This paper illustrates the use of\nattentive models for producing item recommendations in a mobile game scenario.\nThe methodology comprises a combination of supervised and unsupervised\napproaches to create user-level recommendations while introducing a novel\nscale-invariant approach to the prediction. The methodology is subsequently\napplied to a bundle recommendation in Candy Crush Saga. The strategy of\ndeployment, maintenance, and monitoring of ML models that are scaled up to\nserve millions of users is presented, along with the best practices and design\npatterns adopted to minimize technical debt typical of ML systems. The\nrecommendation approach is evaluated both offline and online, with a focus on\nunderstanding the increase in engagement, click- and take rates, novelty\neffects, recommendation diversity, and the impact of degenerate feedback loops.\nWe have demonstrated that the recommendation enhances user engagement by 30%\nconcerning click rate and by more than 40% concerning take rate. In addition,\nwe empirically quantify the diminishing effects of recommendation accuracy on\nuser engagement."
                },
                "authors": [
                    {
                        "name": "Styliani Katsarou"
                    },
                    {
                        "name": "Francesca Carminati"
                    },
                    {
                        "name": "Martin Dlask"
                    },
                    {
                        "name": "Marta Braojos"
                    },
                    {
                        "name": "Lavena Patra"
                    },
                    {
                        "name": "Richard Perkins"
                    },
                    {
                        "name": "Carlos Garcia Ling"
                    },
                    {
                        "name": "Maria Paskevich"
                    }
                ],
                "author_detail": {
                    "name": "Maria Paskevich"
                },
                "author": "Maria Paskevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09021v2",
                "updated": "2024-08-14T04:52:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    4,
                    52,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-06-13T11:55:40Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    11,
                    55,
                    40,
                    3,
                    165,
                    0
                ],
                "title": "Contextual Distillation Model for Diversified Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Distillation Model for Diversified Recommendation"
                },
                "summary": "The diversity of recommendation is equally crucial as accuracy in improving\nuser experience. Existing studies, e.g., Determinantal Point Process (DPP) and\nMaximal Marginal Relevance (MMR), employ a greedy paradigm to iteratively\nselect items that optimize both accuracy and diversity. However, prior methods\ntypically exhibit quadratic complexity, limiting their applications to the\nre-ranking stage and are not applicable to other recommendation stages with a\nlarger pool of candidate items, such as the pre-ranking and ranking stages. In\nthis paper, we propose Contextual Distillation Model (CDM), an efficient\nrecommendation model that addresses diversification, suitable for the\ndeployment in all stages of industrial recommendation pipelines. Specifically,\nCDM utilizes the candidate items in the same user request as context to enhance\nthe diversification of the results. We propose a contrastive context encoder\nthat employs attention mechanisms to model both positive and negative contexts.\nFor the training of CDM, we compare each target item with its context embedding\nand utilize the knowledge distillation framework to learn the win probability\nof each target item under the MMR algorithm, where the teacher is derived from\nMMR outputs. During inference, ranking is performed through a linear\ncombination of the recommendation and student model scores, ensuring both\ndiversity and efficiency. We perform offline evaluations on two industrial\ndatasets and conduct online A/B test of CDM on the short-video platform\nKuaiShou. The considerable enhancements observed in both recommendation quality\nand diversity, as shown by metrics, provide strong superiority for the\neffectiveness of CDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diversity of recommendation is equally crucial as accuracy in improving\nuser experience. Existing studies, e.g., Determinantal Point Process (DPP) and\nMaximal Marginal Relevance (MMR), employ a greedy paradigm to iteratively\nselect items that optimize both accuracy and diversity. However, prior methods\ntypically exhibit quadratic complexity, limiting their applications to the\nre-ranking stage and are not applicable to other recommendation stages with a\nlarger pool of candidate items, such as the pre-ranking and ranking stages. In\nthis paper, we propose Contextual Distillation Model (CDM), an efficient\nrecommendation model that addresses diversification, suitable for the\ndeployment in all stages of industrial recommendation pipelines. Specifically,\nCDM utilizes the candidate items in the same user request as context to enhance\nthe diversification of the results. We propose a contrastive context encoder\nthat employs attention mechanisms to model both positive and negative contexts.\nFor the training of CDM, we compare each target item with its context embedding\nand utilize the knowledge distillation framework to learn the win probability\nof each target item under the MMR algorithm, where the teacher is derived from\nMMR outputs. During inference, ranking is performed through a linear\ncombination of the recommendation and student model scores, ensuring both\ndiversity and efficiency. We perform offline evaluations on two industrial\ndatasets and conduct online A/B test of CDM on the short-video platform\nKuaiShou. The considerable enhancements observed in both recommendation quality\nand diversity, as shown by metrics, provide strong superiority for the\neffectiveness of CDM."
                },
                "authors": [
                    {
                        "name": "Fan Li"
                    },
                    {
                        "name": "Xu Si"
                    },
                    {
                        "name": "Shisong Tang"
                    },
                    {
                        "name": "Dingmin Wang"
                    },
                    {
                        "name": "Kunyan Han"
                    },
                    {
                        "name": "Bing Han"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Hechang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hechang Chen"
                },
                "author": "Hechang Chen",
                "arxiv_doi": "10.1145/3637528.3671514",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3637528.3671514",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.09021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted by KDD 2024 v2",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07291v1",
                "updated": "2024-08-14T04:49:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    4,
                    49,
                    30,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T04:49:30Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    4,
                    49,
                    30,
                    2,
                    227,
                    0
                ],
                "title": "Evaluating Large Language Model based Personal Information Extraction\n  and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Model based Personal Information Extraction\n  and Countermeasures"
                },
                "summary": "Automatically extracting personal information--such as name, phone number,\nand email address--from publicly available profiles at a large scale is a\nstepstone to many other security attacks including spear phishing. Traditional\nmethods--such as regular expression, keyword search, and entity\ndetection--achieve limited success at such personal information extraction. In\nthis work, we perform a systematic measurement study to benchmark large\nlanguage model (LLM) based personal information extraction and countermeasures.\nTowards this goal, we present a framework for LLM-based extraction attacks;\ncollect three datasets including a synthetic dataset generated by GPT-4 and two\nreal-world datasets with manually labeled 8 categories of personal information;\nintroduce a novel mitigation strategy based on \\emph{prompt injection}; and\nsystematically benchmark LLM-based attacks and countermeasures using 10 LLMs\nand our 3 datasets. Our key findings include: LLM can be misused by attackers\nto accurately extract various personal information from personal profiles; LLM\noutperforms conventional methods at such extraction; and prompt injection can\nmitigate such risk to a large extent and outperforms conventional\ncountermeasures. Our code and data are available at:\n\\url{https://github.com/liu00222/LLM-Based-Personal-Profile-Extraction}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically extracting personal information--such as name, phone number,\nand email address--from publicly available profiles at a large scale is a\nstepstone to many other security attacks including spear phishing. Traditional\nmethods--such as regular expression, keyword search, and entity\ndetection--achieve limited success at such personal information extraction. In\nthis work, we perform a systematic measurement study to benchmark large\nlanguage model (LLM) based personal information extraction and countermeasures.\nTowards this goal, we present a framework for LLM-based extraction attacks;\ncollect three datasets including a synthetic dataset generated by GPT-4 and two\nreal-world datasets with manually labeled 8 categories of personal information;\nintroduce a novel mitigation strategy based on \\emph{prompt injection}; and\nsystematically benchmark LLM-based attacks and countermeasures using 10 LLMs\nand our 3 datasets. Our key findings include: LLM can be misused by attackers\nto accurately extract various personal information from personal profiles; LLM\noutperforms conventional methods at such extraction; and prompt injection can\nmitigate such risk to a large extent and outperforms conventional\ncountermeasures. Our code and data are available at:\n\\url{https://github.com/liu00222/LLM-Based-Personal-Profile-Extraction}."
                },
                "authors": [
                    {
                        "name": "Yupei Liu"
                    },
                    {
                        "name": "Yuqi Jia"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Neil Zhenqiang Gong"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zhenqiang Gong"
                },
                "author": "Neil Zhenqiang Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07288v1",
                "updated": "2024-08-14T04:36:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    4,
                    36,
                    8,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T04:36:08Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    4,
                    36,
                    8,
                    2,
                    227,
                    0
                ],
                "title": "A Modeling Framework for Equitable Deployment of Energy Storage in\n  Disadvantaged Communities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Modeling Framework for Equitable Deployment of Energy Storage in\n  Disadvantaged Communities"
                },
                "summary": "This paper provides an analytical framework to incorporate the deployment of\nbehind-the-meter energy storage coupled with rooftop solar, and their\nassociated revenue streams, in the context of equitable energy policy\ninterventions. We propose an extension to the Justice40 optimization model by\nadding storage and incorporating more realistic solar compensation mechanisms,\nsuch as net-billing, which allows for temporal revenue differentiation and the\neconomic viability of behind-the-meter energy storage devices. The extended\nmodel includes household-level PV plus storage co-deployment alongside existing\ninterventions, such as weatherization, rooftop PV only, community solar, and\ncommunity wind. From a modeling perspective, we propose a novel approximation\nmethod to represent storage operations and revenue streams without expanding\nthe temporal dimension of model, thus maintaining its computational efficiency.\nThe proposed model is validated using a case study in Wayne County, Michigan,\ninvolving 3,651 energy insecure households.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an analytical framework to incorporate the deployment of\nbehind-the-meter energy storage coupled with rooftop solar, and their\nassociated revenue streams, in the context of equitable energy policy\ninterventions. We propose an extension to the Justice40 optimization model by\nadding storage and incorporating more realistic solar compensation mechanisms,\nsuch as net-billing, which allows for temporal revenue differentiation and the\neconomic viability of behind-the-meter energy storage devices. The extended\nmodel includes household-level PV plus storage co-deployment alongside existing\ninterventions, such as weatherization, rooftop PV only, community solar, and\ncommunity wind. From a modeling perspective, we propose a novel approximation\nmethod to represent storage operations and revenue streams without expanding\nthe temporal dimension of model, thus maintaining its computational efficiency.\nThe proposed model is validated using a case study in Wayne County, Michigan,\ninvolving 3,651 energy insecure households."
                },
                "authors": [
                    {
                        "name": "Miguel Heleno"
                    },
                    {
                        "name": "Paul Lesur"
                    },
                    {
                        "name": "Alexandre Moreira"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Moreira"
                },
                "author": "Alexandre Moreira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.13764v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.13764v3",
                "updated": "2024-08-15T10:03:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    10,
                    3,
                    37,
                    3,
                    228,
                    0
                ],
                "published": "2023-12-21T11:43:41Z",
                "published_parsed": [
                    2023,
                    12,
                    21,
                    11,
                    43,
                    41,
                    3,
                    355,
                    0
                ],
                "title": "A Semantic Space is Worth 256 Language Descriptions: Make Stronger\n  Segmentation Models with Descriptive Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Semantic Space is Worth 256 Language Descriptions: Make Stronger\n  Segmentation Models with Descriptive Properties"
                },
                "summary": "This paper introduces ProLab, a novel approach using property-level label\nspace for creating strong interpretable segmentation models. Instead of relying\nsolely on category-specific annotations, ProLab uses descriptive properties\ngrounded in common sense knowledge for supervising segmentation models. It is\nbased on two core designs. First, we employ Large Language Models (LLMs) and\ncarefully crafted prompts to generate descriptions of all involved categories\nthat carry meaningful common sense knowledge and follow a structured format.\nSecond, we introduce a description embedding model preserving semantic\ncorrelation across descriptions and then cluster them into a set of descriptive\nproperties (e.g., 256) using K-Means. These properties are based on\ninterpretable common sense knowledge consistent with theories of human\nrecognition. We empirically show that our approach makes segmentation models\nperform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal\nContext, Cityscapes, and BDD). Our method also shows better scalability with\nextended training steps than category-level supervision. Our interpretable\nsegmentation framework also emerges with the generalization ability to segment\nout-of-domain or unknown categories using only in-domain descriptive\nproperties. Code is available at https://github.com/lambert-x/ProLab.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ProLab, a novel approach using property-level label\nspace for creating strong interpretable segmentation models. Instead of relying\nsolely on category-specific annotations, ProLab uses descriptive properties\ngrounded in common sense knowledge for supervising segmentation models. It is\nbased on two core designs. First, we employ Large Language Models (LLMs) and\ncarefully crafted prompts to generate descriptions of all involved categories\nthat carry meaningful common sense knowledge and follow a structured format.\nSecond, we introduce a description embedding model preserving semantic\ncorrelation across descriptions and then cluster them into a set of descriptive\nproperties (e.g., 256) using K-Means. These properties are based on\ninterpretable common sense knowledge consistent with theories of human\nrecognition. We empirically show that our approach makes segmentation models\nperform stronger on five classic benchmarks (e.g., ADE20K, COCO-Stuff, Pascal\nContext, Cityscapes, and BDD). Our method also shows better scalability with\nextended training steps than category-level supervision. Our interpretable\nsegmentation framework also emerges with the generalization ability to segment\nout-of-domain or unknown categories using only in-domain descriptive\nproperties. Code is available at https://github.com/lambert-x/ProLab."
                },
                "authors": [
                    {
                        "name": "Junfei Xiao"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Wenxuan Li"
                    },
                    {
                        "name": "Shiyi Lan"
                    },
                    {
                        "name": "Jieru Mei"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Cihang Xie"
                    }
                ],
                "author_detail": {
                    "name": "Cihang Xie"
                },
                "author": "Cihang Xie",
                "arxiv_comment": "Accepted to ECCV 2024. Code is available at\n  https://github.com/lambert-x/ProLab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.13764v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.13764v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07272v1",
                "updated": "2024-08-14T03:42:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    3,
                    42,
                    53,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T03:42:53Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    3,
                    42,
                    53,
                    2,
                    227,
                    0
                ],
                "title": "NL2OR: Solve Complex Operations Research Problems Using Natural Language\n  Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL2OR: Solve Complex Operations Research Problems Using Natural Language\n  Inputs"
                },
                "summary": "Operations research (OR) uses mathematical models to enhance decision-making,\nbut developing these models requires expert knowledge and can be\ntime-consuming. Automated mathematical programming (AMP) has emerged to\nsimplify this process, but existing systems have limitations. This paper\nintroduces a novel methodology that uses recent advances in Large Language\nModel (LLM) to create and edit OR solutions from non-expert user queries\nexpressed using Natural Language. This reduces the need for domain expertise\nand the time to formulate a problem. The paper presents an end-to-end pipeline,\nnamed NL2OR, that generates solutions to OR problems from natural language\ninput, and shares experimental results on several important OR problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operations research (OR) uses mathematical models to enhance decision-making,\nbut developing these models requires expert knowledge and can be\ntime-consuming. Automated mathematical programming (AMP) has emerged to\nsimplify this process, but existing systems have limitations. This paper\nintroduces a novel methodology that uses recent advances in Large Language\nModel (LLM) to create and edit OR solutions from non-expert user queries\nexpressed using Natural Language. This reduces the need for domain expertise\nand the time to formulate a problem. The paper presents an end-to-end pipeline,\nnamed NL2OR, that generates solutions to OR problems from natural language\ninput, and shares experimental results on several important OR problems."
                },
                "authors": [
                    {
                        "name": "Junxuan Li"
                    },
                    {
                        "name": "Ryan Wickman"
                    },
                    {
                        "name": "Sahil Bhatnagar"
                    },
                    {
                        "name": "Raj Kumar Maity"
                    },
                    {
                        "name": "Arko Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Arko Mukherjee"
                },
                "author": "Arko Mukherjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06457v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06457v2",
                "updated": "2024-08-14T02:41:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    2,
                    41,
                    48,
                    2,
                    227,
                    0
                ],
                "published": "2024-02-09T15:02:56Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    15,
                    2,
                    56,
                    4,
                    40,
                    0
                ],
                "title": "V-STaR: Training Verifiers for Self-Taught Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-STaR: Training Verifiers for Self-Taught Reasoners"
                },
                "summary": "Common self-improvement approaches for large language models (LLMs), such as\nSTaR, iteratively fine-tune LLMs on self-generated solutions to improve their\nproblem-solving ability. However, these approaches discard the large amounts of\nincorrect solutions generated during this process, potentially neglecting\nvaluable information in such solutions. To address this shortcoming, we propose\nV-STaR that utilizes both the correct and incorrect solutions generated during\nthe self-improvement process to train a verifier using DPO that judges\ncorrectness of model-generated solutions. This verifier is used at inference\ntime to select one solution among many candidate solutions. Running V-STaR for\nmultiple iterations results in progressively better reasoners and verifiers,\ndelivering a 4% to 17% test accuracy improvement over existing self-improvement\nand verification approaches on common code generation and math reasoning\nbenchmarks with LLaMA2 models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Common self-improvement approaches for large language models (LLMs), such as\nSTaR, iteratively fine-tune LLMs on self-generated solutions to improve their\nproblem-solving ability. However, these approaches discard the large amounts of\nincorrect solutions generated during this process, potentially neglecting\nvaluable information in such solutions. To address this shortcoming, we propose\nV-STaR that utilizes both the correct and incorrect solutions generated during\nthe self-improvement process to train a verifier using DPO that judges\ncorrectness of model-generated solutions. This verifier is used at inference\ntime to select one solution among many candidate solutions. Running V-STaR for\nmultiple iterations results in progressively better reasoners and verifiers,\ndelivering a 4% to 17% test accuracy improvement over existing self-improvement\nand verification approaches on common code generation and math reasoning\nbenchmarks with LLaMA2 models."
                },
                "authors": [
                    {
                        "name": "Arian Hosseini"
                    },
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Nikolay Malkin"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Alessandro Sordoni"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Agarwal"
                },
                "author": "Rishabh Agarwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06457v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06457v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05577v2",
                "updated": "2024-08-14T02:20:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    2,
                    20,
                    50,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-10T15:01:19Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    15,
                    1,
                    19,
                    5,
                    223,
                    0
                ],
                "title": "Camera Perspective Transformation to Bird's Eye View via Spatial\n  Transformer Model for Road Intersection Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Camera Perspective Transformation to Bird's Eye View via Spatial\n  Transformer Model for Road Intersection Monitoring"
                },
                "summary": "Road intersection monitoring and control research often utilize bird's eye\nview (BEV) simulators. In real traffic settings, achieving a BEV akin to that\nin a simulator necessitates the deployment of drones or specific sensor\nmounting, which is neither feasible nor practical. Consequently, traffic\nintersection management remains confined to simulation environments given these\nconstraints. In this paper, we address the gap between simulated environments\nand real-world implementation by introducing a novel deep-learning model that\nconverts a single camera's perspective of a road intersection into a BEV. We\ncreated a simulation environment that closely resembles a real-world traffic\njunction. The proposed model transforms the vehicles into BEV images,\nfacilitating road intersection monitoring and control model processing.\nInspired by image transformation techniques, we propose a Spatial-Transformer\nDouble Decoder-UNet (SDD-UNet) model that aims to eliminate the transformed\nimage distortions. In addition, the model accurately estimates the vehicle's\npositions and enables the direct application of simulation-trained models in\nreal-world contexts. SDD-UNet model achieves an average dice similarity\ncoefficient (DSC) above 95% which is 40% better than the original UNet model.\nThe mean absolute error (MAE) is 0.102 and the centroid of the predicted mask\nis 0.14 meters displaced, on average, indicating high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Road intersection monitoring and control research often utilize bird's eye\nview (BEV) simulators. In real traffic settings, achieving a BEV akin to that\nin a simulator necessitates the deployment of drones or specific sensor\nmounting, which is neither feasible nor practical. Consequently, traffic\nintersection management remains confined to simulation environments given these\nconstraints. In this paper, we address the gap between simulated environments\nand real-world implementation by introducing a novel deep-learning model that\nconverts a single camera's perspective of a road intersection into a BEV. We\ncreated a simulation environment that closely resembles a real-world traffic\njunction. The proposed model transforms the vehicles into BEV images,\nfacilitating road intersection monitoring and control model processing.\nInspired by image transformation techniques, we propose a Spatial-Transformer\nDouble Decoder-UNet (SDD-UNet) model that aims to eliminate the transformed\nimage distortions. In addition, the model accurately estimates the vehicle's\npositions and enables the direct application of simulation-trained models in\nreal-world contexts. SDD-UNet model achieves an average dice similarity\ncoefficient (DSC) above 95% which is 40% better than the original UNet model.\nThe mean absolute error (MAE) is 0.102 and the centroid of the predicted mask\nis 0.14 meters displaced, on average, indicating high accuracy."
                },
                "authors": [
                    {
                        "name": "Rukesh Prajapati"
                    },
                    {
                        "name": "Amr S. El-Wakeel"
                    }
                ],
                "author_detail": {
                    "name": "Amr S. El-Wakeel"
                },
                "author": "Amr S. El-Wakeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17575v3",
                "updated": "2024-08-14T01:55:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    55,
                    14,
                    2,
                    227,
                    0
                ],
                "published": "2024-01-31T03:39:33Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    3,
                    39,
                    33,
                    2,
                    31,
                    0
                ],
                "title": "Can We Improve Channel Reciprocity via Loop-back Compensation for\n  RIS-assisted Physical Layer Key Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Improve Channel Reciprocity via Loop-back Compensation for\n  RIS-assisted Physical Layer Key Generation"
                },
                "summary": "Reconfigurable intelligent surface (RIS) facilitates the extraction of\nunpredictable channel features for physical layer key generation (PKG),\nsecuring communications among legitimate users with symmetric keys. Previous\nworks have demonstrated that channel reciprocity plays a crucial role in\ngenerating symmetric keys in PKG systems, whereas, in reality, reciprocity is\ngreatly affected by hardware interference and RIS-based jamming attacks. This\nmotivates us to propose LoCKey, a novel approach that aims to improve channel\nreciprocity by mitigating interferences and attacks with a loop-back\ncompensation scheme, thus maximizing the secrecy performance of the PKG system.\nSpecifically, our proposed LoCKey is capable of effectively compensating for\nthe CSI non-reciprocity by the combination of transmit-back signal value and\nerror minimization module. Firstly, we introduce the entire flowchart of our\nmethod and provide an in-depth discussion of each step. Following that, we\ndelve into a theoretical analysis of the performance optimizations when our\nLoCKey is applied for CSI reciprocity enhancement. Finally, we conduct\nexperiments to verify the effectiveness of the proposed LoCKey in improving\nchannel reciprocity under various interferences for RIS-assisted wireless\ncommunications. The results demonstrate a significant improvement in both the\nrate of key generation assisted by the RIS and the consistency of the generated\nkeys, showing great potential for the practical deployment of our LoCKey in\nfuture wireless systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) facilitates the extraction of\nunpredictable channel features for physical layer key generation (PKG),\nsecuring communications among legitimate users with symmetric keys. Previous\nworks have demonstrated that channel reciprocity plays a crucial role in\ngenerating symmetric keys in PKG systems, whereas, in reality, reciprocity is\ngreatly affected by hardware interference and RIS-based jamming attacks. This\nmotivates us to propose LoCKey, a novel approach that aims to improve channel\nreciprocity by mitigating interferences and attacks with a loop-back\ncompensation scheme, thus maximizing the secrecy performance of the PKG system.\nSpecifically, our proposed LoCKey is capable of effectively compensating for\nthe CSI non-reciprocity by the combination of transmit-back signal value and\nerror minimization module. Firstly, we introduce the entire flowchart of our\nmethod and provide an in-depth discussion of each step. Following that, we\ndelve into a theoretical analysis of the performance optimizations when our\nLoCKey is applied for CSI reciprocity enhancement. Finally, we conduct\nexperiments to verify the effectiveness of the proposed LoCKey in improving\nchannel reciprocity under various interferences for RIS-assisted wireless\ncommunications. The results demonstrate a significant improvement in both the\nrate of key generation assisted by the RIS and the consistency of the generated\nkeys, showing great potential for the practical deployment of our LoCKey in\nfuture wireless systems."
                },
                "authors": [
                    {
                        "name": "Ningya Xu"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Xiaofeng Tao"
                    },
                    {
                        "name": "Na Li"
                    },
                    {
                        "name": "Pengxuan Mao"
                    },
                    {
                        "name": "Tianyuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyuan Yang"
                },
                "author": "Tianyuan Yang",
                "arxiv_comment": "Accepted by ICC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06578v2",
                "updated": "2024-08-14T01:37:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    37,
                    39,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-13T02:35:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    2,
                    35,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "OpenEP: Open-Ended Future Event Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenEP: Open-Ended Future Event Prediction"
                },
                "summary": "Future event prediction (FEP) is a long-standing and crucial task in the\nworld, as understanding the evolution of events enables early risk\nidentification, informed decision-making, and strategic planning. Existing work\ntypically treats event prediction as classification tasks and confines the\noutcomes of future events to a fixed scope, such as yes/no questions, candidate\nset, and taxonomy, which is difficult to include all possible outcomes of\nfuture events. In this paper, we introduce OpenEP (an Open-Ended Future Event\nPrediction task), which generates flexible and diverse predictions aligned with\nreal-world scenarios. This is mainly reflected in two aspects: firstly, the\npredictive questions are diverse, covering different stages of event\ndevelopment and perspectives; secondly, the outcomes are flexible, without\nconstraints on scope or format. To facilitate the study of this task, we\nconstruct OpenEPBench, an open-ended future event prediction dataset. For\nquestion construction, we pose questions from seven perspectives, including\nlocation, time, event development, event outcome, event impact, event response,\nand other, to facilitate an in-depth analysis and understanding of the\ncomprehensive evolution of events. For outcome construction, we collect\nfree-form text containing the outcomes as ground truth to provide semantically\ncomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, a\nstakeholder-enhanced future event prediction framework, that incorporates event\ncharacteristics for open-ended settings. Our method extracts stakeholders\ninvolved in events to extend questions to gather diverse information. We also\ncollect historically events that are relevant and similar to the question to\nreveal potential evolutionary patterns. Experiment results indicate that\naccurately predicting future events in open-ended settings is challenging for\nexisting LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future event prediction (FEP) is a long-standing and crucial task in the\nworld, as understanding the evolution of events enables early risk\nidentification, informed decision-making, and strategic planning. Existing work\ntypically treats event prediction as classification tasks and confines the\noutcomes of future events to a fixed scope, such as yes/no questions, candidate\nset, and taxonomy, which is difficult to include all possible outcomes of\nfuture events. In this paper, we introduce OpenEP (an Open-Ended Future Event\nPrediction task), which generates flexible and diverse predictions aligned with\nreal-world scenarios. This is mainly reflected in two aspects: firstly, the\npredictive questions are diverse, covering different stages of event\ndevelopment and perspectives; secondly, the outcomes are flexible, without\nconstraints on scope or format. To facilitate the study of this task, we\nconstruct OpenEPBench, an open-ended future event prediction dataset. For\nquestion construction, we pose questions from seven perspectives, including\nlocation, time, event development, event outcome, event impact, event response,\nand other, to facilitate an in-depth analysis and understanding of the\ncomprehensive evolution of events. For outcome construction, we collect\nfree-form text containing the outcomes as ground truth to provide semantically\ncomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, a\nstakeholder-enhanced future event prediction framework, that incorporates event\ncharacteristics for open-ended settings. Our method extracts stakeholders\ninvolved in events to extend questions to gather diverse information. We also\ncollect historically events that are relevant and similar to the question to\nreveal potential evolutionary patterns. Experiment results indicate that\naccurately predicting future events in open-ended settings is challenging for\nexisting LLMs."
                },
                "authors": [
                    {
                        "name": "Yong Guan"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07249v1",
                "updated": "2024-08-14T01:24:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    24,
                    9,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T01:24:09Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    24,
                    9,
                    2,
                    227,
                    0
                ],
                "title": "GQE: Generalized Query Expansion for Enhanced Text-Video Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GQE: Generalized Query Expansion for Enhanced Text-Video Retrieval"
                },
                "summary": "In the rapidly expanding domain of web video content, the task of text-video\nretrieval has become increasingly critical, bridging the semantic gap between\ntextual queries and video data. This paper introduces a novel data-centric\napproach, Generalized Query Expansion (GQE), to address the inherent\ninformation imbalance between text and video, enhancing the effectiveness of\ntext-video retrieval systems. Unlike traditional model-centric methods that\nfocus on designing intricate cross-modal interaction mechanisms, GQE aims to\nexpand the text queries associated with videos both during training and testing\nphases. By adaptively segmenting videos into short clips and employing\nzero-shot captioning, GQE enriches the training dataset with comprehensive\nscene descriptions, effectively bridging the data imbalance gap. Furthermore,\nduring retrieval, GQE utilizes Large Language Models (LLM) to generate a\ndiverse set of queries and a query selection module to filter these queries\nbased on relevance and diversity, thus optimizing retrieval performance while\nreducing computational overhead. Our contributions include a detailed\nexamination of the information imbalance challenge, a novel approach to query\nexpansion in video-text datasets, and the introduction of a query selection\nstrategy that enhances retrieval accuracy without increasing computational\ncosts. GQE achieves state-of-the-art performance on several benchmarks,\nincluding MSR-VTT, MSVD, LSMDC, and VATEX, demonstrating the effectiveness of\naddressing text-video retrieval from a data-centric perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly expanding domain of web video content, the task of text-video\nretrieval has become increasingly critical, bridging the semantic gap between\ntextual queries and video data. This paper introduces a novel data-centric\napproach, Generalized Query Expansion (GQE), to address the inherent\ninformation imbalance between text and video, enhancing the effectiveness of\ntext-video retrieval systems. Unlike traditional model-centric methods that\nfocus on designing intricate cross-modal interaction mechanisms, GQE aims to\nexpand the text queries associated with videos both during training and testing\nphases. By adaptively segmenting videos into short clips and employing\nzero-shot captioning, GQE enriches the training dataset with comprehensive\nscene descriptions, effectively bridging the data imbalance gap. Furthermore,\nduring retrieval, GQE utilizes Large Language Models (LLM) to generate a\ndiverse set of queries and a query selection module to filter these queries\nbased on relevance and diversity, thus optimizing retrieval performance while\nreducing computational overhead. Our contributions include a detailed\nexamination of the information imbalance challenge, a novel approach to query\nexpansion in video-text datasets, and the introduction of a query selection\nstrategy that enhances retrieval accuracy without increasing computational\ncosts. GQE achieves state-of-the-art performance on several benchmarks,\nincluding MSR-VTT, MSVD, LSMDC, and VATEX, demonstrating the effectiveness of\naddressing text-video retrieval from a data-centric perspective."
                },
                "authors": [
                    {
                        "name": "Zechen Bai"
                    },
                    {
                        "name": "Tianjun Xiao"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Thomas Brox"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "18 pages including appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07246v1",
                "updated": "2024-08-14T01:16:40Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    16,
                    40,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T01:16:40Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    1,
                    16,
                    40,
                    2,
                    227,
                    0
                ],
                "title": "Seeing and Understanding: Bridging Vision with Chemical Knowledge Via\n  ChemVLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seeing and Understanding: Bridging Vision with Chemical Knowledge Via\n  ChemVLM"
                },
                "summary": "In this technical report, we propose ChemVLM, the first open-source\nmultimodal large language model dedicated to the fields of chemistry, designed\nto address the incompatibility between chemical image understanding and text\nanalysis. Built upon the VIT-MLP-LLM architecture, we leverage ChemLLM-20B as\nthe foundational large model, endowing our model with robust capabilities in\nunderstanding and utilizing chemical text knowledge. Additionally, we employ\nInternVIT-6B as a powerful image encoder. We have curated high-quality data\nfrom the chemical domain, including molecules, reaction formulas, and chemistry\nexamination data, and compiled these into a bilingual multimodal\nquestion-answering dataset. We test the performance of our model on multiple\nopen-source benchmarks and three custom evaluation sets. Experimental results\ndemonstrate that our model achieves excellent performance, securing\nstate-of-the-art results in five out of six involved tasks. Our model can be\nfound at https://huggingface.co/AI4Chem/ChemVLM-26B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this technical report, we propose ChemVLM, the first open-source\nmultimodal large language model dedicated to the fields of chemistry, designed\nto address the incompatibility between chemical image understanding and text\nanalysis. Built upon the VIT-MLP-LLM architecture, we leverage ChemLLM-20B as\nthe foundational large model, endowing our model with robust capabilities in\nunderstanding and utilizing chemical text knowledge. Additionally, we employ\nInternVIT-6B as a powerful image encoder. We have curated high-quality data\nfrom the chemical domain, including molecules, reaction formulas, and chemistry\nexamination data, and compiled these into a bilingual multimodal\nquestion-answering dataset. We test the performance of our model on multiple\nopen-source benchmarks and three custom evaluation sets. Experimental results\ndemonstrate that our model achieves excellent performance, securing\nstate-of-the-art results in five out of six involved tasks. Our model can be\nfound at https://huggingface.co/AI4Chem/ChemVLM-26B."
                },
                "authors": [
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Xunzhi Wang"
                    },
                    {
                        "name": "Zeying Hao"
                    },
                    {
                        "name": "Jingdi Lei"
                    },
                    {
                        "name": "Qian Tan"
                    },
                    {
                        "name": "Cai Zhou"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Weiyun Wang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "arxiv_comment": "Techical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14268v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14268v3",
                "updated": "2024-08-14T00:48:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    0,
                    48,
                    43,
                    2,
                    227,
                    0
                ],
                "published": "2024-01-25T16:02:56Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    16,
                    2,
                    56,
                    3,
                    25,
                    0
                ],
                "title": "GPTVoiceTasker: Advancing Multi-step Mobile Task Efficiency Through\n  Dynamic Interface Exploration and Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPTVoiceTasker: Advancing Multi-step Mobile Task Efficiency Through\n  Dynamic Interface Exploration and Learning"
                },
                "summary": "Virtual assistants have the potential to play an important role in helping\nusers achieves different tasks. However, these systems face challenges in their\nreal-world usability, characterized by inefficiency and struggles in grasping\nuser intentions. Leveraging recent advances in Large Language Models (LLMs), we\nintroduce GptVoiceTasker, a virtual assistant poised to enhance user\nexperiences and task efficiency on mobile devices. GptVoiceTasker excels at\nintelligently deciphering user commands and executing relevant device\ninteractions to streamline task completion. The system continually learns from\nhistorical user commands to automate subsequent usages, further enhancing\nexecution efficiency. Our experiments affirm GptVoiceTasker's exceptional\ncommand interpretation abilities and the precision of its task automation\nmodule. In our user study, GptVoiceTasker boosted task efficiency in real-world\nscenarios by 34.85%, accompanied by positive participant feedback. We made\nGptVoiceTasker open-source, inviting further research into LLMs utilization for\ndiverse tasks through prompt engineering and leveraging user usage data to\nimprove efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual assistants have the potential to play an important role in helping\nusers achieves different tasks. However, these systems face challenges in their\nreal-world usability, characterized by inefficiency and struggles in grasping\nuser intentions. Leveraging recent advances in Large Language Models (LLMs), we\nintroduce GptVoiceTasker, a virtual assistant poised to enhance user\nexperiences and task efficiency on mobile devices. GptVoiceTasker excels at\nintelligently deciphering user commands and executing relevant device\ninteractions to streamline task completion. The system continually learns from\nhistorical user commands to automate subsequent usages, further enhancing\nexecution efficiency. Our experiments affirm GptVoiceTasker's exceptional\ncommand interpretation abilities and the precision of its task automation\nmodule. In our user study, GptVoiceTasker boosted task efficiency in real-world\nscenarios by 34.85%, accompanied by positive participant feedback. We made\nGptVoiceTasker open-source, inviting further research into LLMs utilization for\ndiverse tasks through prompt engineering and leveraging user usage data to\nimprove efficiency."
                },
                "authors": [
                    {
                        "name": "Minh Duc Vu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Zhuang Li"
                    },
                    {
                        "name": "Jieshan Chen"
                    },
                    {
                        "name": "Shengdong Zhao"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Chunyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chunyang Chen"
                },
                "author": "Chunyang Chen",
                "arxiv_doi": "10.1145/3654777.3676356",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3654777.3676356",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14268v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14268v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted by UIST 2024",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07238v1",
                "updated": "2024-08-13T23:59:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    23,
                    59,
                    36,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T23:59:36Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    23,
                    59,
                    36,
                    1,
                    226,
                    0
                ],
                "title": "Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge\n  Distillation Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge\n  Distillation Approach"
                },
                "summary": "Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior\nperformance in complex human-like interactions. But they are costly, or too\nlarge for edge devices such as smartphones and harder to self-host, leading to\nsecurity and privacy concerns. This paper introduces a novel interpretable\nknowledge distillation approach to enhance the performance of smaller, more\neconomical LLMs that firms can self-host. We study this problem in the context\nof building a customer service agent aimed at achieving high customer\nsatisfaction through goal-oriented dialogues. Unlike traditional knowledge\ndistillation, where the \"student\" model learns directly from the \"teacher\"\nmodel's responses via fine-tuning, our interpretable \"strategy\" teaching\napproach involves the teacher providing strategies to improve the student's\nperformance in various scenarios. This method alternates between a \"scenario\ngeneration\" step and a \"strategies for improvement\" step, creating a customized\nlibrary of scenarios and optimized strategies for automated prompting. The\nmethod requires only black-box access to both student and teacher models; hence\nit can be used without manipulating model parameters. In our customer service\napplication, the method improves performance, and the learned strategies are\ntransferable to other LLMs and scenarios beyond the training set. The method's\ninterpretabilty helps safeguard against potential harms through human audit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior\nperformance in complex human-like interactions. But they are costly, or too\nlarge for edge devices such as smartphones and harder to self-host, leading to\nsecurity and privacy concerns. This paper introduces a novel interpretable\nknowledge distillation approach to enhance the performance of smaller, more\neconomical LLMs that firms can self-host. We study this problem in the context\nof building a customer service agent aimed at achieving high customer\nsatisfaction through goal-oriented dialogues. Unlike traditional knowledge\ndistillation, where the \"student\" model learns directly from the \"teacher\"\nmodel's responses via fine-tuning, our interpretable \"strategy\" teaching\napproach involves the teacher providing strategies to improve the student's\nperformance in various scenarios. This method alternates between a \"scenario\ngeneration\" step and a \"strategies for improvement\" step, creating a customized\nlibrary of scenarios and optimized strategies for automated prompting. The\nmethod requires only black-box access to both student and teacher models; hence\nit can be used without manipulating model parameters. In our customer service\napplication, the method improves performance, and the learned strategies are\ntransferable to other LLMs and scenarios beyond the training set. The method's\ninterpretabilty helps safeguard against potential harms through human audit."
                },
                "authors": [
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "K. Sudhir"
                    },
                    {
                        "name": "Dat Hong"
                    }
                ],
                "author_detail": {
                    "name": "Dat Hong"
                },
                "author": "Dat Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07237v1",
                "updated": "2024-08-13T23:58:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    23,
                    58,
                    45,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T23:58:45Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    23,
                    58,
                    45,
                    1,
                    226,
                    0
                ],
                "title": "Neural embedding of beliefs reveals the role of relative dissonance in\n  human decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural embedding of beliefs reveals the role of relative dissonance in\n  human decision-making"
                },
                "summary": "Beliefs serve as the foundation for human cognition and decision-making. They\nguide individuals in deriving meaning from their lives, shaping their\nbehaviors, and forming social connections. Therefore, a model that encapsulates\nbeliefs and their interrelationships is crucial for quantitatively studying the\ninfluence of beliefs on our actions. Despite its importance, research on the\ninterplay between human beliefs has often been limited to a small set of\nbeliefs pertaining to specific issues, with a heavy reliance on surveys or\nexperiments. Here, we propose a method for extracting nuanced relations between\nthousands of beliefs by leveraging large-scale user participation data from an\nonline debate platform and mapping these beliefs to an embedding space using a\nfine-tuned large language model (LLM). This belief embedding space effectively\nencapsulates the interconnectedness of diverse beliefs as well as polarization\nacross various social issues. We discover that the positions within this belief\nspace predict new beliefs of individuals. Furthermore, we find that the\nrelative distance between one's existing beliefs and new beliefs can serve as a\nquantitative estimate of cognitive dissonance, allowing us to predict new\nbeliefs. Our study highlights how modern LLMs, when combined with collective\nonline records of human beliefs, can offer insights into the fundamental\nprinciples that govern human belief formation and decision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beliefs serve as the foundation for human cognition and decision-making. They\nguide individuals in deriving meaning from their lives, shaping their\nbehaviors, and forming social connections. Therefore, a model that encapsulates\nbeliefs and their interrelationships is crucial for quantitatively studying the\ninfluence of beliefs on our actions. Despite its importance, research on the\ninterplay between human beliefs has often been limited to a small set of\nbeliefs pertaining to specific issues, with a heavy reliance on surveys or\nexperiments. Here, we propose a method for extracting nuanced relations between\nthousands of beliefs by leveraging large-scale user participation data from an\nonline debate platform and mapping these beliefs to an embedding space using a\nfine-tuned large language model (LLM). This belief embedding space effectively\nencapsulates the interconnectedness of diverse beliefs as well as polarization\nacross various social issues. We discover that the positions within this belief\nspace predict new beliefs of individuals. Furthermore, we find that the\nrelative distance between one's existing beliefs and new beliefs can serve as a\nquantitative estimate of cognitive dissonance, allowing us to predict new\nbeliefs. Our study highlights how modern LLMs, when combined with collective\nonline records of human beliefs, can offer insights into the fundamental\nprinciples that govern human belief formation and decision-making processes."
                },
                "authors": [
                    {
                        "name": "Byunghwee Lee"
                    },
                    {
                        "name": "Rachith Aiyappa"
                    },
                    {
                        "name": "Yong-Yeol Ahn"
                    },
                    {
                        "name": "Haewoon Kwak"
                    },
                    {
                        "name": "Jisun An"
                    }
                ],
                "author_detail": {
                    "name": "Jisun An"
                },
                "author": "Jisun An",
                "arxiv_comment": "26 pages, 6 figures, SI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08850v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08850v3",
                "updated": "2024-08-13T23:41:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    23,
                    41,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-11T20:18:19Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    20,
                    18,
                    19,
                    3,
                    193,
                    0
                ],
                "title": "UICrit: Enhancing Automated Design Evaluation with a UICritique Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UICrit: Enhancing Automated Design Evaluation with a UICritique Dataset"
                },
                "summary": "Automated UI evaluation can be beneficial for the design process; for\nexample, to compare different UI designs, or conduct automated heuristic\nevaluation. LLM-based UI evaluation, in particular, holds the promise of\ngeneralizability to a wide variety of UI types and evaluation tasks. However,\ncurrent LLM-based techniques do not yet match the performance of human\nevaluators. We hypothesize that automatic evaluation can be improved by\ncollecting a targeted UI feedback dataset and then using this dataset to\nenhance the performance of general-purpose LLMs. We present a targeted dataset\nof 3,059 design critiques and quality ratings for 983 mobile UIs, collected\nfrom seven experienced designers. We carried out an in-depth analysis to\ncharacterize the dataset's features. We then applied this dataset to achieve a\n55% performance gain in LLM-generated UI feedback via various few-shot and\nvisual prompting techniques. We also discuss future applications of this\ndataset, including training a reward model for generative UI techniques, and\nfine-tuning a tool-agnostic multi-modal LLM that automates UI evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated UI evaluation can be beneficial for the design process; for\nexample, to compare different UI designs, or conduct automated heuristic\nevaluation. LLM-based UI evaluation, in particular, holds the promise of\ngeneralizability to a wide variety of UI types and evaluation tasks. However,\ncurrent LLM-based techniques do not yet match the performance of human\nevaluators. We hypothesize that automatic evaluation can be improved by\ncollecting a targeted UI feedback dataset and then using this dataset to\nenhance the performance of general-purpose LLMs. We present a targeted dataset\nof 3,059 design critiques and quality ratings for 983 mobile UIs, collected\nfrom seven experienced designers. We carried out an in-depth analysis to\ncharacterize the dataset's features. We then applied this dataset to achieve a\n55% performance gain in LLM-generated UI feedback via various few-shot and\nvisual prompting techniques. We also discuss future applications of this\ndataset, including training a reward model for generative UI techniques, and\nfine-tuning a tool-agnostic multi-modal LLM that automates UI evaluation."
                },
                "authors": [
                    {
                        "name": "Peitong Duan"
                    },
                    {
                        "name": "Chin-yi Chen"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Bjoern Hartmann"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "Accepted to ACM UIST 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08850v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08850v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03626v2",
                "updated": "2024-08-13T22:01:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    22,
                    1,
                    42,
                    1,
                    226,
                    0
                ],
                "published": "2024-04-04T17:48:28Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    17,
                    48,
                    28,
                    3,
                    95,
                    0
                ],
                "title": "Training LLMs over Neurally Compressed Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs over Neurally Compressed Text"
                },
                "summary": "In this paper, we explore the idea of training large language models (LLMs)\nover highly compressed text. While standard subword tokenizers compress text by\na small factor, neural text compressors can achieve much higher rates of\ncompression. If it were possible to train LLMs directly over neurally\ncompressed text, this would confer advantages in training and serving\nefficiency, as well as easier handling of long text spans. The main obstacle to\nthis goal is that strong compression tends to produce opaque outputs that are\nnot well-suited for learning. In particular, we find that text na\\\"ively\ncompressed via Arithmetic Coding is not readily learnable by LLMs. To overcome\nthis, we propose Equal-Info Windows, a novel compression technique whereby text\nis segmented into blocks that each compress to the same bit length. Using this\nmethod, we demonstrate effective learning over neurally compressed text that\nimproves with scale, and outperforms byte-level baselines by a wide margin on\nperplexity and inference speed benchmarks. While our method delivers worse\nperplexity than subword tokenizers for models trained with the same parameter\ncount, it has the benefit of shorter sequence lengths. Shorter sequence lengths\nrequire fewer autoregressive generation steps, and reduce latency. Finally, we\nprovide extensive analysis of the properties that contribute to learnability,\nand offer concrete suggestions for how to further improve the performance of\nhigh-compression tokenizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the idea of training large language models (LLMs)\nover highly compressed text. While standard subword tokenizers compress text by\na small factor, neural text compressors can achieve much higher rates of\ncompression. If it were possible to train LLMs directly over neurally\ncompressed text, this would confer advantages in training and serving\nefficiency, as well as easier handling of long text spans. The main obstacle to\nthis goal is that strong compression tends to produce opaque outputs that are\nnot well-suited for learning. In particular, we find that text na\\\"ively\ncompressed via Arithmetic Coding is not readily learnable by LLMs. To overcome\nthis, we propose Equal-Info Windows, a novel compression technique whereby text\nis segmented into blocks that each compress to the same bit length. Using this\nmethod, we demonstrate effective learning over neurally compressed text that\nimproves with scale, and outperforms byte-level baselines by a wide margin on\nperplexity and inference speed benchmarks. While our method delivers worse\nperplexity than subword tokenizers for models trained with the same parameter\ncount, it has the benefit of shorter sequence lengths. Shorter sequence lengths\nrequire fewer autoregressive generation steps, and reduce latency. Finally, we\nprovide extensive analysis of the properties that contribute to learnability,\nand offer concrete suggestions for how to further improve the performance of\nhigh-compression tokenizers."
                },
                "authors": [
                    {
                        "name": "Brian Lester"
                    },
                    {
                        "name": "Jaehoon Lee"
                    },
                    {
                        "name": "Alex Alemi"
                    },
                    {
                        "name": "Jeffrey Pennington"
                    },
                    {
                        "name": "Adam Roberts"
                    },
                    {
                        "name": "Jascha Sohl-Dickstein"
                    },
                    {
                        "name": "Noah Constant"
                    }
                ],
                "author_detail": {
                    "name": "Noah Constant"
                },
                "author": "Noah Constant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07215v1",
                "updated": "2024-08-13T21:54:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    21,
                    54,
                    10,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T21:54:10Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    21,
                    54,
                    10,
                    1,
                    226,
                    0
                ],
                "title": "Can Large Language Models Reason? A Characterization via 3-SAT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Reason? A Characterization via 3-SAT"
                },
                "summary": "Large Language Models (LLMs) are said to possess advanced reasoning\nabilities. However, some skepticism exists as recent works show how LLMs often\nbypass true reasoning using shortcuts. Current methods for assessing the\nreasoning abilities of LLMs typically rely on open-source benchmarks that may\nbe overrepresented in LLM training data, potentially skewing performance. We\ninstead provide a computational theory perspective of reasoning, using 3-SAT --\nthe prototypical NP-complete problem that lies at the core of logical reasoning\nand constraint satisfaction tasks. By examining the phase transitions in 3-SAT,\nwe empirically characterize the reasoning abilities of LLMs and show how they\nvary with the inherent hardness of the problems. Our experimental evidence\nshows that LLMs cannot perform true reasoning, as is required for solving 3-SAT\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are said to possess advanced reasoning\nabilities. However, some skepticism exists as recent works show how LLMs often\nbypass true reasoning using shortcuts. Current methods for assessing the\nreasoning abilities of LLMs typically rely on open-source benchmarks that may\nbe overrepresented in LLM training data, potentially skewing performance. We\ninstead provide a computational theory perspective of reasoning, using 3-SAT --\nthe prototypical NP-complete problem that lies at the core of logical reasoning\nand constraint satisfaction tasks. By examining the phase transitions in 3-SAT,\nwe empirically characterize the reasoning abilities of LLMs and show how they\nvary with the inherent hardness of the problems. Our experimental evidence\nshows that LLMs cannot perform true reasoning, as is required for solving 3-SAT\nproblems."
                },
                "authors": [
                    {
                        "name": "Rishi Hazra"
                    },
                    {
                        "name": "Gabriele Venturato"
                    },
                    {
                        "name": "Pedro Zuidberg Dos Martires"
                    },
                    {
                        "name": "Luc De Raedt"
                    }
                ],
                "author_detail": {
                    "name": "Luc De Raedt"
                },
                "author": "Luc De Raedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20192v2",
                "updated": "2024-08-13T21:40:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    21,
                    40,
                    7,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-29T17:19:40Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    17,
                    19,
                    40,
                    0,
                    211,
                    0
                ],
                "title": "Time series forecasting with high stakes: A field study of the air cargo\n  industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting with high stakes: A field study of the air cargo\n  industry"
                },
                "summary": "Time series forecasting in the air cargo industry presents unique challenges\ndue to volatile market dynamics and the significant impact of accurate\nforecasts on generated revenue. This paper explores a comprehensive approach to\ndemand forecasting at the origin-destination (O\\&D) level, focusing on the\ndevelopment and implementation of machine learning models in decision-making\nfor the air cargo industry. We leverage a mixture of experts framework,\ncombining statistical and advanced deep learning models to provide reliable\nforecasts for cargo demand over a six-month horizon. The results demonstrate\nthat our approach outperforms industry benchmarks, offering actionable insights\nfor cargo capacity allocation and strategic decision-making in the air cargo\nindustry. While this work is applied in the airline industry, the methodology\nis broadly applicable to any field where forecast-based decision-making in a\nvolatile environment is crucial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting in the air cargo industry presents unique challenges\ndue to volatile market dynamics and the significant impact of accurate\nforecasts on generated revenue. This paper explores a comprehensive approach to\ndemand forecasting at the origin-destination (O\\&D) level, focusing on the\ndevelopment and implementation of machine learning models in decision-making\nfor the air cargo industry. We leverage a mixture of experts framework,\ncombining statistical and advanced deep learning models to provide reliable\nforecasts for cargo demand over a six-month horizon. The results demonstrate\nthat our approach outperforms industry benchmarks, offering actionable insights\nfor cargo capacity allocation and strategic decision-making in the air cargo\nindustry. While this work is applied in the airline industry, the methodology\nis broadly applicable to any field where forecast-based decision-making in a\nvolatile environment is crucial."
                },
                "authors": [
                    {
                        "name": "Abhinav Garg"
                    },
                    {
                        "name": "Naman Shukla"
                    },
                    {
                        "name": "Maarten Wormer"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Wormer"
                },
                "author": "Maarten Wormer",
                "arxiv_comment": "The 10th Mining and Learning from Time Series Workshop: From\n  Classical Methods to LLMs. SIGKDD, Barcelona, Spain, 6 page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07199v1",
                "updated": "2024-08-13T20:52:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    20,
                    52,
                    13,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T20:52:13Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    20,
                    52,
                    13,
                    1,
                    226,
                    0
                ],
                "title": "Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage tasks requiring complex reasoning, yet their application in agentic,\nmulti-step reasoning within interactive environments remains a difficult\nchallenge. Traditional supervised pre-training on static datasets falls short\nin enabling autonomous agent capabilities needed to perform complex\ndecision-making in dynamic settings like web navigation. Previous attempts to\nbridge this ga-through supervised fine-tuning on curated expert\ndemonstrations-often suffer from compounding errors and limited exploration\ndata, resulting in sub-optimal policy outcomes. To overcome these challenges,\nwe propose a framework that combines guided Monte Carlo Tree Search (MCTS)\nsearch with a self-critique mechanism and iterative fine-tuning on agent\ninteractions using an off-policy variant of the Direct Preference Optimization\n(DPO) algorithm. Our method allows LLM agents to learn effectively from both\nsuccessful and unsuccessful trajectories, thereby improving their\ngeneralization in complex, multi-step reasoning tasks. We validate our approach\nin the WebShop environment-a simulated e-commerce platform where it\nconsistently outperforms behavior cloning and reinforced fine-tuning baseline,\nand beats average human performance when equipped with the capability to do\nonline search. In real-world booking scenarios, our methodology boosts Llama-3\n70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340%\nrelative increase) after a single day of data collection and further to 95.4%\nwith online search. We believe this represents a substantial leap forward in\nthe capabilities of autonomous agents, paving the way for more sophisticated\nand reliable decision-making in real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage tasks requiring complex reasoning, yet their application in agentic,\nmulti-step reasoning within interactive environments remains a difficult\nchallenge. Traditional supervised pre-training on static datasets falls short\nin enabling autonomous agent capabilities needed to perform complex\ndecision-making in dynamic settings like web navigation. Previous attempts to\nbridge this ga-through supervised fine-tuning on curated expert\ndemonstrations-often suffer from compounding errors and limited exploration\ndata, resulting in sub-optimal policy outcomes. To overcome these challenges,\nwe propose a framework that combines guided Monte Carlo Tree Search (MCTS)\nsearch with a self-critique mechanism and iterative fine-tuning on agent\ninteractions using an off-policy variant of the Direct Preference Optimization\n(DPO) algorithm. Our method allows LLM agents to learn effectively from both\nsuccessful and unsuccessful trajectories, thereby improving their\ngeneralization in complex, multi-step reasoning tasks. We validate our approach\nin the WebShop environment-a simulated e-commerce platform where it\nconsistently outperforms behavior cloning and reinforced fine-tuning baseline,\nand beats average human performance when equipped with the capability to do\nonline search. In real-world booking scenarios, our methodology boosts Llama-3\n70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340%\nrelative increase) after a single day of data collection and further to 95.4%\nwith online search. We believe this represents a substantial leap forward in\nthe capabilities of autonomous agents, paving the way for more sophisticated\nand reliable decision-making in real-world settings."
                },
                "authors": [
                    {
                        "name": "Pranav Putta"
                    },
                    {
                        "name": "Edmund Mills"
                    },
                    {
                        "name": "Naman Garg"
                    },
                    {
                        "name": "Sumeet Motwani"
                    },
                    {
                        "name": "Chelsea Finn"
                    },
                    {
                        "name": "Divyansh Garg"
                    },
                    {
                        "name": "Rafael Rafailov"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Rafailov"
                },
                "author": "Rafael Rafailov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05346v2",
                "updated": "2024-08-13T20:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    20,
                    46,
                    18,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-09T21:31:33Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    21,
                    31,
                    33,
                    4,
                    222,
                    0
                ],
                "title": "DataNarrative: Automated Data-Driven Storytelling with Visualizations\n  and Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataNarrative: Automated Data-Driven Storytelling with Visualizations\n  and Texts"
                },
                "summary": "Data-driven storytelling is a powerful method for conveying insights by\ncombining narrative techniques with visualizations and text. These stories\nintegrate visual aids, such as highlighted bars and lines in charts, along with\ntextual annotations explaining insights. However, creating such stories\nrequires a deep understanding of the data and meticulous narrative planning,\noften necessitating human intervention, which can be time-consuming and\nmentally taxing. While Large Language Models (LLMs) excel in various NLP tasks,\ntheir ability to generate coherent and comprehensive data stories remains\nunderexplored. In this work, we introduce a novel task for data story\ngeneration and a benchmark containing 1,449 stories from diverse sources. To\naddress the challenges of crafting coherent data stories, we propose a\nmultiagent framework employing two LLM agents designed to replicate the human\nstorytelling process: one for understanding and describing the data\n(Reflection), generating the outline, and narration, and another for\nverification at each intermediary step. While our agentic framework generally\noutperforms non-agentic counterparts in both model-based and human evaluations,\nthe results also reveal unique challenges in data story generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven storytelling is a powerful method for conveying insights by\ncombining narrative techniques with visualizations and text. These stories\nintegrate visual aids, such as highlighted bars and lines in charts, along with\ntextual annotations explaining insights. However, creating such stories\nrequires a deep understanding of the data and meticulous narrative planning,\noften necessitating human intervention, which can be time-consuming and\nmentally taxing. While Large Language Models (LLMs) excel in various NLP tasks,\ntheir ability to generate coherent and comprehensive data stories remains\nunderexplored. In this work, we introduce a novel task for data story\ngeneration and a benchmark containing 1,449 stories from diverse sources. To\naddress the challenges of crafting coherent data stories, we propose a\nmultiagent framework employing two LLM agents designed to replicate the human\nstorytelling process: one for understanding and describing the data\n(Reflection), generating the outline, and narration, and another for\nverification at each intermediary step. While our agentic framework generally\noutperforms non-agentic counterparts in both model-based and human evaluations,\nthe results also reveal unique challenges in data story generation."
                },
                "authors": [
                    {
                        "name": "Mohammed Saidul Islam"
                    },
                    {
                        "name": "Md Tahmid Rahman Laskar"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    },
                    {
                        "name": "Enamul Hoque"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05345v2",
                "updated": "2024-08-13T19:39:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    19,
                    39,
                    52,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-09T21:28:31Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    21,
                    28,
                    31,
                    4,
                    222,
                    0
                ],
                "title": "Explainable AI Reloaded: Challenging the XAI Status Quo in the Era of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable AI Reloaded: Challenging the XAI Status Quo in the Era of\n  Large Language Models"
                },
                "summary": "When the initial vision of Explainable (XAI) was articulated, the most\npopular framing was to open the (proverbial) \"black-box\" of AI so that we could\nunderstand the inner workings. With the advent of Large Language Models (LLMs),\nthe very ability to open the black-box is increasingly limited especially when\nit comes to non-AI expert end-users. In this paper, we challenge the assumption\nof \"opening\" the black-box in the LLM era and argue for a shift in our XAI\nexpectations. Highlighting the epistemic blind spots of an algorithm-centered\nXAI view, we argue that a human-centered perspective can be a path forward. We\noperationalize the argument by synthesizing XAI research along three\ndimensions: explainability outside the black-box, explainability around the\nedges of the black box, and explainability that leverages infrastructural\nseams. We conclude with takeaways that reflexively inform XAI as a domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When the initial vision of Explainable (XAI) was articulated, the most\npopular framing was to open the (proverbial) \"black-box\" of AI so that we could\nunderstand the inner workings. With the advent of Large Language Models (LLMs),\nthe very ability to open the black-box is increasingly limited especially when\nit comes to non-AI expert end-users. In this paper, we challenge the assumption\nof \"opening\" the black-box in the LLM era and argue for a shift in our XAI\nexpectations. Highlighting the epistemic blind spots of an algorithm-centered\nXAI view, we argue that a human-centered perspective can be a path forward. We\noperationalize the argument by synthesizing XAI research along three\ndimensions: explainability outside the black-box, explainability around the\nedges of the black box, and explainability that leverages infrastructural\nseams. We conclude with takeaways that reflexively inform XAI as a domain."
                },
                "authors": [
                    {
                        "name": "Upol Ehsan"
                    },
                    {
                        "name": "Mark O. Riedl"
                    }
                ],
                "author_detail": {
                    "name": "Mark O. Riedl"
                },
                "author": "Mark O. Riedl",
                "arxiv_doi": "10.1145/3686169.3686185",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3686169.3686185",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.05345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM HTTF 2024",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19631v2",
                "updated": "2024-08-13T19:34:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    19,
                    34,
                    13,
                    1,
                    226,
                    0
                ],
                "published": "2024-03-28T17:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    17,
                    47,
                    19,
                    3,
                    88,
                    0
                ],
                "title": "Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs) have shown proficiency in question-answering\ntasks but often struggle to integrate real-time knowledge, leading to\npotentially outdated or inaccurate responses. This problem becomes even more\nchallenging when dealing with multi-hop questions, since they require LLMs to\nupdate and integrate multiple knowledge pieces relevant to the questions. To\ntackle the problem, we propose the Retrieval-Augmented model Editing (RAE)\nframework for multi-hop question answering. RAE first retrieves edited facts\nand then refines the language model through in-context learning. Specifically,\nour retrieval approach, based on mutual information maximization, leverages the\nreasoning abilities of LLMs to identify chain facts that traditional\nsimilarity-based searches might miss. In addition, our framework includes a\npruning strategy to eliminate redundant information from the retrieved facts,\nwhich enhances the editing accuracy and mitigates the hallucination problem.\nOur framework is supported by theoretical justification for its fact retrieval\nefficacy. Finally, comprehensive evaluation across various LLMs validates RAE's\nability in providing accurate answers with updated knowledge. Our code is\navailable at: https://github.com/sycny/RAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown proficiency in question-answering\ntasks but often struggle to integrate real-time knowledge, leading to\npotentially outdated or inaccurate responses. This problem becomes even more\nchallenging when dealing with multi-hop questions, since they require LLMs to\nupdate and integrate multiple knowledge pieces relevant to the questions. To\ntackle the problem, we propose the Retrieval-Augmented model Editing (RAE)\nframework for multi-hop question answering. RAE first retrieves edited facts\nand then refines the language model through in-context learning. Specifically,\nour retrieval approach, based on mutual information maximization, leverages the\nreasoning abilities of LLMs to identify chain facts that traditional\nsimilarity-based searches might miss. In addition, our framework includes a\npruning strategy to eliminate redundant information from the retrieved facts,\nwhich enhances the editing accuracy and mitigates the hallucination problem.\nOur framework is supported by theoretical justification for its fact retrieval\nefficacy. Finally, comprehensive evaluation across various LLMs validates RAE's\nability in providing accurate answers with updated knowledge. Our code is\navailable at: https://github.com/sycny/RAE."
                },
                "authors": [
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Qiaoyu Tan"
                    },
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Kaixiong Zhou"
                    },
                    {
                        "name": "Ninghao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ninghao Liu"
                },
                "author": "Ninghao Liu",
                "arxiv_comment": "Accepted by CIKM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14962v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14962v4",
                "updated": "2024-08-13T19:17:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    19,
                    17,
                    32,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-20T18:48:35Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    18,
                    48,
                    35,
                    5,
                    202,
                    0
                ],
                "title": "Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives"
                },
                "summary": "The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community."
                },
                "authors": [
                    {
                        "name": "Desta Haileselassie Hagos"
                    },
                    {
                        "name": "Rick Battle"
                    },
                    {
                        "name": "Danda B. Rawat"
                    }
                ],
                "author_detail": {
                    "name": "Danda B. Rawat"
                },
                "author": "Danda B. Rawat",
                "arxiv_comment": "This version is accepted for publication in the Journal of IEEE\n  Transactions on Artificial Intelligence (TAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14962v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14962v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05241v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05241v2",
                "updated": "2024-08-13T19:09:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    19,
                    9,
                    57,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-05T20:49:48Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    20,
                    49,
                    48,
                    0,
                    218,
                    0
                ],
                "title": "Large Model Strategic Thinking, Small Model Efficiency: Transferring\n  Theory of Mind in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Model Strategic Thinking, Small Model Efficiency: Transferring\n  Theory of Mind in Large Language Models"
                },
                "summary": "As the performance of larger, newer Large Language Models continues to\nimprove for strategic Theory of Mind (ToM) tasks, the demand for these state of\nthe art models increases commensurately. However, their deployment is costly\nboth in terms of processing power and time. In this paper, we investigate the\nfeasibility of creating smaller, simulation-ready agents by way of fine-tuning.\nTo do this, we present a large pre-trained model with 20 unique scenarios that\ncombine a social context with a social dilemma, recording its answers, and\nusing them for Q\\&A fine-tuning on a smaller model of the same family. Our\nfocus is on in-context game-theoretic decision-making, the same domain within\nwhich human interaction occurs and that requires both a theory of mind (or a\nsemblance thereof) and an understanding of social dynamics. We find that the\nfine-tuned smaller language model exhibited significant performance closer to\nthat of its larger relative, and that their improvements extended in areas and\ncontexts beyond the ones provided in the training examples. On average for all\ngames, through fine-tuning, the smaller model showed a \\%46 improvement in\naligning with the behavior of the larger model, with \\%100 representing\ncomplete alignment. This suggests that our pipeline represents an efficient\nmethod to transmit some form of theory of mind to smaller models, creating\nimproved and cheaply deployable algorithms in the process. Despite their\nsimplicity and their associated shortcomings and limitations, our findings\nrepresent a stepping stone in the pursuit and training of specialized models\nfor strategic and social decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the performance of larger, newer Large Language Models continues to\nimprove for strategic Theory of Mind (ToM) tasks, the demand for these state of\nthe art models increases commensurately. However, their deployment is costly\nboth in terms of processing power and time. In this paper, we investigate the\nfeasibility of creating smaller, simulation-ready agents by way of fine-tuning.\nTo do this, we present a large pre-trained model with 20 unique scenarios that\ncombine a social context with a social dilemma, recording its answers, and\nusing them for Q\\&A fine-tuning on a smaller model of the same family. Our\nfocus is on in-context game-theoretic decision-making, the same domain within\nwhich human interaction occurs and that requires both a theory of mind (or a\nsemblance thereof) and an understanding of social dynamics. We find that the\nfine-tuned smaller language model exhibited significant performance closer to\nthat of its larger relative, and that their improvements extended in areas and\ncontexts beyond the ones provided in the training examples. On average for all\ngames, through fine-tuning, the smaller model showed a \\%46 improvement in\naligning with the behavior of the larger model, with \\%100 representing\ncomplete alignment. This suggests that our pipeline represents an efficient\nmethod to transmit some form of theory of mind to smaller models, creating\nimproved and cheaply deployable algorithms in the process. Despite their\nsimplicity and their associated shortcomings and limitations, our findings\nrepresent a stepping stone in the pursuit and training of specialized models\nfor strategic and social decision making."
                },
                "authors": [
                    {
                        "name": "Nunzio Lore"
                    },
                    {
                        "name": "Alireza Sepehr Ilami"
                    },
                    {
                        "name": "Babak Heydari"
                    }
                ],
                "author_detail": {
                    "name": "Babak Heydari"
                },
                "author": "Babak Heydari",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05241v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05241v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09198v2",
                "updated": "2024-08-13T19:04:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    19,
                    4,
                    18,
                    1,
                    226,
                    0
                ],
                "published": "2023-11-15T18:42:44Z",
                "published_parsed": [
                    2023,
                    11,
                    15,
                    18,
                    42,
                    44,
                    2,
                    319,
                    0
                ],
                "title": "Never Lost in the Middle: Mastering Long-Context Question Answering with\n  Position-Agnostic Decompositional Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Never Lost in the Middle: Mastering Long-Context Question Answering with\n  Position-Agnostic Decompositional Training"
                },
                "summary": "While large language models (LLMs) are equipped with longer text input\ncapabilities than before, they are struggling to seek correct information in\nlong contexts. The \"lost in the middle\" problem challenges most LLMs, referring\nto the dramatic decline in accuracy when correct information is located in the\nmiddle. To overcome this crucial issue, this paper proposes to enhance the\ninformation searching and reflection ability of LLMs in long contexts via\nspecially designed tasks called Attention Strengthening Multi-doc QA (ASM QA).\nFollowing these tasks, our model excels in focusing more precisely on the\ndesired information. Experimental results show substantial improvement in\nMulti-doc QA and other benchmarks, superior to state-of-the-art models by 13.7%\nabsolute gain in shuffled settings, by 21.5% in passage retrieval task. We\nrelease our model, Ziya-Reader to promote related research in the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are equipped with longer text input\ncapabilities than before, they are struggling to seek correct information in\nlong contexts. The \"lost in the middle\" problem challenges most LLMs, referring\nto the dramatic decline in accuracy when correct information is located in the\nmiddle. To overcome this crucial issue, this paper proposes to enhance the\ninformation searching and reflection ability of LLMs in long contexts via\nspecially designed tasks called Attention Strengthening Multi-doc QA (ASM QA).\nFollowing these tasks, our model excels in focusing more precisely on the\ndesired information. Experimental results show substantial improvement in\nMulti-doc QA and other benchmarks, superior to state-of-the-art models by 13.7%\nabsolute gain in shuffled settings, by 21.5% in passage retrieval task. We\nrelease our model, Ziya-Reader to promote related research in the community."
                },
                "authors": [
                    {
                        "name": "Junqing He"
                    },
                    {
                        "name": "Kunhao Pan"
                    },
                    {
                        "name": "Xiaoqun Dong"
                    },
                    {
                        "name": "Zhuoyang Song"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Qianguo Sun"
                    },
                    {
                        "name": "Yuxin Liang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Enming Zhang"
                    },
                    {
                        "name": "Jiaxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxing Zhang"
                },
                "author": "Jiaxing Zhang",
                "arxiv_comment": "Accepted by ACL 2024 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06304v2",
                "updated": "2024-08-13T18:56:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    18,
                    56,
                    20,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-12T17:17:16Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    17,
                    16,
                    0,
                    225,
                    0
                ],
                "title": "Control-Flow Attestation: Concepts, Solutions, and Open Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-Flow Attestation: Concepts, Solutions, and Open Challenges"
                },
                "summary": "Control-flow attestation unifies the worlds of control-flow integrity and\nplatform attestation by measuring and reporting a target's run-time behaviour\nto a verifier. Trust assurances in the target are provided by testing whether\nits execution follows an authorised control-flow path. The problem has been\nexplored in various settings, such as assessing the trustworthiness of\ncyber-physical systems, Internet of Things devices, cloud platforms, and many\nothers. Despite a significant number of proposals being made in recent years,\nthe area remains fragmented, addressing different adversarial behaviours,\nverification paradigms, and deployment challenges. In this paper, we present\nthe first survey of control-flow attestation, examining the core ideas and\nsolutions in state-of-the-art schemes. In total, we survey over 30 papers\npublished between 2016-2024, consolidate and compare their key features, and\npose several challenges and recommendations for future research in the area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow attestation unifies the worlds of control-flow integrity and\nplatform attestation by measuring and reporting a target's run-time behaviour\nto a verifier. Trust assurances in the target are provided by testing whether\nits execution follows an authorised control-flow path. The problem has been\nexplored in various settings, such as assessing the trustworthiness of\ncyber-physical systems, Internet of Things devices, cloud platforms, and many\nothers. Despite a significant number of proposals being made in recent years,\nthe area remains fragmented, addressing different adversarial behaviours,\nverification paradigms, and deployment challenges. In this paper, we present\nthe first survey of control-flow attestation, examining the core ideas and\nsolutions in state-of-the-art schemes. In total, we survey over 30 papers\npublished between 2016-2024, consolidate and compare their key features, and\npose several challenges and recommendations for future research in the area."
                },
                "authors": [
                    {
                        "name": "Zhanyu Sha"
                    },
                    {
                        "name": "Carlton Shepherd"
                    },
                    {
                        "name": "Amir Rafi"
                    },
                    {
                        "name": "Konstantinos Markantonakis"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Markantonakis"
                },
                "author": "Konstantinos Markantonakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07137v1",
                "updated": "2024-08-13T18:12:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    18,
                    12,
                    0,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T18:12:00Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    18,
                    12,
                    0,
                    1,
                    226,
                    0
                ],
                "title": "ELLA: Empowering LLMs for Interpretable, Accurate and Informative Legal\n  Advice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELLA: Empowering LLMs for Interpretable, Accurate and Informative Legal\n  Advice"
                },
                "summary": "Despite remarkable performance in legal consultation exhibited by legal Large\nLanguage Models(LLMs) combined with legal article retrieval components, there\nare still cases when the advice given is incorrect or baseless. To alleviate\nthese problems, we propose {\\bf ELLA}, a tool for {\\bf E}mpowering {\\bf L}LMs\nfor interpretable, accurate, and informative {\\bf L}egal {\\bf A}dvice. ELLA\nvisually presents the correlation between legal articles and LLM's response by\ncalculating their similarities, providing users with an intuitive legal basis\nfor the responses. Besides, based on the users' queries, ELLA retrieves\nrelevant legal articles and displays them to users. Users can interactively\nselect legal articles for LLM to generate more accurate responses. ELLA also\nretrieves relevant legal cases for user reference. Our user study shows that\npresenting the legal basis for the response helps users understand better. The\naccuracy of LLM's responses also improves when users intervene in selecting\nlegal articles for LLM. Providing relevant legal cases also aids individuals in\nobtaining comprehensive information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite remarkable performance in legal consultation exhibited by legal Large\nLanguage Models(LLMs) combined with legal article retrieval components, there\nare still cases when the advice given is incorrect or baseless. To alleviate\nthese problems, we propose {\\bf ELLA}, a tool for {\\bf E}mpowering {\\bf L}LMs\nfor interpretable, accurate, and informative {\\bf L}egal {\\bf A}dvice. ELLA\nvisually presents the correlation between legal articles and LLM's response by\ncalculating their similarities, providing users with an intuitive legal basis\nfor the responses. Besides, based on the users' queries, ELLA retrieves\nrelevant legal articles and displays them to users. Users can interactively\nselect legal articles for LLM to generate more accurate responses. ELLA also\nretrieves relevant legal cases for user reference. Our user study shows that\npresenting the legal basis for the response helps users understand better. The\naccuracy of LLM's responses also improves when users intervene in selecting\nlegal articles for LLM. Providing relevant legal cases also aids individuals in\nobtaining comprehensive information."
                },
                "authors": [
                    {
                        "name": "Yutong Hu"
                    },
                    {
                        "name": "Kangcheng Luo"
                    },
                    {
                        "name": "Yansong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Feng"
                },
                "author": "Yansong Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04614v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04614v2",
                "updated": "2024-08-13T18:00:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    18,
                    0,
                    57,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-08T17:42:32Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    17,
                    42,
                    32,
                    3,
                    221,
                    0
                ],
                "title": "Better Alignment with Instruction Back-and-Forth Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Alignment with Instruction Back-and-Forth Translation"
                },
                "summary": "We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment."
                },
                "authors": [
                    {
                        "name": "Thao Nguyen"
                    },
                    {
                        "name": "Jeffrey Li"
                    },
                    {
                        "name": "Sewoong Oh"
                    },
                    {
                        "name": "Ludwig Schmidt"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Xian Li"
                    }
                ],
                "author_detail": {
                    "name": "Xian Li"
                },
                "author": "Xian Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04614v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04614v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07060v1",
                "updated": "2024-08-13T17:50:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    50,
                    28,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T17:50:28Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    50,
                    28,
                    1,
                    226,
                    0
                ],
                "title": "Diversity Empowers Intelligence: Integrating Expertise of Software\n  Engineering Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity Empowers Intelligence: Integrating Expertise of Software\n  Engineering Agents"
                },
                "summary": "Large language model (LLM) agents have shown great potential in solving\nreal-world software engineering (SWE) problems. The most advanced open-source\nSWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.\nHowever, these sophisticated agent frameworks exhibit varying strengths,\nexcelling in certain tasks while underperforming in others. To fully harness\nthe diversity of these agents, we propose DEI (Diversity Empowered\nIntelligence), a framework that leverages their unique expertise. DEI functions\nas a meta-module atop existing SWE agent frameworks, managing agent collectives\nfor enhanced problem-solving. Experimental results show that a DEI-guided\ncommittee of agents is able to surpass the best individual agent's performance\nby a large margin. For instance, a group of open-source SWE agents, with a\nmaximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%\nresolve rate with DEI, making a 25% improvement and beating most closed-source\nsolutions. Our best-performing group excels with a 55% resolve rate, securing\nthe highest ranking on SWE-Bench Lite. Our findings contribute to the growing\nbody of research on collaborative AI systems and their potential to solve\ncomplex software engineering challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents have shown great potential in solving\nreal-world software engineering (SWE) problems. The most advanced open-source\nSWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.\nHowever, these sophisticated agent frameworks exhibit varying strengths,\nexcelling in certain tasks while underperforming in others. To fully harness\nthe diversity of these agents, we propose DEI (Diversity Empowered\nIntelligence), a framework that leverages their unique expertise. DEI functions\nas a meta-module atop existing SWE agent frameworks, managing agent collectives\nfor enhanced problem-solving. Experimental results show that a DEI-guided\ncommittee of agents is able to surpass the best individual agent's performance\nby a large margin. For instance, a group of open-source SWE agents, with a\nmaximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%\nresolve rate with DEI, making a 25% improvement and beating most closed-source\nsolutions. Our best-performing group excels with a 55% resolve rate, securing\nthe highest ranking on SWE-Bench Lite. Our findings contribute to the growing\nbody of research on collaborative AI systems and their potential to solve\ncomplex software engineering challenges."
                },
                "authors": [
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Rithesh Murthy"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Renze Lou"
                    },
                    {
                        "name": "Jiacheng Xu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07055v1",
                "updated": "2024-08-13T17:46:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    46,
                    12,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T17:46:12Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    46,
                    12,
                    1,
                    226,
                    0
                ],
                "title": "LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs"
                },
                "summary": "Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter."
                },
                "authors": [
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Linzhi Zheng"
                    },
                    {
                        "name": "Siqi Zhu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.04381v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.04381v3",
                "updated": "2024-08-13T17:11:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    17,
                    11,
                    25,
                    1,
                    226,
                    0
                ],
                "published": "2023-08-08T16:32:41Z",
                "published_parsed": [
                    2023,
                    8,
                    8,
                    16,
                    32,
                    41,
                    1,
                    220,
                    0
                ],
                "title": "Gromov-Wasserstein unsupervised alignment reveals structural\n  correspondences between the color similarity structures of humans and large\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gromov-Wasserstein unsupervised alignment reveals structural\n  correspondences between the color similarity structures of humans and large\n  language models"
                },
                "summary": "Large Language Models (LLMs), such as the General Pre-trained Transformer\n(GPT), have shown remarkable performance in various cognitive tasks. However,\nit remains unclear whether these models have the ability to accurately infer\nhuman perceptual representations. Previous research has addressed this question\nby quantifying correlations between similarity response patterns of humans and\nLLMs. Correlation provides a measure of similarity, but it relies pre-defined\nitem labels and does not distinguish category- and item- level similarity,\nfalling short of characterizing detailed structural correspondence between\nhumans and LLMs. To assess their structural equivalence in more detail, we\npropose the use of an unsupervised alignment method based on Gromov-Wasserstein\noptimal transport (GWOT). GWOT allows for the comparison of similarity\nstructures without relying on pre-defined label correspondences and can reveal\nfine-grained structural similarities and differences that may not be detected\nby simple correlation analysis. Using a large dataset of similarity judgments\nof 93 colors, we compared the color similarity structures of humans\n(color-neurotypical and color-atypical participants) and two GPT models\n(GPT-3.5 and GPT-4). Our results show that the similarity structure of\ncolor-neurotypical participants can be remarkably well aligned with that of\nGPT-4 and, to a lesser extent, to that of GPT-3.5. These results contribute to\nthe methodological advancements of comparing LLMs with human perception, and\nhighlight the potential of unsupervised alignment methods to reveal detailed\nstructural correspondences. This work has been published in Scientific Reports,\nDOI: https://doi.org/10.1038/s41598-024-65604-1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as the General Pre-trained Transformer\n(GPT), have shown remarkable performance in various cognitive tasks. However,\nit remains unclear whether these models have the ability to accurately infer\nhuman perceptual representations. Previous research has addressed this question\nby quantifying correlations between similarity response patterns of humans and\nLLMs. Correlation provides a measure of similarity, but it relies pre-defined\nitem labels and does not distinguish category- and item- level similarity,\nfalling short of characterizing detailed structural correspondence between\nhumans and LLMs. To assess their structural equivalence in more detail, we\npropose the use of an unsupervised alignment method based on Gromov-Wasserstein\noptimal transport (GWOT). GWOT allows for the comparison of similarity\nstructures without relying on pre-defined label correspondences and can reveal\nfine-grained structural similarities and differences that may not be detected\nby simple correlation analysis. Using a large dataset of similarity judgments\nof 93 colors, we compared the color similarity structures of humans\n(color-neurotypical and color-atypical participants) and two GPT models\n(GPT-3.5 and GPT-4). Our results show that the similarity structure of\ncolor-neurotypical participants can be remarkably well aligned with that of\nGPT-4 and, to a lesser extent, to that of GPT-3.5. These results contribute to\nthe methodological advancements of comparing LLMs with human perception, and\nhighlight the potential of unsupervised alignment methods to reveal detailed\nstructural correspondences. This work has been published in Scientific Reports,\nDOI: https://doi.org/10.1038/s41598-024-65604-1."
                },
                "authors": [
                    {
                        "name": "Genji Kawakita"
                    },
                    {
                        "name": "Ariel Zeleznikow-Johnston"
                    },
                    {
                        "name": "Naotsugu Tsuchiya"
                    },
                    {
                        "name": "Masafumi Oizumi"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oizumi"
                },
                "author": "Masafumi Oizumi",
                "arxiv_doi": "10.1038/s41598-024-65604-1 10.1038/s41598-024-65604-1\n  10.1038/s41598-024-65604-1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41598-024-65604-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41598-024-65604-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41598-024-65604-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.04381v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.04381v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Sci Rep 14, 15917 (2024)",
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07004v1",
                "updated": "2024-08-13T16:08:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    8,
                    37,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T16:08:37Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    8,
                    37,
                    1,
                    226,
                    0
                ],
                "title": "Casper: Prompt Sanitization for Protecting User Privacy in Web-Based\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Casper: Prompt Sanitization for Protecting User Privacy in Web-Based\n  Large Language Models"
                },
                "summary": "Web-based Large Language Model (LLM) services have been widely adopted and\nhave become an integral part of our Internet experience. Third-party plugins\nenhance the functionalities of LLM by enabling access to real-world data and\nservices. However, the privacy consequences associated with these services and\ntheir third-party plugins are not well understood. Sensitive prompt data are\nstored, processed, and shared by cloud-based LLM providers and third-party\nplugins. In this paper, we propose Casper, a prompt sanitization technique that\naims to protect user privacy by detecting and removing sensitive information\nfrom user inputs before sending them to LLM services. Casper runs entirely on\nthe user's device as a browser extension and does not require any changes to\nthe online LLM services. At the core of Casper is a three-layered sanitization\nmechanism consisting of a rule-based filter, a Machine Learning (ML)-based\nnamed entity recognizer, and a browser-based local LLM topic identifier. We\nevaluate Casper on a dataset of 4000 synthesized prompts and show that it can\neffectively filter out Personal Identifiable Information (PII) and\nprivacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web-based Large Language Model (LLM) services have been widely adopted and\nhave become an integral part of our Internet experience. Third-party plugins\nenhance the functionalities of LLM by enabling access to real-world data and\nservices. However, the privacy consequences associated with these services and\ntheir third-party plugins are not well understood. Sensitive prompt data are\nstored, processed, and shared by cloud-based LLM providers and third-party\nplugins. In this paper, we propose Casper, a prompt sanitization technique that\naims to protect user privacy by detecting and removing sensitive information\nfrom user inputs before sending them to LLM services. Casper runs entirely on\nthe user's device as a browser extension and does not require any changes to\nthe online LLM services. At the core of Casper is a three-layered sanitization\nmechanism consisting of a rule-based filter, a Machine Learning (ML)-based\nnamed entity recognizer, and a browser-based local LLM topic identifier. We\nevaluate Casper on a dataset of 4000 synthesized prompts and show that it can\neffectively filter out Personal Identifiable Information (PII) and\nprivacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively."
                },
                "authors": [
                    {
                        "name": "Chun Jie Chong"
                    },
                    {
                        "name": "Chenxi Hou"
                    },
                    {
                        "name": "Zhihao Yao"
                    },
                    {
                        "name": "Seyed Mohammadjavad Seyed Talebi"
                    }
                ],
                "author_detail": {
                    "name": "Seyed Mohammadjavad Seyed Talebi"
                },
                "author": "Seyed Mohammadjavad Seyed Talebi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07003v1",
                "updated": "2024-08-13T16:07:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    7,
                    16,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T16:07:16Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    16,
                    7,
                    16,
                    1,
                    226,
                    0
                ],
                "title": "Generative AI for automatic topic labelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI for automatic topic labelling"
                },
                "summary": "Topic Modeling has become a prominent tool for the study of scientific\nfields, as they allow for a large scale interpretation of research trends.\nNevertheless, the output of these models is structured as a list of keywords\nwhich requires a manual interpretation for the labelling. This paper proposes\nto assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini\nfor topic labelling. Drawing on previous research leveraging BERTopic, we\ngenerate topics from a dataset of all the scientific articles (n=34,797)\nauthored by all biology professors in Switzerland (n=465) between 2008 and\n2020, as recorded in the Web of Science database. We assess the output of the\nthree models both quantitatively and qualitatively and find that, first, both\nGPT models are capable of accurately and precisely label topics from the\nmodels' output keywords. Second, 3-word labels are preferable to grasp the\ncomplexity of research topics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic Modeling has become a prominent tool for the study of scientific\nfields, as they allow for a large scale interpretation of research trends.\nNevertheless, the output of these models is structured as a list of keywords\nwhich requires a manual interpretation for the labelling. This paper proposes\nto assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini\nfor topic labelling. Drawing on previous research leveraging BERTopic, we\ngenerate topics from a dataset of all the scientific articles (n=34,797)\nauthored by all biology professors in Switzerland (n=465) between 2008 and\n2020, as recorded in the Web of Science database. We assess the output of the\nthree models both quantitatively and qualitatively and find that, first, both\nGPT models are capable of accurately and precisely label topics from the\nmodels' output keywords. Second, 3-word labels are preferable to grasp the\ncomplexity of research topics."
                },
                "authors": [
                    {
                        "name": "Diego Kozlowski"
                    },
                    {
                        "name": "Carolina Pradier"
                    },
                    {
                        "name": "Pierre Benz"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Benz"
                },
                "author": "Pierre Benz",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06993v1",
                "updated": "2024-08-13T15:53:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    53,
                    58,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T15:53:58Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    53,
                    58,
                    1,
                    226,
                    0
                ],
                "title": "LLMs can Schedule",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can Schedule"
                },
                "summary": "The job shop scheduling problem (JSSP) remains a significant hurdle in\noptimizing production processes. This challenge involves efficiently allocating\njobs to a limited number of machines while minimizing factors like total\nprocessing time or job delays. While recent advancements in artificial\nintelligence have yielded promising solutions, such as reinforcement learning\nand graph neural networks, this paper explores the potential of Large Language\nModels (LLMs) for JSSP. We introduce the very first supervised 120k dataset\nspecifically designed to train LLMs for JSSP. Surprisingly, our findings\ndemonstrate that LLM-based scheduling can achieve performance comparable to\nother neural approaches. Furthermore, we propose a sampling method that\nenhances the effectiveness of LLMs in tackling JSSP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The job shop scheduling problem (JSSP) remains a significant hurdle in\noptimizing production processes. This challenge involves efficiently allocating\njobs to a limited number of machines while minimizing factors like total\nprocessing time or job delays. While recent advancements in artificial\nintelligence have yielded promising solutions, such as reinforcement learning\nand graph neural networks, this paper explores the potential of Large Language\nModels (LLMs) for JSSP. We introduce the very first supervised 120k dataset\nspecifically designed to train LLMs for JSSP. Surprisingly, our findings\ndemonstrate that LLM-based scheduling can achieve performance comparable to\nother neural approaches. Furthermore, we propose a sampling method that\nenhances the effectiveness of LLMs in tackling JSSP."
                },
                "authors": [
                    {
                        "name": "Henrik Abgaryan"
                    },
                    {
                        "name": "Ararat Harutyunyan"
                    },
                    {
                        "name": "Tristan Cazenave"
                    }
                ],
                "author_detail": {
                    "name": "Tristan Cazenave"
                },
                "author": "Tristan Cazenave",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12261v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12261v4",
                "updated": "2024-08-13T15:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    20,
                    13,
                    1,
                    226,
                    0
                ],
                "published": "2024-02-19T16:19:15Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    16,
                    19,
                    15,
                    0,
                    50,
                    0
                ],
                "title": "NEO-BENCH: Evaluating Robustness of Large Language Models with\n  Neologisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO-BENCH: Evaluating Robustness of Large Language Models with\n  Neologisms"
                },
                "summary": "The performance of Large Language Models (LLMs) degrades from the temporal\ndrift between data used for model training and newer text seen during\ninference. One understudied avenue of language change causing data drift is the\nemergence of neologisms -- new word forms -- over time. We create a diverse\nresource of recent English neologisms by using several popular collection\nmethods. We analyze temporal drift using neologisms by comparing sentences\ncontaining new words with near-identical sentences that replace neologisms with\nexisting substitute words. Model performance is nearly halved in machine\ntranslation when a single neologism is introduced in a sentence. Motivated by\nthese results, we construct a benchmark to evaluate LLMs' ability to generalize\nto neologisms with various natural language understanding tasks and model\nperplexity. Models with later knowledge cutoff dates yield lower perplexities\nand perform better in downstream tasks. LLMs are also affected differently\nbased on the linguistic origins of words, indicating that neologisms are\ncomplex for static LLMs to address. We will release our benchmark and code for\nreproducing our experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of Large Language Models (LLMs) degrades from the temporal\ndrift between data used for model training and newer text seen during\ninference. One understudied avenue of language change causing data drift is the\nemergence of neologisms -- new word forms -- over time. We create a diverse\nresource of recent English neologisms by using several popular collection\nmethods. We analyze temporal drift using neologisms by comparing sentences\ncontaining new words with near-identical sentences that replace neologisms with\nexisting substitute words. Model performance is nearly halved in machine\ntranslation when a single neologism is introduced in a sentence. Motivated by\nthese results, we construct a benchmark to evaluate LLMs' ability to generalize\nto neologisms with various natural language understanding tasks and model\nperplexity. Models with later knowledge cutoff dates yield lower perplexities\nand perform better in downstream tasks. LLMs are also affected differently\nbased on the linguistic origins of words, indicating that neologisms are\ncomplex for static LLMs to address. We will release our benchmark and code for\nreproducing our experiments."
                },
                "authors": [
                    {
                        "name": "Jonathan Zheng"
                    },
                    {
                        "name": "Alan Ritter"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "accepted to ACL 2024 main conference, 9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12261v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12261v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06956v1",
                "updated": "2024-08-13T15:15:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    15,
                    6,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T15:15:06Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    15,
                    6,
                    1,
                    226,
                    0
                ],
                "title": "PayOff: A Regulated Central Bank Digital Currency with Private Offline\n  Payments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PayOff: A Regulated Central Bank Digital Currency with Private Offline\n  Payments"
                },
                "summary": "The European Central Bank is preparing for the potential issuance of a\ncentral bank digital currency (CBDC), called the digital euro. A recent\nregulatory proposal by the European Commission defines several requirements for\nthe digital euro, such as support for both online and offline payments. Offline\npayments are expected to enable cash-like privacy, local payment settlement,\nand the enforcement of holding limits. While other central banks have expressed\nsimilar desired functionality, achieving such offline payments poses a novel\ntechnical challenge. We observe that none of the existing research solutions,\nincluding offline E-cash schemes, are fully compliant. Proposed solutions based\non secure elements offer no guarantees in case of compromise and can therefore\nlead to significant payment fraud.\n  The main contribution of this paper is PayOff, a novel CBDC design motivated\nby the digital euro regulation, which focuses on offline payments. We analyze\nthe security implications of local payment settlement and identify new security\nobjectives. PayOff protects user privacy, supports complex regulations such as\nholding limits, and implements safeguards to increase robustness against secure\nelement failure. Our analysis shows that PayOff provides strong privacy and\nidentifies residual leakages that may arise in real-world deployments. Our\nevaluation shows that offline payments can be fast and that the central bank\ncan handle high payment loads with moderate computing resources. However, the\nmain limitation of PayOff is that offline payment messages and storage\nrequirements grow in the number of payments that the sender makes or receives\nwithout going online in between.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The European Central Bank is preparing for the potential issuance of a\ncentral bank digital currency (CBDC), called the digital euro. A recent\nregulatory proposal by the European Commission defines several requirements for\nthe digital euro, such as support for both online and offline payments. Offline\npayments are expected to enable cash-like privacy, local payment settlement,\nand the enforcement of holding limits. While other central banks have expressed\nsimilar desired functionality, achieving such offline payments poses a novel\ntechnical challenge. We observe that none of the existing research solutions,\nincluding offline E-cash schemes, are fully compliant. Proposed solutions based\non secure elements offer no guarantees in case of compromise and can therefore\nlead to significant payment fraud.\n  The main contribution of this paper is PayOff, a novel CBDC design motivated\nby the digital euro regulation, which focuses on offline payments. We analyze\nthe security implications of local payment settlement and identify new security\nobjectives. PayOff protects user privacy, supports complex regulations such as\nholding limits, and implements safeguards to increase robustness against secure\nelement failure. Our analysis shows that PayOff provides strong privacy and\nidentifies residual leakages that may arise in real-world deployments. Our\nevaluation shows that offline payments can be fast and that the central bank\ncan handle high payment loads with moderate computing resources. However, the\nmain limitation of PayOff is that offline payment messages and storage\nrequirements grow in the number of payments that the sender makes or receives\nwithout going online in between."
                },
                "authors": [
                    {
                        "name": "Carolin Beer"
                    },
                    {
                        "name": "Sheila Zingg"
                    },
                    {
                        "name": "Kari Kostiainen"
                    },
                    {
                        "name": "Karl Wüst"
                    },
                    {
                        "name": "Vedran Capkun"
                    },
                    {
                        "name": "Srdjan Capkun"
                    }
                ],
                "author_detail": {
                    "name": "Srdjan Capkun"
                },
                "author": "Srdjan Capkun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.02404v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.02404v4",
                "updated": "2024-08-13T15:02:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    2,
                    18,
                    1,
                    226,
                    0
                ],
                "published": "2024-01-04T18:43:26Z",
                "published_parsed": [
                    2024,
                    1,
                    4,
                    18,
                    43,
                    26,
                    3,
                    4,
                    0
                ],
                "title": "Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for\n  Spatial Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correctness Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for\n  Spatial Tasks"
                },
                "summary": "Generative AI including large language models (LLMs) has recently gained\nsignificant interest in the geo-science community through its versatile\ntask-solving capabilities including programming, arithmetic reasoning,\ngeneration of sample data, time-series forecasting, toponym recognition, or\nimage classification. Most existing performance assessments of LLMs for spatial\ntasks have primarily focused on ChatGPT, whereas other chatbots received less\nattention. To narrow this research gap, this study conducts a zero-shot\ncorrectness evaluation for a set of 76 spatial tasks across seven task\ncategories assigned to four prominent chatbots, i.e., ChatGPT-4, Gemini,\nClaude-3, and Copilot. The chatbots generally performed well on tasks related\nto spatial literacy, GIS theory, and interpretation of programming code and\nfunctions, but revealed weaknesses in mapping, code writing, and spatial\nreasoning. Furthermore, there was a significant difference in correctness of\nresults between the four chatbots. Responses from repeated tasks assigned to\neach chatbot showed a high level of consistency in responses with matching\nrates of over 80% for most task categories in the four chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI including large language models (LLMs) has recently gained\nsignificant interest in the geo-science community through its versatile\ntask-solving capabilities including programming, arithmetic reasoning,\ngeneration of sample data, time-series forecasting, toponym recognition, or\nimage classification. Most existing performance assessments of LLMs for spatial\ntasks have primarily focused on ChatGPT, whereas other chatbots received less\nattention. To narrow this research gap, this study conducts a zero-shot\ncorrectness evaluation for a set of 76 spatial tasks across seven task\ncategories assigned to four prominent chatbots, i.e., ChatGPT-4, Gemini,\nClaude-3, and Copilot. The chatbots generally performed well on tasks related\nto spatial literacy, GIS theory, and interpretation of programming code and\nfunctions, but revealed weaknesses in mapping, code writing, and spatial\nreasoning. Furthermore, there was a significant difference in correctness of\nresults between the four chatbots. Responses from repeated tasks assigned to\neach chatbot showed a high level of consistency in responses with matching\nrates of over 80% for most task categories in the four chatbots."
                },
                "authors": [
                    {
                        "name": "Hartwig H. Hochmair"
                    },
                    {
                        "name": "Levente Juhasz"
                    },
                    {
                        "name": "Takoda Kemp"
                    }
                ],
                "author_detail": {
                    "name": "Takoda Kemp"
                },
                "author": "Takoda Kemp",
                "arxiv_doi": "10.1111/tgis.13233",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1111/tgis.13233",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.02404v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.02404v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Transactions in GIS",
                "arxiv_journal_ref": "Hochmair, H., Juh\\'asz, L. and Kemp, T. (2024), Correctness\n  Comparison of ChatGPT-4, Gemini, Claude-3, and Copilot for Spatial Tasks.\n  Transactions in GIS. (ahead of print)",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06941v1",
                "updated": "2024-08-13T14:59:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    59,
                    44,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T14:59:44Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    59,
                    44,
                    1,
                    226,
                    0
                ],
                "title": "OpenResearcher: Unleashing AI for Accelerated Scientific Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenResearcher: Unleashing AI for Accelerated Scientific Research"
                },
                "summary": "The rapid growth of scientific literature imposes significant challenges for\nresearchers endeavoring to stay updated with the latest advancements in their\nfields and delve into new areas. We introduce OpenResearcher, an innovative\nplatform that leverages Artificial Intelligence (AI) techniques to accelerate\nthe research process by answering diverse questions from researchers.\nOpenResearcher is built based on Retrieval-Augmented Generation (RAG) to\nintegrate Large Language Models (LLMs) with up-to-date, domain-specific\nknowledge. Moreover, we develop various tools for OpenResearcher to understand\nresearchers' queries, search from the scientific literature, filter retrieved\ninformation, provide accurate and comprehensive answers, and self-refine these\nanswers. OpenResearcher can flexibly use these tools to balance efficiency and\neffectiveness. As a result, OpenResearcher enables researchers to save time and\nincrease their potential to discover new insights and drive scientific\nbreakthroughs. Demo, video, and code are available at:\nhttps://github.com/GAIR-NLP/OpenResearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of scientific literature imposes significant challenges for\nresearchers endeavoring to stay updated with the latest advancements in their\nfields and delve into new areas. We introduce OpenResearcher, an innovative\nplatform that leverages Artificial Intelligence (AI) techniques to accelerate\nthe research process by answering diverse questions from researchers.\nOpenResearcher is built based on Retrieval-Augmented Generation (RAG) to\nintegrate Large Language Models (LLMs) with up-to-date, domain-specific\nknowledge. Moreover, we develop various tools for OpenResearcher to understand\nresearchers' queries, search from the scientific literature, filter retrieved\ninformation, provide accurate and comprehensive answers, and self-refine these\nanswers. OpenResearcher can flexibly use these tools to balance efficiency and\neffectiveness. As a result, OpenResearcher enables researchers to save time and\nincrease their potential to discover new insights and drive scientific\nbreakthroughs. Demo, video, and code are available at:\nhttps://github.com/GAIR-NLP/OpenResearcher."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zheng"
                    },
                    {
                        "name": "Shichao Sun"
                    },
                    {
                        "name": "Lin Qiu"
                    },
                    {
                        "name": "Dongyu Ru"
                    },
                    {
                        "name": "Cheng Jiayang"
                    },
                    {
                        "name": "Xuefeng Li"
                    },
                    {
                        "name": "Jifan Lin"
                    },
                    {
                        "name": "Binjie Wang"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Renjie Pan"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingkai Min"
                    },
                    {
                        "name": "Zizhao Zhang"
                    },
                    {
                        "name": "Yiwen Wang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06273v2",
                "updated": "2024-08-13T14:57:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    57,
                    25,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-12T16:34:56Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    34,
                    56,
                    0,
                    225,
                    0
                ],
                "title": "FuxiTranyu: A Multilingual Large Language Model Trained with Balanced\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuxiTranyu: A Multilingual Large Language Model Trained with Balanced\n  Data"
                },
                "summary": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is\ntrained from scratch on a meticulously balanced multilingual data repository\nthat contains 600 billion tokens covering 43 natural languages and 16\nprogramming languages. In addition to the base model, we also develop two\ninstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse\nmultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined\nwith DPO on a preference dataset for enhanced alignment ability. Extensive\nexperiments on a wide range of multilingual benchmarks demonstrate the\ncompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,\nBLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability\nanalyses at both the neuron and representation level suggest that FuxiTranyu is\nable to learn consistent multilingual representations across different\nlanguages. To promote further research into multilingual LLMs and their working\nmechanisms, we release both the base and instruction-tuned FuxiTranyu models\ntogether with 58 pretraining checkpoints at HuggingFace and Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is\ntrained from scratch on a meticulously balanced multilingual data repository\nthat contains 600 billion tokens covering 43 natural languages and 16\nprogramming languages. In addition to the base model, we also develop two\ninstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse\nmultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined\nwith DPO on a preference dataset for enhanced alignment ability. Extensive\nexperiments on a wide range of multilingual benchmarks demonstrate the\ncompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,\nBLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability\nanalyses at both the neuron and representation level suggest that FuxiTranyu is\nable to learn consistent multilingual representations across different\nlanguages. To promote further research into multilingual LLMs and their working\nmechanisms, we release both the base and instruction-tuned FuxiTranyu models\ntogether with 58 pretraining checkpoints at HuggingFace and Github."
                },
                "authors": [
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Shaoyang Xu"
                    },
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Supryadi"
                    },
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Jiangcun Du"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Juesi Xiao"
                    },
                    {
                        "name": "Shaolin Zhu"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06929v1",
                "updated": "2024-08-13T14:32:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    32,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T14:32:43Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    32,
                    43,
                    1,
                    226,
                    0
                ],
                "title": "Evaluating Cultural Adaptability of a Large Language Model via\n  Simulation of Synthetic Personas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Cultural Adaptability of a Large Language Model via\n  Simulation of Synthetic Personas"
                },
                "summary": "The success of Large Language Models (LLMs) in multicultural environments\nhinges on their ability to understand users' diverse cultural backgrounds. We\nmeasure this capability by having an LLM simulate human profiles representing\nvarious nationalities within the scope of a questionnaire-style psychological\nexperiment. Specifically, we employ GPT-3.5 to reproduce reactions to\npersuasive news articles of 7,286 participants from 15 countries; comparing the\nresults with a dataset of real participants sharing the same demographic\ntraits. Our analysis shows that specifying a person's country of residence\nimproves GPT-3.5's alignment with their responses. In contrast, using native\nlanguage prompting introduces shifts that significantly reduce overall\nalignment, with some languages particularly impairing performance. These\nfindings suggest that while direct nationality information enhances the model's\ncultural adaptability, native language cues do not reliably improve simulation\nfidelity and can detract from the model's effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLMs) in multicultural environments\nhinges on their ability to understand users' diverse cultural backgrounds. We\nmeasure this capability by having an LLM simulate human profiles representing\nvarious nationalities within the scope of a questionnaire-style psychological\nexperiment. Specifically, we employ GPT-3.5 to reproduce reactions to\npersuasive news articles of 7,286 participants from 15 countries; comparing the\nresults with a dataset of real participants sharing the same demographic\ntraits. Our analysis shows that specifying a person's country of residence\nimproves GPT-3.5's alignment with their responses. In contrast, using native\nlanguage prompting introduces shifts that significantly reduce overall\nalignment, with some languages particularly impairing performance. These\nfindings suggest that while direct nationality information enhances the model's\ncultural adaptability, native language cues do not reliably improve simulation\nfidelity and can detract from the model's effectiveness."
                },
                "authors": [
                    {
                        "name": "Louis Kwok"
                    },
                    {
                        "name": "Michal Bravansky"
                    },
                    {
                        "name": "Lewis D. Griffin"
                    }
                ],
                "author_detail": {
                    "name": "Lewis D. Griffin"
                },
                "author": "Lewis D. Griffin",
                "arxiv_comment": "18 pages, 8 figures, Published as a conference paper at COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06926v1",
                "updated": "2024-08-13T14:26:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    26,
                    30,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T14:26:30Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    26,
                    30,
                    1,
                    226,
                    0
                ],
                "title": "SceneGPT: A Language Model for 3D Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SceneGPT: A Language Model for 3D Scene Understanding"
                },
                "summary": "Building models that can understand and reason about 3D scenes is difficult\nowing to the lack of data sources for 3D supervised training and large-scale\ntraining regimes. In this work we ask - How can the knowledge in a pre-trained\nlanguage model be leveraged for 3D scene understanding without any 3D\npre-training. The aim of this work is to establish whether pre-trained LLMs\npossess priors/knowledge required for reasoning in 3D space and how can we\nprompt them such that they can be used for general purpose spatial reasoning\nand object understanding in 3D. To this end, we present SceneGPT, an LLM based\nscene understanding system which can perform 3D spatial reasoning without\ntraining or explicit 3D supervision. The key components of our framework are -\n1) a 3D scene graph, that serves as scene representation, encoding the objects\nin the scene and their spatial relationships 2) a pre-trained LLM that can be\nadapted with in context learning for 3D spatial reasoning. We evaluate our\nframework qualitatively on object and scene understanding tasks including\nobject semantics, physical properties and affordances (object-level) and\nspatial understanding (scene-level).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building models that can understand and reason about 3D scenes is difficult\nowing to the lack of data sources for 3D supervised training and large-scale\ntraining regimes. In this work we ask - How can the knowledge in a pre-trained\nlanguage model be leveraged for 3D scene understanding without any 3D\npre-training. The aim of this work is to establish whether pre-trained LLMs\npossess priors/knowledge required for reasoning in 3D space and how can we\nprompt them such that they can be used for general purpose spatial reasoning\nand object understanding in 3D. To this end, we present SceneGPT, an LLM based\nscene understanding system which can perform 3D spatial reasoning without\ntraining or explicit 3D supervision. The key components of our framework are -\n1) a 3D scene graph, that serves as scene representation, encoding the objects\nin the scene and their spatial relationships 2) a pre-trained LLM that can be\nadapted with in context learning for 3D spatial reasoning. We evaluate our\nframework qualitatively on object and scene understanding tasks including\nobject semantics, physical properties and affordances (object-level) and\nspatial understanding (scene-level)."
                },
                "authors": [
                    {
                        "name": "Shivam Chandhok"
                    }
                ],
                "author_detail": {
                    "name": "Shivam Chandhok"
                },
                "author": "Shivam Chandhok",
                "arxiv_comment": "UBC Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06904v1",
                "updated": "2024-08-13T13:58:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    58,
                    23,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:58:23Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    58,
                    23,
                    1,
                    226,
                    0
                ],
                "title": "Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge\n  Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge\n  Perspectives"
                },
                "summary": "As large language models (LLMs) continue to scale, their enhanced performance\noften proves insufficient for solving domain-specific tasks. Systematically\nanalyzing their failures and effectively enhancing their performance remain\nsignificant challenges. This paper introduces the Re-TASK framework, a novel\ntheoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge\nperspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space\nTheory. The Re-TASK framework provides a systematic methodology to deepen our\nunderstanding, evaluation, and enhancement of LLMs for domain-specific tasks.\nIt explores the interplay among an LLM's capabilities, the knowledge it\nprocesses, and the skills it applies, elucidating how these elements are\ninterconnected and impact task performance. Our application of the Re-TASK\nframework reveals that many failures in domain-specific tasks can be attributed\nto insufficient knowledge or inadequate skill adaptation. With this insight, we\npropose structured strategies for enhancing LLMs through targeted knowledge\ninjection and skill adaptation. Specifically, we identify key capability items\nassociated with tasks and employ a deliberately designed prompting strategy to\nenhance task performance, thereby reducing the need for extensive fine-tuning.\nAlternatively, we fine-tune the LLM using capability-specific instructions,\nfurther validating the efficacy of our framework. Experimental results confirm\nthe framework's effectiveness, demonstrating substantial improvements in both\nthe performance and applicability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, their enhanced performance\noften proves insufficient for solving domain-specific tasks. Systematically\nanalyzing their failures and effectively enhancing their performance remain\nsignificant challenges. This paper introduces the Re-TASK framework, a novel\ntheoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge\nperspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space\nTheory. The Re-TASK framework provides a systematic methodology to deepen our\nunderstanding, evaluation, and enhancement of LLMs for domain-specific tasks.\nIt explores the interplay among an LLM's capabilities, the knowledge it\nprocesses, and the skills it applies, elucidating how these elements are\ninterconnected and impact task performance. Our application of the Re-TASK\nframework reveals that many failures in domain-specific tasks can be attributed\nto insufficient knowledge or inadequate skill adaptation. With this insight, we\npropose structured strategies for enhancing LLMs through targeted knowledge\ninjection and skill adaptation. Specifically, we identify key capability items\nassociated with tasks and employ a deliberately designed prompting strategy to\nenhance task performance, thereby reducing the need for extensive fine-tuning.\nAlternatively, we fine-tune the LLM using capability-specific instructions,\nfurther validating the efficacy of our framework. Experimental results confirm\nthe framework's effectiveness, demonstrating substantial improvements in both\nthe performance and applicability of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhihu Wang"
                    },
                    {
                        "name": "Shiwan Zhao"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Heyuan Huang"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Sitao Xie"
                    },
                    {
                        "name": "Zhixing Wang"
                    },
                    {
                        "name": "Yubo Zhang"
                    },
                    {
                        "name": "Hongyan Li"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16205v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16205v3",
                "updated": "2024-08-13T13:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    46,
                    18,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-23T06:14:41Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    6,
                    14,
                    41,
                    1,
                    205,
                    0
                ],
                "title": "Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models"
                },
                "summary": "The rapid development of Large Language Models (LLMs) has brought remarkable\ngenerative capabilities across diverse tasks. However, despite the impressive\nachievements, these LLMs still have numerous inherent vulnerabilities,\nparticularly when faced with jailbreak attacks. By investigating jailbreak\nattacks, we can uncover hidden weaknesses in LLMs and inform the development of\nmore robust defense mechanisms to fortify their security. In this paper, we\nfurther explore the boundary of jailbreak attacks on LLMs and propose\nAnalyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes\nadvantage of LLMs' growing analyzing and reasoning capability and reveals their\nunderlying vulnerabilities when facing analyzing-based tasks. We conduct a\ndetailed evaluation of ABJ across various open-source and closed-source LLMs,\nwhich achieves 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE)\non GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and\nefficiency. Our research highlights the importance of prioritizing and\nenhancing the safety of LLMs to mitigate the risks of misuse. The code is\npublicly available at hhttps://github.com/theshi-1128/ABJ-Attack. Warning: This\npaper contains examples of LLMs that might be offensive or harmful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) has brought remarkable\ngenerative capabilities across diverse tasks. However, despite the impressive\nachievements, these LLMs still have numerous inherent vulnerabilities,\nparticularly when faced with jailbreak attacks. By investigating jailbreak\nattacks, we can uncover hidden weaknesses in LLMs and inform the development of\nmore robust defense mechanisms to fortify their security. In this paper, we\nfurther explore the boundary of jailbreak attacks on LLMs and propose\nAnalyzing-based Jailbreak (ABJ). This effective jailbreak attack method takes\nadvantage of LLMs' growing analyzing and reasoning capability and reveals their\nunderlying vulnerabilities when facing analyzing-based tasks. We conduct a\ndetailed evaluation of ABJ across various open-source and closed-source LLMs,\nwhich achieves 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE)\non GPT-4-turbo-0409, demonstrating state-of-the-art attack effectiveness and\nefficiency. Our research highlights the importance of prioritizing and\nenhancing the safety of LLMs to mitigate the risks of misuse. The code is\npublicly available at hhttps://github.com/theshi-1128/ABJ-Attack. Warning: This\npaper contains examples of LLMs that might be offensive or harmful."
                },
                "authors": [
                    {
                        "name": "Shi Lin"
                    },
                    {
                        "name": "Rongchang Li"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Wenpeng Xing"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16205v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16205v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06874v1",
                "updated": "2024-08-13T13:11:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    11,
                    53,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T13:11:53Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    11,
                    53,
                    1,
                    226,
                    0
                ],
                "title": "Leveraging Language Models for Emotion and Behavior Analysis in\n  Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Language Models for Emotion and Behavior Analysis in\n  Education"
                },
                "summary": "The analysis of students' emotions and behaviors is crucial for enhancing\nlearning outcomes and personalizing educational experiences. Traditional\nmethods often rely on intrusive visual and physiological data collection,\nposing privacy concerns and scalability issues. This paper proposes a novel\nmethod leveraging large language models (LLMs) and prompt engineering to\nanalyze textual data from students. Our approach utilizes tailored prompts to\nguide LLMs in detecting emotional and engagement states, providing a\nnon-intrusive and scalable solution. We conducted experiments using Qwen,\nChatGPT, Claude2, and GPT-4, comparing our method against baseline models and\nchain-of-thought (CoT) prompting. Results demonstrate that our method\nsignificantly outperforms the baselines in both accuracy and contextual\nunderstanding. This study highlights the potential of LLMs combined with prompt\nengineering to offer practical and effective tools for educational emotion and\nbehavior analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The analysis of students' emotions and behaviors is crucial for enhancing\nlearning outcomes and personalizing educational experiences. Traditional\nmethods often rely on intrusive visual and physiological data collection,\nposing privacy concerns and scalability issues. This paper proposes a novel\nmethod leveraging large language models (LLMs) and prompt engineering to\nanalyze textual data from students. Our approach utilizes tailored prompts to\nguide LLMs in detecting emotional and engagement states, providing a\nnon-intrusive and scalable solution. We conducted experiments using Qwen,\nChatGPT, Claude2, and GPT-4, comparing our method against baseline models and\nchain-of-thought (CoT) prompting. Results demonstrate that our method\nsignificantly outperforms the baselines in both accuracy and contextual\nunderstanding. This study highlights the potential of LLMs combined with prompt\nengineering to offer practical and effective tools for educational emotion and\nbehavior analysis."
                },
                "authors": [
                    {
                        "name": "Kaito Tanaka"
                    },
                    {
                        "name": "Benjamin Tan"
                    },
                    {
                        "name": "Brian Wong"
                    }
                ],
                "author_detail": {
                    "name": "Brian Wong"
                },
                "author": "Brian Wong",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06571v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06571v3",
                "updated": "2024-08-13T12:49:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    49,
                    20,
                    1,
                    226,
                    0
                ],
                "published": "2024-06-03T16:43:04Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    16,
                    43,
                    4,
                    0,
                    155,
                    0
                ],
                "title": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling\n  for LLM"
                },
                "summary": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have achieved remarkable success in\nvarious fields, the efficiency of training and inference remains a major\nchallenge. To address this issue, we propose SUBLLM, short for\nSubsampling-Upsampling-Bypass Large Language Model, an innovative architecture\nthat extends the core decoder-only framework by incorporating subsampling,\nupsampling, and bypass modules. The subsampling modules are responsible for\nshortening the sequence, while the upsampling modules restore the sequence\nlength, and the bypass modules enhance convergence. In comparison to LLaMA, the\nproposed SUBLLM exhibits significant enhancements in both training and\ninference speeds as well as memory usage, while maintaining competitive\nfew-shot performance. During training, SUBLLM increases speeds by 26% and cuts\nmemory by 10GB per GPU. In inference, it boosts speeds by up to 37% and reduces\nmemory by 1GB per GPU. The training and inference speeds can be enhanced by 34%\nand 52% respectively when the context window is expanded to 8192. Our code is\navailable at https://github.com/XiaoMi/subllm."
                },
                "authors": [
                    {
                        "name": "Quandong Wang"
                    },
                    {
                        "name": "Yuxuan Yuan"
                    },
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Ruike Zhang"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Daniel Povey"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_comment": "9 pages, 3 figures, accepted by ECAI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06571v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06571v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04416v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04416v3",
                "updated": "2024-08-14T21:30:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    21,
                    30,
                    52,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-05T11:07:13Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    11,
                    7,
                    13,
                    4,
                    187,
                    0
                ],
                "title": "Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions"
                },
                "summary": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online."
                },
                "authors": [
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Dongya Jia"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Yuanzhe Chen"
                    },
                    {
                        "name": "Zhengxi Liu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xubo Liu"
                    },
                    {
                        "name": "Xiyuan Kang"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    },
                    {
                        "name": "Wenwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Wang"
                },
                "author": "Wenwu Wang",
                "arxiv_comment": "5 pages with 1 appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04416v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04416v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06854v1",
                "updated": "2024-08-13T12:31:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    31,
                    30,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T12:31:30Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    31,
                    30,
                    1,
                    226,
                    0
                ],
                "title": "LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) with high parameter efficiency for\ndownstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA)\nsignificantly reduces the number of trainable parameters for fine-tuning.\nAlthough it has demonstrated commendable performance, updating parameters\nwithin a single scale may not be the optimal choice for complex downstream\ntasks.In this paper, we extend the LoRA to multiple scales, dubbed as LoRA$^2$.\nWe first combine orthogonal projection theory to train a set of LoRAs in two\nmutually orthogonal planes. Then, we improve the importance score algorithm,\nwhich reduce parameter sensitivity score calculations by approximately 98.5\\%.\nBy pruning singular values with lower importance scores, thereby enhancing\nadaptability to various downstream tasks. Extensive experiments are conducted\non two widely used pre-trained models to validate the effectiveness of\nLoRA$^2$. Results show that it significantly reduces the number of trainable\nparameters to just 0.72\\% compared to full fine-tuning, while still delivering\nhighly impressive performance. Even when the parameters are further reduced to\n0.17M, it still achieves comparable results to the baseline with 8 times more\nparameters. Our code is available here:\nhttps://anonymous.4open.science/r/LoRA-2-5B4C",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) with high parameter efficiency for\ndownstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA)\nsignificantly reduces the number of trainable parameters for fine-tuning.\nAlthough it has demonstrated commendable performance, updating parameters\nwithin a single scale may not be the optimal choice for complex downstream\ntasks.In this paper, we extend the LoRA to multiple scales, dubbed as LoRA$^2$.\nWe first combine orthogonal projection theory to train a set of LoRAs in two\nmutually orthogonal planes. Then, we improve the importance score algorithm,\nwhich reduce parameter sensitivity score calculations by approximately 98.5\\%.\nBy pruning singular values with lower importance scores, thereby enhancing\nadaptability to various downstream tasks. Extensive experiments are conducted\non two widely used pre-trained models to validate the effectiveness of\nLoRA$^2$. Results show that it significantly reduces the number of trainable\nparameters to just 0.72\\% compared to full fine-tuning, while still delivering\nhighly impressive performance. Even when the parameters are further reduced to\n0.17M, it still achieves comparable results to the baseline with 8 times more\nparameters. Our code is available here:\nhttps://anonymous.4open.science/r/LoRA-2-5B4C"
                },
                "authors": [
                    {
                        "name": "Jia-Chen Zhang"
                    },
                    {
                        "name": "Yu-Jie Xiong"
                    },
                    {
                        "name": "He-Xi Qiu"
                    },
                    {
                        "name": "Dong-Hai Zhu"
                    },
                    {
                        "name": "Chun-Ming Xia"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Ming Xia"
                },
                "author": "Chun-Ming Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03627v3",
                "updated": "2024-08-13T12:27:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    27,
                    10,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-04T04:30:04Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    4,
                    30,
                    4,
                    3,
                    186,
                    0
                ],
                "title": "DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSLR: Document Refinement with Sentence-Level Re-ranking and\n  Reconstruction to Enhance Retrieval-Augmented Generation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their performance across various Natural Language Processing (NLP)\ntasks. However, LLMs still struggle with generating non-factual responses due\nto limitations in their parametric memory. Retrieval-Augmented Generation (RAG)\nsystems address this issue by incorporating external knowledge with a retrieval\nmodule. Despite their successes, however, current RAG systems face challenges\nwith retrieval failures and the limited ability of LLMs to filter out\nirrelevant information. Therefore, in this work, we propose DSLR (Document\nRefinement with Sentence-Level Re-ranking and Reconstruction), an unsupervised\nframework that decomposes retrieved documents into sentences, filters out\nirrelevant sentences, and reconstructs them again into coherent passages. We\nexperimentally validate DSLR on multiple open-domain QA datasets and the\nresults demonstrate that DSLR significantly enhances the RAG performance over\nconventional fixed-size passage. Furthermore, our DSLR enhances performance in\nspecific, yet realistic scenarios without the need for additional training,\nproviding an effective and efficient solution for refining retrieved documents\nin RAG systems."
                },
                "authors": [
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Soyeong Jeong"
                    },
                    {
                        "name": "Sukmin Cho"
                    },
                    {
                        "name": "SeungYoon Han"
                    },
                    {
                        "name": "Jong C. Park"
                    }
                ],
                "author_detail": {
                    "name": "Jong C. Park"
                },
                "author": "Jong C. Park",
                "arxiv_comment": "20 pages",
                "arxiv_journal_ref": "KnowledgeNLP@ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06849v1",
                "updated": "2024-08-13T12:22:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    22,
                    26,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T12:22:26Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    22,
                    26,
                    1,
                    226,
                    0
                ],
                "title": "Causal Agent based on Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Agent based on Large Language Model"
                },
                "summary": "Large language models (LLMs) have achieved significant success across various\ndomains. However, the inherent complexity of causal problems and causal theory\nposes challenges in accurately describing them in natural language, making it\ndifficult for LLMs to comprehend and use them effectively. Causal methods are\nnot easily conveyed through natural language, which hinders LLMs' ability to\napply them accurately. Additionally, causal datasets are typically tabular,\nwhile LLMs excel in handling natural language data, creating a structural\nmismatch that impedes effective reasoning with tabular data. This lack of\ncausal reasoning capability limits the development of LLMs. To address these\nchallenges, we have equipped the LLM with causal tools within an agent\nframework, named the Causal Agent, enabling it to tackle causal problems. The\ncausal agent comprises tools, memory, and reasoning modules. In the tools\nmodule, the causal agent applies causal methods to align tabular data with\nnatural language. In the reasoning module, the causal agent employs the ReAct\nframework to perform reasoning through multiple iterations with the tools. In\nthe memory module, the causal agent maintains a dictionary instance where the\nkeys are unique names and the values are causal graphs. To verify the causal\nability of the causal agent, we established a benchmark consisting of four\nlevels of causal problems: variable level, edge level, causal graph level, and\ncausal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for\nthese four levels of issues and tested the causal agent on the datasets. Our\nmethodology demonstrates remarkable efficacy on the four-level causal problems,\nwith accuracy rates all above 80%. For further insights and implementation\ndetails, our code is accessible via the GitHub repository\nhttps://github.com/Kairong-Han/Causal_Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved significant success across various\ndomains. However, the inherent complexity of causal problems and causal theory\nposes challenges in accurately describing them in natural language, making it\ndifficult for LLMs to comprehend and use them effectively. Causal methods are\nnot easily conveyed through natural language, which hinders LLMs' ability to\napply them accurately. Additionally, causal datasets are typically tabular,\nwhile LLMs excel in handling natural language data, creating a structural\nmismatch that impedes effective reasoning with tabular data. This lack of\ncausal reasoning capability limits the development of LLMs. To address these\nchallenges, we have equipped the LLM with causal tools within an agent\nframework, named the Causal Agent, enabling it to tackle causal problems. The\ncausal agent comprises tools, memory, and reasoning modules. In the tools\nmodule, the causal agent applies causal methods to align tabular data with\nnatural language. In the reasoning module, the causal agent employs the ReAct\nframework to perform reasoning through multiple iterations with the tools. In\nthe memory module, the causal agent maintains a dictionary instance where the\nkeys are unique names and the values are causal graphs. To verify the causal\nability of the causal agent, we established a benchmark consisting of four\nlevels of causal problems: variable level, edge level, causal graph level, and\ncausal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for\nthese four levels of issues and tested the causal agent on the datasets. Our\nmethodology demonstrates remarkable efficacy on the four-level causal problems,\nwith accuracy rates all above 80%. For further insights and implementation\ndetails, our code is accessible via the GitHub repository\nhttps://github.com/Kairong-Han/Causal_Agent."
                },
                "authors": [
                    {
                        "name": "Kairong Han"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Junjian Ye"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06845v1",
                "updated": "2024-08-13T12:11:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    11,
                    47,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T12:11:47Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    12,
                    11,
                    47,
                    1,
                    226,
                    0
                ],
                "title": "DracoGPT: Extracting Visualization Design Preferences from Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DracoGPT: Extracting Visualization Design Preferences from Large\n  Language Models"
                },
                "summary": "Trained on vast corpora, Large Language Models (LLMs) have the potential to\nencode visualization design knowledge and best practices. However, if they fail\nto do so, they might provide unreliable visualization recommendations. What\nvisualization design preferences, then, have LLMs learned? We contribute\nDracoGPT, a method for extracting, modeling, and assessing visualization design\npreferences from LLMs. To assess varied tasks, we develop two\npipelines--DracoGPT-Rank and DracoGPT-Recommend--to model LLMs prompted to\neither rank or recommend visual encoding specifications. We use Draco as a\nshared knowledge base in which to represent LLM design preferences and compare\nthem to best practices from empirical research. We demonstrate that DracoGPT\ncan accurately model the preferences expressed by LLMs, enabling analysis in\nterms of Draco design constraints. Across a suite of backing LLMs, we find that\nDracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both\nsubstantially diverge from guidelines drawn from human subjects experiments.\nFuture work can build on our approach to expand Draco's knowledge base to model\na richer set of preferences and to provide a robust and cost-effective stand-in\nfor LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trained on vast corpora, Large Language Models (LLMs) have the potential to\nencode visualization design knowledge and best practices. However, if they fail\nto do so, they might provide unreliable visualization recommendations. What\nvisualization design preferences, then, have LLMs learned? We contribute\nDracoGPT, a method for extracting, modeling, and assessing visualization design\npreferences from LLMs. To assess varied tasks, we develop two\npipelines--DracoGPT-Rank and DracoGPT-Recommend--to model LLMs prompted to\neither rank or recommend visual encoding specifications. We use Draco as a\nshared knowledge base in which to represent LLM design preferences and compare\nthem to best practices from empirical research. We demonstrate that DracoGPT\ncan accurately model the preferences expressed by LLMs, enabling analysis in\nterms of Draco design constraints. Across a suite of backing LLMs, we find that\nDracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both\nsubstantially diverge from guidelines drawn from human subjects experiments.\nFuture work can build on our approach to expand Draco's knowledge base to model\na richer set of preferences and to provide a robust and cost-effective stand-in\nfor LLMs."
                },
                "authors": [
                    {
                        "name": "Huichen Will Wang"
                    },
                    {
                        "name": "Mitchell Gordon"
                    },
                    {
                        "name": "Leilani Battle"
                    },
                    {
                        "name": "Jeffrey Heer"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Heer"
                },
                "author": "Jeffrey Heer",
                "arxiv_comment": "IEEE Transactions on Visualization and Computer Graphics (Proc. VIS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06837v1",
                "updated": "2024-08-13T11:54:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    54,
                    18,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T11:54:18Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    54,
                    18,
                    1,
                    226,
                    0
                ],
                "title": "How Aligned are Human Chart Takeaways and LLM Predictions? A Case Study\n  on Bar Charts with Varying Layouts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Aligned are Human Chart Takeaways and LLM Predictions? A Case Study\n  on Bar Charts with Varying Layouts"
                },
                "summary": "Large Language Models (LLMs) have been adopted for a variety of\nvisualizations tasks, but how far are we from perceptually aware LLMs that can\npredict human takeaways? Graphical perception literature has shown that human\nchart takeaways are sensitive to visualization design choices, such as spatial\nlayouts. In this work, we examine the extent to which LLMs exhibit such\nsensitivity when generating takeaways, using bar charts with varying spatial\nlayouts as a case study. We conducted three experiments and tested four common\nbar chart layouts: vertically juxtaposed, horizontally juxtaposed, overlaid,\nand stacked. In Experiment 1, we identified the optimal configurations to\ngenerate meaningful chart takeaways by testing four LLMs, two temperature\nsettings, nine chart specifications, and two prompting strategies. We found\nthat even state-of-the-art LLMs struggled to generate semantically diverse and\nfactually accurate takeaways. In Experiment 2, we used the optimal\nconfigurations to generate 30 chart takeaways each for eight visualizations\nacross four layouts and two datasets in both zero-shot and one-shot settings.\nCompared to human takeaways, we found that the takeaways LLMs generated often\ndid not match the types of comparisons made by humans. In Experiment 3, we\nexamined the effect of chart context and data on LLM takeaways. We found that\nLLMs, unlike humans, exhibited variation in takeaway comparison types for\ndifferent bar charts using the same bar layout. Overall, our case study\nevaluates the ability of LLMs to emulate human interpretations of data and\npoints to challenges and opportunities in using LLMs to predict human chart\ntakeaways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been adopted for a variety of\nvisualizations tasks, but how far are we from perceptually aware LLMs that can\npredict human takeaways? Graphical perception literature has shown that human\nchart takeaways are sensitive to visualization design choices, such as spatial\nlayouts. In this work, we examine the extent to which LLMs exhibit such\nsensitivity when generating takeaways, using bar charts with varying spatial\nlayouts as a case study. We conducted three experiments and tested four common\nbar chart layouts: vertically juxtaposed, horizontally juxtaposed, overlaid,\nand stacked. In Experiment 1, we identified the optimal configurations to\ngenerate meaningful chart takeaways by testing four LLMs, two temperature\nsettings, nine chart specifications, and two prompting strategies. We found\nthat even state-of-the-art LLMs struggled to generate semantically diverse and\nfactually accurate takeaways. In Experiment 2, we used the optimal\nconfigurations to generate 30 chart takeaways each for eight visualizations\nacross four layouts and two datasets in both zero-shot and one-shot settings.\nCompared to human takeaways, we found that the takeaways LLMs generated often\ndid not match the types of comparisons made by humans. In Experiment 3, we\nexamined the effect of chart context and data on LLM takeaways. We found that\nLLMs, unlike humans, exhibited variation in takeaway comparison types for\ndifferent bar charts using the same bar layout. Overall, our case study\nevaluates the ability of LLMs to emulate human interpretations of data and\npoints to challenges and opportunities in using LLMs to predict human chart\ntakeaways."
                },
                "authors": [
                    {
                        "name": "Huichen Will Wang"
                    },
                    {
                        "name": "Jane Hoffswell"
                    },
                    {
                        "name": "Sao Myat Thazin Thane"
                    },
                    {
                        "name": "Victor S. Bursztyn"
                    },
                    {
                        "name": "Cindy Xiong Bearfield"
                    }
                ],
                "author_detail": {
                    "name": "Cindy Xiong Bearfield"
                },
                "author": "Cindy Xiong Bearfield",
                "arxiv_comment": "IEEE Transactions on Visualization and Computer Graphics (Proc. VIS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10908v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10908v3",
                "updated": "2024-08-13T11:46:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    46,
                    52,
                    1,
                    226,
                    0
                ],
                "published": "2024-06-16T12:11:46Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    12,
                    11,
                    46,
                    6,
                    168,
                    0
                ],
                "title": "MICL: Improving In-Context Learning through Multiple-Label Words in\n  Demonstration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MICL: Improving In-Context Learning through Multiple-Label Words in\n  Demonstration"
                },
                "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks by using sample-label pairs as demonstrations. However, variations in\ndemonstrations can lead to significantly different performances. Current\nresearch mainly focuses on selecting demonstration samples, preassuming the\nclass name to be the label word when creating sample-label pairs. However, the\nchoice of label words is crucial for ICL performance. Besides, we observe that\nusing a single class name in demonstration may not yield optimal results while\nusing multiple label words in one sample-label pair can enhance ICL\nperformance. In this paper, we propose a comprehensive approach that organizes\nboth samples and labels in demonstrations based on LLMs' output space\ndistribution. This approach uses multiple label words in one sample-label pair\nto enhance label instruction. Evaluation results from seven classification\ndatasets show that this demonstration organization method, which incorporates\nmultiple label words to provide diverse label information, improves ICL\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks by using sample-label pairs as demonstrations. However, variations in\ndemonstrations can lead to significantly different performances. Current\nresearch mainly focuses on selecting demonstration samples, preassuming the\nclass name to be the label word when creating sample-label pairs. However, the\nchoice of label words is crucial for ICL performance. Besides, we observe that\nusing a single class name in demonstration may not yield optimal results while\nusing multiple label words in one sample-label pair can enhance ICL\nperformance. In this paper, we propose a comprehensive approach that organizes\nboth samples and labels in demonstrations based on LLMs' output space\ndistribution. This approach uses multiple label words in one sample-label pair\nto enhance label instruction. Evaluation results from seven classification\ndatasets show that this demonstration organization method, which incorporates\nmultiple label words to provide diverse label information, improves ICL\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhu Zixiao"
                    },
                    {
                        "name": "Feng Zijian"
                    },
                    {
                        "name": "Zhou Hanzhang"
                    },
                    {
                        "name": "Qian Junlang"
                    },
                    {
                        "name": "Mao Kezhi"
                    }
                ],
                "author_detail": {
                    "name": "Mao Kezhi"
                },
                "author": "Mao Kezhi",
                "arxiv_comment": "19 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10908v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10908v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06816v1",
                "updated": "2024-08-13T11:17:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    17,
                    31,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-13T11:17:31Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    17,
                    31,
                    1,
                    226,
                    0
                ],
                "title": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data\n  Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data\n  Uncertainty"
                },
                "summary": "Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are capable of performing various\ntasks, they still suffer from producing plausible but incorrect responses. To\nimprove the reliability of LLMs, recent research has focused on uncertainty\nquantification to predict whether a response is correct or not. However, most\nuncertainty quantification methods have been evaluated on questions requiring a\nsingle clear answer, ignoring the existence of data uncertainty that arises\nfrom irreducible randomness. Instead, these methods only consider model\nuncertainty, which arises from a lack of knowledge. In this paper, we\ninvestigate previous uncertainty quantification methods under the presence of\ndata uncertainty. Our contributions are two-fold: 1) proposing a new\nMulti-Answer Question Answering dataset, MAQA, consisting of world knowledge,\nmathematical reasoning, and commonsense reasoning tasks to evaluate uncertainty\nquantification regarding data uncertainty, and 2) assessing 5 uncertainty\nquantification methods of diverse white- and black-box LLMs. Our findings show\nthat entropy and consistency-based methods estimate the model uncertainty well\neven under data uncertainty, while other methods for white- and black-box LLMs\nstruggle depending on the tasks. Additionally, methods designed for white-box\nLLMs suffer from overconfidence in reasoning tasks compared to simple knowledge\nqueries. We believe our observations will pave the way for future work on\nuncertainty quantification in realistic setting."
                },
                "authors": [
                    {
                        "name": "Yongjin Yang"
                    },
                    {
                        "name": "Haneul Yoo"
                    },
                    {
                        "name": "Hwaran Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hwaran Lee"
                },
                "author": "Hwaran Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06276v2",
                "updated": "2024-08-13T11:05:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    5,
                    10,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-12T16:39:03Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    39,
                    3,
                    0,
                    225,
                    0
                ],
                "title": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Jieyong Kim"
                    },
                    {
                        "name": "Hyunseo Kim"
                    },
                    {
                        "name": "Hyunjin Cho"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Buru Chang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]