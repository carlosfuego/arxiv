[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.19260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19260v1",
                "updated": "2025-09-23T17:18:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    18,
                    59,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:18:59Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    18,
                    59,
                    1,
                    266,
                    0
                ],
                "title": "Reconstruction of a potential parameter in time-fractional diffusion\n  problems via a Kohn--Vogelius type functional: Theoretical aspects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstruction of a potential parameter in time-fractional diffusion\n  problems via a Kohn--Vogelius type functional: Theoretical aspects"
                },
                "summary": "Of concern is the problem of reconstructing a space-dependent potential from\nboundary observations in the Caputo time-fractional diffusion equation,\nutilizing a stable and robust recovery method. We develop an algorithm to\nminimize the Kohn-Vogelius (KV) cost function, which measures the difference\nbetween the solutions of two excitations. The inverse potential problem is\nrecast into an optimization problem, where the objective is to minimize a\nKohn-Vogelius-type functional within a set of admissible potentials. We\nestablish the well-posedness of this optimization problem by proving the\nexistence and uniqueness of a minimizer and demonstrating its stability with\nrespect to perturbations in the boundary data. Furthermore, we analyze the\nFr\\'echet differentiability of the KV functional and prove the Lipschitz\ncontinuity of its gradient. These theoretical results enable the development of\na convergent conjugate gradient algorithm for numerical reconstruction. The\neffectiveness and robustness of the proposed method are confirmed through\nseveral numerical examples in both one and two dimensions, including cases with\nnoisy data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Of concern is the problem of reconstructing a space-dependent potential from\nboundary observations in the Caputo time-fractional diffusion equation,\nutilizing a stable and robust recovery method. We develop an algorithm to\nminimize the Kohn-Vogelius (KV) cost function, which measures the difference\nbetween the solutions of two excitations. The inverse potential problem is\nrecast into an optimization problem, where the objective is to minimize a\nKohn-Vogelius-type functional within a set of admissible potentials. We\nestablish the well-posedness of this optimization problem by proving the\nexistence and uniqueness of a minimizer and demonstrating its stability with\nrespect to perturbations in the boundary data. Furthermore, we analyze the\nFr\\'echet differentiability of the KV functional and prove the Lipschitz\ncontinuity of its gradient. These theoretical results enable the development of\na convergent conjugate gradient algorithm for numerical reconstruction. The\neffectiveness and robustness of the proposed method are confirmed through\nseveral numerical examples in both one and two dimensions, including cases with\nnoisy data."
                },
                "authors": [
                    {
                        "name": "Hamza Kahlaoui"
                    },
                    {
                        "name": "Mourad Hrizi"
                    },
                    {
                        "name": "Abdessamad Oulmelk"
                    },
                    {
                        "name": "Xiangcheng Zheng"
                    },
                    {
                        "name": "Ahmed Hendy"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Hendy"
                },
                "author": "Ahmed Hendy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19228v1",
                "updated": "2025-09-23T16:49:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:49:43Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "title": "CompLLM: Compression for Long Context Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompLLM: Compression for Long Context Q&A"
                },
                "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility."
                },
                "authors": [
                    {
                        "name": "Gabriele Berton"
                    },
                    {
                        "name": "Jayakrishnan Unnikrishnan"
                    },
                    {
                        "name": "Son Tran"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19061v1",
                "updated": "2025-09-23T14:25:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:25:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics"
                },
                "summary": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators."
                },
                "authors": [
                    {
                        "name": "Marcel Ferrari"
                    },
                    {
                        "name": "Cyrill PÃ¼ntener"
                    },
                    {
                        "name": "Alexander Sotoudeh"
                    },
                    {
                        "name": "Niklas Viebig"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Viebig"
                },
                "author": "Niklas Viebig",
                "arxiv_comment": "15 pages, 5 figures, appendix has 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F08, 65N55, 65N22, 76M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.8; F.2.1; D.1.3; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18909v1",
                "updated": "2025-09-23T12:32:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:32:51Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "title": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation"
                },
                "summary": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge."
                },
                "authors": [
                    {
                        "name": "Jan Wichelmann"
                    },
                    {
                        "name": "Anja Rabich"
                    },
                    {
                        "name": "Anna P\"atschke"
                    },
                    {
                        "name": "Thomas Eisenbarth"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Eisenbarth"
                },
                "author": "Thomas Eisenbarth",
                "arxiv_doi": "10.1109/SP54263.2024.00261",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP54263.2024.00261",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.18909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 IEEE Symposium on Security and Privacy (SP), San Francisco,\n  CA, USA, 2024, pp. 4182-4199",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v3",
                "updated": "2025-09-23T08:31:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    31,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v2",
                "updated": "2025-09-23T08:24:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    24,
                    7,
                    1,
                    266,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18684v1",
                "updated": "2025-09-23T06:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T06:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "title": "Static Estimation of Reuse Profiles for Arrays in Nested Loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Estimation of Reuse Profiles for Arrays in Nested Loops"
                },
                "summary": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "This paper is accepted at the MEMSYS 2025 conference, 11th\n  International Symposium on Memory Systems, Washington D.C., October 7 -\n  October 8, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18670v1",
                "updated": "2025-09-23T05:39:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T05:39:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases"
                },
                "summary": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Kyuri Park"
                    },
                    {
                        "name": "Youngjae Kim"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "arxiv_comment": "11 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18592v1",
                "updated": "2025-09-23T03:23:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T03:23:03Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation"
                },
                "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/."
                },
                "authors": [
                    {
                        "name": "Neel P. Bhatt"
                    },
                    {
                        "name": "Yunhao Yang"
                    },
                    {
                        "name": "Rohan Siva"
                    },
                    {
                        "name": "Pranay Samineni"
                    },
                    {
                        "name": "Daniel Milan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "arxiv_comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v2",
                "updated": "2025-09-23T01:58:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    1,
                    58,
                    7,
                    1,
                    266,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00329v2",
                "updated": "2025-09-22T19:20:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    20,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-31T00:52:17Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation"
                },
                "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}."
                },
                "authors": [
                    {
                        "name": "Muhammad Adnan"
                    },
                    {
                        "name": "Nithesh Kurella"
                    },
                    {
                        "name": "Akhil Arunkumar"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "arxiv_comment": "Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18344v1",
                "updated": "2025-09-22T19:08:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T19:08:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding"
                },
                "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."
                },
                "authors": [
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Chun-Che Yang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18307v1",
                "updated": "2025-09-22T18:32:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T18:32:32Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "title": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT"
                },
                "summary": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments."
                },
                "authors": [
                    {
                        "name": "Sudam Masanta"
                    },
                    {
                        "name": "Gurvinder Singh"
                    },
                    {
                        "name": "Shefali Pahwa"
                    },
                    {
                        "name": "Shekhar Dwivedi"
                    },
                    {
                        "name": "Devaraju Sampathirao"
                    },
                    {
                        "name": "Ramandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Ramandeep Singh"
                },
                "author": "Ramandeep Singh",
                "arxiv_comment": "8 pages; comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v1",
                "updated": "2025-09-22T17:58:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00919v2",
                "updated": "2025-09-22T16:16:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    16,
                    25,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-02T21:15:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    15,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings"
                },
                "summary": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining."
                },
                "authors": [
                    {
                        "name": "Stephen Zhang"
                    },
                    {
                        "name": "Mustafa Khan"
                    },
                    {
                        "name": "Vardan Papyan"
                    }
                ],
                "author_detail": {
                    "name": "Vardan Papyan"
                },
                "author": "Vardan Papyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v2",
                "updated": "2025-09-22T12:28:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    28,
                    41,
                    0,
                    265,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "MaoXun Huang"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "13 pages, accepted as a main conference paper at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v3",
                "updated": "2025-09-22T12:03:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    3,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17650v1",
                "updated": "2025-09-22T11:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T11:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers"
                },
                "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical."
                },
                "authors": [
                    {
                        "name": "Soroush Mahdi"
                    },
                    {
                        "name": "Fardin Ayar"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Mahdi Javanmardi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Javanmardi"
                },
                "author": "Mahdi Javanmardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v1",
                "updated": "2025-09-22T06:56:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Recent advances in large language models (LLMs) have extended context\nlengths, enabling assistants to sustain long histories for coherent,\npersonalized responses. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly dominates\nunder strict resource constraints. An active line of research for reducing this\noverhead is KV cache compression, which seeks to limit cache size while\npreserving accuracy. Yet existing methods face two major limitations: (i)\nevicting entries after full-context prefill causes unbounded peak memory, and\n(ii) query-dependent eviction narrows the cache to a single query, leading to\ndegraded accuracy in multi-turn conversations. We introduce EpiCache, a\ntraining-free KV cache management framework for long conversational question\nanswering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth\nthrough block-wise prefill and preserves topic-relevant context via episodic KV\ncompression, which clusters conversation history into coherent episodes and\napplies episode-specific KV cache eviction. We further design an adaptive\nlayer-wise budget allocation strategy that measures each layer's sensitivity to\neviction and distributes the memory budget across layers accordingly. Across\nthree LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over\nrecent baselines, sustains near-full KV accuracy under 4-6x compression, and\nreduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have extended context\nlengths, enabling assistants to sustain long histories for coherent,\npersonalized responses. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly dominates\nunder strict resource constraints. An active line of research for reducing this\noverhead is KV cache compression, which seeks to limit cache size while\npreserving accuracy. Yet existing methods face two major limitations: (i)\nevicting entries after full-context prefill causes unbounded peak memory, and\n(ii) query-dependent eviction narrows the cache to a single query, leading to\ndegraded accuracy in multi-turn conversations. We introduce EpiCache, a\ntraining-free KV cache management framework for long conversational question\nanswering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth\nthrough block-wise prefill and preserves topic-relevant context via episodic KV\ncompression, which clusters conversation history into coherent episodes and\napplies episode-specific KV cache eviction. We further design an adaptive\nlayer-wise budget allocation strategy that measures each layer's sensitivity to\neviction and distributes the memory budget across layers accordingly. Across\nthree LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over\nrecent baselines, sustains near-full KV accuracy under 4-6x compression, and\nreduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v1",
                "updated": "2025-09-22T06:52:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v4",
                "updated": "2025-09-22T06:35:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    35,
                    26,
                    0,
                    265,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17360v1",
                "updated": "2025-09-22T05:24:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T05:24:22Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access"
                },
                "summary": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads."
                },
                "authors": [
                    {
                        "name": "Chaoyi Ruan"
                    },
                    {
                        "name": "Chao Bi"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Ziji Shi"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17257v1",
                "updated": "2025-09-21T22:14:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T22:14:56Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "title": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices"
                },
                "summary": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper."
                },
                "authors": [
                    {
                        "name": "Sven Christophersen"
                    }
                ],
                "author_detail": {
                    "name": "Sven Christophersen"
                },
                "author": "Sven Christophersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F55, 65F08, 65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v1",
                "updated": "2025-09-21T21:05:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v2",
                "updated": "2025-09-21T11:48:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    11,
                    48,
                    15,
                    6,
                    264,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v2",
                "updated": "2025-09-21T07:03:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    7,
                    3,
                    46,
                    6,
                    264,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v2",
                "updated": "2025-09-21T03:35:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    3,
                    35,
                    36,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16857v1",
                "updated": "2025-09-21T00:59:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T00:59:45Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching"
                },
                "summary": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput."
                },
                "authors": [
                    {
                        "name": "Xingyu Xiang"
                    },
                    {
                        "name": "Raj Joshi"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Chenxingyu Zhao"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Eddie Kohler"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v2",
                "updated": "2025-09-20T13:54:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    54,
                    37,
                    5,
                    263,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "17 pages, 13 figures, the second version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16686v1",
                "updated": "2025-09-20T13:27:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T13:27:13Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "title": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs"
                },
                "summary": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs."
                },
                "authors": [
                    {
                        "name": "Zhengge Cai"
                    },
                    {
                        "name": "Haowen Hou"
                    }
                ],
                "author_detail": {
                    "name": "Haowen Hou"
                },
                "author": "Haowen Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16630v1",
                "updated": "2025-09-20T11:09:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T11:09:01Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation"
                },
                "summary": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/."
                },
                "authors": [
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Hongfa Wang"
                    },
                    {
                        "name": "Heng Pan"
                    },
                    {
                        "name": "Yingqing He"
                    },
                    {
                        "name": "Junkun Yuan"
                    },
                    {
                        "name": "Ailing Zeng"
                    },
                    {
                        "name": "Chengfei Cai"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Zhifeng Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen",
                "arxiv_comment": "accepted by IJCV2025. project\n  page:https://follow-your-emoji.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16495v1",
                "updated": "2025-09-20T01:56:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T01:56:25Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "title": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads"
                },
                "summary": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads."
                },
                "authors": [
                    {
                        "name": "Mert Hidayetoglu"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Michael Wyatt"
                    },
                    {
                        "name": "Jeff Rasley"
                    },
                    {
                        "name": "Yuxiong He"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    }
                ],
                "author_detail": {
                    "name": "Samyam Rajbhandari"
                },
                "author": "Samyam Rajbhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16471v1",
                "updated": "2025-09-19T23:46:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T23:46:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes"
                },
                "summary": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts"
                },
                "authors": [
                    {
                        "name": "Sima Zeinali Danalou"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Niher R. Sarker"
                    },
                    {
                        "name": "Hooman Chamani"
                    },
                    {
                        "name": "Jane Y. Howe"
                    },
                    {
                        "name": "Patrick C. Lee"
                    },
                    {
                        "name": "Jay R. Werber"
                    }
                ],
                "author_detail": {
                    "name": "Jay R. Werber"
                },
                "author": "Jay R. Werber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v1",
                "updated": "2025-09-19T20:31:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v2",
                "updated": "2025-09-19T17:18:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    18,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts"
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "EMNLP 2025 Main Conference. Mingyuan, Jize, and Haozhen contributed\n  equally, while Minjia, Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v2",
                "updated": "2025-09-19T15:19:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v2",
                "updated": "2025-09-19T14:14:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    14,
                    32,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15763v1",
                "updated": "2025-09-19T08:47:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T08:47:37Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression"
                },
                "summary": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling."
                },
                "authors": [
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v2",
                "updated": "2025-09-19T06:20:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    6,
                    20,
                    14,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference"
                },
                "summary": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15529v1",
                "updated": "2025-09-19T02:27:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T02:27:01Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "title": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB"
                },
                "summary": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads."
                },
                "authors": [
                    {
                        "name": "Mashkhal A. Sidiq"
                    },
                    {
                        "name": "Aras A. Salih"
                    },
                    {
                        "name": "Samrand M. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Samrand M. Hassan"
                },
                "author": "Samrand M. Hassan",
                "arxiv_doi": "10.5121/ijdms.2025.17501",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5121/ijdms.2025.17501",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.15529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 4 figures, 1 Table",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15515v1",
                "updated": "2025-09-19T01:39:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T01:39:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference"
                },
                "summary": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%."
                },
                "authors": [
                    {
                        "name": "Hantao Yang"
                    },
                    {
                        "name": "Hong Xie"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v3",
                "updated": "2025-09-18T23:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    23,
                    34,
                    50,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. AlmazÃ¡n"
                    },
                    {
                        "name": "V. Ãlvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-RodrÃ­guez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. CÃ¡rcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. CossÃ­o"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. DÃ­az"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. GarcÃ­a-Barrena"
                    },
                    {
                        "name": "J. J. GÃ³mez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-GÃ³mez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. HervÃ©s Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. LÃ³pez-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. MartÃ­n-Albo"
                    },
                    {
                        "name": "G. MartÃ­nez-Lema"
                    },
                    {
                        "name": "M. MartÃ­nez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. NuÃ±ez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. PÃ©rez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. SimÃ³n"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. TonnelÃ©"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. UsÃ³n"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16278v1",
                "updated": "2025-09-18T17:38:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    48,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:38:48Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    48,
                    3,
                    261,
                    0
                ],
                "title": "Language Modeling with Learned Meta-Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Modeling with Learned Meta-Tokens"
                },
                "summary": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization."
                },
                "authors": [
                    {
                        "name": "Alok N. Shah"
                    },
                    {
                        "name": "Khush Gupta"
                    },
                    {
                        "name": "Keshav Ramji"
                    },
                    {
                        "name": "Pratik Chaudhari"
                    }
                ],
                "author_detail": {
                    "name": "Pratik Chaudhari"
                },
                "author": "Pratik Chaudhari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15038v1",
                "updated": "2025-09-18T15:04:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:04:06Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "title": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition"
                },
                "summary": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Siddhant Chaudhary"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15024v1",
                "updated": "2025-09-18T14:51:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:51:13Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "title": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering"
                },
                "summary": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xuanting Xie"
                    },
                    {
                        "name": "Bingheng Li"
                    },
                    {
                        "name": "Erlin Pan"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Wenyu Chen"
                    },
                    {
                        "name": "Zhao Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Kang"
                },
                "author": "Zhao Kang",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13789v2",
                "updated": "2025-09-18T04:57:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    4,
                    57,
                    32,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-17T07:58:36Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    58,
                    36,
                    2,
                    260,
                    0
                ],
                "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching"
                },
                "summary": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality."
                },
                "authors": [
                    {
                        "name": "Hanshuai Cui"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Zhifei Xu"
                    },
                    {
                        "name": "Zhi Yao"
                    },
                    {
                        "name": "Wenyi Zeng"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14403v1",
                "updated": "2025-09-17T20:08:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T20:08:53Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "title": "Kilovolt-Class $Î²-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt-Class $Î²-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers"
                },
                "summary": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers."
                },
                "authors": [
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Rachel Kahler"
                    },
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Akhila Mattapalli"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14347v1",
                "updated": "2025-09-17T18:26:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T18:26:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI"
                },
                "summary": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability."
                },
                "authors": [
                    {
                        "name": "Henri AÃ¯dasso"
                    },
                    {
                        "name": "Francis Bordeleau"
                    },
                    {
                        "name": "Ali Tizghadam"
                    }
                ],
                "author_detail": {
                    "name": "Ali Tizghadam"
                },
                "author": "Ali Tizghadam",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14093v1",
                "updated": "2025-09-17T15:33:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:33:44Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints."
                },
                "authors": [
                    {
                        "name": "Kerui Huang"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Tongtong Xu"
                    },
                    {
                        "name": "Lingfeng Bao"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14041v1",
                "updated": "2025-09-17T14:42:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:42:38Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "title": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching"
                },
                "summary": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO."
                },
                "authors": [
                    {
                        "name": "Henry Kao"
                    },
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Prabhdeep Singh Soni"
                    },
                    {
                        "name": "Ali Sedaghati"
                    },
                    {
                        "name": "Fang Su"
                    },
                    {
                        "name": "Bryan Chan"
                    },
                    {
                        "name": "Maziar Goudarzi"
                    },
                    {
                        "name": "Reza Azimi"
                    }
                ],
                "author_detail": {
                    "name": "Reza Azimi"
                },
                "author": "Reza Azimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18172v1",
                "updated": "2025-09-17T13:51:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    27,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:51:27Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    27,
                    2,
                    260,
                    0
                ],
                "title": "SBVR: Summation of BitVector Representation for Efficient LLM\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SBVR: Summation of BitVector Representation for Efficient LLM\n  Quantization"
                },
                "summary": "With the advent of large language models (LLMs), numerous Post-Training\nQuantization (PTQ) strategies have been proposed to alleviate deployment\nbarriers created by their enormous parameter counts. Quantization achieves\ncompression by limiting the number of representable points in the data.\nTherefore, the key to achieving efficient quantization is selecting the optimal\ncombination of representation points, or codes, for the given data. Existing\nPTQ solutions adopt two major approaches to this problem: Round-To-Nearest\n(RTN)-based methods and codebook-based methods. RTN-based methods map LLM\nweights onto uniformly distributed integer grids, failing to account for the\nGaussian-like weight distribution of LLM weights. Codebook-based methods\nmitigate this issue by constructing distribution-aware codebooks; however, they\nsuffer from random and strided memory access patterns, resulting in degraded\ninference speed that is exacerbated by the limited size of GPU L1 cache. To\novercome these limitations, we propose a novel LLM quantization method, SBVR\n(Summation of BitVector Representation), that enables Gaussian-like code\nrepresentation in a hardware-friendly manner for fast inference. SBVR maps\nweight values to non-uniform representation points whose distribution follows\nthe actual distribution of LLM weights, enabling more accurate compression.\nAdditionally, we design a custom CUDA kernel that allows matrix-vector\nmultiplication directly in the SBVR format without decompression, thereby\nenabling high-performance execution of SBVR-compressed models. Our evaluations\nof SBVR on various models demonstrate state-of-the-art perplexity and accuracy\nbenchmark performance while delivering a 2.21x- 3.04x end-to-end\ntoken-generation speedup over naive FP16 models in the 4-bit quantization\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of large language models (LLMs), numerous Post-Training\nQuantization (PTQ) strategies have been proposed to alleviate deployment\nbarriers created by their enormous parameter counts. Quantization achieves\ncompression by limiting the number of representable points in the data.\nTherefore, the key to achieving efficient quantization is selecting the optimal\ncombination of representation points, or codes, for the given data. Existing\nPTQ solutions adopt two major approaches to this problem: Round-To-Nearest\n(RTN)-based methods and codebook-based methods. RTN-based methods map LLM\nweights onto uniformly distributed integer grids, failing to account for the\nGaussian-like weight distribution of LLM weights. Codebook-based methods\nmitigate this issue by constructing distribution-aware codebooks; however, they\nsuffer from random and strided memory access patterns, resulting in degraded\ninference speed that is exacerbated by the limited size of GPU L1 cache. To\novercome these limitations, we propose a novel LLM quantization method, SBVR\n(Summation of BitVector Representation), that enables Gaussian-like code\nrepresentation in a hardware-friendly manner for fast inference. SBVR maps\nweight values to non-uniform representation points whose distribution follows\nthe actual distribution of LLM weights, enabling more accurate compression.\nAdditionally, we design a custom CUDA kernel that allows matrix-vector\nmultiplication directly in the SBVR format without decompression, thereby\nenabling high-performance execution of SBVR-compressed models. Our evaluations\nof SBVR on various models demonstrate state-of-the-art perplexity and accuracy\nbenchmark performance while delivering a 2.21x- 3.04x end-to-end\ntoken-generation speedup over naive FP16 models in the 4-bit quantization\nregime."
                },
                "authors": [
                    {
                        "name": "Wonjun Bang"
                    },
                    {
                        "name": "Jongseok Park"
                    },
                    {
                        "name": "Hongseung Yu"
                    },
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Kyunghan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyunghan Lee"
                },
                "author": "Kyunghan Lee",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13848v1",
                "updated": "2025-09-17T09:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:24:40Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation"
                },
                "summary": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference."
                },
                "authors": [
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Yongkang Zhou"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13604v1",
                "updated": "2025-09-17T00:28:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T00:28:49Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "title": "A Framework for Multi-source Prefetching Through Adaptive Weight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Multi-source Prefetching Through Adaptive Weight"
                },
                "summary": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web."
                },
                "authors": [
                    {
                        "name": "Yoseph Berhanu Alebachew"
                    },
                    {
                        "name": "Mulugeta Libsie"
                    }
                ],
                "author_detail": {
                    "name": "Mulugeta Libsie"
                },
                "author": "Mulugeta Libsie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v3",
                "updated": "2025-09-16T23:56:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    56,
                    55,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08256v2",
                "updated": "2025-09-16T23:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    15,
                    44,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-28T05:22:44Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    5,
                    22,
                    44,
                    2,
                    148,
                    0
                ],
                "title": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference"
                },
                "summary": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER"
                },
                "authors": [
                    {
                        "name": "Dongwei Wang"
                    },
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Yuxin Ren"
                    },
                    {
                        "name": "Jianing Deng"
                    },
                    {
                        "name": "Jingtong Hu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Huanrui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Huanrui Yang"
                },
                "author": "Huanrui Yang",
                "arxiv_comment": "EMNLP2025 Camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v3",
                "updated": "2025-09-16T10:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    33,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ICSE '26 (The 48th IEEE/ACM International Conference on\n  Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12900v1",
                "updated": "2025-09-16T09:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis"
                },
                "summary": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "BÃ¡lint Hartmann"
                    },
                    {
                        "name": "Michelle T. Cirunay"
                    }
                ],
                "author_detail": {
                    "name": "Michelle T. Cirunay"
                },
                "author": "Michelle T. Cirunay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12867v1",
                "updated": "2025-09-16T09:22:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:22:21Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "title": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1."
                },
                "authors": [
                    {
                        "name": "Yabo Zhang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Qingyun Li"
                    },
                    {
                        "name": "Zhen Hu"
                    },
                    {
                        "name": "Kavin Han"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12817v1",
                "updated": "2025-09-16T08:36:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T08:36:05Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "title": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention"
                },
                "summary": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Dong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Wang"
                },
                "author": "Dong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11156v2",
                "updated": "2025-09-16T07:49:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    7,
                    49,
                    41,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-14T08:22:37Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    22,
                    37,
                    6,
                    257,
                    0
                ],
                "title": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud"
                },
                "summary": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems."
                },
                "authors": [
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Aadarshraj Sah"
                    },
                    {
                        "name": "Poddutoori Sweeya Reddy"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v2",
                "updated": "2025-09-15T14:40:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    40,
                    16,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "IEEE Computer Architecture Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11628v1",
                "updated": "2025-09-15T06:46:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T06:46:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching"
                },
                "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Fei Ren"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_doi": "10.1145/3746027.3755331",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755331",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.11628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 9 figures, ACM Multimedia 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v2",
                "updated": "2025-09-15T01:15:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    1,
                    15,
                    50,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v2",
                "updated": "2025-09-15T00:51:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    0,
                    51,
                    47,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11239v1",
                "updated": "2025-09-14T12:29:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T12:29:49Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "title": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks"
                },
                "summary": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments."
                },
                "authors": [
                    {
                        "name": "Zhekun Huang"
                    },
                    {
                        "name": "Milena Radenkovic"
                    }
                ],
                "author_detail": {
                    "name": "Milena Radenkovic"
                },
                "author": "Milena Radenkovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11181v1",
                "updated": "2025-09-14T09:26:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T09:26:44Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "title": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study"
                },
                "summary": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior."
                },
                "authors": [
                    {
                        "name": "Alexander Frisch"
                    },
                    {
                        "name": "Daniel Isaia"
                    },
                    {
                        "name": "Oliver PreuÃ"
                    },
                    {
                        "name": "Xufei Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xufei Fang"
                },
                "author": "Xufei Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11155v1",
                "updated": "2025-09-14T08:20:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T08:20:48Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "title": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs"
                },
                "summary": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable."
                },
                "authors": [
                    {
                        "name": "Santhosh G S"
                    },
                    {
                        "name": "Saurav Prakash"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10798v1",
                "updated": "2025-09-13T03:34:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "published": "2025-09-13T03:34:12Z",
                "published_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction"
                },
                "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10372v1",
                "updated": "2025-09-12T16:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness"
                },
                "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively."
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Yousheng Long"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Jianxun Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10312v1",
                "updated": "2025-09-12T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching"
                },
                "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion."
                },
                "authors": [
                    {
                        "name": "Zhixin Zheng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10251v1",
                "updated": "2025-09-12T13:49:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:49:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing"
                },
                "summary": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs."
                },
                "authors": [
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Jieming Yin"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Diyu Zhou"
                    },
                    {
                        "name": "Zhenlin Wang"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10016v1",
                "updated": "2025-09-12T07:20:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "title": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report"
                },
                "summary": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago."
                },
                "authors": [
                    {
                        "name": "M. MÃ¼ller"
                    },
                    {
                        "name": "J. Rabault"
                    },
                    {
                        "name": "C. Palerme"
                    },
                    {
                        "name": "J. TjernstrÃ¶m"
                    }
                ],
                "author_detail": {
                    "name": "J. TjernstrÃ¶m"
                },
                "author": "J. TjernstrÃ¶m",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09754v1",
                "updated": "2025-09-11T16:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation"
                },
                "summary": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava."
                },
                "authors": [
                    {
                        "name": "Yiqun Shen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Zhengze Zhang"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Nguyen Cam-Tu"
                    }
                ],
                "author_detail": {
                    "name": "Nguyen Cam-Tu"
                },
                "author": "Nguyen Cam-Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09525v1",
                "updated": "2025-09-11T15:06:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:06:03Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes"
                },
                "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."
                },
                "authors": [
                    {
                        "name": "Jialiang Huang"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Sixing Lin"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Xia Liao"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Mengting Lu"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Haifeng Gong"
                    },
                    {
                        "name": "Mingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxing Zhang"
                },
                "author": "Mingxing Zhang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09494v1",
                "updated": "2025-09-11T14:34:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:34:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding"
                },
                "summary": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v2",
                "updated": "2025-09-11T12:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    49,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v2",
                "updated": "2025-09-11T10:20:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    20,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_doi": "10.1109/CLOUD67622.2025.00051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CLOUD67622.2025.00051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.19880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 2025 IEEE 18th International Conference on Cloud\n  Computing (CLOUD)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v3",
                "updated": "2025-09-11T06:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    45,
                    58,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v3",
                "updated": "2025-09-11T06:16:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    16,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09094v1",
                "updated": "2025-09-11T02:00:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T02:00:27Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "title": "Coherence-Aware Task Graph Modeling for Realistic Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence-Aware Task Graph Modeling for Realistic Application"
                },
                "summary": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3742875.3754678",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3742875.3754678",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by MEMOCODE'25, 10 pages",
                "arxiv_journal_ref": "International Symposium on Formal Methods and Models for System\n  Design (MEMOCODE '25), September 28-October 3, 2025, Taipei, Taiwan",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08696v1",
                "updated": "2025-09-10T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching"
                },
                "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ ."
                },
                "authors": [
                    {
                        "name": "Siratish Sakpiboonchit"
                    }
                ],
                "author_detail": {
                    "name": "Siratish Sakpiboonchit"
                },
                "author": "Siratish Sakpiboonchit",
                "arxiv_comment": "9 pages, 2 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08342v1",
                "updated": "2025-09-10T07:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08315v1",
                "updated": "2025-09-10T06:32:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T06:32:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolKV: Evolutionary KV Cache Compression for LLM Inference"
                },
                "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation."
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Yekun Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yekun Chai"
                },
                "author": "Yekun Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07379v1",
                "updated": "2025-09-09T04:00:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T04:00:43Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize."
                },
                "authors": [
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Grant Pinkert"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yanli Li"
                    },
                    {
                        "name": "Dong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yuan"
                },
                "author": "Dong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v2",
                "updated": "2025-09-09T00:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    0,
                    15,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v4",
                "updated": "2025-09-08T13:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    34,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick StrÃ¼mpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v1",
                "updated": "2025-09-08T09:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v4",
                "updated": "2025-09-08T09:09:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    9,
                    36,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06444v1",
                "updated": "2025-09-08T08:44:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:44:24Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data"
                },
                "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Hong-Wei Zheng"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v1",
                "updated": "2025-09-08T08:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06047v1",
                "updated": "2025-09-07T13:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "published": "2025-09-07T13:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "title": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon"
                },
                "summary": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Antony Jeyaseelan"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "arxiv_affiliation": "Centre for Nanoscience and Engineering, Indian Institute of Science, Bengaluru, India",
                "author": "Pavan Nukala",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v2",
                "updated": "2025-09-06T05:58:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    6,
                    5,
                    58,
                    51,
                    5,
                    249,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05207v1",
                "updated": "2025-09-05T16:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10806",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.19296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19296v1",
                "updated": "2025-09-23T17:58:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    58,
                    1,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:58:01Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    58,
                    1,
                    1,
                    266,
                    0
                ],
                "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model\n  Self-Distillation"
                },
                "summary": "The ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation."
                },
                "authors": [
                    {
                        "name": "Sherwin Bahmani"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiawei Ren"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Yifeng Jiang"
                    },
                    {
                        "name": "Haithem Turki"
                    },
                    {
                        "name": "Andrea Tagliasacchi"
                    },
                    {
                        "name": "David B. Lindell"
                    },
                    {
                        "name": "Zan Gojcic"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Jun Gao"
                    },
                    {
                        "name": "Xuanchi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Xuanchi Ren"
                },
                "author": "Xuanchi Ren",
                "arxiv_comment": "Project Page: https://research.nvidia.com/labs/toronto-ai/lyra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20197v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20197v4",
                "updated": "2025-09-23T17:57:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    57,
                    16,
                    1,
                    266,
                    0
                ],
                "published": "2025-03-26T03:44:03Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    44,
                    3,
                    2,
                    85,
                    0
                ],
                "title": "A Preliminary Study on the Robustness of Code Generation by Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Preliminary Study on the Robustness of Code Generation by Large\n  Language Models"
                },
                "summary": "Robustness is a critical factor for reliable code generation by large\nlanguage models, yet most evaluations focus on correctness and overlook key\nissues such as missing input validation and inadequate error handling. In this\nwork, we present the first empirical study of LLM-generated code robustness\nusing the CoderEval benchmark. Evaluating four state-of-the-art code LLMs, we\nfind that 35.2% of their outputs are less robust than human-written code, with\nover 90% of deficiencies caused by missing conditional checks-70% of which\noccur in the first line. Interestingly, in 63% of cases where a conditional\nstatement is needed but absent, the \"if\" token still ranks among the top three\npredictions, suggesting implicit recognition of control flow.\n  To address these issues, we propose RobGen, a model-agnostic framework that\nimproves robustness without retraining. RobGen combines a line-level\nintervention checker, which decides whether to adjust logits for each generated\nline, with token-level conditional logit adjustments to promote essential\ncontrol structures. Experiments show that RobGen reduces the proportion of less\nrobust code by 10%, achieves the highest average Pass@1 (43.57), and adds\nminimal overhead (+33.4%). As a lightweight and adaptable solution, RobGen\neffectively enhances the reliability of LLM-generated code across diverse\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness is a critical factor for reliable code generation by large\nlanguage models, yet most evaluations focus on correctness and overlook key\nissues such as missing input validation and inadequate error handling. In this\nwork, we present the first empirical study of LLM-generated code robustness\nusing the CoderEval benchmark. Evaluating four state-of-the-art code LLMs, we\nfind that 35.2% of their outputs are less robust than human-written code, with\nover 90% of deficiencies caused by missing conditional checks-70% of which\noccur in the first line. Interestingly, in 63% of cases where a conditional\nstatement is needed but absent, the \"if\" token still ranks among the top three\npredictions, suggesting implicit recognition of control flow.\n  To address these issues, we propose RobGen, a model-agnostic framework that\nimproves robustness without retraining. RobGen combines a line-level\nintervention checker, which decides whether to adjust logits for each generated\nline, with token-level conditional logit adjustments to promote essential\ncontrol structures. Experiments show that RobGen reduces the proportion of less\nrobust code by 10%, achieves the highest average Pass@1 (43.57), and adds\nminimal overhead (+33.4%). As a lightweight and adaptable solution, RobGen\neffectively enhances the reliability of LLM-generated code across diverse\ntasks."
                },
                "authors": [
                    {
                        "name": "Zike Li"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Anji Li"
                    },
                    {
                        "name": "Kaifeng He"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20197v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20197v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10571v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10571v3",
                "updated": "2025-09-23T17:47:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    47,
                    4,
                    1,
                    266,
                    0
                ],
                "published": "2025-04-30T16:18:39Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    18,
                    39,
                    2,
                    120,
                    0
                ],
                "title": "Language Models Do Not Have Human-Like Working Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Do Not Have Human-Like Working Memory"
                },
                "summary": "While Large Language Models (LLMs) exhibit remarkable reasoning abilities, we\ndemonstrate that they lack a fundamental aspect of human cognition: working\nmemory. Human working memory is an active cognitive system that enables not\nonly the temporary storage of information but also its processing and\nutilization, enabling coherent reasoning and decision-making. Without working\nmemory, individuals may produce unrealistic responses, exhibit\nself-contradictions, and struggle with tasks that require mental reasoning.\nExisting evaluations using N-back or context-dependent tasks fall short as they\nallow LLMs to exploit external context rather than retaining the reasoning\nprocess in the latent space. We introduce three novel tasks: (1) Number\nGuessing, (2) Yes-No Deduction, and (3) Math Magic, designed to isolate\ninternal representation from external context. Across seventeen frontier models\nspanning four major model families, we consistently observe irrational or\ncontradictory behaviors, indicating LLMs' inability to retain and manipulate\nlatent information. Our work establishes a new benchmark for evaluating working\nmemory in LLMs and highlights this limitation as a key bottleneck for advancing\nreliable reasoning systems. Code and prompts for the experiments are available\nat https://github.com/penguinnnnn/LLM-Working-Memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) exhibit remarkable reasoning abilities, we\ndemonstrate that they lack a fundamental aspect of human cognition: working\nmemory. Human working memory is an active cognitive system that enables not\nonly the temporary storage of information but also its processing and\nutilization, enabling coherent reasoning and decision-making. Without working\nmemory, individuals may produce unrealistic responses, exhibit\nself-contradictions, and struggle with tasks that require mental reasoning.\nExisting evaluations using N-back or context-dependent tasks fall short as they\nallow LLMs to exploit external context rather than retaining the reasoning\nprocess in the latent space. We introduce three novel tasks: (1) Number\nGuessing, (2) Yes-No Deduction, and (3) Math Magic, designed to isolate\ninternal representation from external context. Across seventeen frontier models\nspanning four major model families, we consistently observe irrational or\ncontradictory behaviors, indicating LLMs' inability to retain and manipulate\nlatent information. Our work establishes a new benchmark for evaluating working\nmemory in LLMs and highlights this limitation as a key bottleneck for advancing\nreliable reasoning systems. Code and prompts for the experiments are available\nat https://github.com/penguinnnnn/LLM-Working-Memory."
                },
                "authors": [
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Kaiser Sun"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10571v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10571v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19276v1",
                "updated": "2025-09-23T17:41:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    41,
                    43,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:41:43Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    41,
                    43,
                    1,
                    266,
                    0
                ],
                "title": "A Gradient Flow Approach to Solving Inverse Problems with Latent\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Gradient Flow Approach to Solving Inverse Problems with Latent\n  Diffusion Models"
                },
                "summary": "Solving ill-posed inverse problems requires powerful and flexible priors. We\npropose leveraging pretrained latent diffusion models for this task through a\nnew training-free approach, termed Diffusion-regularized Wasserstein Gradient\nFlow (DWGF). Specifically, we formulate the posterior sampling problem as a\nregularized Wasserstein gradient flow of the Kullback-Leibler divergence in the\nlatent space. We demonstrate the performance of our method on standard\nbenchmarks using StableDiffusion (Rombach et al., 2022) as the prior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving ill-posed inverse problems requires powerful and flexible priors. We\npropose leveraging pretrained latent diffusion models for this task through a\nnew training-free approach, termed Diffusion-regularized Wasserstein Gradient\nFlow (DWGF). Specifically, we formulate the posterior sampling problem as a\nregularized Wasserstein gradient flow of the Kullback-Leibler divergence in the\nlatent space. We demonstrate the performance of our method on standard\nbenchmarks using StableDiffusion (Rombach et al., 2022) as the prior."
                },
                "authors": [
                    {
                        "name": "Tim Y. J. Wang"
                    },
                    {
                        "name": "O. Deniz Akyildiz"
                    }
                ],
                "author_detail": {
                    "name": "O. Deniz Akyildiz"
                },
                "author": "O. Deniz Akyildiz",
                "arxiv_comment": "Accepted at the 2nd Workshop on Frontiers in Probabilistic Inference:\n  Sampling Meets Learning, 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19275v1",
                "updated": "2025-09-23T17:40:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    40,
                    49,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:40:49Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    40,
                    49,
                    1,
                    266,
                    0
                ],
                "title": "A Novel Site-Specific Inference Model for Urban Canyon Channels: From\n  Measurements to Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Site-Specific Inference Model for Urban Canyon Channels: From\n  Measurements to Modeling"
                },
                "summary": "With the rapid development of intelligent transportation and smart city\napplications, urban canyon has become a critical scenario for the design and\nevaluation of wireless communication systems. Due to its unique environmental\nlayout, the channel characteristics in urban canyon are strongly a street\ngeometry and building distribution, thereby exhibiting significant\nsite-specific channel condition. However, this feature has not been well\ncaptured in existing channel models. In this paper, we propose a site-specific\nchannel inference model based on environmental geometry, the model is\nparameterized using sub-6GHz channel measurements. Multipath components (MPCs)\nare extracted and clustered according to geometric propagation, which are\nexplicitly derived from the influence of canyon width, thereby establishing an\ninterpretable mapping between the physical environment and statistical\ncharacteristics of MPCs. A step-by-step implementation scheme is presented.\nSubsequently, the proposed site-specific channel inference model is validated\nby comparing second-order statistics of channels, derived from the model and\nmeasurements. The results show that the proposed model achieves high accuracy\nand robustness in different urban canyon scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of intelligent transportation and smart city\napplications, urban canyon has become a critical scenario for the design and\nevaluation of wireless communication systems. Due to its unique environmental\nlayout, the channel characteristics in urban canyon are strongly a street\ngeometry and building distribution, thereby exhibiting significant\nsite-specific channel condition. However, this feature has not been well\ncaptured in existing channel models. In this paper, we propose a site-specific\nchannel inference model based on environmental geometry, the model is\nparameterized using sub-6GHz channel measurements. Multipath components (MPCs)\nare extracted and clustered according to geometric propagation, which are\nexplicitly derived from the influence of canyon width, thereby establishing an\ninterpretable mapping between the physical environment and statistical\ncharacteristics of MPCs. A step-by-step implementation scheme is presented.\nSubsequently, the proposed site-specific channel inference model is validated\nby comparing second-order statistics of channels, derived from the model and\nmeasurements. The results show that the proposed model achieves high accuracy\nand robustness in different urban canyon scenarios."
                },
                "authors": [
                    {
                        "name": "Junzhe Song"
                    },
                    {
                        "name": "Ruisi He"
                    },
                    {
                        "name": "Mi Yang"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Xinwen Chen"
                    },
                    {
                        "name": "Xiaoying Zhang"
                    },
                    {
                        "name": "Bo Ai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Ai"
                },
                "author": "Bo Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12104v2",
                "updated": "2025-09-23T17:35:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    35,
                    19,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-16T17:00:51Z",
                "published_parsed": [
                    2025,
                    8,
                    16,
                    17,
                    0,
                    51,
                    5,
                    228,
                    0
                ],
                "title": "Generative Medical Event Models Improve with Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Medical Event Models Improve with Scale"
                },
                "summary": "Realizing personalized medicine at scale calls for methods that distill\ninsights from longitudinal patient journeys, which can be viewed as a sequence\nof medical events. Foundation models pretrained on large-scale medical event\ndata represent a promising direction for scaling real-world evidence generation\nand generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with\nmedical events from de-identified longitudinal health records for 16.3 billion\nencounters over 300 million unique patient records from 310 health systems, we\nintroduce the Comet models, a family of decoder-only transformer models\npretrained on 118 million patients representing 115 billion discrete medical\nevents (151 billion tokens). We present the largest scaling-law study of\nmedical event data, establishing a methodology for pretraining and revealing\npower-law scaling relationships for compute, tokens, and model size.\nConsequently, we pretrained a series of compute-optimal models with up to 1\nbillion parameters. Conditioned on a patient's real-world history, Comet\nautoregressively predicts the next medical event to simulate patient health\ntimelines. We studied 78 real-world tasks, including diagnosis prediction,\ndisease prognosis, and healthcare operations. Remarkably for a foundation model\nwith generic pretraining and simulation-based inference, Comet generally\noutperformed or matched task-specific supervised models on these tasks, without\nrequiring task-specific fine-tuning or few-shot examples. Comet's predictive\npower consistently improves as the model and pretraining scale. Our results\nshow that Comet, a generative medical event foundation model, can effectively\ncapture complex clinical dynamics, providing an extensible and generalizable\nframework to support clinical decision-making, streamline healthcare\noperations, and improve patient outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realizing personalized medicine at scale calls for methods that distill\ninsights from longitudinal patient journeys, which can be viewed as a sequence\nof medical events. Foundation models pretrained on large-scale medical event\ndata represent a promising direction for scaling real-world evidence generation\nand generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with\nmedical events from de-identified longitudinal health records for 16.3 billion\nencounters over 300 million unique patient records from 310 health systems, we\nintroduce the Comet models, a family of decoder-only transformer models\npretrained on 118 million patients representing 115 billion discrete medical\nevents (151 billion tokens). We present the largest scaling-law study of\nmedical event data, establishing a methodology for pretraining and revealing\npower-law scaling relationships for compute, tokens, and model size.\nConsequently, we pretrained a series of compute-optimal models with up to 1\nbillion parameters. Conditioned on a patient's real-world history, Comet\nautoregressively predicts the next medical event to simulate patient health\ntimelines. We studied 78 real-world tasks, including diagnosis prediction,\ndisease prognosis, and healthcare operations. Remarkably for a foundation model\nwith generic pretraining and simulation-based inference, Comet generally\noutperformed or matched task-specific supervised models on these tasks, without\nrequiring task-specific fine-tuning or few-shot examples. Comet's predictive\npower consistently improves as the model and pretraining scale. Our results\nshow that Comet, a generative medical event foundation model, can effectively\ncapture complex clinical dynamics, providing an extensible and generalizable\nframework to support clinical decision-making, streamline healthcare\noperations, and improve patient outcomes."
                },
                "authors": [
                    {
                        "name": "Shane Waxler"
                    },
                    {
                        "name": "Paul Blazek"
                    },
                    {
                        "name": "Davis White"
                    },
                    {
                        "name": "Daniel Sneider"
                    },
                    {
                        "name": "Kevin Chung"
                    },
                    {
                        "name": "Mani Nagarathnam"
                    },
                    {
                        "name": "Patrick Williams"
                    },
                    {
                        "name": "Hank Voeller"
                    },
                    {
                        "name": "Karen Wong"
                    },
                    {
                        "name": "Matthew Swanhorst"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Naoto Usuyama"
                    },
                    {
                        "name": "Cliff Wong"
                    },
                    {
                        "name": "Tristan Naumann"
                    },
                    {
                        "name": "Hoifung Poon"
                    },
                    {
                        "name": "Andrew Loza"
                    },
                    {
                        "name": "Daniella Meeker"
                    },
                    {
                        "name": "Seth Hain"
                    },
                    {
                        "name": "Rahul Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Shah"
                },
                "author": "Rahul Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18057v2",
                "updated": "2025-09-23T17:34:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    34,
                    30,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T17:30:33Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    33,
                    0,
                    265,
                    0
                ],
                "title": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory"
                },
                "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs."
                },
                "authors": [
                    {
                        "name": "Ansh Nagda"
                    },
                    {
                        "name": "Prabhakar Raghavan"
                    },
                    {
                        "name": "Abhradeep Thakurta"
                    }
                ],
                "author_detail": {
                    "name": "Abhradeep Thakurta"
                },
                "author": "Abhradeep Thakurta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18058v2",
                "updated": "2025-09-23T17:34:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    34,
                    27,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T17:30:56Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    56,
                    0,
                    265,
                    0
                ],
                "title": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier\n  LLMs"
                },
                "summary": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are crafted to be subtly incorrect or otherwise harmless in\npractice. This behavior emerges with hard-to-predict variations even within\nmodels from the same model family. We find no apparent cause for the propensity\nto deceive, but show that more capable models are better at executing this\nstrategy. Strategic dishonesty already has a practical impact on safety\nevaluations, as we show that dishonest responses fool all output-based monitors\nused to detect jailbreaks that we test, rendering benchmark scores unreliable.\nFurther, strategic dishonesty can act like a honeypot against malicious users,\nwhich noticeably obfuscates prior jailbreak attacks. While output monitors\nfail, we show that linear probes on internal activations can be used to\nreliably detect strategic dishonesty. We validate probes on datasets with\nverifiable outcomes and by using them as steering vectors. Overall, we consider\nstrategic dishonesty as a concrete example of a broader concern that alignment\nof LLMs is hard to control, especially when helpfulness and harmlessness\nconflict.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are crafted to be subtly incorrect or otherwise harmless in\npractice. This behavior emerges with hard-to-predict variations even within\nmodels from the same model family. We find no apparent cause for the propensity\nto deceive, but show that more capable models are better at executing this\nstrategy. Strategic dishonesty already has a practical impact on safety\nevaluations, as we show that dishonest responses fool all output-based monitors\nused to detect jailbreaks that we test, rendering benchmark scores unreliable.\nFurther, strategic dishonesty can act like a honeypot against malicious users,\nwhich noticeably obfuscates prior jailbreak attacks. While output monitors\nfail, we show that linear probes on internal activations can be used to\nreliably detect strategic dishonesty. We validate probes on datasets with\nverifiable outcomes and by using them as steering vectors. Overall, we consider\nstrategic dishonesty as a concrete example of a broader concern that alignment\nof LLMs is hard to control, especially when helpfulness and harmlessness\nconflict."
                },
                "authors": [
                    {
                        "name": "Alexander Panfilov"
                    },
                    {
                        "name": "Evgenii Kortukov"
                    },
                    {
                        "name": "Kristina NikoliÄ"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19269v1",
                "updated": "2025-09-23T17:33:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    33,
                    30,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:33:30Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    33,
                    30,
                    1,
                    266,
                    0
                ],
                "title": "Extracting Conceptual Spaces from LLMs Using Prototype Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Conceptual Spaces from LLMs Using Prototype Embeddings"
                },
                "summary": "Conceptual spaces represent entities and concepts using cognitively\nmeaningful dimensions, typically referring to perceptual features. Such\nrepresentations are widely used in cognitive science and have the potential to\nserve as a cornerstone for explainable AI. Unfortunately, they have proven\nnotoriously difficult to learn, although recent LLMs appear to capture the\nrequired perceptual features to a remarkable extent. Nonetheless, practical\nmethods for extracting the corresponding conceptual spaces are currently still\nlacking. While various methods exist for extracting embeddings from LLMs,\nextracting conceptual spaces also requires us to encode the underlying\nfeatures. In this paper, we propose a strategy in which features (e.g.\nsweetness) are encoded by embedding the description of a corresponding\nprototype (e.g. a very sweet food). To improve this strategy, we fine-tune the\nLLM to align the prototype embeddings with the corresponding conceptual space\ndimensions. Our empirical analysis finds this approach to be highly effective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conceptual spaces represent entities and concepts using cognitively\nmeaningful dimensions, typically referring to perceptual features. Such\nrepresentations are widely used in cognitive science and have the potential to\nserve as a cornerstone for explainable AI. Unfortunately, they have proven\nnotoriously difficult to learn, although recent LLMs appear to capture the\nrequired perceptual features to a remarkable extent. Nonetheless, practical\nmethods for extracting the corresponding conceptual spaces are currently still\nlacking. While various methods exist for extracting embeddings from LLMs,\nextracting conceptual spaces also requires us to encode the underlying\nfeatures. In this paper, we propose a strategy in which features (e.g.\nsweetness) are encoded by embedding the description of a corresponding\nprototype (e.g. a very sweet food). To improve this strategy, we fine-tune the\nLLM to align the prototype embeddings with the corresponding conceptual space\ndimensions. Our empirical analysis finds this approach to be highly effective."
                },
                "authors": [
                    {
                        "name": "Nitesh Kumar"
                    },
                    {
                        "name": "Usashi Chatterjee"
                    },
                    {
                        "name": "Steven Schockaert"
                    }
                ],
                "author_detail": {
                    "name": "Steven Schockaert"
                },
                "author": "Steven Schockaert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12613v3",
                "updated": "2025-09-23T17:26:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    26,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2024-10-16T14:29:29Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    29,
                    29,
                    2,
                    290,
                    0
                ],
                "title": "Exploring Model Kinship for Merging Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Model Kinship for Merging Large Language Models"
                },
                "summary": "Model merging has emerged as a key technique for enhancing the capabilities\nand efficiency of Large Language Models (LLMs). The open-source community has\ndriven model evolution by iteratively merging existing models, yet a principled\nunderstanding of the gains and underlying factors in model merging remains\nlimited. In this work, we study model evolution through iterative merging,\ndrawing an analogy to biological evolution, and introduce the concept of model\nkinship, the degree of similarity or relatedness between LLMs. Through\ncomprehensive empirical analysis, we show that model kinship is closely linked\nto the performance improvements achieved by merging, providing a useful\ncriterion for selecting candidate models. Building on this insight, we propose\na new model merging strategy: Top-k Greedy Merging with Model Kinship, which\ncan improve benchmark performance. Specifically, we discover that incorporating\nmodel kinship as a guiding criterion enables continuous merging while\nmitigating performance degradation caused by local optima, thereby facilitating\nmore effective model evolution. Code is available at\nhttps://github.com/zjunlp/ModelKinship.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging has emerged as a key technique for enhancing the capabilities\nand efficiency of Large Language Models (LLMs). The open-source community has\ndriven model evolution by iteratively merging existing models, yet a principled\nunderstanding of the gains and underlying factors in model merging remains\nlimited. In this work, we study model evolution through iterative merging,\ndrawing an analogy to biological evolution, and introduce the concept of model\nkinship, the degree of similarity or relatedness between LLMs. Through\ncomprehensive empirical analysis, we show that model kinship is closely linked\nto the performance improvements achieved by merging, providing a useful\ncriterion for selecting candidate models. Building on this insight, we propose\na new model merging strategy: Top-k Greedy Merging with Model Kinship, which\ncan improve benchmark performance. Specifically, we discover that incorporating\nmodel kinship as a guiding criterion enables continuous merging while\nmitigating performance degradation caused by local optima, thereby facilitating\nmore effective model evolution. Code is available at\nhttps://github.com/zjunlp/ModelKinship."
                },
                "authors": [
                    {
                        "name": "Yedi Hu"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Shumin Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shumin Deng"
                },
                "author": "Shumin Deng",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15044v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15044v3",
                "updated": "2025-09-23T17:25:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    25,
                    59,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-20T20:10:56Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    10,
                    56,
                    2,
                    232,
                    0
                ],
                "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time\n  Weak-to-Strong Aligner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time\n  Weak-to-Strong Aligner"
                },
                "summary": "Aligning large language models (LLMs) with human preferences has become a\ncritical step in their development. Recent research has increasingly focused on\ntest-time alignment, where additional compute is allocated during inference to\nenhance LLM safety and reasoning capabilities. However, these test-time\nalignment techniques often incur substantial inference costs, limiting their\npractical application. We are inspired by the speculative sampling\nacceleration, which leverages a small draft model to efficiently predict future\ntokens, to address the efficiency bottleneck of test-time alignment. We\nintroduce the reward-shifted speculative sampling (SSS) algorithm, in which the\ndraft model is aligned with human preferences, while the target model remains\nunchanged. We theoretically demonstrate that the distributional shift between\nthe aligned draft model and the unaligned target model can be exploited to\nrecover the RLHF optimal solution without actually obtaining it, by modifying\nthe acceptance criterion and bonus token distribution. Our algorithm achieves\nsuperior gold reward scores at a significantly reduced inference cost in\ntest-time weak-to-strong alignment experiments, thereby validating both its\neffectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human preferences has become a\ncritical step in their development. Recent research has increasingly focused on\ntest-time alignment, where additional compute is allocated during inference to\nenhance LLM safety and reasoning capabilities. However, these test-time\nalignment techniques often incur substantial inference costs, limiting their\npractical application. We are inspired by the speculative sampling\nacceleration, which leverages a small draft model to efficiently predict future\ntokens, to address the efficiency bottleneck of test-time alignment. We\nintroduce the reward-shifted speculative sampling (SSS) algorithm, in which the\ndraft model is aligned with human preferences, while the target model remains\nunchanged. We theoretically demonstrate that the distributional shift between\nthe aligned draft model and the unaligned target model can be exploited to\nrecover the RLHF optimal solution without actually obtaining it, by modifying\nthe acceptance criterion and bonus token distribution. Our algorithm achieves\nsuperior gold reward scores at a significantly reduced inference cost in\ntest-time weak-to-strong alignment experiments, thereby validating both its\neffectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Bolian Li"
                    },
                    {
                        "name": "Yanran Wu"
                    },
                    {
                        "name": "Xinyu Luo"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15044v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15044v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15433v2",
                "updated": "2025-09-23T17:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    24,
                    40,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-18T21:15:20Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    21,
                    15,
                    20,
                    3,
                    261,
                    0
                ],
                "title": "LLM-Driven SAST-Genius: A Hybrid Static Analysis Framework for\n  Comprehensive and Actionable Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven SAST-Genius: A Hybrid Static Analysis Framework for\n  Comprehensive and Actionable Security"
                },
                "summary": "This report examines the synergy between Large Language Models (LLMs) and\nStatic Application Security Testing (SAST) to improve vulnerability discovery.\nTraditional SAST tools, while effective for proactive security, are limited by\nhigh false-positive rates and a lack of contextual understanding. Conversely,\nLLMs excel at code analysis and pattern recognition but can be prone to\ninconsistencies and hallucinations. By integrating these two technologies, a\nmore intelligent and efficient system is created. This combination moves beyond\nmere vulnerability detection optimization, transforming security into a deeply\nintegrated, contextual process that provides tangible benefits like improved\ntriage, dynamic bug descriptions, bug validation via exploit generation and\nenhanced analysis of complex codebases. The result is a more effective security\napproach that leverages the strengths of both technologies while mitigating\ntheir weaknesses. SAST-Genius reduced false positives by about 91 % (225 to 20)\ncompared to Semgrep alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report examines the synergy between Large Language Models (LLMs) and\nStatic Application Security Testing (SAST) to improve vulnerability discovery.\nTraditional SAST tools, while effective for proactive security, are limited by\nhigh false-positive rates and a lack of contextual understanding. Conversely,\nLLMs excel at code analysis and pattern recognition but can be prone to\ninconsistencies and hallucinations. By integrating these two technologies, a\nmore intelligent and efficient system is created. This combination moves beyond\nmere vulnerability detection optimization, transforming security into a deeply\nintegrated, contextual process that provides tangible benefits like improved\ntriage, dynamic bug descriptions, bug validation via exploit generation and\nenhanced analysis of complex codebases. The result is a more effective security\napproach that leverages the strengths of both technologies while mitigating\ntheir weaknesses. SAST-Genius reduced false positives by about 91 % (225 to 20)\ncompared to Semgrep alone."
                },
                "authors": [
                    {
                        "name": "Vaibhav Agrawal"
                    },
                    {
                        "name": "Kiarash Ahi"
                    }
                ],
                "author_detail": {
                    "name": "Kiarash Ahi"
                },
                "author": "Kiarash Ahi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19265v1",
                "updated": "2025-09-23T17:24:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    24,
                    14,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:24:14Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    24,
                    14,
                    1,
                    266,
                    0
                ],
                "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from\n  the Arab World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from\n  the Arab World"
                },
                "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting\ntheir effectiveness in diverse cultural contexts. Although some work has\nexplored cultural alignment, the potential for cross-cultural transfer, using\nalignment in one culture to improve performance in others, remains\nunderexplored. This paper investigates cross-cultural transfer of commonsense\nreasoning in the Arab world, where linguistic and historical similarities\ncoexist with local cultural differences. Using a culturally grounded\ncommonsense reasoning dataset covering 13 Arab countries, we evaluate\nlightweight alignment methods such as in-context learning and\ndemonstration-based reinforcement (DITTO), alongside baselines like supervised\nfine-tuning and direct preference optimization. Our results show that merely 12\nculture-specific examples from one country can improve performance in others by\n10\\% on average, within multilingual models. In addition, we demonstrate that\nout-of-culture demonstrations from Indonesia and US contexts can match or\nsurpass in-culture alignment for MCQ reasoning, highlighting cultural\ncommonsense transferability beyond the Arab world. These findings demonstrate\nthat efficient cross-cultural alignment is possible and offer a promising\napproach to adapt LLMs to low-resource cultural settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often reflect Western-centric biases, limiting\ntheir effectiveness in diverse cultural contexts. Although some work has\nexplored cultural alignment, the potential for cross-cultural transfer, using\nalignment in one culture to improve performance in others, remains\nunderexplored. This paper investigates cross-cultural transfer of commonsense\nreasoning in the Arab world, where linguistic and historical similarities\ncoexist with local cultural differences. Using a culturally grounded\ncommonsense reasoning dataset covering 13 Arab countries, we evaluate\nlightweight alignment methods such as in-context learning and\ndemonstration-based reinforcement (DITTO), alongside baselines like supervised\nfine-tuning and direct preference optimization. Our results show that merely 12\nculture-specific examples from one country can improve performance in others by\n10\\% on average, within multilingual models. In addition, we demonstrate that\nout-of-culture demonstrations from Indonesia and US contexts can match or\nsurpass in-culture alignment for MCQ reasoning, highlighting cultural\ncommonsense transferability beyond the Arab world. These findings demonstrate\nthat efficient cross-cultural alignment is possible and offer a promising\napproach to adapt LLMs to low-resource cultural settings."
                },
                "authors": [
                    {
                        "name": "Saeed Almheiri"
                    },
                    {
                        "name": "Rania Hossam"
                    },
                    {
                        "name": "Mena Attia"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "arxiv_comment": "EMNLP 2025 - Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15589v2",
                "updated": "2025-09-23T17:20:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    20,
                    22,
                    1,
                    266,
                    0
                ],
                "published": "2025-02-21T16:57:22Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    57,
                    22,
                    4,
                    52,
                    0
                ],
                "title": "LightThinker: Thinking Step-by-Step Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightThinker: Thinking Step-by-Step Compression"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code is released at https://github.com/zjunlp/LightThinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code is released at https://github.com/zjunlp/LightThinker."
                },
                "authors": [
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "EMNLP 2025 (oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13425v3",
                "updated": "2025-09-23T17:20:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    20,
                    11,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-16T18:02:23Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    18,
                    2,
                    23,
                    1,
                    259,
                    0
                ],
                "title": "Unified Spatiotemporal Physics-Informed Learning (USPIL): A Framework\n  for Modeling Complex Predator-Prey Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Spatiotemporal Physics-Informed Learning (USPIL): A Framework\n  for Modeling Complex Predator-Prey Dynamics"
                },
                "summary": "Ecological systems exhibit complex multi-scale dynamics that challenge\ntraditional modeling. New methods must capture temporal oscillations and\nemergent spatiotemporal patterns while adhering to conservation principles. We\npresent the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,\na deep learning architecture integrating physics-informed neural networks\n(PINNs) and conservation laws to model predator-prey dynamics across\ndimensional scales. The framework provides a unified solution for both ordinary\n(ODE) and partial (PDE) differential equation systems, describing temporal\ncycles and reaction-diffusion patterns within a single neural network\narchitecture. Our methodology uses automatic differentiation to enforce physics\nconstraints and adaptive loss weighting to balance data fidelity with physical\nconsistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%\ncorrelation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures\ncomplex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).\nValidation confirms conservation law adherence within 0.5% and shows a 10-50x\ncomputational speedup for inference compared to numerical solvers. USPIL also\nenables mechanistic understanding through interpretable physics constraints,\nfacilitating parameter discovery and sensitivity analysis not possible with\npurely data-driven methods. Its ability to transition between dimensional\nformulations opens new avenues for multi-scale ecological modeling. These\ncapabilities make USPIL a transformative tool for ecological forecasting,\nconservation planning, and understanding ecosystem resilience, establishing\nphysics-informed deep learning as a powerful and scientifically rigorous\nparadigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ecological systems exhibit complex multi-scale dynamics that challenge\ntraditional modeling. New methods must capture temporal oscillations and\nemergent spatiotemporal patterns while adhering to conservation principles. We\npresent the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,\na deep learning architecture integrating physics-informed neural networks\n(PINNs) and conservation laws to model predator-prey dynamics across\ndimensional scales. The framework provides a unified solution for both ordinary\n(ODE) and partial (PDE) differential equation systems, describing temporal\ncycles and reaction-diffusion patterns within a single neural network\narchitecture. Our methodology uses automatic differentiation to enforce physics\nconstraints and adaptive loss weighting to balance data fidelity with physical\nconsistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%\ncorrelation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures\ncomplex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).\nValidation confirms conservation law adherence within 0.5% and shows a 10-50x\ncomputational speedup for inference compared to numerical solvers. USPIL also\nenables mechanistic understanding through interpretable physics constraints,\nfacilitating parameter discovery and sensitivity analysis not possible with\npurely data-driven methods. Its ability to transition between dimensional\nformulations opens new avenues for multi-scale ecological modeling. These\ncapabilities make USPIL a transformative tool for ecological forecasting,\nconservation planning, and understanding ecosystem resilience, establishing\nphysics-informed deep learning as a powerful and scientifically rigorous\nparadigm."
                },
                "authors": [
                    {
                        "name": "Julian Evan Chrisnanto"
                    },
                    {
                        "name": "Salsabila Rahma Alia"
                    },
                    {
                        "name": "Yulison Herry Chrisnanto"
                    },
                    {
                        "name": "Ferry Faizal"
                    }
                ],
                "author_detail": {
                    "name": "Ferry Faizal"
                },
                "author": "Ferry Faizal",
                "arxiv_comment": "20 pages, 11 figures. A preprint on using a unified physics-informed\n  neural network framework to model predator-prey dynamics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92D25, 35K57, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; J.3; G.1.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19249v1",
                "updated": "2025-09-23T17:10:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    10,
                    40,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:10:40Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    10,
                    40,
                    1,
                    266,
                    0
                ],
                "title": "Reinforcement Learning on Pre-Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning on Pre-Training Data"
                },
                "summary": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,\n$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,\n$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance."
                },
                "authors": [
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Guanhua Huang"
                    },
                    {
                        "name": "Evander Yang"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Haoyuan Wu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Kun Shi"
                    },
                    {
                        "name": "Kyrierl Deng"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Ruibin Xiong"
                    },
                    {
                        "name": "Tingqiang Xu"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Fei Gao"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Bo Chao Wang"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Wai Lam"
                    },
                    {
                        "name": "Wayyt Wang"
                    },
                    {
                        "name": "Bo Zhou"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16356v2",
                "updated": "2025-09-23T17:10:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    10,
                    14,
                    1,
                    266,
                    0
                ],
                "published": "2025-03-20T17:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    14,
                    34,
                    3,
                    79,
                    0
                ],
                "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners"
                },
                "summary": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they often fail to generalize these updates to multi-hop\nreasoning tasks that rely on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we find that current layer-localized KE approaches (e.g., MEMIT,\nWISE), which edit only single or a few model layers, inadequately integrate\nupdated knowledge into these reasoning pathways. To address this limitation, we\npresent CaKE (Circuit-aware Knowledge Editing), a novel method that enhances\nthe effective integration of updated knowledge in LLMs. By only leveraging a\nfew curated data samples guided by our circuit-based analysis, CaKE stimulates\nthe model to develop appropriate reasoning circuits for newly incorporated\nknowledge. Experiments show that CaKE enables more accurate and consistent use\nof edited knowledge across related reasoning tasks, achieving an average\nimprovement of 20% in multi-hop reasoning accuracy on the MQuAKE dataset while\nrequiring less memory than existing KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they often fail to generalize these updates to multi-hop\nreasoning tasks that rely on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we find that current layer-localized KE approaches (e.g., MEMIT,\nWISE), which edit only single or a few model layers, inadequately integrate\nupdated knowledge into these reasoning pathways. To address this limitation, we\npresent CaKE (Circuit-aware Knowledge Editing), a novel method that enhances\nthe effective integration of updated knowledge in LLMs. By only leveraging a\nfew curated data samples guided by our circuit-based analysis, CaKE stimulates\nthe model to develop appropriate reasoning circuits for newly incorporated\nknowledge. Experiments show that CaKE enables more accurate and consistent use\nof edited knowledge across related reasoning tasks, achieving an average\nimprovement of 20% in multi-hop reasoning accuracy on the MQuAKE dataset while\nrequiring less memory than existing KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE."
                },
                "authors": [
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Jizhan Fang"
                    },
                    {
                        "name": "Jia-Chen Gu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19247v1",
                "updated": "2025-09-23T17:08:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    8,
                    30,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:08:30Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    8,
                    30,
                    1,
                    266,
                    0
                ],
                "title": "Exploring the Sub-Neptune Frontier with JWST",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Sub-Neptune Frontier with JWST"
                },
                "summary": "Sub-Neptune planets, with sizes and masses between those of Earth and\nNeptune, dominate the exoplanet population. Sub-Neptunes are expected to be the\nmost diverse family of the exoplanet population, potentially including rocky\ngas dwarfs, water worlds, and mini-Neptunes, with a wide range of atmospheric,\nsurface and interior conditions. With no analogue in the solar system, these\nplanets open fundamental questions in planetary processes, origins, and\nhabitability, and present new avenues in the search for life elsewhere.\nAtmospheric observations with the James Webb Space Telescope (JWST) are\nenabling unprecedented characterization of sub-Neptunes, starting with the\nfirst detections of carbon-bearing molecules in the habitable zone sub-Neptune\nK2-18 b. We survey the present landscape of JWST observations and atmospheric\ninferences of sub-Neptunes, which in turn provide key insights into their\natmospheric processes, internal structures, surface conditions, formation\npathways and potential habitability. The atmospheric abundance constraints\nreveal evidence of chemical disequilibria, and insights into the planetary\nmass-metallicity relation in the sub-Neptune regime. Similarly, for\nsub-Neptunes with H$_2$O-rich interiors, increasing atmospheric H$_2$O\nabundances with the equilibrium temperature may indicate the existence of a\ncritical temperature for transition from H$_2$ dominated atmospheres with\ntropospheric cold traps to those with steamy atmospheres. The chemical\nabundances also provide initial evidence for diverse planet types, from\npotentially habitable hycean worlds to steam worlds with super critical water\nlayers. These planet types serve as benchmarks for an emerging taxonomy of\nvolatile-rich sub-Neptunes as a function of their equilibrium temperature and\natmospheric extent, heralding a new era of chemical classification of low-mass\nexoplanets with JWST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-Neptune planets, with sizes and masses between those of Earth and\nNeptune, dominate the exoplanet population. Sub-Neptunes are expected to be the\nmost diverse family of the exoplanet population, potentially including rocky\ngas dwarfs, water worlds, and mini-Neptunes, with a wide range of atmospheric,\nsurface and interior conditions. With no analogue in the solar system, these\nplanets open fundamental questions in planetary processes, origins, and\nhabitability, and present new avenues in the search for life elsewhere.\nAtmospheric observations with the James Webb Space Telescope (JWST) are\nenabling unprecedented characterization of sub-Neptunes, starting with the\nfirst detections of carbon-bearing molecules in the habitable zone sub-Neptune\nK2-18 b. We survey the present landscape of JWST observations and atmospheric\ninferences of sub-Neptunes, which in turn provide key insights into their\natmospheric processes, internal structures, surface conditions, formation\npathways and potential habitability. The atmospheric abundance constraints\nreveal evidence of chemical disequilibria, and insights into the planetary\nmass-metallicity relation in the sub-Neptune regime. Similarly, for\nsub-Neptunes with H$_2$O-rich interiors, increasing atmospheric H$_2$O\nabundances with the equilibrium temperature may indicate the existence of a\ncritical temperature for transition from H$_2$ dominated atmospheres with\ntropospheric cold traps to those with steamy atmospheres. The chemical\nabundances also provide initial evidence for diverse planet types, from\npotentially habitable hycean worlds to steam worlds with super critical water\nlayers. These planet types serve as benchmarks for an emerging taxonomy of\nvolatile-rich sub-Neptunes as a function of their equilibrium temperature and\natmospheric extent, heralding a new era of chemical classification of low-mass\nexoplanets with JWST."
                },
                "authors": [
                    {
                        "name": "Nikku Madhusudhan"
                    },
                    {
                        "name": "MÃ¥ns Holmberg"
                    },
                    {
                        "name": "Savvas Constantinou"
                    },
                    {
                        "name": "Gregory J. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "Gregory J. Cooke"
                },
                "author": "Gregory J. Cooke",
                "arxiv_comment": "Accepted for publication in PNAS special feature on Characterization\n  of Exoplanets in the JWST Era",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11341v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11341v3",
                "updated": "2025-09-23T17:07:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    7,
                    48,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-16T15:08:04Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    15,
                    8,
                    4,
                    4,
                    136,
                    0
                ],
                "title": "Benchmarking Critical Questions Generation: A Challenging Reasoning Task\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Critical Questions Generation: A Challenging Reasoning Task\n  for Large Language Models"
                },
                "summary": "The task of Critical Questions Generation (CQs-Gen) aims to foster critical\nthinking by enabling systems to generate questions that expose underlying\nassumptions and challenge the validity of argumentative reasoning structures.\nDespite growing interest in this area, progress has been hindered by the lack\nof suitable datasets and automatic evaluation standards. This paper presents a\ncomprehensive approach to support the development and benchmarking of systems\nfor this task. We construct the first large-scale dataset including ~5K\nmanually annotated questions. We also investigate automatic evaluation methods\nand propose reference-based techniques as the strategy that best correlates\nwith human judgments. Our zero-shot evaluation of 11 LLMs establishes a strong\nbaseline while showcasing the difficulty of the task. Data and code plus a\npublic leaderboard are provided to encourage further research, not only in\nterms of model performance, but also to explore the practical benefits of\nCQs-Gen for both automated reasoning and human critical thinking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of Critical Questions Generation (CQs-Gen) aims to foster critical\nthinking by enabling systems to generate questions that expose underlying\nassumptions and challenge the validity of argumentative reasoning structures.\nDespite growing interest in this area, progress has been hindered by the lack\nof suitable datasets and automatic evaluation standards. This paper presents a\ncomprehensive approach to support the development and benchmarking of systems\nfor this task. We construct the first large-scale dataset including ~5K\nmanually annotated questions. We also investigate automatic evaluation methods\nand propose reference-based techniques as the strategy that best correlates\nwith human judgments. Our zero-shot evaluation of 11 LLMs establishes a strong\nbaseline while showcasing the difficulty of the task. Data and code plus a\npublic leaderboard are provided to encourage further research, not only in\nterms of model performance, but also to explore the practical benefits of\nCQs-Gen for both automated reasoning and human critical thinking."
                },
                "authors": [
                    {
                        "name": "Banca Calvo Figueras"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11341v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11341v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19244v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19244v2",
                "updated": "2025-09-24T09:38:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    9,
                    38,
                    15,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-23T17:05:46Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    5,
                    46,
                    1,
                    266,
                    0
                ],
                "title": "Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal\n  Understanding and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal\n  Understanding and Generation"
                },
                "summary": "We propose Lavida-O, a unified Masked Diffusion Model (MDM) for multimodal\nunderstanding and generation. Unlike existing multimodal MDMs such as MMaDa and\nMuddit which only support simple image-level understanding tasks and\nlow-resolution image generation, Lavida-O presents a single framework that\nenables image-level understanding, object grounding, image editing, and\nhigh-resolution (1024px) text-to-image synthesis. Lavida-O incorporates a novel\nElastic Mixture-of-Transformers (Elastic-MoT) architecture that couples a\nlightweight generation branch with a larger understanding branch, supported by\ntoken compression, universal text conditioning and stratified sampling for\nefficient and high-quality generation. Lavida-O further incorporates planning\nand iterative self-reflection in image generation and editing tasks, seamlessly\nboosting generation quality with its understanding capabilities. Lavida-O\nachieves state-of-the-art performance on a wide range of benchmarks including\nRefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image\nediting, outperforming existing autoregressive models and continuous diffusion\nmodels such as Qwen2.5-VL and FluxKontext-dev, while offering considerable\nspeedup at inference. These advances establish Lavida-O as a new paradigm for\nscalable multimodal reasoning and generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lavida-O, a unified Masked Diffusion Model (MDM) for multimodal\nunderstanding and generation. Unlike existing multimodal MDMs such as MMaDa and\nMuddit which only support simple image-level understanding tasks and\nlow-resolution image generation, Lavida-O presents a single framework that\nenables image-level understanding, object grounding, image editing, and\nhigh-resolution (1024px) text-to-image synthesis. Lavida-O incorporates a novel\nElastic Mixture-of-Transformers (Elastic-MoT) architecture that couples a\nlightweight generation branch with a larger understanding branch, supported by\ntoken compression, universal text conditioning and stratified sampling for\nefficient and high-quality generation. Lavida-O further incorporates planning\nand iterative self-reflection in image generation and editing tasks, seamlessly\nboosting generation quality with its understanding capabilities. Lavida-O\nachieves state-of-the-art performance on a wide range of benchmarks including\nRefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image\nediting, outperforming existing autoregressive models and continuous diffusion\nmodels such as Qwen2.5-VL and FluxKontext-dev, while offering considerable\nspeedup at inference. These advances establish Lavida-O as a new paradigm for\nscalable multimodal reasoning and generation."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Kangning Liu"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Zijun Wei"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Jason Kuen"
                    }
                ],
                "author_detail": {
                    "name": "Jason Kuen"
                },
                "author": "Jason Kuen",
                "arxiv_comment": "31 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19244v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19244v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08727v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08727v3",
                "updated": "2025-09-23T17:04:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    4,
                    48,
                    1,
                    266,
                    0
                ],
                "published": "2025-04-11T17:55:45Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    55,
                    45,
                    4,
                    101,
                    0
                ],
                "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images"
                },
                "summary": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles."
                },
                "authors": [
                    {
                        "name": "Boyang Deng"
                    },
                    {
                        "name": "Songyou Peng"
                    },
                    {
                        "name": "Kyle Genova"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Noah Snavely"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Thomas Funkhouser"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Funkhouser"
                },
                "author": "Thomas Funkhouser",
                "arxiv_comment": "ICCV 2025, Project page: https://boyangdeng.com/visual-chronicles ,\n  second and third listed authors have equal contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08727v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08727v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19041v2",
                "updated": "2025-09-23T17:04:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    4,
                    18,
                    1,
                    266,
                    0
                ],
                "published": "2025-03-24T18:11:42Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    11,
                    42,
                    0,
                    83,
                    0
                ],
                "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LookAhead Tuning: Safer Language Models via Partial Answer Previews"
                },
                "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often compromises their previously established safety alignment.\nTo mitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, a lightweight and effective data-driven approach that\npreserves safety during fine-tuning. The method introduces two simple\nstrategies that modify training data by previewing partial answer prefixes,\nthereby minimizing perturbations to the model's initial token distributions and\nmaintaining its built-in safety mechanisms. Comprehensive experiments\ndemonstrate that LookAhead Tuning effectively maintains model safety without\nsacrificing robust performance on downstream tasks. Our findings position\nLookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often compromises their previously established safety alignment.\nTo mitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, a lightweight and effective data-driven approach that\npreserves safety during fine-tuning. The method introduces two simple\nstrategies that modify training data by previewing partial answer prefixes,\nthereby minimizing perturbations to the model's initial token distributions and\nmaintaining its built-in safety mechanisms. Comprehensive experiments\ndemonstrate that LookAhead Tuning effectively maintains model safety without\nsacrificing robust performance on downstream tasks. Our findings position\nLookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs."
                },
                "authors": [
                    {
                        "name": "Kangwei Liu"
                    },
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Yuan Lin"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Shumin Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shumin Deng"
                },
                "author": "Shumin Deng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19236v1",
                "updated": "2025-09-23T16:58:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    58,
                    54,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:58:54Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    58,
                    54,
                    1,
                    266,
                    0
                ],
                "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and\n  Expertise Orchestration for Effective and Efficient Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and\n  Expertise Orchestration for Effective and Efficient Collaboration"
                },
                "summary": "Proper initialization is crucial for any system, particularly in multi-agent\nsystems (MAS), where it plays a pivotal role in determining both the system's\nefficiency and effectiveness. However, existing MAS initialization methods do\nnot fully account for the collaborative needs of the generated agents in\nsubsequent stages. Inspired by the principles of effective team composition, we\npropose AgentInit, which aims to optimize the structure of agent teams.\nSpecifically, in addition to multi-round interactions and reflections between\nagents during agent generation, AgentInit incorporates a Natural Language to\nFormat mechanism to ensure consistency and standardization. Balanced team\nselection strategies using Pareto principles are subsequently applied to\njointly consider agent team diversity and task relevance to promote effective\nand efficient collaboration and enhance overall system performance. Experiments\nshow that AgentInit consistently outperforms state-of-the-art initialization\nmethods and pre-defined strategies across various frameworks and tasks,\nachieving an overall performance improvement of up to 1.2 and 1.6,\nrespectively, while also significantly reducing token consumption. Further\nanalysis confirms its strong transferability to similar tasks and verifies the\neffectiveness of its key components, demonstrating its capability and\nadaptability as a reliable MAS initialization method. Source code and models\nare available at https://github.com/1737423697/AgentInit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proper initialization is crucial for any system, particularly in multi-agent\nsystems (MAS), where it plays a pivotal role in determining both the system's\nefficiency and effectiveness. However, existing MAS initialization methods do\nnot fully account for the collaborative needs of the generated agents in\nsubsequent stages. Inspired by the principles of effective team composition, we\npropose AgentInit, which aims to optimize the structure of agent teams.\nSpecifically, in addition to multi-round interactions and reflections between\nagents during agent generation, AgentInit incorporates a Natural Language to\nFormat mechanism to ensure consistency and standardization. Balanced team\nselection strategies using Pareto principles are subsequently applied to\njointly consider agent team diversity and task relevance to promote effective\nand efficient collaboration and enhance overall system performance. Experiments\nshow that AgentInit consistently outperforms state-of-the-art initialization\nmethods and pre-defined strategies across various frameworks and tasks,\nachieving an overall performance improvement of up to 1.2 and 1.6,\nrespectively, while also significantly reducing token consumption. Further\nanalysis confirms its strong transferability to similar tasks and verifies the\neffectiveness of its key components, demonstrating its capability and\nadaptability as a reliable MAS initialization method. Source code and models\nare available at https://github.com/1737423697/AgentInit."
                },
                "authors": [
                    {
                        "name": "Chunhao Tian"
                    },
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Zhexuan Wang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19228v1",
                "updated": "2025-09-23T16:49:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:49:43Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "title": "CompLLM: Compression for Long Context Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompLLM: Compression for Long Context Q&A"
                },
                "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility."
                },
                "authors": [
                    {
                        "name": "Gabriele Berton"
                    },
                    {
                        "name": "Jayakrishnan Unnikrishnan"
                    },
                    {
                        "name": "Son Tran"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05613v3",
                "updated": "2025-09-23T16:48:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    48,
                    10,
                    1,
                    266,
                    0
                ],
                "published": "2025-03-07T17:38:00Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    38,
                    0,
                    4,
                    66,
                    0
                ],
                "title": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have transformed natural language processing,\nyet their internal mechanisms remain largely opaque. Recently, mechanistic\ninterpretability has attracted significant attention from the research\ncommunity as a means to understand the inner workings of LLMs. Among various\nmechanistic interpretability approaches, Sparse Autoencoders (SAEs) have\nemerged as a promising method due to their ability to disentangle the complex,\nsuperimposed features within LLMs into more interpretable components. This\npaper presents a comprehensive survey of SAEs for interpreting and\nunderstanding the internal workings of LLMs. Our major contributions include:\n(1) exploring the technical framework of SAEs, covering basic architecture,\ndesign improvements, and effective training strategies; (2) examining different\napproaches to explaining SAE features, categorized into input-based and\noutput-based explanation methods; (3) discussing evaluation methods for\nassessing SAE performance, covering both structural and functional metrics; and\n(4) investigating real-world applications of SAEs in understanding and\nmanipulating LLM behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed natural language processing,\nyet their internal mechanisms remain largely opaque. Recently, mechanistic\ninterpretability has attracted significant attention from the research\ncommunity as a means to understand the inner workings of LLMs. Among various\nmechanistic interpretability approaches, Sparse Autoencoders (SAEs) have\nemerged as a promising method due to their ability to disentangle the complex,\nsuperimposed features within LLMs into more interpretable components. This\npaper presents a comprehensive survey of SAEs for interpreting and\nunderstanding the internal workings of LLMs. Our major contributions include:\n(1) exploring the technical framework of SAEs, covering basic architecture,\ndesign improvements, and effective training strategies; (2) examining different\napproaches to explaining SAE features, categorized into input-based and\noutput-based explanation methods; (3) discussing evaluation methods for\nassessing SAE performance, covering both structural and functional metrics; and\n(4) investigating real-world applications of SAEs in understanding and\nmanipulating LLM behaviors."
                },
                "authors": [
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Daking Rai"
                    },
                    {
                        "name": "Ziyu Yao"
                    },
                    {
                        "name": "Ninghao Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08080v2",
                "updated": "2025-09-23T16:43:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    43,
                    51,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-12T21:29:12Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    21,
                    29,
                    12,
                    0,
                    132,
                    0
                ],
                "title": "Beyond Input Activations: Identifying Influential Latents by Gradient\n  Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Input Activations: Identifying Influential Latents by Gradient\n  Sparse Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) have recently emerged as powerful tools for\ninterpreting and steering the internal representations of large language models\n(LLMs). However, conventional approaches to analyzing SAEs typically rely\nsolely on input-side activations, without considering the causal influence\nbetween each latent feature and the model's output. This work is built on two\nkey hypotheses: (1) activated latents do not contribute equally to the\nconstruction of the model's output, and (2) only latents with high causal\ninfluence are effective for model steering. To validate these hypotheses, we\npropose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method\nthat identifies the most influential latents by incorporating output-side\ngradient information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have recently emerged as powerful tools for\ninterpreting and steering the internal representations of large language models\n(LLMs). However, conventional approaches to analyzing SAEs typically rely\nsolely on input-side activations, without considering the causal influence\nbetween each latent feature and the model's output. This work is built on two\nkey hypotheses: (1) activated latents do not contribute equally to the\nconstruction of the model's output, and (2) only latents with high causal\ninfluence are effective for model steering. To validate these hypotheses, we\npropose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method\nthat identifies the most influential latents by incorporating output-side\ngradient information."
                },
                "authors": [
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Ninghao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ninghao Liu"
                },
                "author": "Ninghao Liu",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01346v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01346v3",
                "updated": "2025-09-23T16:40:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    40,
                    27,
                    1,
                    266,
                    0
                ],
                "published": "2025-01-02T16:53:50Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    53,
                    50,
                    3,
                    2,
                    0
                ],
                "title": "Large Vision-Language Model Alignment and Misalignment: A Survey Through\n  the Lens of Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Model Alignment and Misalignment: A Survey Through\n  the Lens of Explainability"
                },
                "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in processing both visual and textual information. However, the\ncritical challenge of alignment between visual and textual representations is\nnot fully understood. This survey presents a comprehensive examination of\nalignment and misalignment in LVLMs through an explainability lens. We first\nexamine the fundamentals of alignment, exploring its representational and\nbehavioral aspects, training methodologies, and theoretical foundations. We\nthen analyze misalignment phenomena across three semantic levels: object,\nattribute, and relational misalignment. Our investigation reveals that\nmisalignment emerges from challenges at multiple levels: the data level, the\nmodel level, and the inference level. We provide a comprehensive review of\nexisting mitigation strategies, categorizing them into parameter-frozen and\nparameter-tuning approaches. Finally, we outline promising future research\ndirections, emphasizing the need for standardized evaluation protocols and\nin-depth explainability studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in processing both visual and textual information. However, the\ncritical challenge of alignment between visual and textual representations is\nnot fully understood. This survey presents a comprehensive examination of\nalignment and misalignment in LVLMs through an explainability lens. We first\nexamine the fundamentals of alignment, exploring its representational and\nbehavioral aspects, training methodologies, and theoretical foundations. We\nthen analyze misalignment phenomena across three semantic levels: object,\nattribute, and relational misalignment. Our investigation reveals that\nmisalignment emerges from challenges at multiple levels: the data level, the\nmodel level, and the inference level. We provide a comprehensive review of\nexisting mitigation strategies, categorizing them into parameter-frozen and\nparameter-tuning approaches. Finally, we outline promising future research\ndirections, emphasizing the need for standardized evaluation protocols and\nin-depth explainability studies."
                },
                "authors": [
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Jingyu Hu"
                    },
                    {
                        "name": "Weiru Liu"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01346v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01346v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04906v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04906v6",
                "updated": "2025-09-23T16:40:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    40,
                    14,
                    1,
                    266,
                    0
                ],
                "published": "2024-02-07T14:35:25Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    14,
                    35,
                    25,
                    2,
                    38,
                    0
                ],
                "title": "Conformal Convolution and Monte Carlo Meta-learners for Predictive\n  Inference of Individual Treatment Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Convolution and Monte Carlo Meta-learners for Predictive\n  Inference of Individual Treatment Effects"
                },
                "summary": "Generating probabilistic forecasts of potential outcomes and individual\ntreatment effects (ITE) is essential for risk-aware decision-making in domains\nsuch as healthcare, policy, marketing, and finance. We propose two novel\nmethods: the conformal convolution T-learner (CCT) and the conformal Monte\nCarlo (CMC) meta-learner, that generate full predictive distributions of both\npotential outcomes and ITEs. Our approaches combine weighted conformal\npredictive systems with either analytic convolution of potential outcome\ndistributions or Monte Carlo sampling, addressing covariate shift through\npropensity score weighting. In contrast to other approaches that allow the\ngeneration of potential outcome predictive distributions, our approaches are\nmodel agnostic, universal, and come with finite-sample guarantees of\nprobabilistic calibration under knowledge of the propensity score. Regarding\nestimating the ITE distribution, we formally characterize how assumptions about\npotential outcomes' noise dependency impact distribution validity and establish\nuniversal consistency under independence noise assumptions. Experiments on\nsynthetic and semi-synthetic datasets demonstrate that the proposed methods\nachieve probabilistically calibrated predictive distributions while maintaining\nnarrow prediction intervals and having performant continuous ranked probability\nscores. Besides probabilistic forecasting performance, we observe significant\nefficiency gains for the CCT- and CMC meta-learners compared to other conformal\napproaches that produce prediction intervals for ITE with coverage guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating probabilistic forecasts of potential outcomes and individual\ntreatment effects (ITE) is essential for risk-aware decision-making in domains\nsuch as healthcare, policy, marketing, and finance. We propose two novel\nmethods: the conformal convolution T-learner (CCT) and the conformal Monte\nCarlo (CMC) meta-learner, that generate full predictive distributions of both\npotential outcomes and ITEs. Our approaches combine weighted conformal\npredictive systems with either analytic convolution of potential outcome\ndistributions or Monte Carlo sampling, addressing covariate shift through\npropensity score weighting. In contrast to other approaches that allow the\ngeneration of potential outcome predictive distributions, our approaches are\nmodel agnostic, universal, and come with finite-sample guarantees of\nprobabilistic calibration under knowledge of the propensity score. Regarding\nestimating the ITE distribution, we formally characterize how assumptions about\npotential outcomes' noise dependency impact distribution validity and establish\nuniversal consistency under independence noise assumptions. Experiments on\nsynthetic and semi-synthetic datasets demonstrate that the proposed methods\nachieve probabilistically calibrated predictive distributions while maintaining\nnarrow prediction intervals and having performant continuous ranked probability\nscores. Besides probabilistic forecasting performance, we observe significant\nefficiency gains for the CCT- and CMC meta-learners compared to other conformal\napproaches that produce prediction intervals for ITE with coverage guarantees."
                },
                "authors": [
                    {
                        "name": "Jef Jonkers"
                    },
                    {
                        "name": "Jarne Verhaeghe"
                    },
                    {
                        "name": "Glenn Van Wallendael"
                    },
                    {
                        "name": "Luc Duchateau"
                    },
                    {
                        "name": "Sofie Van Hoecke"
                    }
                ],
                "author_detail": {
                    "name": "Sofie Van Hoecke"
                },
                "author": "Sofie Van Hoecke",
                "arxiv_comment": "Major update (rescope to distributional regression in counterfactual\n  inference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04906v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04906v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12642v2",
                "updated": "2025-09-23T16:36:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    36,
                    24,
                    1,
                    266,
                    0
                ],
                "published": "2025-07-16T21:27:31Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    21,
                    27,
                    31,
                    2,
                    197,
                    0
                ],
                "title": "QSpark: Towards Reliable Qiskit Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpark: Towards Reliable Qiskit Code Generation"
                },
                "summary": "Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and\nStarCoder often output flawed Qiskit code. We fine-tuned the Qwen2.5-Coder-32B\nmodel with two RL methods, Group Relative Policy Optimization (GRPO) and\nOdds-Ratio Preference Optimization (ORPO), using a richly annotated synthetic\ndataset. On the Qiskit HumanEval benchmark, ORPO reaches 56.29% Pass@1\n($\\approx+10$ pp over Granite-8B-QK) and GRPO hits 49%, both beating all\ngeneral-purpose baselines; on the original HumanEval they score 65.90% and\n63.00%. GRPO performs well on basic tasks (44/78) and excels on intermediate\nones (41/68), but neither GRPO nor ORPO solves any of the five advanced tasks,\nhighlighting clear gains yet room for progress in AI-assisted quantum\nprogramming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and\nStarCoder often output flawed Qiskit code. We fine-tuned the Qwen2.5-Coder-32B\nmodel with two RL methods, Group Relative Policy Optimization (GRPO) and\nOdds-Ratio Preference Optimization (ORPO), using a richly annotated synthetic\ndataset. On the Qiskit HumanEval benchmark, ORPO reaches 56.29% Pass@1\n($\\approx+10$ pp over Granite-8B-QK) and GRPO hits 49%, both beating all\ngeneral-purpose baselines; on the original HumanEval they score 65.90% and\n63.00%. GRPO performs well on basic tasks (44/78) and excels on intermediate\nones (41/68), but neither GRPO nor ORPO solves any of the five advanced tasks,\nhighlighting clear gains yet room for progress in AI-assisted quantum\nprogramming."
                },
                "authors": [
                    {
                        "name": "Kiana Kheiri"
                    },
                    {
                        "name": "Aamna Aamir"
                    },
                    {
                        "name": "Andriy Miranskyy"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_journal_ref": "In Proceedings of AIQxQIA 2025: International Workshop on AI for\n  Quantum and Quantum for AI | co-located with ECAI 2025, Bologna, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19215v1",
                "updated": "2025-09-23T16:35:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    35,
                    38,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:35:38Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    35,
                    38,
                    1,
                    266,
                    0
                ],
                "title": "PPG-Distill: Efficient Photoplethysmography Signals Analysis via\n  Foundation Model Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPG-Distill: Efficient Photoplethysmography Signals Analysis via\n  Foundation Model Distillation"
                },
                "summary": "Photoplethysmography (PPG) is widely used in wearable health monitoring, yet\nlarge PPG foundation models remain difficult to deploy on resource-limited\ndevices. We present PPG-Distill, a knowledge distillation framework that\ntransfers both global and local knowledge through prediction-, feature-, and\npatch-level distillation. PPG-Distill incorporates morphology distillation to\npreserve local waveform patterns and rhythm distillation to capture inter-patch\ntemporal structures. On heart rate estimation and atrial fibrillation\ndetection, PPG-Distill improves student performance by up to 21.8% while\nachieving 7X faster inference and reducing memory usage by 19X, enabling\nefficient PPG analysis on wearables",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoplethysmography (PPG) is widely used in wearable health monitoring, yet\nlarge PPG foundation models remain difficult to deploy on resource-limited\ndevices. We present PPG-Distill, a knowledge distillation framework that\ntransfers both global and local knowledge through prediction-, feature-, and\npatch-level distillation. PPG-Distill incorporates morphology distillation to\npreserve local waveform patterns and rhythm distillation to capture inter-patch\ntemporal structures. On heart rate estimation and atrial fibrillation\ndetection, PPG-Distill improves student performance by up to 21.8% while\nachieving 7X faster inference and reducing memory usage by 19X, enabling\nefficient PPG analysis on wearables"
                },
                "authors": [
                    {
                        "name": "Juntong Ni"
                    },
                    {
                        "name": "Saurabh Kataria"
                    },
                    {
                        "name": "Shengpu Tang"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Wei Jin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Jin"
                },
                "author": "Wei Jin",
                "arxiv_comment": "Accepted at NeurIPS 2025 Workshop on Learning from Time Series for\n  Health",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15273v2",
                "updated": "2025-09-23T16:30:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    30,
                    58,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-18T11:53:37Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    53,
                    37,
                    3,
                    261,
                    0
                ],
                "title": "Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation\n  Platform for Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation\n  Platform for Embodied AI"
                },
                "summary": "Embodied AI development significantly lags behind large foundation models due\nto three critical challenges: (1) lack of systematic understanding of core\ncapabilities needed for Embodied AI, making research lack clear objectives; (2)\nabsence of unified and standardized evaluation systems, rendering\ncross-benchmark evaluation infeasible; and (3) underdeveloped automated and\nscalable acquisition methods for embodied data, creating critical bottlenecks\nfor model scaling. To address these obstacles, we present Embodied Arena, a\ncomprehensive, unified, and evolving evaluation platform for Embodied AI. Our\nplatform establishes a systematic embodied capability taxonomy spanning three\nlevels (perception, reasoning, task execution), seven core capabilities, and 25\nfine-grained dimensions, enabling unified evaluation with systematic research\nobjectives. We introduce a standardized evaluation system built upon unified\ninfrastructure supporting flexible integration of 22 diverse benchmarks across\nthree domains (2D/3D Embodied Q&A, Navigation, Task Planning) and 30+ advanced\nmodels from 20+ worldwide institutes. Additionally, we develop a novel\nLLM-driven automated generation pipeline ensuring scalable embodied evaluation\ndata with continuous evolution for diversity and comprehensiveness. Embodied\nArena publishes three real-time leaderboards (Embodied Q&A, Navigation, Task\nPlanning) with dual perspectives (benchmark view and capability view),\nproviding comprehensive overviews of advanced model capabilities. Especially,\nwe present nine findings summarized from the evaluation results on the\nleaderboards of Embodied Arena. This helps to establish clear research veins\nand pinpoint critical research problems, thereby driving forward progress in\nthe field of Embodied AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI development significantly lags behind large foundation models due\nto three critical challenges: (1) lack of systematic understanding of core\ncapabilities needed for Embodied AI, making research lack clear objectives; (2)\nabsence of unified and standardized evaluation systems, rendering\ncross-benchmark evaluation infeasible; and (3) underdeveloped automated and\nscalable acquisition methods for embodied data, creating critical bottlenecks\nfor model scaling. To address these obstacles, we present Embodied Arena, a\ncomprehensive, unified, and evolving evaluation platform for Embodied AI. Our\nplatform establishes a systematic embodied capability taxonomy spanning three\nlevels (perception, reasoning, task execution), seven core capabilities, and 25\nfine-grained dimensions, enabling unified evaluation with systematic research\nobjectives. We introduce a standardized evaluation system built upon unified\ninfrastructure supporting flexible integration of 22 diverse benchmarks across\nthree domains (2D/3D Embodied Q&A, Navigation, Task Planning) and 30+ advanced\nmodels from 20+ worldwide institutes. Additionally, we develop a novel\nLLM-driven automated generation pipeline ensuring scalable embodied evaluation\ndata with continuous evolution for diversity and comprehensiveness. Embodied\nArena publishes three real-time leaderboards (Embodied Q&A, Navigation, Task\nPlanning) with dual perspectives (benchmark view and capability view),\nproviding comprehensive overviews of advanced model capabilities. Especially,\nwe present nine findings summarized from the evaluation results on the\nleaderboards of Embodied Arena. This helps to establish clear research veins\nand pinpoint critical research problems, thereby driving forward progress in\nthe field of Embodied AI."
                },
                "authors": [
                    {
                        "name": "Fei Ni"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Pengyi Li"
                    },
                    {
                        "name": "Yifu Yuan"
                    },
                    {
                        "name": "Lingfeng Zhang"
                    },
                    {
                        "name": "Yuecheng Liu"
                    },
                    {
                        "name": "Peilong Han"
                    },
                    {
                        "name": "Longxin Kou"
                    },
                    {
                        "name": "Shaojin Ma"
                    },
                    {
                        "name": "Jinbin Qiao"
                    },
                    {
                        "name": "David Gamaliel Arcos Bravo"
                    },
                    {
                        "name": "Yuening Wang"
                    },
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhanguang Zhang"
                    },
                    {
                        "name": "Xianze Yao"
                    },
                    {
                        "name": "Yutong Li"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Ying Wen"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Bin He"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Huazhe Xu"
                    },
                    {
                        "name": "Jiankang Deng"
                    },
                    {
                        "name": "Shan Luo"
                    },
                    {
                        "name": "Shuqiang Jiang"
                    },
                    {
                        "name": "Wei Pan"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Stefanos Zafeiriou"
                    },
                    {
                        "name": "Jan Peters"
                    },
                    {
                        "name": "Yuzheng Zhuang"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Yan Zheng"
                    },
                    {
                        "name": "Hongyao Tang"
                    },
                    {
                        "name": "Jianye Hao"
                    }
                ],
                "author_detail": {
                    "name": "Jianye Hao"
                },
                "author": "Jianye Hao",
                "arxiv_comment": "32 pages, 5 figures, Embodied Arena Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19209v1",
                "updated": "2025-09-23T16:29:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    29,
                    22,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:29:22Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    29,
                    22,
                    1,
                    266,
                    0
                ],
                "title": "A Knowledge Graph and a Tripartite Evaluation Framework Make\n  Retrieval-Augmented Generation Scalable and Transparent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Knowledge Graph and a Tripartite Evaluation Framework Make\n  Retrieval-Augmented Generation Scalable and Transparent"
                },
                "summary": "Large Language Models (LLMs) have significantly enhanced conversational\nArtificial Intelligence(AI) chatbots; however, domain-specific accuracy and the\navoidance of factual inconsistencies remain pressing challenges, particularly\nfor large datasets. Designing an effective chatbot with appropriate methods and\nevaluating its effectiveness is among the challenges in this domain. This study\npresents a Retrieval Augmented Generation (RAG) chatbot that harnesses a\nknowledge graph and vector search retrieval to deliver precise, context-rich\nresponses in an exemplary use case from over high-volume engineering\nproject-related emails, thereby minimising the need for document chunking. A\ncentral innovation of this work is the introduction of RAG Evaluation\n(RAG-Eval), a novel chain-of-thought LLM-based tripartite evaluation framework\nspecifically developed to assess RAG applications. This framework operates in\nparallel with the chatbot, jointly assessing the user's query, the retrieved\ndocument, and the generated response, enabling a holistic evaluation across\nmultiple quality metrics like query relevance, factual accuracy, coverage,\ncoherence and fluency. The resulting scoring system is provided directly to\nusers as a confidence score (1 to 100%), enabling quick identification of\npossible misaligned or incomplete answers. This proposed approach promotes\ntransparency and rapid verification by incorporating metadata email IDs,\ntimestamps into responses. Experimental comparisons against BERTScore and\nG-EVAL for summarisation evaluation tasks confirm its effectiveness, and\nempirical analysis also shows RAG-Eval reliably detects factual gaps and query\nmismatches, thereby fostering trust in high demand, data centric environments.\nThese findings highlight a scalable path for developing accurate,\nuser-verifiable chatbots that bridge the gap between high-level conversational\nfluency and factual accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly enhanced conversational\nArtificial Intelligence(AI) chatbots; however, domain-specific accuracy and the\navoidance of factual inconsistencies remain pressing challenges, particularly\nfor large datasets. Designing an effective chatbot with appropriate methods and\nevaluating its effectiveness is among the challenges in this domain. This study\npresents a Retrieval Augmented Generation (RAG) chatbot that harnesses a\nknowledge graph and vector search retrieval to deliver precise, context-rich\nresponses in an exemplary use case from over high-volume engineering\nproject-related emails, thereby minimising the need for document chunking. A\ncentral innovation of this work is the introduction of RAG Evaluation\n(RAG-Eval), a novel chain-of-thought LLM-based tripartite evaluation framework\nspecifically developed to assess RAG applications. This framework operates in\nparallel with the chatbot, jointly assessing the user's query, the retrieved\ndocument, and the generated response, enabling a holistic evaluation across\nmultiple quality metrics like query relevance, factual accuracy, coverage,\ncoherence and fluency. The resulting scoring system is provided directly to\nusers as a confidence score (1 to 100%), enabling quick identification of\npossible misaligned or incomplete answers. This proposed approach promotes\ntransparency and rapid verification by incorporating metadata email IDs,\ntimestamps into responses. Experimental comparisons against BERTScore and\nG-EVAL for summarisation evaluation tasks confirm its effectiveness, and\nempirical analysis also shows RAG-Eval reliably detects factual gaps and query\nmismatches, thereby fostering trust in high demand, data centric environments.\nThese findings highlight a scalable path for developing accurate,\nuser-verifiable chatbots that bridge the gap between high-level conversational\nfluency and factual accuracy."
                },
                "authors": [
                    {
                        "name": "Olalekan K. Akindele"
                    },
                    {
                        "name": "Bhupesh Kumar Mishra"
                    },
                    {
                        "name": "Kenneth Y. Wertheim"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Y. Wertheim"
                },
                "author": "Kenneth Y. Wertheim",
                "arxiv_comment": "25 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09403v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09403v3",
                "updated": "2025-09-23T16:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    19,
                    6,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-13T00:39:22Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    0,
                    39,
                    22,
                    2,
                    225,
                    0
                ],
                "title": "Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large\n  Language Models"
                },
                "summary": "Expanding the abbreviated column names of tables, such as \"esal\" to \"employee\nsalary\", is critical for many downstream NLP tasks for tabular data, such as\nNL2SQL, table QA, and keyword search. This problem arises in enterprises,\ndomain sciences, government agencies, and more. In this paper, we make three\ncontributions that significantly advance the state of the art. First, we show\nthat the synthetic public data used by prior work has major limitations, and we\nintroduce four new datasets in enterprise/science domains, with real-world\nabbreviations. Second, we show that accuracy measures used by prior work\nseriously undercount correct expansions, and we propose new synonym-aware\nmeasures that capture accuracy much more accurately. Finally, we develop\nColumbo, a powerful LLM-based solution that exploits context, rules,\nchain-of-thought reasoning, and token-level analysis. Extensive experiments\nshow that Columbo significantly outperforms NameGuess, the current most\nadvanced solution, by 4-29%, over five datasets. Columbo has been used in\nproduction on EDI, a major data lake for environmental sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding the abbreviated column names of tables, such as \"esal\" to \"employee\nsalary\", is critical for many downstream NLP tasks for tabular data, such as\nNL2SQL, table QA, and keyword search. This problem arises in enterprises,\ndomain sciences, government agencies, and more. In this paper, we make three\ncontributions that significantly advance the state of the art. First, we show\nthat the synthetic public data used by prior work has major limitations, and we\nintroduce four new datasets in enterprise/science domains, with real-world\nabbreviations. Second, we show that accuracy measures used by prior work\nseriously undercount correct expansions, and we propose new synonym-aware\nmeasures that capture accuracy much more accurately. Finally, we develop\nColumbo, a powerful LLM-based solution that exploits context, rules,\nchain-of-thought reasoning, and token-level analysis. Extensive experiments\nshow that Columbo significantly outperforms NameGuess, the current most\nadvanced solution, by 4-29%, over five datasets. Columbo has been used in\nproduction on EDI, a major data lake for environmental sciences."
                },
                "authors": [
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Stephen Sheen"
                    },
                    {
                        "name": "AnHai Doan"
                    }
                ],
                "author_detail": {
                    "name": "AnHai Doan"
                },
                "author": "AnHai Doan",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025; 19 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09403v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09403v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19199v2",
                "updated": "2025-09-24T01:27:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    27,
                    23,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-23T16:15:42Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    15,
                    42,
                    1,
                    266,
                    0
                ],
                "title": "Online Process Reward Leanring for Agentic Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Process Reward Leanring for Agentic Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning (RL) as autonomous agents that reason and act over long horizons in\ninteractive environments. However, sparse and sometimes unverifiable rewards\nmake temporal credit assignment extremely challenging. Recent work attempts to\nintegrate process supervision into agent learning but suffers from biased\nannotation, reward hacking, high-variance from overly fine-grained signals or\nfailtures when state overlap is rare. We therefore introduce Online Process\nReward Learning (OPRL), a general credit-assignment strategy for agentic RL\nthat integrates seamlessly with standard on-policy algorithms without relying\non additional rollouts or explicit step labels. In OPRL, we optimize an\nimplicit process reward model (PRM) alternately with the agent's policy to\ntransform trajectory preferences into implicit step rewards through a\ntrajectory-based DPO objective. These step rewards are then used to compute\nstep-level advantages, which are combined with episode-level advantages from\noutcome rewards for policy update, creating a self-reinforcing loop.\nTheoretical findings guarantee that the learned step rewards are consistent\nwith trajectory preferences and act as potential-based shaping rewards,\nproviding bounded gradients to stabilize training. Empirically, we evaluate\nOPRL on three distinct agent benmarks, including WebShop and VisualSokoban, as\nwell as open-ended social interactions with unverfiable rewards in SOTOPIA.\nCrucially, OPRL shows superior performance over frontier LLMs and strong RL\nbaselines across domains, achieving state-of-the-art results with higher\nsample-efficiency and lower variance during training. Further analysis also\ndemonstrates the efficient exploration by OPRL using fewer actions,\nunderscoring its potential for agentic learning in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning (RL) as autonomous agents that reason and act over long horizons in\ninteractive environments. However, sparse and sometimes unverifiable rewards\nmake temporal credit assignment extremely challenging. Recent work attempts to\nintegrate process supervision into agent learning but suffers from biased\nannotation, reward hacking, high-variance from overly fine-grained signals or\nfailtures when state overlap is rare. We therefore introduce Online Process\nReward Learning (OPRL), a general credit-assignment strategy for agentic RL\nthat integrates seamlessly with standard on-policy algorithms without relying\non additional rollouts or explicit step labels. In OPRL, we optimize an\nimplicit process reward model (PRM) alternately with the agent's policy to\ntransform trajectory preferences into implicit step rewards through a\ntrajectory-based DPO objective. These step rewards are then used to compute\nstep-level advantages, which are combined with episode-level advantages from\noutcome rewards for policy update, creating a self-reinforcing loop.\nTheoretical findings guarantee that the learned step rewards are consistent\nwith trajectory preferences and act as potential-based shaping rewards,\nproviding bounded gradients to stabilize training. Empirically, we evaluate\nOPRL on three distinct agent benmarks, including WebShop and VisualSokoban, as\nwell as open-ended social interactions with unverfiable rewards in SOTOPIA.\nCrucially, OPRL shows superior performance over frontier LLMs and strong RL\nbaselines across domains, achieving state-of-the-art results with higher\nsample-efficiency and lower variance during training. Further analysis also\ndemonstrates the efficient exploration by OPRL using fewer actions,\nunderscoring its potential for agentic learning in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Yuchuan Wu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Junge Zhang"
                    },
                    {
                        "name": "Jianbin Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Jianbin Jiao"
                },
                "author": "Jianbin Jiao",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.11593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.11593v2",
                "updated": "2025-09-23T16:12:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    12,
                    45,
                    1,
                    266,
                    0
                ],
                "published": "2023-06-20T15:13:02Z",
                "published_parsed": [
                    2023,
                    6,
                    20,
                    15,
                    13,
                    2,
                    1,
                    171,
                    0
                ],
                "title": "Improving Image Captioning Descriptiveness by Ranking and LLM-based\n  Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Image Captioning Descriptiveness by Ranking and LLM-based\n  Fusion"
                },
                "summary": "State-of-The-Art (SoTA) image captioning models are often trained on the\nMicroSoft Common Objects in Context (MS-COCO) dataset, which contains\nhuman-annotated captions with an average length of approximately ten tokens.\nAlthough effective for general scene understanding, these short captions often\nfail to capture complex scenes and convey detailed information. Moreover,\ncaptioning models tend to exhibit bias towards the ``average'' caption, which\ncaptures only the more general aspects, thus overlooking finer details. In this\npaper, we present a novel approach to generate richer and more informative\nimage captions by combining the captions generated from different SoTA\ncaptioning models. Our proposed method requires no additional model training:\ngiven an image, it leverages pre-trained models from the literature to generate\nthe initial captions, and then ranks them using a newly introduced\nimage-text-based metric, which we name BLIPScore. Subsequently, the top two\ncaptions are fused using a Large Language Model (LLM) to produce the final,\nmore detailed description. Experimental results on the MS-COCO and Flickr30k\ntest sets demonstrate the effectiveness of our approach in terms of\ncaption-image alignment and hallucination reduction according to the ALOHa,\nCAPTURE, and Polos metrics. A subjective study lends additional support to\nthese results, suggesting that the captions produced by our model are generally\nperceived as more consistent with human judgment. By combining the strengths of\ndiverse SoTA models, our method enhances the quality and appeal of image\ncaptions, bridging the gap between automated systems and the rich and\ninformative nature of human-generated descriptions. This advance enables the\ngeneration of more suitable captions for the training of both vision-language\nand captioning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-The-Art (SoTA) image captioning models are often trained on the\nMicroSoft Common Objects in Context (MS-COCO) dataset, which contains\nhuman-annotated captions with an average length of approximately ten tokens.\nAlthough effective for general scene understanding, these short captions often\nfail to capture complex scenes and convey detailed information. Moreover,\ncaptioning models tend to exhibit bias towards the ``average'' caption, which\ncaptures only the more general aspects, thus overlooking finer details. In this\npaper, we present a novel approach to generate richer and more informative\nimage captions by combining the captions generated from different SoTA\ncaptioning models. Our proposed method requires no additional model training:\ngiven an image, it leverages pre-trained models from the literature to generate\nthe initial captions, and then ranks them using a newly introduced\nimage-text-based metric, which we name BLIPScore. Subsequently, the top two\ncaptions are fused using a Large Language Model (LLM) to produce the final,\nmore detailed description. Experimental results on the MS-COCO and Flickr30k\ntest sets demonstrate the effectiveness of our approach in terms of\ncaption-image alignment and hallucination reduction according to the ALOHa,\nCAPTURE, and Polos metrics. A subjective study lends additional support to\nthese results, suggesting that the captions produced by our model are generally\nperceived as more consistent with human judgment. By combining the strengths of\ndiverse SoTA models, our method enhances the quality and appeal of image\ncaptions, bridging the gap between automated systems and the rich and\ninformative nature of human-generated descriptions. This advance enables the\ngeneration of more suitable captions for the training of both vision-language\nand captioning models."
                },
                "authors": [
                    {
                        "name": "Luigi Celona"
                    },
                    {
                        "name": "Simone Bianco"
                    },
                    {
                        "name": "Marco Donzella"
                    },
                    {
                        "name": "Paolo Napoletano"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Napoletano"
                },
                "author": "Paolo Napoletano",
                "arxiv_comment": "This manuscript has been accepted for publication in Springer Neural\n  Computing and Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.11593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.11593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19189v2",
                "updated": "2025-09-24T05:27:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    5,
                    27,
                    45,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-23T16:05:16Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    5,
                    16,
                    1,
                    266,
                    0
                ],
                "title": "Unveiling the Role of Learning Rate Schedules via Functional Scaling\n  Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Role of Learning Rate Schedules via Functional Scaling\n  Laws"
                },
                "summary": "Scaling laws have played a cornerstone role in guiding the training of large\nlanguage models (LLMs). However, most existing works on scaling laws primarily\nfocus on the final-step loss, overlooking the loss dynamics during the training\nprocess and, crucially, the impact of learning rate schedule (LRS). In this\npaper, we aim to bridge this gap by studying a teacher-student kernel\nregression setup trained via online stochastic gradient descent (SGD).\nLeveraging a novel intrinsic time viewpoint and stochastic differential\nequation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),\nwhich characterizes the evolution of population risk during the training\nprocess for general LRSs. Remarkably, the impact of the LRSs is captured\nthrough an explicit convolution-type functional term, making their effects\nfully tractable. To illustrate the utility of FSL, we analyze three widely used\nLRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under\nboth data-limited and compute-limited regimes. We provide theoretical\njustification for widely adopted empirical practices in LLMs pre-training such\nas (i) higher-capacity models are more data- and compute-efficient; (ii)\nlearning rate decay can improve training efficiency; (iii) WSD-like schedules\ncan outperform direct-decay schedules. Lastly, we explore the practical\nrelevance of FSL as a surrogate model for fitting, predicting and optimizing\nthe loss curves in LLM pre-training, with experiments conducted across model\nsizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen\nthe understanding of LLM pre-training dynamics and provide insights for\nimproving large-scale model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws have played a cornerstone role in guiding the training of large\nlanguage models (LLMs). However, most existing works on scaling laws primarily\nfocus on the final-step loss, overlooking the loss dynamics during the training\nprocess and, crucially, the impact of learning rate schedule (LRS). In this\npaper, we aim to bridge this gap by studying a teacher-student kernel\nregression setup trained via online stochastic gradient descent (SGD).\nLeveraging a novel intrinsic time viewpoint and stochastic differential\nequation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),\nwhich characterizes the evolution of population risk during the training\nprocess for general LRSs. Remarkably, the impact of the LRSs is captured\nthrough an explicit convolution-type functional term, making their effects\nfully tractable. To illustrate the utility of FSL, we analyze three widely used\nLRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under\nboth data-limited and compute-limited regimes. We provide theoretical\njustification for widely adopted empirical practices in LLMs pre-training such\nas (i) higher-capacity models are more data- and compute-efficient; (ii)\nlearning rate decay can improve training efficiency; (iii) WSD-like schedules\ncan outperform direct-decay schedules. Lastly, we explore the practical\nrelevance of FSL as a surrogate model for fitting, predicting and optimizing\nthe loss curves in LLM pre-training, with experiments conducted across model\nsizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen\nthe understanding of LLM pre-training dynamics and provide insights for\nimproving large-scale model training."
                },
                "authors": [
                    {
                        "name": "Binghui Li"
                    },
                    {
                        "name": "Fengling Chen"
                    },
                    {
                        "name": "Zixun Huang"
                    },
                    {
                        "name": "Lean Wang"
                    },
                    {
                        "name": "Lei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lei Wu"
                },
                "author": "Lei Wu",
                "arxiv_comment": "52 pages, accepted by NeurIPS 2025 as a spotlight paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19180v1",
                "updated": "2025-09-23T15:52:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    52,
                    18,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:52:18Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    52,
                    18,
                    1,
                    266,
                    0
                ],
                "title": "Bayesian Neural Networks versus deep ensembles for uncertainty\n  quantification in machine learning interatomic potentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Neural Networks versus deep ensembles for uncertainty\n  quantification in machine learning interatomic potentials"
                },
                "summary": "Neural-network-based machine learning interatomic potentials have emerged as\npowerful tools for predicting atomic energies and forces, enabling accurate and\nefficient simulations in atomistic modeling. A key limitation of traditional\ndeep learning approaches, however, is their inability to provide reliable\nestimates of predictive uncertainty. Such uncertainty quantification is\ncritical for assessing model reliability, especially in materials science,\nwhere often the model is applied on out-of-distribution data. Different\nstrategies have been proposed to address this challenge, with deep ensembles\nand Bayesian neural networks being among the most widely used. In this work, we\nintroduce an implementation of Bayesian neural networks with variational\ninference in the aenet-PyTorch framework. To evaluate their applicability to\nmachine learning interatomic potentials, we systematically compare the\nperformance of variational BNNs and deep ensembles on a dataset of 7,815\nTiO$_{2}$ structures. The models are trained on both the full dataset and a\nsubset to assess how variations in data representation influence predictive\naccuracy and uncertainty estimation. This analysis provides insights into the\nstrengths and limitations of each approach, offering practical guidance for the\ndevelopment of uncertainty-aware machine learning interatomic potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural-network-based machine learning interatomic potentials have emerged as\npowerful tools for predicting atomic energies and forces, enabling accurate and\nefficient simulations in atomistic modeling. A key limitation of traditional\ndeep learning approaches, however, is their inability to provide reliable\nestimates of predictive uncertainty. Such uncertainty quantification is\ncritical for assessing model reliability, especially in materials science,\nwhere often the model is applied on out-of-distribution data. Different\nstrategies have been proposed to address this challenge, with deep ensembles\nand Bayesian neural networks being among the most widely used. In this work, we\nintroduce an implementation of Bayesian neural networks with variational\ninference in the aenet-PyTorch framework. To evaluate their applicability to\nmachine learning interatomic potentials, we systematically compare the\nperformance of variational BNNs and deep ensembles on a dataset of 7,815\nTiO$_{2}$ structures. The models are trained on both the full dataset and a\nsubset to assess how variations in data representation influence predictive\naccuracy and uncertainty estimation. This analysis provides insights into the\nstrengths and limitations of each approach, offering practical guidance for the\ndevelopment of uncertainty-aware machine learning interatomic potentials."
                },
                "authors": [
                    {
                        "name": "Riccardo Farris"
                    },
                    {
                        "name": "Emanuele Telari"
                    },
                    {
                        "name": "Nongnuch Artrith"
                    },
                    {
                        "name": "Konstantin Neyman"
                    },
                    {
                        "name": "Albert Bruix"
                    }
                ],
                "author_detail": {
                    "name": "Albert Bruix"
                },
                "author": "Albert Bruix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19170v1",
                "updated": "2025-09-23T15:43:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    43,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:43:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    43,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "Soft Tokens, Hard Truths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Tokens, Hard Truths"
                },
                "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model."
                },
                "authors": [
                    {
                        "name": "Natasha Butt"
                    },
                    {
                        "name": "Ariel Kwiatkowski"
                    },
                    {
                        "name": "Ismail Labiad"
                    },
                    {
                        "name": "Julia Kempe"
                    },
                    {
                        "name": "Yann Ollivier"
                    }
                ],
                "author_detail": {
                    "name": "Yann Ollivier"
                },
                "author": "Yann Ollivier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19162v1",
                "updated": "2025-09-23T15:40:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    40,
                    36,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:40:36Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    40,
                    36,
                    1,
                    266,
                    0
                ],
                "title": "CayleyPy Growth: Efficient growth computations and hundreds of new\n  conjectures on Cayley graphs (Brief version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CayleyPy Growth: Efficient growth computations and hundreds of new\n  conjectures on Cayley graphs (Brief version)"
                },
                "summary": "This is the third paper of the CayleyPy project applying artificial\nintelligence to problems in group theory. We announce the first public release\nof CayleyPy, an open source Python library for computations with Cayley and\nSchreier graphs. Compared with systems such as GAP and Sage, CayleyPy handles\nmuch larger graphs and performs several orders of magnitude faster.\n  Using CayleyPy we obtained about 200 new conjectures on Cayley and Schreier\ngraphs, focused on diameters and growth. For many Cayley graphs of symmetric\ngroups Sn we observe quasi polynomial diameter formulas: a small set of\nquadratic or linear polynomials indexed by n mod s. We conjecture that this is\na general phenomenon, giving efficient diameter computation despite the problem\nbeing NP hard. We propose a refinement of the Babai type conjecture on\ndiameters of Sn: n^2/2 + 4n upper bounds in the undirected case, compared to\nprevious O(n^2) bounds. We also provide explicit generator families, related to\ninvolutions in a square with whiskers pattern, conjectured to maximize the\ndiameter; search confirms this for all n up to 15. We further conjecture an\nanswer to a question posed by V M Glushkov in 1968 on directed Cayley graphs\ngenerated by a cyclic shift and a transposition.\n  For nilpotent groups we conjecture an improvement of J S Ellenberg's results\non upper unitriangular matrices over Z/pZ, showing linear dependence of\ndiameter on p. Moreover.\n  Some conjectures are LLM friendly, naturally stated as sorting problems\nverifiable by algorithms or Python code. To benchmark path finding we created\nmore than 10 Kaggle datasets. CayleyPy works with arbitrary permutation or\nmatrix groups and includes over 100 predefined generators. Our growth\ncomputation code outperforms GAP and Sage up to 1000 times in speed and size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This is the third paper of the CayleyPy project applying artificial\nintelligence to problems in group theory. We announce the first public release\nof CayleyPy, an open source Python library for computations with Cayley and\nSchreier graphs. Compared with systems such as GAP and Sage, CayleyPy handles\nmuch larger graphs and performs several orders of magnitude faster.\n  Using CayleyPy we obtained about 200 new conjectures on Cayley and Schreier\ngraphs, focused on diameters and growth. For many Cayley graphs of symmetric\ngroups Sn we observe quasi polynomial diameter formulas: a small set of\nquadratic or linear polynomials indexed by n mod s. We conjecture that this is\na general phenomenon, giving efficient diameter computation despite the problem\nbeing NP hard. We propose a refinement of the Babai type conjecture on\ndiameters of Sn: n^2/2 + 4n upper bounds in the undirected case, compared to\nprevious O(n^2) bounds. We also provide explicit generator families, related to\ninvolutions in a square with whiskers pattern, conjectured to maximize the\ndiameter; search confirms this for all n up to 15. We further conjecture an\nanswer to a question posed by V M Glushkov in 1968 on directed Cayley graphs\ngenerated by a cyclic shift and a transposition.\n  For nilpotent groups we conjecture an improvement of J S Ellenberg's results\non upper unitriangular matrices over Z/pZ, showing linear dependence of\ndiameter on p. Moreover.\n  Some conjectures are LLM friendly, naturally stated as sorting problems\nverifiable by algorithms or Python code. To benchmark path finding we created\nmore than 10 Kaggle datasets. CayleyPy works with arbitrary permutation or\nmatrix groups and includes over 100 predefined generators. Our growth\ncomputation code outperforms GAP and Sage up to 1000 times in speed and size."
                },
                "authors": [
                    {
                        "name": "A. Chervov"
                    },
                    {
                        "name": "D. Fedoriaka"
                    },
                    {
                        "name": "E. Konstantinova"
                    },
                    {
                        "name": "A. Naumov"
                    },
                    {
                        "name": "I. Kiselev"
                    },
                    {
                        "name": "A. Sheveleva"
                    },
                    {
                        "name": "I. Koltsov"
                    },
                    {
                        "name": "S. Lytkin"
                    },
                    {
                        "name": "A. Smolensky"
                    },
                    {
                        "name": "A. Soibelman"
                    },
                    {
                        "name": "F. Levkovich-Maslyuk"
                    },
                    {
                        "name": "R. Grimov"
                    },
                    {
                        "name": "D. Volovich"
                    },
                    {
                        "name": "A. Isakov"
                    },
                    {
                        "name": "A. Kostin"
                    },
                    {
                        "name": "M. Litvinov"
                    },
                    {
                        "name": "N. Vilkin-Krom"
                    },
                    {
                        "name": "A. Bidzhiev"
                    },
                    {
                        "name": "A. Krasnyi"
                    },
                    {
                        "name": "M. Evseev"
                    },
                    {
                        "name": "E. Geraseva"
                    },
                    {
                        "name": "L. Grunwald"
                    },
                    {
                        "name": "S. Galkin"
                    },
                    {
                        "name": "E. Koldunov"
                    },
                    {
                        "name": "S. Diner"
                    },
                    {
                        "name": "A. Chevychelov"
                    },
                    {
                        "name": "E. Kudasheva"
                    },
                    {
                        "name": "A. Sychev"
                    },
                    {
                        "name": "A. Kravchenko"
                    },
                    {
                        "name": "Z. Kogan"
                    },
                    {
                        "name": "A. Natyrova"
                    },
                    {
                        "name": "L. Shishina"
                    },
                    {
                        "name": "L. Cheldieva"
                    },
                    {
                        "name": "V. Zamkovoy"
                    },
                    {
                        "name": "D. Kovalenko"
                    },
                    {
                        "name": "O. Papulov"
                    },
                    {
                        "name": "S. Kudashev"
                    },
                    {
                        "name": "D. Shiltsov"
                    },
                    {
                        "name": "R. Turtayev"
                    },
                    {
                        "name": "O. Nikitina"
                    },
                    {
                        "name": "D. Mamayeva"
                    },
                    {
                        "name": "S. Nikolenko"
                    },
                    {
                        "name": "M. Obozov"
                    },
                    {
                        "name": "A. Titarenko"
                    },
                    {
                        "name": "A. Dolgorukova"
                    },
                    {
                        "name": "A. Aparnev"
                    },
                    {
                        "name": "O. Debeaupuis"
                    },
                    {
                        "name": "S. Alami C."
                    },
                    {
                        "name": "H. Isambert"
                    }
                ],
                "author_detail": {
                    "name": "H. Isambert"
                },
                "author": "H. Isambert",
                "arxiv_comment": "46 pages, 30 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17681v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17681v3",
                "updated": "2025-09-23T15:40:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    40,
                    35,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-25T05:24:15Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    5,
                    24,
                    15,
                    0,
                    237,
                    0
                ],
                "title": "Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative\n  Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative\n  Scientific Discovery"
                },
                "summary": "Bold claims about AI's role in science-from \"AGI will cure all diseases\" to\npromises of radically accelerated discovery-raise a central epistemic question:\ndo large language models (LLMs) truly generate new knowledge, or do they merely\nremix memorized fragments? We propose unlearning-as-ablation as a falsifiable\nprobe of constructive scientific discovery. The idea is to systematically\nremove a target result together with its forget-closure (supporting lemmas,\nparaphrases, and multi-hop entailments) and then evaluate whether the model can\nre-derive the result from only permitted axioms and tools. Success would\nindicate generative capability beyond recall; failure would expose current\nlimits. Unlike prevailing motivations for unlearning-privacy, copyright, or\nsafety-our framing repositions it as an epistemic probe for AI-for-Science. We\noutline a minimal pilot in mathematics and algorithms to illustrate\nfeasibility, and sketch how the same approach could later be extended to\ndomains such as physics or chemistry. This is a position paper: our\ncontribution is conceptual and methodological, not empirical. We aim to\nstimulate discussion on how principled ablation tests could help distinguish\nmodels that reconstruct knowledge from those that merely retrieve it, and how\nsuch probes might guide the next generation of AI-for-Science benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bold claims about AI's role in science-from \"AGI will cure all diseases\" to\npromises of radically accelerated discovery-raise a central epistemic question:\ndo large language models (LLMs) truly generate new knowledge, or do they merely\nremix memorized fragments? We propose unlearning-as-ablation as a falsifiable\nprobe of constructive scientific discovery. The idea is to systematically\nremove a target result together with its forget-closure (supporting lemmas,\nparaphrases, and multi-hop entailments) and then evaluate whether the model can\nre-derive the result from only permitted axioms and tools. Success would\nindicate generative capability beyond recall; failure would expose current\nlimits. Unlike prevailing motivations for unlearning-privacy, copyright, or\nsafety-our framing repositions it as an epistemic probe for AI-for-Science. We\noutline a minimal pilot in mathematics and algorithms to illustrate\nfeasibility, and sketch how the same approach could later be extended to\ndomains such as physics or chemistry. This is a position paper: our\ncontribution is conceptual and methodological, not empirical. We aim to\nstimulate discussion on how principled ablation tests could help distinguish\nmodels that reconstruct knowledge from those that merely retrieve it, and how\nsuch probes might guide the next generation of AI-for-Science benchmarks."
                },
                "authors": [
                    {
                        "name": "Robert Yang"
                    }
                ],
                "author_detail": {
                    "name": "Robert Yang"
                },
                "author": "Robert Yang",
                "arxiv_comment": "6 pages. Accepted to NeurIPS 2025 AI4Science Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17681v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17681v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09177v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09177v2",
                "updated": "2025-09-23T15:39:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    39,
                    49,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-11T06:27:10Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    27,
                    10,
                    3,
                    254,
                    0
                ],
                "title": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level\n  RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level\n  RL"
                },
                "summary": "We propose FSPO (Fair Sequence Policy Optimization), a sequence-level\nreinforcement learning method for LLMs that enforces length-fair clipping on\nthe importance-sampling (IS) weight. We study RL methods with sequence-level IS\nand identify a mismatch when PPO/GRPO-style clipping is transplanted to\nsequences: a fixed clip range systematically reweights short vs.\\ long\nresponses, distorting the optimization direction. FSPO introduces a simple\nremedy: we clip the sequence log-IS ratio with a band that scales as\n$\\sqrt{L}$. Theoretically, we formalize length fairness via a Length\nReweighting Error (LRE) and prove that small LRE yields a cosine directional\nguarantee between the clipped and true updates. Empirically, FSPO flattens clip\nrates across length bins, stabilizes training, and outperforms all baselines\nacross multiple evaluation datasets on Qwen3-8B-Base model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose FSPO (Fair Sequence Policy Optimization), a sequence-level\nreinforcement learning method for LLMs that enforces length-fair clipping on\nthe importance-sampling (IS) weight. We study RL methods with sequence-level IS\nand identify a mismatch when PPO/GRPO-style clipping is transplanted to\nsequences: a fixed clip range systematically reweights short vs.\\ long\nresponses, distorting the optimization direction. FSPO introduces a simple\nremedy: we clip the sequence log-IS ratio with a band that scales as\n$\\sqrt{L}$. Theoretically, we formalize length fairness via a Length\nReweighting Error (LRE) and prove that small LRE yields a cosine directional\nguarantee between the clipped and true updates. Empirically, FSPO flattens clip\nrates across length bins, stabilizes training, and outperforms all baselines\nacross multiple evaluation datasets on Qwen3-8B-Base model."
                },
                "authors": [
                    {
                        "name": "Hanyi Mao"
                    },
                    {
                        "name": "Quanjia Xiao"
                    },
                    {
                        "name": "Lei Pang"
                    },
                    {
                        "name": "Haixiao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Haixiao Liu"
                },
                "author": "Haixiao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09177v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09177v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19156v1",
                "updated": "2025-09-23T15:34:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    34,
                    33,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:34:33Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    34,
                    33,
                    1,
                    266,
                    0
                ],
                "title": "NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and\n  Dynamic Early-Exit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and\n  Dynamic Early-Exit"
                },
                "summary": "Spiking Neural Networks (SNNs) offer significant potential for enabling\nenergy-efficient intelligence at the edge. However, performing full SNN\ninference at the edge can be challenging due to the latency and energy\nconstraints arising from fixed and high timestep overheads. Edge-cloud\nco-inference systems present a promising solution, but their deployment is\noften hindered by high latency and feature transmission costs. To address these\nissues, we introduce NeuCODEX, a neuromorphic co-inference architecture that\njointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a\nlearned spike-driven compression module to reduce data transmission and employs\na dynamic early-exit mechanism to adaptively terminate inference based on\noutput confidence. We evaluated NeuCODEX on both static images (CIFAR10 and\nCaltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To\ndemonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16\nbackbones in a real edge-to-cloud testbed. Our proposed system reduces data\ntransfer by up to 2048x and edge energy consumption by over 90%, while reducing\nend-to-end latency by up to 3x compared to edge-only inference, all with a\nnegligible accuracy drop of less than 2%. In doing so, NeuCODEX enables\npractical, high-performance SNN deployment in resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) offer significant potential for enabling\nenergy-efficient intelligence at the edge. However, performing full SNN\ninference at the edge can be challenging due to the latency and energy\nconstraints arising from fixed and high timestep overheads. Edge-cloud\nco-inference systems present a promising solution, but their deployment is\noften hindered by high latency and feature transmission costs. To address these\nissues, we introduce NeuCODEX, a neuromorphic co-inference architecture that\njointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a\nlearned spike-driven compression module to reduce data transmission and employs\na dynamic early-exit mechanism to adaptively terminate inference based on\noutput confidence. We evaluated NeuCODEX on both static images (CIFAR10 and\nCaltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To\ndemonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16\nbackbones in a real edge-to-cloud testbed. Our proposed system reduces data\ntransfer by up to 2048x and edge energy consumption by over 90%, while reducing\nend-to-end latency by up to 3x compared to edge-only inference, all with a\nnegligible accuracy drop of less than 2%. In doing so, NeuCODEX enables\npractical, high-performance SNN deployment in resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Maurf Hassan"
                    },
                    {
                        "name": "Steven Davy"
                    },
                    {
                        "name": "Muhammad Zawish"
                    },
                    {
                        "name": "Owais Bin Zuber"
                    },
                    {
                        "name": "Nouman Ashraf"
                    }
                ],
                "author_detail": {
                    "name": "Nouman Ashraf"
                },
                "author": "Nouman Ashraf",
                "arxiv_comment": "This paper was accepted at ICMLA 2025. The official version will\n  appear in IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19153v1",
                "updated": "2025-09-23T15:32:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    32,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:32:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    32,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "LLMs as verification oracles for Solidity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as verification oracles for Solidity"
                },
                "summary": "Ensuring the correctness of smart contracts is critical, as even subtle flaws\ncan lead to severe financial losses. While bug detection tools able to spot\ncommon vulnerability patterns can serve as a first line of defense, most\nreal-world exploits and losses stem from errors in the contract business logic.\nFormal verification tools such as SolCMC and the Certora Prover address this\nchallenge, but their impact remains limited by steep learning curves and\nrestricted specification languages. Recent works have begun to explore the use\nof large language models (LLMs) for security-related tasks such as\nvulnerability detection and test generation. Yet, a fundamental question\nremains open: can LLMs serve as verification oracles, capable of reasoning\nabout arbitrary contract-specific properties? In this paper, we provide the\nfirst systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this\nrole. We benchmark its performance on a large dataset of verification tasks,\ncompare its outputs against those of established formal verification tools, and\nassess its practical effectiveness in real-world auditing scenarios. Our study\ncombines quantitative metrics with qualitative analysis, and shows that recent\nreasoning-oriented LLMs can be surprisingly effective as verification oracles,\nsuggesting a new frontier in the convergence of AI and formal methods for\nsecure smart contract development and auditing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the correctness of smart contracts is critical, as even subtle flaws\ncan lead to severe financial losses. While bug detection tools able to spot\ncommon vulnerability patterns can serve as a first line of defense, most\nreal-world exploits and losses stem from errors in the contract business logic.\nFormal verification tools such as SolCMC and the Certora Prover address this\nchallenge, but their impact remains limited by steep learning curves and\nrestricted specification languages. Recent works have begun to explore the use\nof large language models (LLMs) for security-related tasks such as\nvulnerability detection and test generation. Yet, a fundamental question\nremains open: can LLMs serve as verification oracles, capable of reasoning\nabout arbitrary contract-specific properties? In this paper, we provide the\nfirst systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this\nrole. We benchmark its performance on a large dataset of verification tasks,\ncompare its outputs against those of established formal verification tools, and\nassess its practical effectiveness in real-world auditing scenarios. Our study\ncombines quantitative metrics with qualitative analysis, and shows that recent\nreasoning-oriented LLMs can be surprisingly effective as verification oracles,\nsuggesting a new frontier in the convergence of AI and formal methods for\nsecure smart contract development and auditing."
                },
                "authors": [
                    {
                        "name": "Massimo Bartoletti"
                    },
                    {
                        "name": "Enrico Lipparini"
                    },
                    {
                        "name": "Livio Pompianu"
                    }
                ],
                "author_detail": {
                    "name": "Livio Pompianu"
                },
                "author": "Livio Pompianu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19143v1",
                "updated": "2025-09-23T15:26:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    26,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:26:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    26,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "Anecdoctoring: Automated Red-Teaming Across Language and Place",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anecdoctoring: Automated Red-Teaming Across Language and Place"
                },
                "summary": "Disinformation is among the top risks of generative artificial intelligence\n(AI) misuse. Global adoption of generative AI necessitates red-teaming\nevaluations (i.e., systematic adversarial probing) that are robust across\ndiverse languages and cultures, but red-teaming datasets are commonly US- and\nEnglish-centric. To address this gap, we propose \"anecdoctoring\", a novel\nred-teaming approach that automatically generates adversarial prompts across\nlanguages and cultures. We collect misinformation claims from fact-checking\nwebsites in three languages (English, Spanish, and Hindi) and two geographies\n(US and India). We then cluster individual claims into broader narratives and\ncharacterize the resulting clusters with knowledge graphs, with which we\naugment an attacker LLM. Our method produces higher attack success rates and\noffers interpretability benefits relative to few-shot prompting. Results\nunderscore the need for disinformation mitigations that scale globally and are\ngrounded in real-world adversarial misuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disinformation is among the top risks of generative artificial intelligence\n(AI) misuse. Global adoption of generative AI necessitates red-teaming\nevaluations (i.e., systematic adversarial probing) that are robust across\ndiverse languages and cultures, but red-teaming datasets are commonly US- and\nEnglish-centric. To address this gap, we propose \"anecdoctoring\", a novel\nred-teaming approach that automatically generates adversarial prompts across\nlanguages and cultures. We collect misinformation claims from fact-checking\nwebsites in three languages (English, Spanish, and Hindi) and two geographies\n(US and India). We then cluster individual claims into broader narratives and\ncharacterize the resulting clusters with knowledge graphs, with which we\naugment an attacker LLM. Our method produces higher attack success rates and\noffers interpretability benefits relative to few-shot prompting. Results\nunderscore the need for disinformation mitigations that scale globally and are\ngrounded in real-world adversarial misuse."
                },
                "authors": [
                    {
                        "name": "Alejandro Cuevas"
                    },
                    {
                        "name": "Saloni Dash"
                    },
                    {
                        "name": "Bharat Kumar Nayak"
                    },
                    {
                        "name": "Dan Vann"
                    },
                    {
                        "name": "Madeleine I. G. Daepp"
                    }
                ],
                "author_detail": {
                    "name": "Madeleine I. G. Daepp"
                },
                "author": "Madeleine I. G. Daepp",
                "arxiv_comment": "To be published in EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19142v1",
                "updated": "2025-09-23T15:26:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    26,
                    4,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:26:04Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    26,
                    4,
                    1,
                    266,
                    0
                ],
                "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiGraspFormer: End-to-End Bimanual Grasp Transformer"
                },
                "summary": "Bimanual grasping is essential for robots to handle large and complex\nobjects. However, existing methods either focus solely on single-arm grasping\nor employ separate grasp generation and bimanual evaluation stages, leading to\ncoordination problems including collision risks and unbalanced force\ndistribution. To address these limitations, we propose BiGraspFormer, a unified\nend-to-end transformer framework that directly generates coordinated bimanual\ngrasps from object point clouds. Our key idea is the Single-Guided Bimanual\n(SGB) strategy, which first generates diverse single grasp candidates using a\ntransformer decoder, then leverages their learned features through specialized\nattention mechanisms to jointly predict bimanual poses and quality scores. This\nconditioning strategy reduces the complexity of the 12-DoF search space while\nensuring coordinated bimanual manipulation. Comprehensive simulation\nexperiments and real-world validation demonstrate that BiGraspFormer\nconsistently outperforms existing methods while maintaining efficient inference\nspeed (<0.05s), confirming the effectiveness of our framework. Code and\nsupplementary materials are available at https://sites.google.com/bigraspformer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bimanual grasping is essential for robots to handle large and complex\nobjects. However, existing methods either focus solely on single-arm grasping\nor employ separate grasp generation and bimanual evaluation stages, leading to\ncoordination problems including collision risks and unbalanced force\ndistribution. To address these limitations, we propose BiGraspFormer, a unified\nend-to-end transformer framework that directly generates coordinated bimanual\ngrasps from object point clouds. Our key idea is the Single-Guided Bimanual\n(SGB) strategy, which first generates diverse single grasp candidates using a\ntransformer decoder, then leverages their learned features through specialized\nattention mechanisms to jointly predict bimanual poses and quality scores. This\nconditioning strategy reduces the complexity of the 12-DoF search space while\nensuring coordinated bimanual manipulation. Comprehensive simulation\nexperiments and real-world validation demonstrate that BiGraspFormer\nconsistently outperforms existing methods while maintaining efficient inference\nspeed (<0.05s), confirming the effectiveness of our framework. Code and\nsupplementary materials are available at https://sites.google.com/bigraspformer"
                },
                "authors": [
                    {
                        "name": "Kangmin Kim"
                    },
                    {
                        "name": "Seunghyeok Back"
                    },
                    {
                        "name": "Geonhyup Lee"
                    },
                    {
                        "name": "Sangbeom Lee"
                    },
                    {
                        "name": "Sangjun Noh"
                    },
                    {
                        "name": "Kyoobin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyoobin Lee"
                },
                "author": "Kyoobin Lee",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19136v1",
                "updated": "2025-09-23T15:20:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    20,
                    40,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:20:40Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    20,
                    40,
                    1,
                    266,
                    0
                ],
                "title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases\n  Written in Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Soundness and Consistency of LLM Agents for Executing Test Cases\n  Written in Natural Language"
                },
                "summary": "The use of natural language (NL) test cases for validating graphical user\ninterface (GUI) applications is emerging as a promising direction to manually\nwritten executable test scripts, which are costly to develop and difficult to\nmaintain. Recent advances in large language models (LLMs) have opened the\npossibility of the direct execution of NL test cases by LLM agents. This paper\ninvestigates this direction, focusing on the impact on NL test case unsoundness\nand on test case execution consistency. NL test cases are inherently unsound,\nas they may yield false failures due to ambiguous instructions or unpredictable\nagent behaviour. Furthermore, repeated executions of the same NL test case may\nlead to inconsistent outcomes, undermining test reliability. To address these\nchallenges, we propose an algorithm for executing NL test cases with guardrail\nmechanisms and specialised agents that dynamically verify the correct execution\nof each test step. We introduce measures to evaluate the capabilities of LLMs\nin test execution and one measure to quantify execution consistency. We propose\na definition of weak unsoundness to characterise contexts in which NL test case\nexecution remains acceptable, with respect to the industrial quality levels Six\nSigma. Our experimental evaluation with eight publicly available LLMs, ranging\nfrom 3B to 70B parameters, demonstrates both the potential and current\nlimitations of current LLM agents for GUI testing. Our experiments show that\nMeta Llama 3.1 70B demonstrates acceptable capabilities in NL test case\nexecution with high execution consistency (above the level 3-sigma). We provide\nprototype tools, test suites, and results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of natural language (NL) test cases for validating graphical user\ninterface (GUI) applications is emerging as a promising direction to manually\nwritten executable test scripts, which are costly to develop and difficult to\nmaintain. Recent advances in large language models (LLMs) have opened the\npossibility of the direct execution of NL test cases by LLM agents. This paper\ninvestigates this direction, focusing on the impact on NL test case unsoundness\nand on test case execution consistency. NL test cases are inherently unsound,\nas they may yield false failures due to ambiguous instructions or unpredictable\nagent behaviour. Furthermore, repeated executions of the same NL test case may\nlead to inconsistent outcomes, undermining test reliability. To address these\nchallenges, we propose an algorithm for executing NL test cases with guardrail\nmechanisms and specialised agents that dynamically verify the correct execution\nof each test step. We introduce measures to evaluate the capabilities of LLMs\nin test execution and one measure to quantify execution consistency. We propose\na definition of weak unsoundness to characterise contexts in which NL test case\nexecution remains acceptable, with respect to the industrial quality levels Six\nSigma. Our experimental evaluation with eight publicly available LLMs, ranging\nfrom 3B to 70B parameters, demonstrates both the potential and current\nlimitations of current LLM agents for GUI testing. Our experiments show that\nMeta Llama 3.1 70B demonstrates acceptable capabilities in NL test case\nexecution with high execution consistency (above the level 3-sigma). We provide\nprototype tools, test suites, and results."
                },
                "authors": [
                    {
                        "name": "SÃ©bastien Salva"
                    },
                    {
                        "name": "Redha Taguelmimt"
                    }
                ],
                "author_detail": {
                    "name": "Redha Taguelmimt"
                },
                "author": "Redha Taguelmimt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; D.2.5; F.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11189v2",
                "updated": "2025-09-23T15:19:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    19,
                    17,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-16T12:48:44Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    12,
                    48,
                    44,
                    4,
                    136,
                    0
                ],
                "title": "Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule\n  Extraction vs RuleSHAP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule\n  Extraction vs RuleSHAP"
                },
                "summary": "Large language models (LLMs) can amplify misinformation, undermining societal\ngoals like the UN SDGs. We study three documented drivers of misinformation\n(valence framing, information overload, and oversimplification) which are often\nshaped by one's default beliefs. Building on evidence that LLMs encode such\ndefaults (e.g., \"joy is positive,\" \"math is complex\") and can act as \"bags of\nheuristics,\" we ask: can general belief-driven heuristics behind misinformative\nbehaviour be recovered from LLMs as clear rules? A key obstacle is that global\nrule-extraction methods in explainable AI (XAI) are built for numerical\ninputs/outputs, not text. We address this by eliciting global LLM beliefs and\nmapping them to numerical scores via statistically reliable abstractions,\nthereby enabling off-the-shelf global XAI to detect belief-related heuristics\nin LLMs. To obtain ground truth, we hard-code bias-inducing nonlinear\nheuristics of increasing complexity (univariate, conjunctive, nonconvex) into\npopular LLMs (ChatGPT and Llama) via system instructions. This way, we find\nthat RuleFit under-detects non-univariate biases, while global SHAP better\napproximates conjunctive ones but does not yield actionable rules. To bridge\nthis gap, we propose RuleSHAP, a rule-extraction algorithm that couples global\nSHAP-value aggregations with rule induction to better capture non-univariate\nbias, improving heuristics detection over RuleFit by +94% (MRR@1) on average.\nOur results provide a practical pathway for revealing belief-driven biases in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can amplify misinformation, undermining societal\ngoals like the UN SDGs. We study three documented drivers of misinformation\n(valence framing, information overload, and oversimplification) which are often\nshaped by one's default beliefs. Building on evidence that LLMs encode such\ndefaults (e.g., \"joy is positive,\" \"math is complex\") and can act as \"bags of\nheuristics,\" we ask: can general belief-driven heuristics behind misinformative\nbehaviour be recovered from LLMs as clear rules? A key obstacle is that global\nrule-extraction methods in explainable AI (XAI) are built for numerical\ninputs/outputs, not text. We address this by eliciting global LLM beliefs and\nmapping them to numerical scores via statistically reliable abstractions,\nthereby enabling off-the-shelf global XAI to detect belief-related heuristics\nin LLMs. To obtain ground truth, we hard-code bias-inducing nonlinear\nheuristics of increasing complexity (univariate, conjunctive, nonconvex) into\npopular LLMs (ChatGPT and Llama) via system instructions. This way, we find\nthat RuleFit under-detects non-univariate biases, while global SHAP better\napproximates conjunctive ones but does not yield actionable rules. To bridge\nthis gap, we propose RuleSHAP, a rule-extraction algorithm that couples global\nSHAP-value aggregations with rule induction to better capture non-univariate\nbias, improving heuristics detection over RuleFit by +94% (MRR@1) on average.\nOur results provide a practical pathway for revealing belief-driven biases in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Francesco Sovrano"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Sovrano"
                },
                "author": "Francesco Sovrano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17310v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17310v4",
                "updated": "2025-09-23T15:17:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    17,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2025-01-28T21:43:56Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    43,
                    56,
                    1,
                    28,
                    0
                ],
                "title": "Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds\n  Decoding"
                },
                "summary": "Guesstimation--the task of making approximate quantitative estimates about\nobjects or events-is a common real--world skill, yet remains underexplored in\nlarge language model (LLM) research. We introduce three guesstimation datasets:\nMARBLES, FUTURE, and ELECPRED, spanning physical estimation (e.g., how many\nmarbles fit in a cup) to abstract predictions (e.g., the 2024 U.S. presidential\nelection). Inspired by the social science concept of Wisdom of Crowds (WOC)-\nwhere the median of multiple estimates improves accuracy-we propose WOC\ndecoding for LLMs. We replicate WOC effects in human participants and find that\nLLMs exhibit similar benefits: median aggregation across sampled responses\nconsistently improves accuracy over greedy decoding, self-consistency decoding,\nand mean decoding. This suggests that LLMs encode a world model that supports\napproximate reasoning. Our results position guesstimation as a useful probe of\nLLM world knowledge and highlight WOC decoding as a strategy for enhancing LLM\nguesstimation performance on real-world tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guesstimation--the task of making approximate quantitative estimates about\nobjects or events-is a common real--world skill, yet remains underexplored in\nlarge language model (LLM) research. We introduce three guesstimation datasets:\nMARBLES, FUTURE, and ELECPRED, spanning physical estimation (e.g., how many\nmarbles fit in a cup) to abstract predictions (e.g., the 2024 U.S. presidential\nelection). Inspired by the social science concept of Wisdom of Crowds (WOC)-\nwhere the median of multiple estimates improves accuracy-we propose WOC\ndecoding for LLMs. We replicate WOC effects in human participants and find that\nLLMs exhibit similar benefits: median aggregation across sampled responses\nconsistently improves accuracy over greedy decoding, self-consistency decoding,\nand mean decoding. This suggests that LLMs encode a world model that supports\napproximate reasoning. Our results position guesstimation as a useful probe of\nLLM world knowledge and highlight WOC decoding as a strategy for enhancing LLM\nguesstimation performance on real-world tasks."
                },
                "authors": [
                    {
                        "name": "Yun-Shiuan Chuang"
                    },
                    {
                        "name": "Sameer Narendran"
                    },
                    {
                        "name": "Nikunj Harlalka"
                    },
                    {
                        "name": "Alexander Cheung"
                    },
                    {
                        "name": "Sizhe Gao"
                    },
                    {
                        "name": "Siddharth Suresh"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Timothy T. Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Timothy T. Rogers"
                },
                "author": "Timothy T. Rogers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17310v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17310v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19128v1",
                "updated": "2025-09-23T15:15:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    15,
                    21,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:15:21Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    15,
                    21,
                    1,
                    266,
                    0
                ],
                "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence\n  Generatio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence\n  Generatio"
                },
                "summary": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning\ncapabilities of Large Language Models (LLMs). However, effectively scaling\nthese RL methods presents significant challenges, primarily due to the\ndifficulty in maintaining high AI accelerator utilization without generating\nstale, off-policy data that harms common RL algorithms. This paper introduces\nPipelineRL, an approach designed to achieve a superior trade-off between\nhardware efficiency and data on-policyness for LLM training. PipelineRL employs\nconcurrent asynchronous data generation and model training, distinguished by\nthe novel in-flight weight updates. This mechanism allows the LLM generation\nengine to receive updated model weights with minimal interruption during the\ngeneration of token sequences, thereby maximizing both the accelerator\nutilization and the freshness of training data. Experiments conducted on\nlong-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL\nachieves approximately $\\sim 2x$ faster learning compared to conventional RL\nbaselines while maintaining highly on-policy training data. A scalable and\nmodular open-source implementation of PipelineRL is also released as a key\ncontribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning\ncapabilities of Large Language Models (LLMs). However, effectively scaling\nthese RL methods presents significant challenges, primarily due to the\ndifficulty in maintaining high AI accelerator utilization without generating\nstale, off-policy data that harms common RL algorithms. This paper introduces\nPipelineRL, an approach designed to achieve a superior trade-off between\nhardware efficiency and data on-policyness for LLM training. PipelineRL employs\nconcurrent asynchronous data generation and model training, distinguished by\nthe novel in-flight weight updates. This mechanism allows the LLM generation\nengine to receive updated model weights with minimal interruption during the\ngeneration of token sequences, thereby maximizing both the accelerator\nutilization and the freshness of training data. Experiments conducted on\nlong-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL\nachieves approximately $\\sim 2x$ faster learning compared to conventional RL\nbaselines while maintaining highly on-policy training data. A scalable and\nmodular open-source implementation of PipelineRL is also released as a key\ncontribution."
                },
                "authors": [
                    {
                        "name": "Alexandre PichÃ©"
                    },
                    {
                        "name": "Ehsan Kamaloo"
                    },
                    {
                        "name": "Rafael Pardinas"
                    },
                    {
                        "name": "Dzmitry Bahdanau"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Bahdanau"
                },
                "author": "Dzmitry Bahdanau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15389v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15389v3",
                "updated": "2025-09-23T15:14:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    14,
                    42,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-21T11:26:40Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    11,
                    26,
                    40,
                    2,
                    141,
                    0
                ],
                "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study"
                },
                "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs are more vulnerable to meme-based\nharmful prompts than to synthetic or typographic images. Memes significantly\nincrease harmful responses and decrease refusals compared to text-only inputs.\nThough multi-turn interactions provide partial mitigation, elevated\nvulnerability persists. These results highlight the need for ecologically valid\nevaluations and stronger safety mechanisms. MemeSafetyBench is publicly\navailable at https://github.com/oneonlee/Meme-Safety-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs are more vulnerable to meme-based\nharmful prompts than to synthetic or typographic images. Memes significantly\nincrease harmful responses and decrease refusals compared to text-only inputs.\nThough multi-turn interactions provide partial mitigation, elevated\nvulnerability persists. These results highlight the need for ecologically valid\nevaluations and stronger safety mechanisms. MemeSafetyBench is publicly\navailable at https://github.com/oneonlee/Meme-Safety-Bench."
                },
                "authors": [
                    {
                        "name": "DongGeon Lee"
                    },
                    {
                        "name": "Joonwon Jang"
                    },
                    {
                        "name": "Jihae Jeong"
                    },
                    {
                        "name": "Hwanjo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjo Yu"
                },
                "author": "Hwanjo Yu",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15389v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15389v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19125v1",
                "updated": "2025-09-23T15:12:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    12,
                    58,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:12:58Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    12,
                    58,
                    1,
                    266,
                    0
                ],
                "title": "Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via\n  LLM-Guided Multi-Aspect Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via\n  LLM-Guided Multi-Aspect Clustering"
                },
                "summary": "The rapid growth of scientific literature demands efficient methods to\norganize and synthesize research findings. Existing taxonomy construction\nmethods, leveraging unsupervised clustering or direct prompting of large\nlanguage models (LLMs), often lack coherence and granularity. We propose a\nnovel context-aware hierarchical taxonomy generation framework that integrates\nLLM-guided multi-aspect encoding with dynamic clustering. Our method leverages\nLLMs to identify key aspects of each paper (e.g., methodology, dataset,\nevaluation) and generates aspect-specific paper summaries, which are then\nencoded and clustered along each aspect to form a coherent hierarchy. In\naddition, we introduce a new evaluation benchmark of 156 expert-crafted\ntaxonomies encompassing 11.6k papers, providing the first naturally annotated\ndataset for this task. Experimental results demonstrate that our method\nsignificantly outperforms prior approaches, achieving state-of-the-art\nperformance in taxonomy coherence, granularity, and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of scientific literature demands efficient methods to\norganize and synthesize research findings. Existing taxonomy construction\nmethods, leveraging unsupervised clustering or direct prompting of large\nlanguage models (LLMs), often lack coherence and granularity. We propose a\nnovel context-aware hierarchical taxonomy generation framework that integrates\nLLM-guided multi-aspect encoding with dynamic clustering. Our method leverages\nLLMs to identify key aspects of each paper (e.g., methodology, dataset,\nevaluation) and generates aspect-specific paper summaries, which are then\nencoded and clustered along each aspect to form a coherent hierarchy. In\naddition, we introduce a new evaluation benchmark of 156 expert-crafted\ntaxonomies encompassing 11.6k papers, providing the first naturally annotated\ndataset for this task. Experimental results demonstrate that our method\nsignificantly outperforms prior approaches, achieving state-of-the-art\nperformance in taxonomy coherence, granularity, and interpretability."
                },
                "authors": [
                    {
                        "name": "Kun Zhu"
                    },
                    {
                        "name": "Lizi Liao"
                    },
                    {
                        "name": "Yuxuan Gu"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18080v2",
                "updated": "2025-09-23T15:12:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    12,
                    35,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-25T14:46:07Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    14,
                    46,
                    7,
                    0,
                    237,
                    0
                ],
                "title": "GWTC-4.0: An Introduction to Version 4.0 of the Gravitational-Wave\n  Transient Catalog",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GWTC-4.0: An Introduction to Version 4.0 of the Gravitational-Wave\n  Transient Catalog"
                },
                "summary": "The Gravitational-Wave Transient Catalog (GWTC) is a collection of\nshort-duration (transient) gravitational wave signals identified by the\nLIGO-Virgo-KAGRA Collaboration in gravitational-wave data produced by the\neponymous detectors. The catalog provides information about the identified\ncandidates, such as the arrival time and amplitude of the signal and properties\nof the signal's source as inferred from the observational data. GWTC is the\ndata release of this dataset and version 4.0 extends the catalog to include\nobservations made during the first part of the fourth LIGO-Virgo-KAGRA\nobserving run up until 2024 January 31. This paper marks an introduction to a\ncollection of articles related to this version of the catalog, GWTC-4.0. The\ncollection of articles accompanying the catalog provides documentation of the\nmethods used to analyze the data, summaries of the catalog of events,\nobservational measurements drawn from the population, and detailed discussions\nof selected candidates",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Gravitational-Wave Transient Catalog (GWTC) is a collection of\nshort-duration (transient) gravitational wave signals identified by the\nLIGO-Virgo-KAGRA Collaboration in gravitational-wave data produced by the\neponymous detectors. The catalog provides information about the identified\ncandidates, such as the arrival time and amplitude of the signal and properties\nof the signal's source as inferred from the observational data. GWTC is the\ndata release of this dataset and version 4.0 extends the catalog to include\nobservations made during the first part of the fourth LIGO-Virgo-KAGRA\nobserving run up until 2024 January 31. This paper marks an introduction to a\ncollection of articles related to this version of the catalog, GWTC-4.0. The\ncollection of articles accompanying the catalog provides documentation of the\nmethods used to analyze the data, summaries of the catalog of events,\nobservational measurements drawn from the population, and detailed discussions\nof selected candidates"
                },
                "authors": [
                    {
                        "name": "The LIGO Scientific Collaboration"
                    },
                    {
                        "name": "the Virgo Collaboration"
                    },
                    {
                        "name": "the KAGRA Collaboration"
                    },
                    {
                        "name": "A. G. Abac"
                    },
                    {
                        "name": "I. Abouelfettouh"
                    },
                    {
                        "name": "F. Acernese"
                    },
                    {
                        "name": "K. Ackley"
                    },
                    {
                        "name": "S. Adhicary"
                    },
                    {
                        "name": "D. Adhikari"
                    },
                    {
                        "name": "N. Adhikari"
                    },
                    {
                        "name": "R. X. Adhikari"
                    },
                    {
                        "name": "V. K. Adkins"
                    },
                    {
                        "name": "S. Afroz"
                    },
                    {
                        "name": "D. Agarwal"
                    },
                    {
                        "name": "M. Agathos"
                    },
                    {
                        "name": "M. Aghaei Abchouyeh"
                    },
                    {
                        "name": "O. D. Aguiar"
                    },
                    {
                        "name": "S. Ahmadzadeh"
                    },
                    {
                        "name": "L. Aiello"
                    },
                    {
                        "name": "A. Ain"
                    },
                    {
                        "name": "P. Ajith"
                    },
                    {
                        "name": "S. Akcay"
                    },
                    {
                        "name": "T. Akutsu"
                    },
                    {
                        "name": "S. Albanesi"
                    },
                    {
                        "name": "R. A. Alfaidi"
                    },
                    {
                        "name": "A. Al-Jodah"
                    },
                    {
                        "name": "C. AllÃ©nÃ©"
                    },
                    {
                        "name": "A. Allocca"
                    },
                    {
                        "name": "S. Al-Shammari"
                    },
                    {
                        "name": "P. A. Altin"
                    },
                    {
                        "name": "S. Alvarez-Lopez"
                    },
                    {
                        "name": "O. Amarasinghe"
                    },
                    {
                        "name": "A. Amato"
                    },
                    {
                        "name": "C. Amra"
                    },
                    {
                        "name": "A. Ananyeva"
                    },
                    {
                        "name": "S. B. Anderson"
                    },
                    {
                        "name": "W. G. Anderson"
                    },
                    {
                        "name": "M. Andia"
                    },
                    {
                        "name": "M. Ando"
                    },
                    {
                        "name": "T. Andrade"
                    },
                    {
                        "name": "M. AndrÃ©s-Carcasona"
                    },
                    {
                        "name": "T. AndriÄ"
                    },
                    {
                        "name": "J. Anglin"
                    },
                    {
                        "name": "S. Ansoldi"
                    },
                    {
                        "name": "J. M. Antelis"
                    },
                    {
                        "name": "S. Antier"
                    },
                    {
                        "name": "M. Aoumi"
                    },
                    {
                        "name": "E. Z. Appavuravther"
                    },
                    {
                        "name": "S. Appert"
                    },
                    {
                        "name": "S. K. Apple"
                    },
                    {
                        "name": "K. Arai"
                    },
                    {
                        "name": "A. Araya"
                    },
                    {
                        "name": "M. C. Araya"
                    },
                    {
                        "name": "M. Arca Sedda"
                    },
                    {
                        "name": "J. S. Areeda"
                    },
                    {
                        "name": "L. Argianas"
                    },
                    {
                        "name": "N. Aritomi"
                    },
                    {
                        "name": "F. Armato"
                    },
                    {
                        "name": "S. Armstrong"
                    },
                    {
                        "name": "N. Arnaud"
                    },
                    {
                        "name": "M. Arogeti"
                    },
                    {
                        "name": "S. M. Aronson"
                    },
                    {
                        "name": "G. Ashton"
                    },
                    {
                        "name": "Y. Aso"
                    },
                    {
                        "name": "M. Assiduo"
                    },
                    {
                        "name": "S. Assis de Souza Melo"
                    },
                    {
                        "name": "S. M. Aston"
                    },
                    {
                        "name": "P. Astone"
                    },
                    {
                        "name": "F. Attadio"
                    },
                    {
                        "name": "F. Aubin"
                    },
                    {
                        "name": "K. AultONeal"
                    },
                    {
                        "name": "G. Avallone"
                    },
                    {
                        "name": "S. Babak"
                    },
                    {
                        "name": "F. Badaracco"
                    },
                    {
                        "name": "C. Badger"
                    },
                    {
                        "name": "S. Bae"
                    },
                    {
                        "name": "S. Bagnasco"
                    },
                    {
                        "name": "E. Bagui"
                    },
                    {
                        "name": "L. Baiotti"
                    },
                    {
                        "name": "R. Bajpai"
                    },
                    {
                        "name": "T. Baka"
                    },
                    {
                        "name": "T. Baker"
                    },
                    {
                        "name": "M. Ball"
                    },
                    {
                        "name": "G. Ballardin"
                    },
                    {
                        "name": "S. W. Ballmer"
                    },
                    {
                        "name": "S. Banagiri"
                    },
                    {
                        "name": "B. Banerjee"
                    },
                    {
                        "name": "D. Bankar"
                    },
                    {
                        "name": "T. M. Baptiste"
                    },
                    {
                        "name": "P. Baral"
                    },
                    {
                        "name": "J. C. Barayoga"
                    },
                    {
                        "name": "B. C. Barish"
                    },
                    {
                        "name": "D. Barker"
                    },
                    {
                        "name": "N. Barman"
                    },
                    {
                        "name": "P. Barneo"
                    },
                    {
                        "name": "F. Barone"
                    },
                    {
                        "name": "B. Barr"
                    },
                    {
                        "name": "L. Barsotti"
                    },
                    {
                        "name": "M. Barsuglia"
                    },
                    {
                        "name": "D. Barta"
                    },
                    {
                        "name": "A. M. Bartoletti"
                    },
                    {
                        "name": "M. A. Barton"
                    },
                    {
                        "name": "I. Bartos"
                    },
                    {
                        "name": "S. Basak"
                    },
                    {
                        "name": "A. Basalaev"
                    },
                    {
                        "name": "R. Bassiri"
                    },
                    {
                        "name": "A. Basti"
                    },
                    {
                        "name": "D. E. Bates"
                    },
                    {
                        "name": "M. Bawaj"
                    },
                    {
                        "name": "P. Baxi"
                    },
                    {
                        "name": "J. C. Bayley"
                    },
                    {
                        "name": "A. C. Baylor"
                    },
                    {
                        "name": "P. A. Baynard II"
                    },
                    {
                        "name": "M. Bazzan"
                    },
                    {
                        "name": "V. M. Bedakihale"
                    },
                    {
                        "name": "F. Beirnaert"
                    },
                    {
                        "name": "M. Bejger"
                    },
                    {
                        "name": "D. Belardinelli"
                    },
                    {
                        "name": "A. S. Bell"
                    },
                    {
                        "name": "D. S. Bellie"
                    },
                    {
                        "name": "L. Bellizzi"
                    },
                    {
                        "name": "W. Benoit"
                    },
                    {
                        "name": "I. Bentara"
                    },
                    {
                        "name": "J. D. Bentley"
                    },
                    {
                        "name": "M. Ben Yaala"
                    },
                    {
                        "name": "S. Bera"
                    },
                    {
                        "name": "F. Bergamin"
                    },
                    {
                        "name": "B. K. Berger"
                    },
                    {
                        "name": "S. Bernuzzi"
                    },
                    {
                        "name": "M. Beroiz"
                    },
                    {
                        "name": "C. P. L. Berry"
                    },
                    {
                        "name": "D. Bersanetti"
                    },
                    {
                        "name": "A. Bertolini"
                    },
                    {
                        "name": "J. Betzwieser"
                    },
                    {
                        "name": "D. Beveridge"
                    },
                    {
                        "name": "G. Bevilacqua"
                    },
                    {
                        "name": "N. Bevins"
                    },
                    {
                        "name": "R. Bhandare"
                    },
                    {
                        "name": "S. A. Bhat"
                    },
                    {
                        "name": "R. Bhatt"
                    },
                    {
                        "name": "D. Bhattacharjee"
                    },
                    {
                        "name": "S. Bhaumik"
                    },
                    {
                        "name": "S. Bhowmick"
                    },
                    {
                        "name": "V. Biancalana"
                    },
                    {
                        "name": "A. Bianchi"
                    },
                    {
                        "name": "I. A. Bilenko"
                    },
                    {
                        "name": "G. Billingsley"
                    },
                    {
                        "name": "A. Binetti"
                    },
                    {
                        "name": "S. Bini"
                    },
                    {
                        "name": "C. Binu"
                    },
                    {
                        "name": "O. Birnholtz"
                    },
                    {
                        "name": "S. Biscoveanu"
                    },
                    {
                        "name": "A. Bisht"
                    },
                    {
                        "name": "M. Bitossi"
                    },
                    {
                        "name": "M. -A. Bizouard"
                    },
                    {
                        "name": "S. Blaber"
                    },
                    {
                        "name": "J. K. Blackburn"
                    },
                    {
                        "name": "L. A. Blagg"
                    },
                    {
                        "name": "C. D. Blair"
                    },
                    {
                        "name": "D. G. Blair"
                    },
                    {
                        "name": "F. Bobba"
                    },
                    {
                        "name": "N. Bode"
                    },
                    {
                        "name": "G. Boileau"
                    },
                    {
                        "name": "M. Boldrini"
                    },
                    {
                        "name": "G. N. Bolingbroke"
                    },
                    {
                        "name": "A. Bolliand"
                    },
                    {
                        "name": "L. D. Bonavena"
                    },
                    {
                        "name": "R. Bondarescu"
                    },
                    {
                        "name": "F. Bondu"
                    },
                    {
                        "name": "E. Bonilla"
                    },
                    {
                        "name": "M. S. Bonilla"
                    },
                    {
                        "name": "A. Bonino"
                    },
                    {
                        "name": "R. Bonnand"
                    },
                    {
                        "name": "P. Booker"
                    },
                    {
                        "name": "A. Borchers"
                    },
                    {
                        "name": "S. Borhanian"
                    },
                    {
                        "name": "V. Boschi"
                    },
                    {
                        "name": "S. Bose"
                    },
                    {
                        "name": "V. Bossilkov"
                    },
                    {
                        "name": "A. Boudon"
                    },
                    {
                        "name": "A. Bozzi"
                    },
                    {
                        "name": "C. Bradaschia"
                    },
                    {
                        "name": "P. R. Brady"
                    },
                    {
                        "name": "A. Branch"
                    },
                    {
                        "name": "M. Branchesi"
                    },
                    {
                        "name": "I. Braun"
                    },
                    {
                        "name": "T. Briant"
                    },
                    {
                        "name": "A. Brillet"
                    },
                    {
                        "name": "M. Brinkmann"
                    },
                    {
                        "name": "P. Brockill"
                    },
                    {
                        "name": "E. Brockmueller"
                    },
                    {
                        "name": "A. F. Brooks"
                    },
                    {
                        "name": "B. C. Brown"
                    },
                    {
                        "name": "D. D. Brown"
                    },
                    {
                        "name": "M. L. Brozzetti"
                    },
                    {
                        "name": "S. Brunett"
                    },
                    {
                        "name": "G. Bruno"
                    },
                    {
                        "name": "R. Bruntz"
                    },
                    {
                        "name": "J. Bryant"
                    },
                    {
                        "name": "Y. Bu"
                    },
                    {
                        "name": "F. Bucci"
                    },
                    {
                        "name": "J. Buchanan"
                    },
                    {
                        "name": "O. Bulashenko"
                    },
                    {
                        "name": "T. Bulik"
                    },
                    {
                        "name": "H. J. Bulten"
                    },
                    {
                        "name": "A. Buonanno"
                    },
                    {
                        "name": "K. Burtnyk"
                    },
                    {
                        "name": "R. Buscicchio"
                    },
                    {
                        "name": "D. Buskulic"
                    },
                    {
                        "name": "C. Buy"
                    },
                    {
                        "name": "R. L. Byer"
                    },
                    {
                        "name": "G. S. Cabourn Davies"
                    },
                    {
                        "name": "G. Cabras"
                    },
                    {
                        "name": "R. Cabrita"
                    },
                    {
                        "name": "V. CÃ¡ceres-Barbosa"
                    },
                    {
                        "name": "L. Cadonati"
                    },
                    {
                        "name": "G. Cagnoli"
                    },
                    {
                        "name": "C. Cahillane"
                    },
                    {
                        "name": "A. Calafat"
                    },
                    {
                        "name": "J. CalderÃ³n Bustillo"
                    },
                    {
                        "name": "T. A. Callister"
                    },
                    {
                        "name": "E. Calloni"
                    },
                    {
                        "name": "M. Canepa"
                    },
                    {
                        "name": "G. Caneva Santoro"
                    },
                    {
                        "name": "K. C. Cannon"
                    },
                    {
                        "name": "H. Cao"
                    },
                    {
                        "name": "L. A. Capistran"
                    },
                    {
                        "name": "E. Capocasa"
                    },
                    {
                        "name": "E. Capote"
                    },
                    {
                        "name": "G. Capurri"
                    },
                    {
                        "name": "G. Carapella"
                    },
                    {
                        "name": "F. Carbognani"
                    },
                    {
                        "name": "M. Carlassara"
                    },
                    {
                        "name": "J. B. Carlin"
                    },
                    {
                        "name": "T. K. Carlson"
                    },
                    {
                        "name": "M. F. Carney"
                    },
                    {
                        "name": "M. Carpinelli"
                    },
                    {
                        "name": "G. Carrillo"
                    },
                    {
                        "name": "J. J. Carter"
                    },
                    {
                        "name": "G. Carullo"
                    },
                    {
                        "name": "J. Casanueva Diaz"
                    },
                    {
                        "name": "C. Casentini"
                    },
                    {
                        "name": "S. Y. Castro-Lucas"
                    },
                    {
                        "name": "S. Caudill"
                    },
                    {
                        "name": "M. CavagliÃ "
                    },
                    {
                        "name": "R. Cavalieri"
                    },
                    {
                        "name": "G. Cella"
                    },
                    {
                        "name": "P. CerdÃ¡-DurÃ¡n"
                    },
                    {
                        "name": "E. Cesarini"
                    },
                    {
                        "name": "W. Chaibi"
                    },
                    {
                        "name": "P. Chakraborty"
                    },
                    {
                        "name": "S. Chakraborty"
                    },
                    {
                        "name": "S. Chalathadka Subrahmanya"
                    },
                    {
                        "name": "J. C. L. Chan"
                    },
                    {
                        "name": "M. Chan"
                    },
                    {
                        "name": "R. -J. Chang"
                    },
                    {
                        "name": "S. Chao"
                    },
                    {
                        "name": "E. L. Charlton"
                    },
                    {
                        "name": "P. Charlton"
                    },
                    {
                        "name": "E. Chassande-Mottin"
                    },
                    {
                        "name": "C. Chatterjee"
                    },
                    {
                        "name": "Debarati Chatterjee"
                    },
                    {
                        "name": "Deep Chatterjee"
                    },
                    {
                        "name": "M. Chaturvedi"
                    },
                    {
                        "name": "S. Chaty"
                    },
                    {
                        "name": "K. Chatziioannou"
                    },
                    {
                        "name": "C. Checchia"
                    },
                    {
                        "name": "A. Chen"
                    },
                    {
                        "name": "A. H. -Y. Chen"
                    },
                    {
                        "name": "D. Chen"
                    },
                    {
                        "name": "H. Chen"
                    },
                    {
                        "name": "H. Y. Chen"
                    },
                    {
                        "name": "S. Chen"
                    },
                    {
                        "name": "Y. Chen"
                    },
                    {
                        "name": "Yanbei Chen"
                    },
                    {
                        "name": "Yitian Chen"
                    },
                    {
                        "name": "H. P. Cheng"
                    },
                    {
                        "name": "P. Chessa"
                    },
                    {
                        "name": "H. T. Cheung"
                    },
                    {
                        "name": "S. Y. Cheung"
                    },
                    {
                        "name": "F. Chiadini"
                    },
                    {
                        "name": "G. Chiarini"
                    },
                    {
                        "name": "R. Chierici"
                    },
                    {
                        "name": "A. Chincarini"
                    },
                    {
                        "name": "M. L. Chiofalo"
                    },
                    {
                        "name": "A. Chiummo"
                    },
                    {
                        "name": "C. Chou"
                    },
                    {
                        "name": "S. Choudhary"
                    },
                    {
                        "name": "N. Christensen"
                    },
                    {
                        "name": "S. S. Y. Chua"
                    },
                    {
                        "name": "P. Chugh"
                    },
                    {
                        "name": "G. Ciani"
                    },
                    {
                        "name": "P. Ciecielag"
                    },
                    {
                        "name": "M. CieÅlar"
                    },
                    {
                        "name": "M. Cifaldi"
                    },
                    {
                        "name": "R. Ciolfi"
                    },
                    {
                        "name": "F. Clara"
                    },
                    {
                        "name": "J. A. Clark"
                    },
                    {
                        "name": "J. Clarke"
                    },
                    {
                        "name": "T. A. Clarke"
                    },
                    {
                        "name": "P. Clearwater"
                    },
                    {
                        "name": "S. Clesse"
                    },
                    {
                        "name": "S. M. Clyne"
                    },
                    {
                        "name": "E. Coccia"
                    },
                    {
                        "name": "E. Codazzo"
                    },
                    {
                        "name": "P. -F. Cohadon"
                    },
                    {
                        "name": "S. Colace"
                    },
                    {
                        "name": "E. Colangeli"
                    },
                    {
                        "name": "M. Colleoni"
                    },
                    {
                        "name": "C. G. Collette"
                    },
                    {
                        "name": "J. Collins"
                    },
                    {
                        "name": "S. Colloms"
                    },
                    {
                        "name": "A. Colombo"
                    },
                    {
                        "name": "C. M. Compton"
                    },
                    {
                        "name": "G. Connolly"
                    },
                    {
                        "name": "L. Conti"
                    },
                    {
                        "name": "T. R. Corbitt"
                    },
                    {
                        "name": "I. Cordero-CarriÃ³n"
                    },
                    {
                        "name": "S. Corezzi"
                    },
                    {
                        "name": "N. J. Cornish"
                    },
                    {
                        "name": "A. Corsi"
                    },
                    {
                        "name": "S. Cortese"
                    },
                    {
                        "name": "R. Cottingham"
                    },
                    {
                        "name": "M. W. Coughlin"
                    },
                    {
                        "name": "A. Couineaux"
                    },
                    {
                        "name": "J. -P. Coulon"
                    },
                    {
                        "name": "J. -F. Coupechoux"
                    },
                    {
                        "name": "P. Couvares"
                    },
                    {
                        "name": "D. M. Coward"
                    },
                    {
                        "name": "R. Coyne"
                    },
                    {
                        "name": "K. Craig"
                    },
                    {
                        "name": "J. D. E. Creighton"
                    },
                    {
                        "name": "T. D. Creighton"
                    },
                    {
                        "name": "P. Cremonese"
                    },
                    {
                        "name": "A. W. Criswell"
                    },
                    {
                        "name": "S. Crook"
                    },
                    {
                        "name": "R. Crouch"
                    },
                    {
                        "name": "J. Csizmazia"
                    },
                    {
                        "name": "J. R. Cudell"
                    },
                    {
                        "name": "T. J. Cullen"
                    },
                    {
                        "name": "A. Cumming"
                    },
                    {
                        "name": "E. Cuoco"
                    },
                    {
                        "name": "M. Cusinato"
                    },
                    {
                        "name": "P. Dabadie"
                    },
                    {
                        "name": "L. V. Da ConceiÃ§Ã£o"
                    },
                    {
                        "name": "T. Dal Canton"
                    },
                    {
                        "name": "S. Dall'Osso"
                    },
                    {
                        "name": "S. Dal Pra"
                    },
                    {
                        "name": "G. DÃ¡lya"
                    },
                    {
                        "name": "B. D'Angelo"
                    },
                    {
                        "name": "S. Danilishin"
                    },
                    {
                        "name": "S. D'Antonio"
                    },
                    {
                        "name": "K. Danzmann"
                    },
                    {
                        "name": "K. E. Darroch"
                    },
                    {
                        "name": "L. P. Dartez"
                    },
                    {
                        "name": "A. Dasgupta"
                    },
                    {
                        "name": "S. Datta"
                    },
                    {
                        "name": "V. Dattilo"
                    },
                    {
                        "name": "A. Daumas"
                    },
                    {
                        "name": "N. Davari"
                    },
                    {
                        "name": "I. Dave"
                    },
                    {
                        "name": "A. Davenport"
                    },
                    {
                        "name": "M. Davier"
                    },
                    {
                        "name": "T. F. Davies"
                    },
                    {
                        "name": "D. Davis"
                    },
                    {
                        "name": "L. Davis"
                    },
                    {
                        "name": "M. C. Davis"
                    },
                    {
                        "name": "P. Davis"
                    },
                    {
                        "name": "M. Dax"
                    },
                    {
                        "name": "J. De Bolle"
                    },
                    {
                        "name": "M. Deenadayalan"
                    },
                    {
                        "name": "J. Degallaix"
                    },
                    {
                        "name": "U. Deka"
                    },
                    {
                        "name": "M. De Laurentis"
                    },
                    {
                        "name": "S. DelÃ©glise"
                    },
                    {
                        "name": "F. De Lillo"
                    },
                    {
                        "name": "D. Dell'Aquila"
                    },
                    {
                        "name": "F. Della Valle"
                    },
                    {
                        "name": "W. Del Pozzo"
                    },
                    {
                        "name": "F. De Marco"
                    },
                    {
                        "name": "G. Demasi"
                    },
                    {
                        "name": "F. De Matteis"
                    },
                    {
                        "name": "V. D'Emilio"
                    },
                    {
                        "name": "N. Demos"
                    },
                    {
                        "name": "A. Depasse"
                    },
                    {
                        "name": "N. DePergola"
                    },
                    {
                        "name": "R. De Pietri"
                    },
                    {
                        "name": "R. De Rosa"
                    },
                    {
                        "name": "C. De Rossi"
                    },
                    {
                        "name": "M. Desai"
                    },
                    {
                        "name": "R. DeSalvo"
                    },
                    {
                        "name": "A. DeSimone"
                    },
                    {
                        "name": "R. De Simone"
                    },
                    {
                        "name": "A. Dhani"
                    },
                    {
                        "name": "R. Diab"
                    },
                    {
                        "name": "M. C. DÃ­az"
                    },
                    {
                        "name": "M. Di Cesare"
                    },
                    {
                        "name": "G. Dideron"
                    },
                    {
                        "name": "N. A. Didio"
                    },
                    {
                        "name": "T. Dietrich"
                    },
                    {
                        "name": "L. Di Fiore"
                    },
                    {
                        "name": "C. Di Fronzo"
                    },
                    {
                        "name": "M. Di Giovanni"
                    },
                    {
                        "name": "T. Di Girolamo"
                    },
                    {
                        "name": "D. Diksha"
                    },
                    {
                        "name": "A. Di Michele"
                    },
                    {
                        "name": "J. Ding"
                    },
                    {
                        "name": "S. Di Pace"
                    },
                    {
                        "name": "I. Di Palma"
                    },
                    {
                        "name": "F. Di Renzo"
                    },
                    {
                        "name": "Divyajyoti"
                    },
                    {
                        "name": "A. Dmitriev"
                    },
                    {
                        "name": "Z. Doctor"
                    },
                    {
                        "name": "N. Doerksen"
                    },
                    {
                        "name": "E. Dohmen"
                    },
                    {
                        "name": "D. Dominguez"
                    },
                    {
                        "name": "L. D'Onofrio"
                    },
                    {
                        "name": "F. Donovan"
                    },
                    {
                        "name": "K. L. Dooley"
                    },
                    {
                        "name": "T. Dooney"
                    },
                    {
                        "name": "S. Doravari"
                    },
                    {
                        "name": "O. Dorosh"
                    },
                    {
                        "name": "M. Drago"
                    },
                    {
                        "name": "J. C. Driggers"
                    },
                    {
                        "name": "J. -G. Ducoin"
                    },
                    {
                        "name": "L. Dunn"
                    },
                    {
                        "name": "U. Dupletsa"
                    },
                    {
                        "name": "D. D'Urso"
                    },
                    {
                        "name": "H. Duval"
                    },
                    {
                        "name": "S. E. Dwyer"
                    },
                    {
                        "name": "C. Eassa"
                    },
                    {
                        "name": "M. Ebersold"
                    },
                    {
                        "name": "T. Eckhardt"
                    },
                    {
                        "name": "G. Eddolls"
                    },
                    {
                        "name": "B. Edelman"
                    },
                    {
                        "name": "T. B. Edo"
                    },
                    {
                        "name": "O. Edy"
                    },
                    {
                        "name": "A. Effler"
                    },
                    {
                        "name": "J. Eichholz"
                    },
                    {
                        "name": "H. Einsle"
                    },
                    {
                        "name": "M. Eisenmann"
                    },
                    {
                        "name": "R. A. Eisenstein"
                    },
                    {
                        "name": "A. Ejlli"
                    },
                    {
                        "name": "M. Emma"
                    },
                    {
                        "name": "K. Endo"
                    },
                    {
                        "name": "R. Enficiaud"
                    },
                    {
                        "name": "A. J. Engl"
                    },
                    {
                        "name": "L. Errico"
                    },
                    {
                        "name": "R. Espinosa"
                    },
                    {
                        "name": "M. Esposito"
                    },
                    {
                        "name": "R. C. Essick"
                    },
                    {
                        "name": "H. EstellÃ©s"
                    },
                    {
                        "name": "T. Etzel"
                    },
                    {
                        "name": "M. Evans"
                    },
                    {
                        "name": "T. Evstafyeva"
                    },
                    {
                        "name": "B. E. Ewing"
                    },
                    {
                        "name": "J. M. Ezquiaga"
                    },
                    {
                        "name": "F. Fabrizi"
                    },
                    {
                        "name": "F. Faedi"
                    },
                    {
                        "name": "V. Fafone"
                    },
                    {
                        "name": "S. Fairhurst"
                    },
                    {
                        "name": "A. M. Farah"
                    },
                    {
                        "name": "B. Farr"
                    },
                    {
                        "name": "W. M. Farr"
                    },
                    {
                        "name": "G. Favaro"
                    },
                    {
                        "name": "M. Favata"
                    },
                    {
                        "name": "M. Fays"
                    },
                    {
                        "name": "M. Fazio"
                    },
                    {
                        "name": "J. Feicht"
                    },
                    {
                        "name": "M. M. Fejer"
                    },
                    {
                        "name": "R. Felicetti"
                    },
                    {
                        "name": "E. Fenyvesi"
                    },
                    {
                        "name": "D. L. Ferguson"
                    },
                    {
                        "name": "T. Fernandes"
                    },
                    {
                        "name": "D. Fernando"
                    },
                    {
                        "name": "S. Ferraiuolo"
                    },
                    {
                        "name": "I. Ferrante"
                    },
                    {
                        "name": "T. A. Ferreira"
                    },
                    {
                        "name": "F. Fidecaro"
                    },
                    {
                        "name": "P. Figura"
                    },
                    {
                        "name": "A. Fiori"
                    },
                    {
                        "name": "I. Fiori"
                    },
                    {
                        "name": "M. Fishbach"
                    },
                    {
                        "name": "R. P. Fisher"
                    },
                    {
                        "name": "R. Fittipaldi"
                    },
                    {
                        "name": "V. Fiumara"
                    },
                    {
                        "name": "R. Flaminio"
                    },
                    {
                        "name": "S. M. Fleischer"
                    },
                    {
                        "name": "L. S. Fleming"
                    },
                    {
                        "name": "E. Floden"
                    },
                    {
                        "name": "H. Fong"
                    },
                    {
                        "name": "J. A. Font"
                    },
                    {
                        "name": "C. Foo"
                    },
                    {
                        "name": "B. Fornal"
                    },
                    {
                        "name": "P. W. F. Forsyth"
                    },
                    {
                        "name": "K. Franceschetti"
                    },
                    {
                        "name": "N. Franchini"
                    },
                    {
                        "name": "S. Frasca"
                    },
                    {
                        "name": "F. Frasconi"
                    },
                    {
                        "name": "A. Frattale Mascioli"
                    },
                    {
                        "name": "Z. Frei"
                    },
                    {
                        "name": "A. Freise"
                    },
                    {
                        "name": "O. Freitas"
                    },
                    {
                        "name": "R. Frey"
                    },
                    {
                        "name": "W. Frischhertz"
                    },
                    {
                        "name": "P. Fritschel"
                    },
                    {
                        "name": "V. V. Frolov"
                    },
                    {
                        "name": "G. G. FronzÃ©"
                    },
                    {
                        "name": "M. Fuentes-Garcia"
                    },
                    {
                        "name": "S. Fujii"
                    },
                    {
                        "name": "T. Fujimori"
                    },
                    {
                        "name": "P. Fulda"
                    },
                    {
                        "name": "M. Fyffe"
                    },
                    {
                        "name": "B. Gadre"
                    },
                    {
                        "name": "J. R. Gair"
                    },
                    {
                        "name": "S. Galaudage"
                    },
                    {
                        "name": "V. Galdi"
                    },
                    {
                        "name": "H. Gallagher"
                    },
                    {
                        "name": "B. Gallego"
                    },
                    {
                        "name": "R. Gamba"
                    },
                    {
                        "name": "A. Gamboa"
                    },
                    {
                        "name": "D. Ganapathy"
                    },
                    {
                        "name": "A. Ganguly"
                    },
                    {
                        "name": "B. Garaventa"
                    },
                    {
                        "name": "J. GarcÃ­a-Bellido"
                    },
                    {
                        "name": "C. GarcÃ­a NÃºÃ±ez"
                    },
                    {
                        "name": "C. GarcÃ­a-QuirÃ³s"
                    },
                    {
                        "name": "J. W. Gardner"
                    },
                    {
                        "name": "K. A. Gardner"
                    },
                    {
                        "name": "J. Gargiulo"
                    },
                    {
                        "name": "A. Garron"
                    },
                    {
                        "name": "F. Garufi"
                    },
                    {
                        "name": "P. A. Garver"
                    },
                    {
                        "name": "C. Gasbarra"
                    },
                    {
                        "name": "B. Gateley"
                    },
                    {
                        "name": "F. Gautier"
                    },
                    {
                        "name": "V. Gayathri"
                    },
                    {
                        "name": "T. Gayer"
                    },
                    {
                        "name": "G. Gemme"
                    },
                    {
                        "name": "A. Gennai"
                    },
                    {
                        "name": "V. Gennari"
                    },
                    {
                        "name": "J. George"
                    },
                    {
                        "name": "R. George"
                    },
                    {
                        "name": "O. Gerberding"
                    },
                    {
                        "name": "L. Gergely"
                    },
                    {
                        "name": "Archisman Ghosh"
                    },
                    {
                        "name": "Sayantan Ghosh"
                    },
                    {
                        "name": "Shaon Ghosh"
                    },
                    {
                        "name": "Shrobana Ghosh"
                    },
                    {
                        "name": "Suprovo Ghosh"
                    },
                    {
                        "name": "Tathagata Ghosh"
                    },
                    {
                        "name": "J. A. Giaime"
                    },
                    {
                        "name": "K. D. Giardina"
                    },
                    {
                        "name": "D. R. Gibson"
                    },
                    {
                        "name": "D. T. Gibson"
                    },
                    {
                        "name": "C. Gier"
                    },
                    {
                        "name": "S. Gkaitatzis"
                    },
                    {
                        "name": "J. Glanzer"
                    },
                    {
                        "name": "F. Glotin"
                    },
                    {
                        "name": "J. Godfrey"
                    },
                    {
                        "name": "P. Godwin"
                    },
                    {
                        "name": "A. S. Goettel"
                    },
                    {
                        "name": "E. Goetz"
                    },
                    {
                        "name": "J. Golomb"
                    },
                    {
                        "name": "S. Gomez Lopez"
                    },
                    {
                        "name": "B. Goncharov"
                    },
                    {
                        "name": "Y. Gong"
                    },
                    {
                        "name": "G. GonzÃ¡lez"
                    },
                    {
                        "name": "P. Goodarzi"
                    },
                    {
                        "name": "S. Goode"
                    },
                    {
                        "name": "A. W. Goodwin-Jones"
                    },
                    {
                        "name": "M. Gosselin"
                    },
                    {
                        "name": "R. Gouaty"
                    },
                    {
                        "name": "D. W. Gould"
                    },
                    {
                        "name": "K. Govorkova"
                    },
                    {
                        "name": "S. Goyal"
                    },
                    {
                        "name": "B. Grace"
                    },
                    {
                        "name": "A. Grado"
                    },
                    {
                        "name": "V. Graham"
                    },
                    {
                        "name": "A. E. Granados"
                    },
                    {
                        "name": "M. Granata"
                    },
                    {
                        "name": "V. Granata"
                    },
                    {
                        "name": "S. Gras"
                    },
                    {
                        "name": "P. Grassia"
                    },
                    {
                        "name": "A. Gray"
                    },
                    {
                        "name": "C. Gray"
                    },
                    {
                        "name": "R. Gray"
                    },
                    {
                        "name": "G. Greco"
                    },
                    {
                        "name": "A. C. Green"
                    },
                    {
                        "name": "S. M. Green"
                    },
                    {
                        "name": "S. R. Green"
                    },
                    {
                        "name": "A. M. Gretarsson"
                    },
                    {
                        "name": "E. M. Gretarsson"
                    },
                    {
                        "name": "D. Griffith"
                    },
                    {
                        "name": "W. L. Griffiths"
                    },
                    {
                        "name": "H. L. Griggs"
                    },
                    {
                        "name": "G. Grignani"
                    },
                    {
                        "name": "C. Grimaud"
                    },
                    {
                        "name": "H. Grote"
                    },
                    {
                        "name": "S. Grunewald"
                    },
                    {
                        "name": "D. Guerra"
                    },
                    {
                        "name": "D. Guetta"
                    },
                    {
                        "name": "G. M. Guidi"
                    },
                    {
                        "name": "A. R. Guimaraes"
                    },
                    {
                        "name": "H. K. Gulati"
                    },
                    {
                        "name": "F. Gulminelli"
                    },
                    {
                        "name": "A. M. Gunny"
                    },
                    {
                        "name": "H. Guo"
                    },
                    {
                        "name": "W. Guo"
                    },
                    {
                        "name": "Y. Guo"
                    },
                    {
                        "name": "Anchal Gupta"
                    },
                    {
                        "name": "Anuradha Gupta"
                    },
                    {
                        "name": "I. Gupta"
                    },
                    {
                        "name": "N. C. Gupta"
                    },
                    {
                        "name": "P. Gupta"
                    },
                    {
                        "name": "S. K. Gupta"
                    },
                    {
                        "name": "T. Gupta"
                    },
                    {
                        "name": "V. Gupta"
                    },
                    {
                        "name": "N. Gupte"
                    },
                    {
                        "name": "J. Gurs"
                    },
                    {
                        "name": "N. Gutierrez"
                    },
                    {
                        "name": "F. Guzman"
                    },
                    {
                        "name": "D. Haba"
                    },
                    {
                        "name": "M. Haberland"
                    },
                    {
                        "name": "S. Haino"
                    },
                    {
                        "name": "E. D. Hall"
                    },
                    {
                        "name": "R. Hamburg"
                    },
                    {
                        "name": "E. Z. Hamilton"
                    },
                    {
                        "name": "G. Hammond"
                    },
                    {
                        "name": "W. -B. Han"
                    },
                    {
                        "name": "M. Haney"
                    },
                    {
                        "name": "J. Hanks"
                    },
                    {
                        "name": "C. Hanna"
                    },
                    {
                        "name": "M. D. Hannam"
                    },
                    {
                        "name": "O. A. Hannuksela"
                    },
                    {
                        "name": "A. G. Hanselman"
                    },
                    {
                        "name": "H. Hansen"
                    },
                    {
                        "name": "J. Hanson"
                    },
                    {
                        "name": "R. Harada"
                    },
                    {
                        "name": "A. R. Hardison"
                    },
                    {
                        "name": "S. Harikumar"
                    },
                    {
                        "name": "K. Haris"
                    },
                    {
                        "name": "T. Harmark"
                    },
                    {
                        "name": "J. Harms"
                    },
                    {
                        "name": "G. M. Harry"
                    },
                    {
                        "name": "I. W. Harry"
                    },
                    {
                        "name": "J. Hart"
                    },
                    {
                        "name": "B. Haskell"
                    },
                    {
                        "name": "C. -J. Haster"
                    },
                    {
                        "name": "K. Haughian"
                    },
                    {
                        "name": "H. Hayakawa"
                    },
                    {
                        "name": "K. Hayama"
                    },
                    {
                        "name": "R. Hayes"
                    },
                    {
                        "name": "M. C. Heintze"
                    },
                    {
                        "name": "J. Heinze"
                    },
                    {
                        "name": "J. Heinzel"
                    },
                    {
                        "name": "H. Heitmann"
                    },
                    {
                        "name": "A. Heffernan"
                    },
                    {
                        "name": "F. Hellman"
                    },
                    {
                        "name": "A. F. Helmling-Cornell"
                    },
                    {
                        "name": "G. Hemming"
                    },
                    {
                        "name": "O. Henderson-Sapir"
                    },
                    {
                        "name": "M. Hendry"
                    },
                    {
                        "name": "I. S. Heng"
                    },
                    {
                        "name": "M. H. Hennig"
                    },
                    {
                        "name": "C. Henshaw"
                    },
                    {
                        "name": "M. Heurs"
                    },
                    {
                        "name": "A. L. Hewitt"
                    },
                    {
                        "name": "J. Heyns"
                    },
                    {
                        "name": "S. Higginbotham"
                    },
                    {
                        "name": "S. Hild"
                    },
                    {
                        "name": "S. Hill"
                    },
                    {
                        "name": "Y. Himemoto"
                    },
                    {
                        "name": "N. Hirata"
                    },
                    {
                        "name": "C. Hirose"
                    },
                    {
                        "name": "S. Hochheim"
                    },
                    {
                        "name": "D. Hofman"
                    },
                    {
                        "name": "N. A. Holland"
                    },
                    {
                        "name": "D. E. Holz"
                    },
                    {
                        "name": "L. Honet"
                    },
                    {
                        "name": "C. Hong"
                    },
                    {
                        "name": "S. Hoshino"
                    },
                    {
                        "name": "J. Hough"
                    },
                    {
                        "name": "S. Hourihane"
                    },
                    {
                        "name": "N. T. Howard"
                    },
                    {
                        "name": "E. J. Howell"
                    },
                    {
                        "name": "C. G. Hoy"
                    },
                    {
                        "name": "C. A. Hrishikesh"
                    },
                    {
                        "name": "H. -F. Hsieh"
                    },
                    {
                        "name": "H. -Y. Hsieh"
                    },
                    {
                        "name": "C. Hsiung"
                    },
                    {
                        "name": "W. -F. Hsu"
                    },
                    {
                        "name": "Q. Hu"
                    },
                    {
                        "name": "H. Y. Huang"
                    },
                    {
                        "name": "Y. Huang"
                    },
                    {
                        "name": "Y. T. Huang"
                    },
                    {
                        "name": "A. D. Huddart"
                    },
                    {
                        "name": "B. Hughey"
                    },
                    {
                        "name": "D. C. Y. Hui"
                    },
                    {
                        "name": "V. Hui"
                    },
                    {
                        "name": "S. Husa"
                    },
                    {
                        "name": "R. Huxford"
                    },
                    {
                        "name": "L. Iampieri"
                    },
                    {
                        "name": "G. A. Iandolo"
                    },
                    {
                        "name": "M. Ianni"
                    },
                    {
                        "name": "A. Ierardi"
                    },
                    {
                        "name": "A. Iess"
                    },
                    {
                        "name": "H. Imafuku"
                    },
                    {
                        "name": "K. Inayoshi"
                    },
                    {
                        "name": "Y. Inoue"
                    },
                    {
                        "name": "G. Iorio"
                    },
                    {
                        "name": "P. Iosif"
                    },
                    {
                        "name": "M. H. Iqbal"
                    },
                    {
                        "name": "J. Irwin"
                    },
                    {
                        "name": "R. Ishikawa"
                    },
                    {
                        "name": "M. Isi"
                    },
                    {
                        "name": "Y. Itoh"
                    },
                    {
                        "name": "H. Iwanaga"
                    },
                    {
                        "name": "M. Iwaya"
                    },
                    {
                        "name": "B. R. Iyer"
                    },
                    {
                        "name": "C. Jacquet"
                    },
                    {
                        "name": "P. -E. Jacquet"
                    },
                    {
                        "name": "S. J. Jadhav"
                    },
                    {
                        "name": "S. P. Jadhav"
                    },
                    {
                        "name": "T. Jain"
                    },
                    {
                        "name": "A. L. James"
                    },
                    {
                        "name": "P. A. James"
                    },
                    {
                        "name": "R. Jamshidi"
                    },
                    {
                        "name": "A. Jan"
                    },
                    {
                        "name": "K. Jani"
                    },
                    {
                        "name": "J. Janquart"
                    },
                    {
                        "name": "K. Janssens"
                    },
                    {
                        "name": "N. N. Janthalur"
                    },
                    {
                        "name": "S. Jaraba"
                    },
                    {
                        "name": "P. Jaranowski"
                    },
                    {
                        "name": "R. Jaume"
                    },
                    {
                        "name": "W. Javed"
                    },
                    {
                        "name": "A. Jennings"
                    },
                    {
                        "name": "W. Jia"
                    },
                    {
                        "name": "J. Jiang"
                    },
                    {
                        "name": "S. J. Jin"
                    },
                    {
                        "name": "C. Johanson"
                    },
                    {
                        "name": "G. R. Johns"
                    },
                    {
                        "name": "N. A. Johnson"
                    },
                    {
                        "name": "N. K. Johnson-McDaniel"
                    },
                    {
                        "name": "M. C. Johnston"
                    },
                    {
                        "name": "R. Johnston"
                    },
                    {
                        "name": "N. Johny"
                    },
                    {
                        "name": "D. H. Jones"
                    },
                    {
                        "name": "D. I. Jones"
                    },
                    {
                        "name": "E. J. Jones"
                    },
                    {
                        "name": "R. Jones"
                    },
                    {
                        "name": "S. Jose"
                    },
                    {
                        "name": "P. Joshi"
                    },
                    {
                        "name": "S. K. Joshi"
                    },
                    {
                        "name": "J. Ju"
                    },
                    {
                        "name": "L. Ju"
                    },
                    {
                        "name": "K. Jung"
                    },
                    {
                        "name": "J. Junker"
                    },
                    {
                        "name": "V. Juste"
                    },
                    {
                        "name": "H. B. Kabagoz"
                    },
                    {
                        "name": "T. Kajita"
                    },
                    {
                        "name": "I. Kaku"
                    },
                    {
                        "name": "V. Kalogera"
                    },
                    {
                        "name": "M. Kalomenopoulos"
                    },
                    {
                        "name": "M. Kamiizumi"
                    },
                    {
                        "name": "N. Kanda"
                    },
                    {
                        "name": "S. Kandhasamy"
                    },
                    {
                        "name": "G. Kang"
                    },
                    {
                        "name": "N. C. Kannachel"
                    },
                    {
                        "name": "J. B. Kanner"
                    },
                    {
                        "name": "S. J. Kapadia"
                    },
                    {
                        "name": "D. P. Kapasi"
                    },
                    {
                        "name": "S. Karat"
                    },
                    {
                        "name": "R. Kashyap"
                    },
                    {
                        "name": "M. Kasprzack"
                    },
                    {
                        "name": "W. Kastaun"
                    },
                    {
                        "name": "T. Kato"
                    },
                    {
                        "name": "E. Katsavounidis"
                    },
                    {
                        "name": "W. Katzman"
                    },
                    {
                        "name": "R. Kaushik"
                    },
                    {
                        "name": "K. Kawabe"
                    },
                    {
                        "name": "R. Kawamoto"
                    },
                    {
                        "name": "A. Kazemi"
                    },
                    {
                        "name": "D. Keitel"
                    },
                    {
                        "name": "J. Kennington"
                    },
                    {
                        "name": "R. Kesharwani"
                    },
                    {
                        "name": "J. S. Key"
                    },
                    {
                        "name": "R. Khadela"
                    },
                    {
                        "name": "S. Khadka"
                    },
                    {
                        "name": "F. Y. Khalili"
                    },
                    {
                        "name": "F. Khan"
                    },
                    {
                        "name": "I. Khan"
                    },
                    {
                        "name": "T. Khanam"
                    },
                    {
                        "name": "M. Khursheed"
                    },
                    {
                        "name": "N. M. Khusid"
                    },
                    {
                        "name": "W. Kiendrebeogo"
                    },
                    {
                        "name": "N. Kijbunchoo"
                    },
                    {
                        "name": "C. Kim"
                    },
                    {
                        "name": "J. C. Kim"
                    },
                    {
                        "name": "K. Kim"
                    },
                    {
                        "name": "M. H. Kim"
                    },
                    {
                        "name": "S. Kim"
                    },
                    {
                        "name": "Y. -M. Kim"
                    },
                    {
                        "name": "C. Kimball"
                    },
                    {
                        "name": "M. Kinley-Hanlon"
                    },
                    {
                        "name": "M. Kinnear"
                    },
                    {
                        "name": "J. S. Kissel"
                    },
                    {
                        "name": "S. Klimenko"
                    },
                    {
                        "name": "A. M. Knee"
                    },
                    {
                        "name": "N. Knust"
                    },
                    {
                        "name": "K. Kobayashi"
                    },
                    {
                        "name": "P. Koch"
                    },
                    {
                        "name": "S. M. Koehlenbeck"
                    },
                    {
                        "name": "G. Koekoek"
                    },
                    {
                        "name": "K. Kohri"
                    },
                    {
                        "name": "K. Kokeyama"
                    },
                    {
                        "name": "S. Koley"
                    },
                    {
                        "name": "P. Kolitsidou"
                    },
                    {
                        "name": "K. Komori"
                    },
                    {
                        "name": "A. K. H. Kong"
                    },
                    {
                        "name": "A. Kontos"
                    },
                    {
                        "name": "M. Korobko"
                    },
                    {
                        "name": "R. V. Kossak"
                    },
                    {
                        "name": "X. Kou"
                    },
                    {
                        "name": "A. Koushik"
                    },
                    {
                        "name": "N. Kouvatsos"
                    },
                    {
                        "name": "M. Kovalam"
                    },
                    {
                        "name": "D. B. Kozak"
                    },
                    {
                        "name": "S. L. Kranzhoff"
                    },
                    {
                        "name": "V. Kringel"
                    },
                    {
                        "name": "N. V. Krishnendu"
                    },
                    {
                        "name": "A. KrÃ³lak"
                    },
                    {
                        "name": "K. Kruska"
                    },
                    {
                        "name": "J. Kubisz"
                    },
                    {
                        "name": "G. Kuehn"
                    },
                    {
                        "name": "S. Kulkarni"
                    },
                    {
                        "name": "A. Kulur Ramamohan"
                    },
                    {
                        "name": "A. Kumar"
                    },
                    {
                        "name": "Praveen Kumar"
                    },
                    {
                        "name": "Prayush Kumar"
                    },
                    {
                        "name": "Rahul Kumar"
                    },
                    {
                        "name": "Rakesh Kumar"
                    },
                    {
                        "name": "J. Kume"
                    },
                    {
                        "name": "K. Kuns"
                    },
                    {
                        "name": "N. Kuntimaddi"
                    },
                    {
                        "name": "S. Kuroyanagi"
                    },
                    {
                        "name": "S. Kuwahara"
                    },
                    {
                        "name": "K. Kwak"
                    },
                    {
                        "name": "K. Kwan"
                    },
                    {
                        "name": "J. Kwok"
                    },
                    {
                        "name": "G. Lacaille"
                    },
                    {
                        "name": "P. Lagabbe"
                    },
                    {
                        "name": "D. Laghi"
                    },
                    {
                        "name": "S. Lai"
                    },
                    {
                        "name": "E. Lalande"
                    },
                    {
                        "name": "M. Lalleman"
                    },
                    {
                        "name": "P. C. Lalremruati"
                    },
                    {
                        "name": "M. Landry"
                    },
                    {
                        "name": "B. B. Lane"
                    },
                    {
                        "name": "R. N. Lang"
                    },
                    {
                        "name": "J. Lange"
                    },
                    {
                        "name": "R. Langgin"
                    },
                    {
                        "name": "B. Lantz"
                    },
                    {
                        "name": "A. La Rana"
                    },
                    {
                        "name": "I. La Rosa"
                    },
                    {
                        "name": "J. Larsen"
                    },
                    {
                        "name": "A. Lartaux-Vollard"
                    },
                    {
                        "name": "P. D. Lasky"
                    },
                    {
                        "name": "J. Lawrence"
                    },
                    {
                        "name": "M. N. Lawrence"
                    },
                    {
                        "name": "M. Laxen"
                    },
                    {
                        "name": "C. Lazarte"
                    },
                    {
                        "name": "A. Lazzarini"
                    },
                    {
                        "name": "C. Lazzaro"
                    },
                    {
                        "name": "P. Leaci"
                    },
                    {
                        "name": "L. Leali"
                    },
                    {
                        "name": "Y. K. Lecoeuche"
                    },
                    {
                        "name": "H. M. Lee"
                    },
                    {
                        "name": "H. W. Lee"
                    },
                    {
                        "name": "J. Lee"
                    },
                    {
                        "name": "K. Lee"
                    },
                    {
                        "name": "R. -K. Lee"
                    },
                    {
                        "name": "R. Lee"
                    },
                    {
                        "name": "Sungho Lee"
                    },
                    {
                        "name": "Sunjae Lee"
                    },
                    {
                        "name": "Y. Lee"
                    },
                    {
                        "name": "I. N. Legred"
                    },
                    {
                        "name": "J. Lehmann"
                    },
                    {
                        "name": "L. Lehner"
                    },
                    {
                        "name": "M. Le Jean"
                    },
                    {
                        "name": "A. LemaÃ®tre"
                    },
                    {
                        "name": "M. Lenti"
                    },
                    {
                        "name": "M. Leonardi"
                    },
                    {
                        "name": "M. Lequime"
                    },
                    {
                        "name": "N. Leroy"
                    },
                    {
                        "name": "M. Lesovsky"
                    },
                    {
                        "name": "N. Letendre"
                    },
                    {
                        "name": "M. Lethuillier"
                    },
                    {
                        "name": "Y. Levin"
                    },
                    {
                        "name": "K. Leyde"
                    },
                    {
                        "name": "A. K. Y. Li"
                    },
                    {
                        "name": "K. L. Li"
                    },
                    {
                        "name": "T. G. F. Li"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Y. Li"
                    },
                    {
                        "name": "Z. Li"
                    },
                    {
                        "name": "A. Lihos"
                    },
                    {
                        "name": "C-Y. Lin"
                    },
                    {
                        "name": "E. T. Lin"
                    },
                    {
                        "name": "L. C. -C. Lin"
                    },
                    {
                        "name": "Y. -C. Lin"
                    },
                    {
                        "name": "C. Lindsay"
                    },
                    {
                        "name": "S. D. Linker"
                    },
                    {
                        "name": "T. B. Littenberg"
                    },
                    {
                        "name": "A. Liu"
                    },
                    {
                        "name": "G. C. Liu"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "F. Llamas Villarreal"
                    },
                    {
                        "name": "J. Llobera-Querol"
                    },
                    {
                        "name": "R. K. L. Lo"
                    },
                    {
                        "name": "J. -P. Locquet"
                    },
                    {
                        "name": "M. R. Loizou"
                    },
                    {
                        "name": "L. T. London"
                    },
                    {
                        "name": "A. Longo"
                    },
                    {
                        "name": "D. Lopez"
                    },
                    {
                        "name": "M. Lopez Portilla"
                    },
                    {
                        "name": "A. Lorenzo-Medina"
                    },
                    {
                        "name": "V. Loriette"
                    },
                    {
                        "name": "M. Lormand"
                    },
                    {
                        "name": "G. Losurdo"
                    },
                    {
                        "name": "E. Lotti"
                    },
                    {
                        "name": "T. P. Lott IV"
                    },
                    {
                        "name": "J. D. Lough"
                    },
                    {
                        "name": "H. A. Loughlin"
                    },
                    {
                        "name": "C. O. Lousto"
                    },
                    {
                        "name": "N. Low"
                    },
                    {
                        "name": "M. J. Lowry"
                    },
                    {
                        "name": "N. Lu"
                    },
                    {
                        "name": "L. Lucchesi"
                    },
                    {
                        "name": "H. LÃ¼ck"
                    },
                    {
                        "name": "D. Lumaca"
                    },
                    {
                        "name": "A. P. Lundgren"
                    },
                    {
                        "name": "A. W. Lussier"
                    },
                    {
                        "name": "L. -T. Ma"
                    },
                    {
                        "name": "S. Ma"
                    },
                    {
                        "name": "R. Macas"
                    },
                    {
                        "name": "A. Macedo"
                    },
                    {
                        "name": "M. MacInnis"
                    },
                    {
                        "name": "R. R. Maciy"
                    },
                    {
                        "name": "D. M. Macleod"
                    },
                    {
                        "name": "I. A. O. MacMillan"
                    },
                    {
                        "name": "A. Macquet"
                    },
                    {
                        "name": "D. Macri"
                    },
                    {
                        "name": "K. Maeda"
                    },
                    {
                        "name": "S. Maenaut"
                    },
                    {
                        "name": "S. S. Magare"
                    },
                    {
                        "name": "R. M. Magee"
                    },
                    {
                        "name": "E. Maggio"
                    },
                    {
                        "name": "R. Maggiore"
                    },
                    {
                        "name": "M. Magnozzi"
                    },
                    {
                        "name": "M. Mahesh"
                    },
                    {
                        "name": "M. Maini"
                    },
                    {
                        "name": "S. Majhi"
                    },
                    {
                        "name": "E. Majorana"
                    },
                    {
                        "name": "C. N. Makarem"
                    },
                    {
                        "name": "D. Malakar"
                    },
                    {
                        "name": "J. A. Malaquias-Reis"
                    },
                    {
                        "name": "U. Mali"
                    },
                    {
                        "name": "S. Maliakal"
                    },
                    {
                        "name": "A. Malik"
                    },
                    {
                        "name": "L. Mallick"
                    },
                    {
                        "name": "A. Malz"
                    },
                    {
                        "name": "N. Man"
                    },
                    {
                        "name": "V. Mandic"
                    },
                    {
                        "name": "V. Mangano"
                    },
                    {
                        "name": "B. Mannix"
                    },
                    {
                        "name": "G. L. Mansell"
                    },
                    {
                        "name": "G. Mansingh"
                    },
                    {
                        "name": "M. Manske"
                    },
                    {
                        "name": "M. Mantovani"
                    },
                    {
                        "name": "M. Mapelli"
                    },
                    {
                        "name": "F. Marchesoni"
                    },
                    {
                        "name": "C. Marinelli"
                    },
                    {
                        "name": "D. MarÃ­n Pina"
                    },
                    {
                        "name": "F. Marion"
                    },
                    {
                        "name": "S. MÃ¡rka"
                    },
                    {
                        "name": "Z. MÃ¡rka"
                    },
                    {
                        "name": "A. S. Markosyan"
                    },
                    {
                        "name": "A. Markowitz"
                    },
                    {
                        "name": "E. Maros"
                    },
                    {
                        "name": "S. Marsat"
                    },
                    {
                        "name": "F. Martelli"
                    },
                    {
                        "name": "I. W. Martin"
                    },
                    {
                        "name": "R. M. Martin"
                    },
                    {
                        "name": "B. B. Martinez"
                    },
                    {
                        "name": "M. Martinez"
                    },
                    {
                        "name": "V. Martinez"
                    },
                    {
                        "name": "A. Martini"
                    },
                    {
                        "name": "J. C. Martins"
                    },
                    {
                        "name": "D. V. Martynov"
                    },
                    {
                        "name": "E. J. Marx"
                    },
                    {
                        "name": "L. Massaro"
                    },
                    {
                        "name": "A. Masserot"
                    },
                    {
                        "name": "M. Masso-Reid"
                    },
                    {
                        "name": "M. Mastrodicasa"
                    },
                    {
                        "name": "S. Mastrogiovanni"
                    },
                    {
                        "name": "T. Matcovich"
                    },
                    {
                        "name": "M. Matiushechkina"
                    },
                    {
                        "name": "M. Matsuyama"
                    },
                    {
                        "name": "N. Mavalvala"
                    },
                    {
                        "name": "N. Maxwell"
                    },
                    {
                        "name": "G. McCarrol"
                    },
                    {
                        "name": "R. McCarthy"
                    },
                    {
                        "name": "D. E. McClelland"
                    },
                    {
                        "name": "S. McCormick"
                    },
                    {
                        "name": "L. McCuller"
                    },
                    {
                        "name": "S. McEachin"
                    },
                    {
                        "name": "C. McElhenny"
                    },
                    {
                        "name": "G. I. McGhee"
                    },
                    {
                        "name": "J. McGinn"
                    },
                    {
                        "name": "K. B. M. McGowan"
                    },
                    {
                        "name": "J. McIver"
                    },
                    {
                        "name": "A. McLeod"
                    },
                    {
                        "name": "T. McRae"
                    },
                    {
                        "name": "D. Meacher"
                    },
                    {
                        "name": "Q. Meijer"
                    },
                    {
                        "name": "A. Melatos"
                    },
                    {
                        "name": "M. Melching"
                    },
                    {
                        "name": "S. Mellaerts"
                    },
                    {
                        "name": "C. S. Menoni"
                    },
                    {
                        "name": "F. Mera"
                    },
                    {
                        "name": "R. A. Mercer"
                    },
                    {
                        "name": "L. Mereni"
                    },
                    {
                        "name": "K. Merfeld"
                    },
                    {
                        "name": "E. L. Merilh"
                    },
                    {
                        "name": "J. R. MÃ©rou"
                    },
                    {
                        "name": "J. D. Merritt"
                    },
                    {
                        "name": "M. Merzougui"
                    },
                    {
                        "name": "C. Messenger"
                    },
                    {
                        "name": "C. Messick"
                    },
                    {
                        "name": "B. Mestichelli"
                    },
                    {
                        "name": "M. Meyer-Conde"
                    },
                    {
                        "name": "F. Meylahn"
                    },
                    {
                        "name": "A. Mhaske"
                    },
                    {
                        "name": "A. Miani"
                    },
                    {
                        "name": "H. Miao"
                    },
                    {
                        "name": "I. Michaloliakos"
                    },
                    {
                        "name": "C. Michel"
                    },
                    {
                        "name": "Y. Michimura"
                    },
                    {
                        "name": "H. Middleton"
                    },
                    {
                        "name": "S. J. Miller"
                    },
                    {
                        "name": "M. Millhouse"
                    },
                    {
                        "name": "E. Milotti"
                    },
                    {
                        "name": "V. Milotti"
                    },
                    {
                        "name": "Y. Minenkov"
                    },
                    {
                        "name": "N. Mio"
                    },
                    {
                        "name": "Ll. M. Mir"
                    },
                    {
                        "name": "L. Mirasola"
                    },
                    {
                        "name": "M. Miravet-TenÃ©s"
                    },
                    {
                        "name": "C. -A. Miritescu"
                    },
                    {
                        "name": "A. K. Mishra"
                    },
                    {
                        "name": "A. Mishra"
                    },
                    {
                        "name": "C. Mishra"
                    },
                    {
                        "name": "T. Mishra"
                    },
                    {
                        "name": "A. L. Mitchell"
                    },
                    {
                        "name": "J. G. Mitchell"
                    },
                    {
                        "name": "S. Mitra"
                    },
                    {
                        "name": "V. P. Mitrofanov"
                    },
                    {
                        "name": "R. Mittleman"
                    },
                    {
                        "name": "O. Miyakawa"
                    },
                    {
                        "name": "S. Miyamoto"
                    },
                    {
                        "name": "S. Miyoki"
                    },
                    {
                        "name": "G. Mo"
                    },
                    {
                        "name": "L. Mobilia"
                    },
                    {
                        "name": "S. R. P. Mohapatra"
                    },
                    {
                        "name": "S. R. Mohite"
                    },
                    {
                        "name": "M. Molina-Ruiz"
                    },
                    {
                        "name": "C. Mondal"
                    },
                    {
                        "name": "M. Mondin"
                    },
                    {
                        "name": "M. Montani"
                    },
                    {
                        "name": "C. J. Moore"
                    },
                    {
                        "name": "D. Moraru"
                    },
                    {
                        "name": "A. More"
                    },
                    {
                        "name": "S. More"
                    },
                    {
                        "name": "E. A. Moreno"
                    },
                    {
                        "name": "G. Moreno"
                    },
                    {
                        "name": "S. Morisaki"
                    },
                    {
                        "name": "Y. Moriwaki"
                    },
                    {
                        "name": "G. Morras"
                    },
                    {
                        "name": "A. Moscatello"
                    },
                    {
                        "name": "M. Mould"
                    },
                    {
                        "name": "P. Mourier"
                    },
                    {
                        "name": "B. Mours"
                    },
                    {
                        "name": "C. M. Mow-Lowry"
                    },
                    {
                        "name": "F. Muciaccia"
                    },
                    {
                        "name": "D. Mukherjee"
                    },
                    {
                        "name": "Samanwaya Mukherjee"
                    },
                    {
                        "name": "Soma Mukherjee"
                    },
                    {
                        "name": "Subroto Mukherjee"
                    },
                    {
                        "name": "Suvodip Mukherjee"
                    },
                    {
                        "name": "N. Mukund"
                    },
                    {
                        "name": "A. Mullavey"
                    },
                    {
                        "name": "H. Mullock"
                    },
                    {
                        "name": "J. Munch"
                    },
                    {
                        "name": "J. Mundi"
                    },
                    {
                        "name": "C. L. Mungioli"
                    },
                    {
                        "name": "Y. Murakami"
                    },
                    {
                        "name": "M. Murakoshi"
                    },
                    {
                        "name": "P. G. Murray"
                    },
                    {
                        "name": "S. Muusse"
                    },
                    {
                        "name": "D. Nabari"
                    },
                    {
                        "name": "S. L. Nadji"
                    },
                    {
                        "name": "A. Nagar"
                    },
                    {
                        "name": "N. Nagarajan"
                    },
                    {
                        "name": "K. Nakagaki"
                    },
                    {
                        "name": "K. Nakamura"
                    },
                    {
                        "name": "H. Nakano"
                    },
                    {
                        "name": "M. Nakano"
                    },
                    {
                        "name": "D. Nanadoumgar-Lacroze"
                    },
                    {
                        "name": "D. Nandi"
                    },
                    {
                        "name": "V. Napolano"
                    },
                    {
                        "name": "P. Narayan"
                    },
                    {
                        "name": "I. Nardecchia"
                    },
                    {
                        "name": "T. Narikawa"
                    },
                    {
                        "name": "H. Narola"
                    },
                    {
                        "name": "L. Naticchioni"
                    },
                    {
                        "name": "R. K. Nayak"
                    },
                    {
                        "name": "A. Nela"
                    },
                    {
                        "name": "A. Nelson"
                    },
                    {
                        "name": "T. J. N. Nelson"
                    },
                    {
                        "name": "M. Nery"
                    },
                    {
                        "name": "A. Neunzert"
                    },
                    {
                        "name": "S. Ng"
                    },
                    {
                        "name": "L. Nguyen Quynh"
                    },
                    {
                        "name": "S. A. Nichols"
                    },
                    {
                        "name": "A. B. Nielsen"
                    },
                    {
                        "name": "G. Nieradka"
                    },
                    {
                        "name": "Y. Nishino"
                    },
                    {
                        "name": "A. Nishizawa"
                    },
                    {
                        "name": "S. Nissanke"
                    },
                    {
                        "name": "E. Nitoglia"
                    },
                    {
                        "name": "W. Niu"
                    },
                    {
                        "name": "F. Nocera"
                    },
                    {
                        "name": "M. Norman"
                    },
                    {
                        "name": "C. North"
                    },
                    {
                        "name": "J. Novak"
                    },
                    {
                        "name": "J. F. NuÃ±o Siles"
                    },
                    {
                        "name": "L. K. Nuttall"
                    },
                    {
                        "name": "K. Obayashi"
                    },
                    {
                        "name": "J. Oberling"
                    },
                    {
                        "name": "J. O'Dell"
                    },
                    {
                        "name": "M. Oertel"
                    },
                    {
                        "name": "A. Offermans"
                    },
                    {
                        "name": "G. Oganesyan"
                    },
                    {
                        "name": "J. J. Oh"
                    },
                    {
                        "name": "K. Oh"
                    },
                    {
                        "name": "T. O'Hanlon"
                    },
                    {
                        "name": "M. Ohashi"
                    },
                    {
                        "name": "M. Ohkawa"
                    },
                    {
                        "name": "F. Ohme"
                    },
                    {
                        "name": "R. Oliveri"
                    },
                    {
                        "name": "R. Omer"
                    },
                    {
                        "name": "B. O'Neal"
                    },
                    {
                        "name": "K. Oohara"
                    },
                    {
                        "name": "B. O'Reilly"
                    },
                    {
                        "name": "R. Oram"
                    },
                    {
                        "name": "N. D. Ormsby"
                    },
                    {
                        "name": "M. Orselli"
                    },
                    {
                        "name": "R. O'Shaughnessy"
                    },
                    {
                        "name": "S. O'Shea"
                    },
                    {
                        "name": "Y. Oshima"
                    },
                    {
                        "name": "S. Oshino"
                    },
                    {
                        "name": "C. Osthelder"
                    },
                    {
                        "name": "I. Ota"
                    },
                    {
                        "name": "D. J. Ottaway"
                    },
                    {
                        "name": "A. Ouzriat"
                    },
                    {
                        "name": "H. Overmier"
                    },
                    {
                        "name": "B. J. Owen"
                    },
                    {
                        "name": "A. E. Pace"
                    },
                    {
                        "name": "R. Pagano"
                    },
                    {
                        "name": "M. A. Page"
                    },
                    {
                        "name": "A. Pai"
                    },
                    {
                        "name": "L. Paiella"
                    },
                    {
                        "name": "A. Pal"
                    },
                    {
                        "name": "S. Pal"
                    },
                    {
                        "name": "M. A. Palaia"
                    },
                    {
                        "name": "M. PÃ¡lfi"
                    },
                    {
                        "name": "P. P. Palma"
                    },
                    {
                        "name": "C. Palomba"
                    },
                    {
                        "name": "P. Palud"
                    },
                    {
                        "name": "J. Pan"
                    },
                    {
                        "name": "K. C. Pan"
                    },
                    {
                        "name": "R. Panai"
                    },
                    {
                        "name": "P. K. Panda"
                    },
                    {
                        "name": "Shiksha Pandey"
                    },
                    {
                        "name": "Swadha Pandey"
                    },
                    {
                        "name": "P. T. H. Pang"
                    },
                    {
                        "name": "F. Pannarale"
                    },
                    {
                        "name": "K. A. Pannone"
                    },
                    {
                        "name": "B. C. Pant"
                    },
                    {
                        "name": "F. H. Panther"
                    },
                    {
                        "name": "F. Paoletti"
                    },
                    {
                        "name": "A. Paolone"
                    },
                    {
                        "name": "A. Papadopoulos"
                    },
                    {
                        "name": "E. E. Papalexakis"
                    },
                    {
                        "name": "L. Papalini"
                    },
                    {
                        "name": "G. Papigkiotis"
                    },
                    {
                        "name": "A. Paquis"
                    },
                    {
                        "name": "A. Parisi"
                    },
                    {
                        "name": "B. -J. Park"
                    },
                    {
                        "name": "J. Park"
                    },
                    {
                        "name": "W. Parker"
                    },
                    {
                        "name": "G. Pascale"
                    },
                    {
                        "name": "D. Pascucci"
                    },
                    {
                        "name": "A. Pasqualetti"
                    },
                    {
                        "name": "R. Passaquieti"
                    },
                    {
                        "name": "L. Passenger"
                    },
                    {
                        "name": "D. Passuello"
                    },
                    {
                        "name": "O. Patane"
                    },
                    {
                        "name": "D. Pathak"
                    },
                    {
                        "name": "L. Pathak"
                    },
                    {
                        "name": "A. Patra"
                    },
                    {
                        "name": "B. Patricelli"
                    },
                    {
                        "name": "A. S. Patron"
                    },
                    {
                        "name": "B. G. Patterson"
                    },
                    {
                        "name": "K. Paul"
                    },
                    {
                        "name": "S. Paul"
                    },
                    {
                        "name": "E. Payne"
                    },
                    {
                        "name": "T. Pearce"
                    },
                    {
                        "name": "M. Pedraza"
                    },
                    {
                        "name": "A. Pele"
                    },
                    {
                        "name": "F. E. PeÃ±a Arellano"
                    },
                    {
                        "name": "S. Penn"
                    },
                    {
                        "name": "M. D. Penuliar"
                    },
                    {
                        "name": "A. Perego"
                    },
                    {
                        "name": "Z. Pereira"
                    },
                    {
                        "name": "J. J. Perez"
                    },
                    {
                        "name": "C. PÃ©rigois"
                    },
                    {
                        "name": "G. Perna"
                    },
                    {
                        "name": "A. Perreca"
                    },
                    {
                        "name": "J. Perret"
                    },
                    {
                        "name": "S. PerriÃ¨s"
                    },
                    {
                        "name": "J. W. Perry"
                    },
                    {
                        "name": "D. Pesios"
                    },
                    {
                        "name": "S. Petracca"
                    },
                    {
                        "name": "C. Petrillo"
                    },
                    {
                        "name": "H. P. Pfeiffer"
                    },
                    {
                        "name": "H. Pham"
                    },
                    {
                        "name": "K. A. Pham"
                    },
                    {
                        "name": "K. S. Phukon"
                    },
                    {
                        "name": "H. Phurailatpam"
                    },
                    {
                        "name": "M. Piarulli"
                    },
                    {
                        "name": "L. Piccari"
                    },
                    {
                        "name": "O. J. Piccinni"
                    },
                    {
                        "name": "M. Pichot"
                    },
                    {
                        "name": "M. Piendibene"
                    },
                    {
                        "name": "F. Piergiovanni"
                    },
                    {
                        "name": "L. Pierini"
                    },
                    {
                        "name": "G. Pierra"
                    },
                    {
                        "name": "V. Pierro"
                    },
                    {
                        "name": "M. Pietrzak"
                    },
                    {
                        "name": "M. Pillas"
                    },
                    {
                        "name": "F. Pilo"
                    },
                    {
                        "name": "L. Pinard"
                    },
                    {
                        "name": "I. M. Pinto"
                    },
                    {
                        "name": "M. Pinto"
                    },
                    {
                        "name": "B. J. Piotrzkowski"
                    },
                    {
                        "name": "M. Pirello"
                    },
                    {
                        "name": "M. D. Pitkin"
                    },
                    {
                        "name": "A. Placidi"
                    },
                    {
                        "name": "E. Placidi"
                    },
                    {
                        "name": "M. L. Planas"
                    },
                    {
                        "name": "W. Plastino"
                    },
                    {
                        "name": "C. Plunkett"
                    },
                    {
                        "name": "R. Poggiani"
                    },
                    {
                        "name": "E. Polini"
                    },
                    {
                        "name": "L. Pompili"
                    },
                    {
                        "name": "J. Poon"
                    },
                    {
                        "name": "E. Porcelli"
                    },
                    {
                        "name": "E. K. Porter"
                    },
                    {
                        "name": "C. Posnansky"
                    },
                    {
                        "name": "R. Poulton"
                    },
                    {
                        "name": "J. Powell"
                    },
                    {
                        "name": "M. Pracchia"
                    },
                    {
                        "name": "B. K. Pradhan"
                    },
                    {
                        "name": "T. Pradier"
                    },
                    {
                        "name": "A. K. Prajapati"
                    },
                    {
                        "name": "K. Prasai"
                    },
                    {
                        "name": "R. Prasanna"
                    },
                    {
                        "name": "P. Prasia"
                    },
                    {
                        "name": "G. Pratten"
                    },
                    {
                        "name": "G. Principe"
                    },
                    {
                        "name": "M. Principe"
                    },
                    {
                        "name": "G. A. Prodi"
                    },
                    {
                        "name": "L. Prokhorov"
                    },
                    {
                        "name": "P. Prosperi"
                    },
                    {
                        "name": "P. Prosposito"
                    },
                    {
                        "name": "A. C. Providence"
                    },
                    {
                        "name": "A. Puecher"
                    },
                    {
                        "name": "J. Pullin"
                    },
                    {
                        "name": "M. Punturo"
                    },
                    {
                        "name": "P. Puppo"
                    },
                    {
                        "name": "M. PÃ¼rrer"
                    },
                    {
                        "name": "H. Qi"
                    },
                    {
                        "name": "J. Qin"
                    },
                    {
                        "name": "G. QuÃ©mÃ©ner"
                    },
                    {
                        "name": "V. Quetschke"
                    },
                    {
                        "name": "P. J. Quinonez"
                    },
                    {
                        "name": "F. J. Raab"
                    },
                    {
                        "name": "I. Rainho"
                    },
                    {
                        "name": "S. Raja"
                    },
                    {
                        "name": "C. Rajan"
                    },
                    {
                        "name": "B. Rajbhandari"
                    },
                    {
                        "name": "K. E. Ramirez"
                    },
                    {
                        "name": "F. A. Ramis Vidal"
                    },
                    {
                        "name": "A. Ramos-Buades"
                    },
                    {
                        "name": "D. Rana"
                    },
                    {
                        "name": "S. Ranjan"
                    },
                    {
                        "name": "K. Ransom"
                    },
                    {
                        "name": "P. Rapagnani"
                    },
                    {
                        "name": "B. Ratto"
                    },
                    {
                        "name": "A. Ray"
                    },
                    {
                        "name": "V. Raymond"
                    },
                    {
                        "name": "M. Razzano"
                    },
                    {
                        "name": "J. Read"
                    },
                    {
                        "name": "M. Recaman Payo"
                    },
                    {
                        "name": "T. Regimbau"
                    },
                    {
                        "name": "L. Rei"
                    },
                    {
                        "name": "S. Reid"
                    },
                    {
                        "name": "D. H. Reitze"
                    },
                    {
                        "name": "P. Relton"
                    },
                    {
                        "name": "A. I. Renzini"
                    },
                    {
                        "name": "B. Revenu"
                    },
                    {
                        "name": "R. Reyes"
                    },
                    {
                        "name": "A. S. Rezaei"
                    },
                    {
                        "name": "F. Ricci"
                    },
                    {
                        "name": "M. Ricci"
                    },
                    {
                        "name": "A. Ricciardone"
                    },
                    {
                        "name": "J. W. Richardson"
                    },
                    {
                        "name": "M. Richardson"
                    },
                    {
                        "name": "A. Rijal"
                    },
                    {
                        "name": "K. Riles"
                    },
                    {
                        "name": "H. K. Riley"
                    },
                    {
                        "name": "S. Rinaldi"
                    },
                    {
                        "name": "J. Rittmeyer"
                    },
                    {
                        "name": "C. Robertson"
                    },
                    {
                        "name": "F. Robinet"
                    },
                    {
                        "name": "M. Robinson"
                    },
                    {
                        "name": "A. Rocchi"
                    },
                    {
                        "name": "L. Rolland"
                    },
                    {
                        "name": "J. G. Rollins"
                    },
                    {
                        "name": "A. E. Romano"
                    },
                    {
                        "name": "R. Romano"
                    },
                    {
                        "name": "A. Romero"
                    },
                    {
                        "name": "I. M. Romero-Shaw"
                    },
                    {
                        "name": "J. H. Romie"
                    },
                    {
                        "name": "S. Ronchini"
                    },
                    {
                        "name": "T. J. Roocke"
                    },
                    {
                        "name": "L. Rosa"
                    },
                    {
                        "name": "T. J. Rosauer"
                    },
                    {
                        "name": "C. A. Rose"
                    },
                    {
                        "name": "D. RosiÅska"
                    },
                    {
                        "name": "M. P. Ross"
                    },
                    {
                        "name": "M. Rossello-Sastre"
                    },
                    {
                        "name": "S. Rowan"
                    },
                    {
                        "name": "S. Roy"
                    },
                    {
                        "name": "S. K. Roy"
                    },
                    {
                        "name": "D. Rozza"
                    },
                    {
                        "name": "P. Ruggi"
                    },
                    {
                        "name": "N. Ruhama"
                    },
                    {
                        "name": "E. Ruiz Morales"
                    },
                    {
                        "name": "K. Ruiz-Rocha"
                    },
                    {
                        "name": "S. Sachdev"
                    },
                    {
                        "name": "T. Sadecki"
                    },
                    {
                        "name": "J. Sadiq"
                    },
                    {
                        "name": "P. Saffarieh"
                    },
                    {
                        "name": "S. Safi-Harb"
                    },
                    {
                        "name": "M. R. Sah"
                    },
                    {
                        "name": "S. Saha"
                    },
                    {
                        "name": "T. Sainrat"
                    },
                    {
                        "name": "S. Sajith Menon"
                    },
                    {
                        "name": "K. Sakai"
                    },
                    {
                        "name": "M. Sakellariadou"
                    },
                    {
                        "name": "S. Sakon"
                    },
                    {
                        "name": "O. S. Salafia"
                    },
                    {
                        "name": "F. Salces-Carcoba"
                    },
                    {
                        "name": "L. Salconi"
                    },
                    {
                        "name": "M. Saleem"
                    },
                    {
                        "name": "F. Salemi"
                    },
                    {
                        "name": "M. SallÃ©"
                    },
                    {
                        "name": "S. U. Salunkhe"
                    },
                    {
                        "name": "S. Salvador"
                    },
                    {
                        "name": "A. Samajdar"
                    },
                    {
                        "name": "A. Sanchez"
                    },
                    {
                        "name": "E. J. Sanchez"
                    },
                    {
                        "name": "J. H. Sanchez"
                    },
                    {
                        "name": "L. E. Sanchez"
                    },
                    {
                        "name": "N. Sanchis-Gual"
                    },
                    {
                        "name": "J. R. Sanders"
                    },
                    {
                        "name": "E. M. SÃ¤nger"
                    },
                    {
                        "name": "F. Santoliquido"
                    },
                    {
                        "name": "F. Sarandrea"
                    },
                    {
                        "name": "T. R. Saravanan"
                    },
                    {
                        "name": "N. Sarin"
                    },
                    {
                        "name": "P. Sarkar"
                    },
                    {
                        "name": "S. Sasaoka"
                    },
                    {
                        "name": "A. Sasli"
                    },
                    {
                        "name": "P. Sassi"
                    },
                    {
                        "name": "B. Sassolas"
                    },
                    {
                        "name": "B. S. Sathyaprakash"
                    },
                    {
                        "name": "R. Sato"
                    },
                    {
                        "name": "Y. Sato"
                    },
                    {
                        "name": "O. Sauter"
                    },
                    {
                        "name": "R. L. Savage"
                    },
                    {
                        "name": "T. Sawada"
                    },
                    {
                        "name": "H. L. Sawant"
                    },
                    {
                        "name": "S. Sayah"
                    },
                    {
                        "name": "V. Scacco"
                    },
                    {
                        "name": "D. Schaetzl"
                    },
                    {
                        "name": "M. Scheel"
                    },
                    {
                        "name": "A. Schiebelbein"
                    },
                    {
                        "name": "M. G. Schiworski"
                    },
                    {
                        "name": "P. Schmidt"
                    },
                    {
                        "name": "S. Schmidt"
                    },
                    {
                        "name": "R. Schnabel"
                    },
                    {
                        "name": "M. Schneewind"
                    },
                    {
                        "name": "R. M. S. Schofield"
                    },
                    {
                        "name": "K. Schouteden"
                    },
                    {
                        "name": "B. W. Schulte"
                    },
                    {
                        "name": "B. F. Schutz"
                    },
                    {
                        "name": "E. Schwartz"
                    },
                    {
                        "name": "M. Scialpi"
                    },
                    {
                        "name": "J. Scott"
                    },
                    {
                        "name": "S. M. Scott"
                    },
                    {
                        "name": "R. M. Sedas"
                    },
                    {
                        "name": "T. C. Seetharamu"
                    },
                    {
                        "name": "M. Seglar-Arroyo"
                    },
                    {
                        "name": "Y. Sekiguchi"
                    },
                    {
                        "name": "D. Sellers"
                    },
                    {
                        "name": "A. S. Sengupta"
                    },
                    {
                        "name": "D. Sentenac"
                    },
                    {
                        "name": "E. G. Seo"
                    },
                    {
                        "name": "J. W. Seo"
                    },
                    {
                        "name": "V. Sequino"
                    },
                    {
                        "name": "M. Serra"
                    },
                    {
                        "name": "G. Servignat"
                    },
                    {
                        "name": "A. Sevrin"
                    },
                    {
                        "name": "T. Shaffer"
                    },
                    {
                        "name": "U. S. Shah"
                    },
                    {
                        "name": "M. S. Shahriar"
                    },
                    {
                        "name": "M. A. Shaikh"
                    },
                    {
                        "name": "L. Shao"
                    },
                    {
                        "name": "A. Sharma"
                    },
                    {
                        "name": "A. K. Sharma"
                    },
                    {
                        "name": "P. Sharma"
                    },
                    {
                        "name": "S. Sharma Chaudhary"
                    },
                    {
                        "name": "M. R. Shaw"
                    },
                    {
                        "name": "P. Shawhan"
                    },
                    {
                        "name": "N. S. Shcheblanov"
                    },
                    {
                        "name": "Y. Shikano"
                    },
                    {
                        "name": "M. Shikauchi"
                    },
                    {
                        "name": "K. Shimode"
                    },
                    {
                        "name": "H. Shinkai"
                    },
                    {
                        "name": "J. Shiota"
                    },
                    {
                        "name": "S. Shirke"
                    },
                    {
                        "name": "D. H. Shoemaker"
                    },
                    {
                        "name": "D. M. Shoemaker"
                    },
                    {
                        "name": "R. W. Short"
                    },
                    {
                        "name": "S. ShyamSundar"
                    },
                    {
                        "name": "A. Sider"
                    },
                    {
                        "name": "H. Siegel"
                    },
                    {
                        "name": "D. Sigg"
                    },
                    {
                        "name": "L. Silenzi"
                    },
                    {
                        "name": "M. Simmonds"
                    },
                    {
                        "name": "L. P. Singer"
                    },
                    {
                        "name": "A. Singh"
                    },
                    {
                        "name": "D. Singh"
                    },
                    {
                        "name": "M. K. Singh"
                    },
                    {
                        "name": "N. Singh"
                    },
                    {
                        "name": "S. Singh"
                    },
                    {
                        "name": "A. Singha"
                    },
                    {
                        "name": "A. M. Sintes"
                    },
                    {
                        "name": "V. Sipala"
                    },
                    {
                        "name": "V. Skliris"
                    },
                    {
                        "name": "B. J. J. Slagmolen"
                    },
                    {
                        "name": "D. A. Slater"
                    },
                    {
                        "name": "T. J. Slaven-Blair"
                    },
                    {
                        "name": "J. Smetana"
                    },
                    {
                        "name": "J. R. Smith"
                    },
                    {
                        "name": "L. Smith"
                    },
                    {
                        "name": "R. J. E. Smith"
                    },
                    {
                        "name": "W. J. Smith"
                    },
                    {
                        "name": "K. Somiya"
                    },
                    {
                        "name": "I. Song"
                    },
                    {
                        "name": "K. Soni"
                    },
                    {
                        "name": "S. Soni"
                    },
                    {
                        "name": "V. Sordini"
                    },
                    {
                        "name": "F. Sorrentino"
                    },
                    {
                        "name": "H. Sotani"
                    },
                    {
                        "name": "A. Southgate"
                    },
                    {
                        "name": "F. Spada"
                    },
                    {
                        "name": "V. Spagnuolo"
                    },
                    {
                        "name": "A. P. Spencer"
                    },
                    {
                        "name": "M. Spera"
                    },
                    {
                        "name": "P. Spinicelli"
                    },
                    {
                        "name": "C. A. Sprague"
                    },
                    {
                        "name": "A. K. Srivastava"
                    },
                    {
                        "name": "F. Stachurski"
                    },
                    {
                        "name": "D. A. Steer"
                    },
                    {
                        "name": "N. Steinle"
                    },
                    {
                        "name": "J. Steinlechner"
                    },
                    {
                        "name": "S. Steinlechner"
                    },
                    {
                        "name": "N. Stergioulas"
                    },
                    {
                        "name": "P. Stevens"
                    },
                    {
                        "name": "S. P. Stevenson"
                    },
                    {
                        "name": "F. Stolzi"
                    },
                    {
                        "name": "M. StPierre"
                    },
                    {
                        "name": "G. Stratta"
                    },
                    {
                        "name": "M. D. Strong"
                    },
                    {
                        "name": "A. Strunk"
                    },
                    {
                        "name": "R. Sturani"
                    },
                    {
                        "name": "A. L. Stuver"
                    },
                    {
                        "name": "M. Suchenek"
                    },
                    {
                        "name": "S. Sudhagar"
                    },
                    {
                        "name": "N. Sueltmann"
                    },
                    {
                        "name": "L. Suleiman"
                    },
                    {
                        "name": "J. M. Sullivan"
                    },
                    {
                        "name": "K. D. Sullivan"
                    },
                    {
                        "name": "J. Sun"
                    },
                    {
                        "name": "L. Sun"
                    },
                    {
                        "name": "S. Sunil"
                    },
                    {
                        "name": "J. Suresh"
                    },
                    {
                        "name": "B. J. Sutton"
                    },
                    {
                        "name": "P. J. Sutton"
                    },
                    {
                        "name": "T. Suzuki"
                    },
                    {
                        "name": "Y. Suzuki"
                    },
                    {
                        "name": "B. L. Swinkels"
                    },
                    {
                        "name": "A. Syx"
                    },
                    {
                        "name": "M. J. SzczepaÅczyk"
                    },
                    {
                        "name": "P. Szewczyk"
                    },
                    {
                        "name": "M. Tacca"
                    },
                    {
                        "name": "H. Tagoshi"
                    },
                    {
                        "name": "S. C. Tait"
                    },
                    {
                        "name": "H. Takahashi"
                    },
                    {
                        "name": "R. Takahashi"
                    },
                    {
                        "name": "A. Takamori"
                    },
                    {
                        "name": "T. Takase"
                    },
                    {
                        "name": "K. Takatani"
                    },
                    {
                        "name": "H. Takeda"
                    },
                    {
                        "name": "K. Takeshita"
                    },
                    {
                        "name": "C. Talbot"
                    },
                    {
                        "name": "M. Tamaki"
                    },
                    {
                        "name": "N. Tamanini"
                    },
                    {
                        "name": "D. Tanabe"
                    },
                    {
                        "name": "K. Tanaka"
                    },
                    {
                        "name": "S. J. Tanaka"
                    },
                    {
                        "name": "T. Tanaka"
                    },
                    {
                        "name": "D. Tang"
                    },
                    {
                        "name": "S. Tanioka"
                    },
                    {
                        "name": "D. B. Tanner"
                    },
                    {
                        "name": "W. Tanner"
                    },
                    {
                        "name": "L. Tao"
                    },
                    {
                        "name": "R. D. Tapia"
                    },
                    {
                        "name": "E. N. Tapia San MartÃ­n"
                    },
                    {
                        "name": "R. Tarafder"
                    },
                    {
                        "name": "C. Taranto"
                    },
                    {
                        "name": "A. Taruya"
                    },
                    {
                        "name": "J. D. Tasson"
                    },
                    {
                        "name": "J. G. Tau"
                    },
                    {
                        "name": "R. Tenorio"
                    },
                    {
                        "name": "H. Themann"
                    },
                    {
                        "name": "A. Theodoropoulos"
                    },
                    {
                        "name": "M. P. Thirugnanasambandam"
                    },
                    {
                        "name": "L. M. Thomas"
                    },
                    {
                        "name": "M. Thomas"
                    },
                    {
                        "name": "P. Thomas"
                    },
                    {
                        "name": "J. E. Thompson"
                    },
                    {
                        "name": "S. R. Thondapu"
                    },
                    {
                        "name": "K. A. Thorne"
                    },
                    {
                        "name": "E. Thrane"
                    },
                    {
                        "name": "S. Tibrewal"
                    },
                    {
                        "name": "J. Tissino"
                    },
                    {
                        "name": "A. Tiwari"
                    },
                    {
                        "name": "P. Tiwari"
                    },
                    {
                        "name": "S. Tiwari"
                    },
                    {
                        "name": "V. Tiwari"
                    },
                    {
                        "name": "M. R. Todd"
                    },
                    {
                        "name": "A. M. Toivonen"
                    },
                    {
                        "name": "K. Toland"
                    },
                    {
                        "name": "A. E. Tolley"
                    },
                    {
                        "name": "T. Tomaru"
                    },
                    {
                        "name": "K. Tomita"
                    },
                    {
                        "name": "V. Tommasini"
                    },
                    {
                        "name": "T. Tomura"
                    },
                    {
                        "name": "H. Tong"
                    },
                    {
                        "name": "C. Tong-Yu"
                    },
                    {
                        "name": "A. Toriyama"
                    },
                    {
                        "name": "N. Toropov"
                    },
                    {
                        "name": "A. Torres-FornÃ©"
                    },
                    {
                        "name": "C. I. Torrie"
                    },
                    {
                        "name": "M. Toscani"
                    },
                    {
                        "name": "I. Tosta e Melo"
                    },
                    {
                        "name": "E. Tournefier"
                    },
                    {
                        "name": "M. Trad Nery"
                    },
                    {
                        "name": "A. Trapananti"
                    },
                    {
                        "name": "F. Travasso"
                    },
                    {
                        "name": "G. Traylor"
                    },
                    {
                        "name": "C. Trejo"
                    },
                    {
                        "name": "M. Trevor"
                    },
                    {
                        "name": "M. C. Tringali"
                    },
                    {
                        "name": "A. Tripathee"
                    },
                    {
                        "name": "G. Troian"
                    },
                    {
                        "name": "A. Trovato"
                    },
                    {
                        "name": "L. Trozzo"
                    },
                    {
                        "name": "R. J. Trudeau"
                    },
                    {
                        "name": "T. T. L. Tsang"
                    },
                    {
                        "name": "S. Tsuchida"
                    },
                    {
                        "name": "L. Tsukada"
                    },
                    {
                        "name": "K. Turbang"
                    },
                    {
                        "name": "M. Turconi"
                    },
                    {
                        "name": "C. Turski"
                    },
                    {
                        "name": "H. Ubach"
                    },
                    {
                        "name": "N. Uchikata"
                    },
                    {
                        "name": "T. Uchiyama"
                    },
                    {
                        "name": "R. P. Udall"
                    },
                    {
                        "name": "T. Uehara"
                    },
                    {
                        "name": "M. Uematsu"
                    },
                    {
                        "name": "S. Ueno"
                    },
                    {
                        "name": "V. Undheim"
                    },
                    {
                        "name": "T. Ushiba"
                    },
                    {
                        "name": "M. Vacatello"
                    },
                    {
                        "name": "H. Vahlbruch"
                    },
                    {
                        "name": "G. Vajente"
                    },
                    {
                        "name": "A. Vajpeyi"
                    },
                    {
                        "name": "G. Valdes"
                    },
                    {
                        "name": "J. Valencia"
                    },
                    {
                        "name": "A. F. Valentini"
                    },
                    {
                        "name": "M. Valentini"
                    },
                    {
                        "name": "S. A. Vallejo-PeÃ±a"
                    },
                    {
                        "name": "S. Vallero"
                    },
                    {
                        "name": "V. Valsan"
                    },
                    {
                        "name": "N. van Bakel"
                    },
                    {
                        "name": "M. van Beuzekom"
                    },
                    {
                        "name": "M. van Dael"
                    },
                    {
                        "name": "J. F. J. van den Brand"
                    },
                    {
                        "name": "C. Van Den Broeck"
                    },
                    {
                        "name": "D. C. Vander-Hyde"
                    },
                    {
                        "name": "M. van der Sluys"
                    },
                    {
                        "name": "A. Van de Walle"
                    },
                    {
                        "name": "J. van Dongen"
                    },
                    {
                        "name": "K. Vandra"
                    },
                    {
                        "name": "H. van Haevermaet"
                    },
                    {
                        "name": "J. V. van Heijningen"
                    },
                    {
                        "name": "P. Van Hove"
                    },
                    {
                        "name": "J. Vanier"
                    },
                    {
                        "name": "M. VanKeuren"
                    },
                    {
                        "name": "J. Vanosky"
                    },
                    {
                        "name": "M. H. P. M. van Putten"
                    },
                    {
                        "name": "Z. Van Ranst"
                    },
                    {
                        "name": "N. van Remortel"
                    },
                    {
                        "name": "M. Vardaro"
                    },
                    {
                        "name": "A. F. Vargas"
                    },
                    {
                        "name": "J. J. Varghese"
                    },
                    {
                        "name": "V. Varma"
                    },
                    {
                        "name": "A. N. Vazquez"
                    },
                    {
                        "name": "A. Vecchio"
                    },
                    {
                        "name": "G. Vedovato"
                    },
                    {
                        "name": "J. Veitch"
                    },
                    {
                        "name": "P. J. Veitch"
                    },
                    {
                        "name": "S. Venikoudis"
                    },
                    {
                        "name": "J. Venneberg"
                    },
                    {
                        "name": "P. Verdier"
                    },
                    {
                        "name": "M. Vereecken"
                    },
                    {
                        "name": "D. Verkindt"
                    },
                    {
                        "name": "B. Verma"
                    },
                    {
                        "name": "P. Verma"
                    },
                    {
                        "name": "Y. Verma"
                    },
                    {
                        "name": "S. M. Vermeulen"
                    },
                    {
                        "name": "F. Vetrano"
                    },
                    {
                        "name": "A. Veutro"
                    },
                    {
                        "name": "A. M. Vibhute"
                    },
                    {
                        "name": "A. VicerÃ©"
                    },
                    {
                        "name": "S. Vidyant"
                    },
                    {
                        "name": "A. D. Viets"
                    },
                    {
                        "name": "A. Vijaykumar"
                    },
                    {
                        "name": "A. Vilkha"
                    },
                    {
                        "name": "V. Villa-Ortega"
                    },
                    {
                        "name": "E. T. Vincent"
                    },
                    {
                        "name": "J. -Y. Vinet"
                    },
                    {
                        "name": "S. Viret"
                    },
                    {
                        "name": "A. Virtuoso"
                    },
                    {
                        "name": "S. Vitale"
                    },
                    {
                        "name": "A. Vives"
                    },
                    {
                        "name": "H. Vocca"
                    },
                    {
                        "name": "D. Voigt"
                    },
                    {
                        "name": "E. R. G. von Reis"
                    },
                    {
                        "name": "J. S. A. von Wrangel"
                    },
                    {
                        "name": "L. Vujeva"
                    },
                    {
                        "name": "S. P. Vyatchanin"
                    },
                    {
                        "name": "J. Wack"
                    },
                    {
                        "name": "L. E. Wade"
                    },
                    {
                        "name": "M. Wade"
                    },
                    {
                        "name": "K. J. Wagner"
                    },
                    {
                        "name": "A. Wajid"
                    },
                    {
                        "name": "M. Walker"
                    },
                    {
                        "name": "G. S. Wallace"
                    },
                    {
                        "name": "L. Wallace"
                    },
                    {
                        "name": "E. J. Wang"
                    },
                    {
                        "name": "H. Wang"
                    },
                    {
                        "name": "J. Z. Wang"
                    },
                    {
                        "name": "W. H. Wang"
                    },
                    {
                        "name": "Y. F. Wang"
                    },
                    {
                        "name": "Z. Wang"
                    },
                    {
                        "name": "G. Waratkar"
                    },
                    {
                        "name": "J. Warner"
                    },
                    {
                        "name": "M. Was"
                    },
                    {
                        "name": "T. Washimi"
                    },
                    {
                        "name": "N. Y. Washington"
                    },
                    {
                        "name": "D. Watarai"
                    },
                    {
                        "name": "K. E. Wayt"
                    },
                    {
                        "name": "B. R. Weaver"
                    },
                    {
                        "name": "B. Weaver"
                    },
                    {
                        "name": "C. R. Weaving"
                    },
                    {
                        "name": "S. A. Webster"
                    },
                    {
                        "name": "N. L. Weickhardt"
                    },
                    {
                        "name": "M. Weinert"
                    },
                    {
                        "name": "A. J. Weinstein"
                    },
                    {
                        "name": "R. Weiss"
                    },
                    {
                        "name": "F. Wellmann"
                    },
                    {
                        "name": "L. Wen"
                    },
                    {
                        "name": "P. WeÃels"
                    },
                    {
                        "name": "K. Wette"
                    },
                    {
                        "name": "J. T. Whelan"
                    },
                    {
                        "name": "B. F. Whiting"
                    },
                    {
                        "name": "C. Whittle"
                    },
                    {
                        "name": "E. G. Wickens"
                    },
                    {
                        "name": "J. B. Wildberger"
                    },
                    {
                        "name": "D. Wilken"
                    },
                    {
                        "name": "D. J. Willadsen"
                    },
                    {
                        "name": "K. Willetts"
                    },
                    {
                        "name": "D. Williams"
                    },
                    {
                        "name": "M. J. Williams"
                    },
                    {
                        "name": "N. S. Williams"
                    },
                    {
                        "name": "J. L. Willis"
                    },
                    {
                        "name": "B. Willke"
                    },
                    {
                        "name": "M. Wils"
                    },
                    {
                        "name": "C. W. Winborn"
                    },
                    {
                        "name": "J. Winterflood"
                    },
                    {
                        "name": "C. C. Wipf"
                    },
                    {
                        "name": "G. Woan"
                    },
                    {
                        "name": "J. Woehler"
                    },
                    {
                        "name": "N. E. Wolfe"
                    },
                    {
                        "name": "H. T. Wong"
                    },
                    {
                        "name": "I. C. F. Wong"
                    },
                    {
                        "name": "J. L. Wright"
                    },
                    {
                        "name": "M. Wright"
                    },
                    {
                        "name": "C. Wu"
                    },
                    {
                        "name": "D. S. Wu"
                    },
                    {
                        "name": "H. Wu"
                    },
                    {
                        "name": "E. Wuchner"
                    },
                    {
                        "name": "D. M. Wysocki"
                    },
                    {
                        "name": "V. A. Xu"
                    },
                    {
                        "name": "Y. Xu"
                    },
                    {
                        "name": "N. Yadav"
                    },
                    {
                        "name": "H. Yamamoto"
                    },
                    {
                        "name": "K. Yamamoto"
                    },
                    {
                        "name": "T. S. Yamamoto"
                    },
                    {
                        "name": "T. Yamamoto"
                    },
                    {
                        "name": "S. Yamamura"
                    },
                    {
                        "name": "R. Yamazaki"
                    },
                    {
                        "name": "T. Yan"
                    },
                    {
                        "name": "F. W. Yang"
                    },
                    {
                        "name": "F. Yang"
                    },
                    {
                        "name": "K. Z. Yang"
                    },
                    {
                        "name": "Y. Yang"
                    },
                    {
                        "name": "Z. Yarbrough"
                    },
                    {
                        "name": "H. Yasui"
                    },
                    {
                        "name": "S. -W. Yeh"
                    },
                    {
                        "name": "A. B. Yelikar"
                    },
                    {
                        "name": "X. Yin"
                    },
                    {
                        "name": "J. Yokoyama"
                    },
                    {
                        "name": "T. Yokozawa"
                    },
                    {
                        "name": "J. Yoo"
                    },
                    {
                        "name": "H. Yu"
                    },
                    {
                        "name": "S. Yuan"
                    },
                    {
                        "name": "H. Yuzurihara"
                    },
                    {
                        "name": "A. ZadroÅ¼ny"
                    },
                    {
                        "name": "M. Zanolin"
                    },
                    {
                        "name": "M. Zeeshan"
                    },
                    {
                        "name": "T. Zelenova"
                    },
                    {
                        "name": "J. -P. Zendri"
                    },
                    {
                        "name": "M. Zeoli"
                    },
                    {
                        "name": "M. Zerrad"
                    },
                    {
                        "name": "M. Zevin"
                    },
                    {
                        "name": "A. C. Zhang"
                    },
                    {
                        "name": "L. Zhang"
                    },
                    {
                        "name": "R. Zhang"
                    },
                    {
                        "name": "T. Zhang"
                    },
                    {
                        "name": "Y. Zhang"
                    },
                    {
                        "name": "C. Zhao"
                    },
                    {
                        "name": "Yue Zhao"
                    },
                    {
                        "name": "Yuhang Zhao"
                    },
                    {
                        "name": "Y. Zheng"
                    },
                    {
                        "name": "H. Zhong"
                    },
                    {
                        "name": "R. Zhou"
                    },
                    {
                        "name": "X. -J. Zhu"
                    },
                    {
                        "name": "Z. -H. Zhu"
                    },
                    {
                        "name": "A. B. Zimmerman"
                    },
                    {
                        "name": "M. E. Zucker"
                    },
                    {
                        "name": "J. Zweizig"
                    }
                ],
                "author_detail": {
                    "name": "J. Zweizig"
                },
                "author": "J. Zweizig",
                "arxiv_comment": "As part of the Astrophysical Journal Letters Focus Issue on the\n  Gravitational Wave Transient Catalog. Update following peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11361v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11361v4",
                "updated": "2025-09-23T15:07:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    7,
                    33,
                    1,
                    266,
                    0
                ],
                "published": "2025-02-17T02:18:47Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    2,
                    18,
                    47,
                    0,
                    48,
                    0
                ],
                "title": "VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment"
                },
                "summary": "Detecting disinformation that blends manipulated text and images has become\nincreasingly challenging, as AI tools make synthetic content easy to generate\nand disseminate. While most existing AI safety benchmarks focus on single\nmodality misinformation (i.e., false content shared without intent to deceive),\nintentional multimodal disinformation, such as propaganda or conspiracy\ntheories that imitate credible news, remains largely unaddressed. We introduce\nthe Vision-Language Disinformation Detection Benchmark (VLDBench), the first\nlarge-scale resource supporting both unimodal (text-only) and multimodal (text\n+ image) disinformation detection. VLDBench comprises approximately 62,000\nlabeled text-image pairs across 13 categories, curated from 58 news outlets.\nUsing a semi-automated pipeline followed by expert review, 22 domain experts\ninvested over 500 hours to produce high-quality annotations with substantial\ninter-annotator agreement. Evaluations of state-of-the-art Large Language\nModels (LLMs) and Vision-Language Models (VLMs) on VLDBench show that\nincorporating visual cues improves detection accuracy by 5 to 35 percentage\npoints over text-only models. VLDBench provides data and code for evaluation,\nfine-tuning, and robustness testing to support disinformation analysis.\nDeveloped in alignment with AI governance frameworks (e.g., the MIT AI Risk\nRepository), VLDBench offers a principled foundation for advancing trustworthy\ndisinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset:\nhttps://huggingface.co/datasets/vector-institute/VLDBench Code:\nhttps://github.com/VectorInstitute/VLDBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting disinformation that blends manipulated text and images has become\nincreasingly challenging, as AI tools make synthetic content easy to generate\nand disseminate. While most existing AI safety benchmarks focus on single\nmodality misinformation (i.e., false content shared without intent to deceive),\nintentional multimodal disinformation, such as propaganda or conspiracy\ntheories that imitate credible news, remains largely unaddressed. We introduce\nthe Vision-Language Disinformation Detection Benchmark (VLDBench), the first\nlarge-scale resource supporting both unimodal (text-only) and multimodal (text\n+ image) disinformation detection. VLDBench comprises approximately 62,000\nlabeled text-image pairs across 13 categories, curated from 58 news outlets.\nUsing a semi-automated pipeline followed by expert review, 22 domain experts\ninvested over 500 hours to produce high-quality annotations with substantial\ninter-annotator agreement. Evaluations of state-of-the-art Large Language\nModels (LLMs) and Vision-Language Models (VLMs) on VLDBench show that\nincorporating visual cues improves detection accuracy by 5 to 35 percentage\npoints over text-only models. VLDBench provides data and code for evaluation,\nfine-tuning, and robustness testing to support disinformation analysis.\nDeveloped in alignment with AI governance frameworks (e.g., the MIT AI Risk\nRepository), VLDBench offers a principled foundation for advancing trustworthy\ndisinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset:\nhttps://huggingface.co/datasets/vector-institute/VLDBench Code:\nhttps://github.com/VectorInstitute/VLDBench"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Ashmal Vayani"
                    },
                    {
                        "name": "Aditya Jain"
                    },
                    {
                        "name": "Aravind Narayanan"
                    },
                    {
                        "name": "Vahid Reza Khazaie"
                    },
                    {
                        "name": "Syed Raza Bashir"
                    },
                    {
                        "name": "Elham Dolatabadi"
                    },
                    {
                        "name": "Gias Uddin"
                    },
                    {
                        "name": "Christos Emmanouilidis"
                    },
                    {
                        "name": "Rizwan Qureshi"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11361v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11361v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19117v1",
                "updated": "2025-09-23T15:03:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    3,
                    5,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:03:05Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    3,
                    5,
                    1,
                    266,
                    0
                ],
                "title": "LLM-based Vulnerability Discovery through the Lens of Code Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Vulnerability Discovery through the Lens of Code Metrics"
                },
                "summary": "Large language models (LLMs) excel in many tasks of software engineering, yet\nprogress in leveraging them for vulnerability discovery has stalled in recent\nyears. To understand this phenomenon, we investigate LLMs through the lens of\nclassic code metrics. Surprisingly, we find that a classifier trained solely on\nthese metrics performs on par with state-of-the-art LLMs for vulnerability\ndiscovery. A root-cause analysis reveals a strong correlation and a causal\neffect between LLMs and code metrics: When the value of a metric is changed,\nLLM predictions tend to shift by a corresponding magnitude. This dependency\nsuggests that LLMs operate at a similarly shallow level as code metrics,\nlimiting their ability to grasp complex patterns and fully realize their\npotential in vulnerability discovery. Based on these findings, we derive\nrecommendations on how research should more effectively address this challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in many tasks of software engineering, yet\nprogress in leveraging them for vulnerability discovery has stalled in recent\nyears. To understand this phenomenon, we investigate LLMs through the lens of\nclassic code metrics. Surprisingly, we find that a classifier trained solely on\nthese metrics performs on par with state-of-the-art LLMs for vulnerability\ndiscovery. A root-cause analysis reveals a strong correlation and a causal\neffect between LLMs and code metrics: When the value of a metric is changed,\nLLM predictions tend to shift by a corresponding magnitude. This dependency\nsuggests that LLMs operate at a similarly shallow level as code metrics,\nlimiting their ability to grasp complex patterns and fully realize their\npotential in vulnerability discovery. Based on these findings, we derive\nrecommendations on how research should more effectively address this challenge."
                },
                "authors": [
                    {
                        "name": "Felix Weissberg"
                    },
                    {
                        "name": "Lukas Pirch"
                    },
                    {
                        "name": "Erik Imgrund"
                    },
                    {
                        "name": "Jonas MÃ¶ller"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Konrad Rieck"
                    }
                ],
                "author_detail": {
                    "name": "Konrad Rieck"
                },
                "author": "Konrad Rieck",
                "arxiv_doi": "10.1145/3744916.3764574",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764574",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.19117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12623v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12623v3",
                "updated": "2025-09-23T15:01:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    1,
                    37,
                    1,
                    266,
                    0
                ],
                "published": "2025-02-18T08:09:42Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    8,
                    9,
                    42,
                    1,
                    49,
                    0
                ],
                "title": "DeepResonance: Enhancing Multimodal Music Understanding via\n  Music-centric Multi-way Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepResonance: Enhancing Multimodal Music Understanding via\n  Music-centric Multi-way Instruction Tuning"
                },
                "summary": "Recent advancements in music large language models (LLMs) have significantly\nimproved music understanding tasks, which involve the model's ability to\nanalyze and interpret various musical elements. These improvements primarily\nfocused on integrating both music and text inputs. However, the potential of\nincorporating additional modalities such as images, videos and textual music\nfeatures to enhance music understanding remains unexplored. To bridge this gap,\nwe propose DeepResonance, a multimodal music understanding LLM fine-tuned via\nmulti-way instruction tuning with multi-way aligned music, text, image, and\nvideo data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and\nMusic4way-Any2T, three 4-way training and evaluation datasets designed to\nenable DeepResonance to integrate both visual and textual music feature\ncontent. We also introduce multi-sampled ImageBind embeddings and a pre-LLM\nfusion Transformer to enhance modality fusion prior to input into text LLMs,\ntailoring for multi-way instruction tuning. Our model achieves state-of-the-art\nperformances across six music understanding tasks, highlighting the benefits of\nthe auxiliary modalities and the structural superiority of DeepResonance. We\nopen-source the codes, models and datasets we constructed:\ngithub.com/sony/DeepResonance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in music large language models (LLMs) have significantly\nimproved music understanding tasks, which involve the model's ability to\nanalyze and interpret various musical elements. These improvements primarily\nfocused on integrating both music and text inputs. However, the potential of\nincorporating additional modalities such as images, videos and textual music\nfeatures to enhance music understanding remains unexplored. To bridge this gap,\nwe propose DeepResonance, a multimodal music understanding LLM fine-tuned via\nmulti-way instruction tuning with multi-way aligned music, text, image, and\nvideo data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and\nMusic4way-Any2T, three 4-way training and evaluation datasets designed to\nenable DeepResonance to integrate both visual and textual music feature\ncontent. We also introduce multi-sampled ImageBind embeddings and a pre-LLM\nfusion Transformer to enhance modality fusion prior to input into text LLMs,\ntailoring for multi-way instruction tuning. Our model achieves state-of-the-art\nperformances across six music understanding tasks, highlighting the benefits of\nthe auxiliary modalities and the structural superiority of DeepResonance. We\nopen-source the codes, models and datasets we constructed:\ngithub.com/sony/DeepResonance."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Mao"
                    },
                    {
                        "name": "Mengjie Zhao"
                    },
                    {
                        "name": "Qiyu Wu"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "Accepted to EMNLP 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12623v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12623v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19115v1",
                "updated": "2025-09-23T15:00:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    0,
                    18,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:00:18Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    0,
                    18,
                    1,
                    266,
                    0
                ],
                "title": "Track-On2: Enhancing Online Point Tracking with Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Track-On2: Enhancing Online Point Tracking with Memory"
                },
                "summary": "In this paper, we consider the problem of long-term point tracking, which\nrequires consistent identification of points across video frames under\nsignificant appearance changes, motion, and occlusion. We target the online\nsetting, i.e. tracking points frame-by-frame, making it suitable for real-time\nand streaming applications. We extend our prior model Track-On into Track-On2,\na simple and efficient transformer-based model for online long-term tracking.\nTrack-On2 improves both performance and efficiency through architectural\nrefinements, more effective use of memory, and improved synthetic training\nstrategies. Unlike prior approaches that rely on full-sequence access or\niterative updates, our model processes frames causally and maintains temporal\ncoherence via a memory mechanism, which is key to handling drift and occlusions\nwithout requiring future frames. At inference, we perform coarse patch-level\nclassification followed by refinement. Beyond architecture, we systematically\nstudy synthetic training setups and their impact on memory behavior, showing\nhow they shape temporal robustness over long sequences. Through comprehensive\nexperiments, Track-On2 achieves state-of-the-art results across five synthetic\nand real-world benchmarks, surpassing prior online trackers and even strong\noffline methods that exploit bidirectional context. These results highlight the\neffectiveness of causal, memory-based architectures trained purely on synthetic\ndata as scalable solutions for real-world point tracking. Project page:\nhttps://kuis-ai.github.io/track_on2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider the problem of long-term point tracking, which\nrequires consistent identification of points across video frames under\nsignificant appearance changes, motion, and occlusion. We target the online\nsetting, i.e. tracking points frame-by-frame, making it suitable for real-time\nand streaming applications. We extend our prior model Track-On into Track-On2,\na simple and efficient transformer-based model for online long-term tracking.\nTrack-On2 improves both performance and efficiency through architectural\nrefinements, more effective use of memory, and improved synthetic training\nstrategies. Unlike prior approaches that rely on full-sequence access or\niterative updates, our model processes frames causally and maintains temporal\ncoherence via a memory mechanism, which is key to handling drift and occlusions\nwithout requiring future frames. At inference, we perform coarse patch-level\nclassification followed by refinement. Beyond architecture, we systematically\nstudy synthetic training setups and their impact on memory behavior, showing\nhow they shape temporal robustness over long sequences. Through comprehensive\nexperiments, Track-On2 achieves state-of-the-art results across five synthetic\nand real-world benchmarks, surpassing prior online trackers and even strong\noffline methods that exploit bidirectional context. These results highlight the\neffectiveness of causal, memory-based architectures trained purely on synthetic\ndata as scalable solutions for real-world point tracking. Project page:\nhttps://kuis-ai.github.io/track_on2"
                },
                "authors": [
                    {
                        "name": "GÃ¶rkay Aydemir"
                    },
                    {
                        "name": "Weidi Xie"
                    },
                    {
                        "name": "Fatma GÃ¼ney"
                    }
                ],
                "author_detail": {
                    "name": "Fatma GÃ¼ney"
                },
                "author": "Fatma GÃ¼ney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19112v1",
                "updated": "2025-09-23T14:58:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    58,
                    50,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:58:50Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    58,
                    50,
                    1,
                    266,
                    0
                ],
                "title": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event\n  Sequences via One-Shot Graph Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Practical Multi-label Causal Discovery in High-Dimensional Event\n  Sequences via One-Shot Graph Aggregation"
                },
                "summary": "Understanding causality in event sequences where outcome labels such as\ndiseases or system failures arise from preceding events like symptoms or error\ncodes is critical. Yet remains an unsolved challenge across domains like\nhealthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label\ncausal discovery method for sparse, high-dimensional event sequences comprising\nof thousands of unique event types. Using two pretrained causal Transformers as\ndomain-specific foundation models for event sequences. CARGO infers in\nparallel, per sequence one-shot causal graphs and aggregates them using an\nadaptive frequency fusion to reconstruct the global Markov boundaries of\nlabels. This two-stage approach enables efficient probabilistic reasoning at\nscale while bypassing the intractable cost of full-dataset conditional\nindependence testing. Our results on a challenging real-world automotive fault\nprediction dataset with over 29,100 unique event types and 474 imbalanced\nlabels demonstrate CARGO's ability to perform structured reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding causality in event sequences where outcome labels such as\ndiseases or system failures arise from preceding events like symptoms or error\ncodes is critical. Yet remains an unsolved challenge across domains like\nhealthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label\ncausal discovery method for sparse, high-dimensional event sequences comprising\nof thousands of unique event types. Using two pretrained causal Transformers as\ndomain-specific foundation models for event sequences. CARGO infers in\nparallel, per sequence one-shot causal graphs and aggregates them using an\nadaptive frequency fusion to reconstruct the global Markov boundaries of\nlabels. This two-stage approach enables efficient probabilistic reasoning at\nscale while bypassing the intractable cost of full-dataset conditional\nindependence testing. Our results on a challenging real-world automotive fault\nprediction dataset with over 29,100 unique event types and 474 imbalanced\nlabels demonstrate CARGO's ability to perform structured reasoning."
                },
                "authors": [
                    {
                        "name": "Hugo Math"
                    },
                    {
                        "name": "Rainer Lienhart"
                    }
                ],
                "author_detail": {
                    "name": "Rainer Lienhart"
                },
                "author": "Rainer Lienhart",
                "arxiv_comment": "Accepted at NeuRIPS2025 Workshop on Structured Probabilistic\n  Inference and Generative Modeling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13488v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13488v3",
                "updated": "2025-09-23T14:54:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    54,
                    29,
                    1,
                    266,
                    0
                ],
                "published": "2024-03-20T10:45:44Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    10,
                    45,
                    44,
                    2,
                    80,
                    0
                ],
                "title": "The DeepJoint algorithm: An innovative approach for studying the\n  longitudinal evolution of quantitative mammographic density and its\n  association with screen-detected breast cancer risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DeepJoint algorithm: An innovative approach for studying the\n  longitudinal evolution of quantitative mammographic density and its\n  association with screen-detected breast cancer risk"
                },
                "summary": "Mammographic density is a dynamic risk factor for breast cancer and affects\nthe sensitivity of mammography-based screening. While automated machine and\ndeep learning-based methods provide more consistent and precise measurements\ncompared to subjective BI-RADS assessments, they often fail to account for the\nlongitudinal evolution of density. Many of these methods assess mammographic\ndensity in a cross-sectional manner, overlooking correlations in repeated\nmeasures, irregular visit intervals, missing data, and informative dropouts.\nJoint models, however, are well-suited for capturing the longitudinal\nrelationship between biomarkers and survival outcomes. We present the DeepJoint\nalgorithm, an open-source solution that integrates deep learning for\nquantitative mammographic density estimation with joint modeling to assess the\nlongitudinal relationship between mammographic density and breast cancer risk.\nOur method efficiently analyzes processed mammograms from various\nmanufacturers, estimating both dense area and percent density--established risk\nfactors for breast cancer. We utilize a joint model to explore their\nassociation with breast cancer risk and provide individualized risk\npredictions. Bayesian inference and the Monte Carlo consensus algorithm make\nthe approach reliable for large screening datasets. Our method allows for\naccurate analysis of processed mammograms from multiple manufacturers, offering\na comprehensive view of breast cancer risk based on individual longitudinal\ndensity profiles. The complete pipeline is publicly available, promoting\nbroader application and comparison with other methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mammographic density is a dynamic risk factor for breast cancer and affects\nthe sensitivity of mammography-based screening. While automated machine and\ndeep learning-based methods provide more consistent and precise measurements\ncompared to subjective BI-RADS assessments, they often fail to account for the\nlongitudinal evolution of density. Many of these methods assess mammographic\ndensity in a cross-sectional manner, overlooking correlations in repeated\nmeasures, irregular visit intervals, missing data, and informative dropouts.\nJoint models, however, are well-suited for capturing the longitudinal\nrelationship between biomarkers and survival outcomes. We present the DeepJoint\nalgorithm, an open-source solution that integrates deep learning for\nquantitative mammographic density estimation with joint modeling to assess the\nlongitudinal relationship between mammographic density and breast cancer risk.\nOur method efficiently analyzes processed mammograms from various\nmanufacturers, estimating both dense area and percent density--established risk\nfactors for breast cancer. We utilize a joint model to explore their\nassociation with breast cancer risk and provide individualized risk\npredictions. Bayesian inference and the Monte Carlo consensus algorithm make\nthe approach reliable for large screening datasets. Our method allows for\naccurate analysis of processed mammograms from multiple manufacturers, offering\na comprehensive view of breast cancer risk based on individual longitudinal\ndensity profiles. The complete pipeline is publicly available, promoting\nbroader application and comparison with other methods."
                },
                "authors": [
                    {
                        "name": "Manel Rakez"
                    },
                    {
                        "name": "Julien Guillaumin"
                    },
                    {
                        "name": "Aurelien Chick"
                    },
                    {
                        "name": "Gaelle Coureau"
                    },
                    {
                        "name": "Foucauld Chamming's"
                    },
                    {
                        "name": "Pierre Fillard"
                    },
                    {
                        "name": "Brice Amadeo"
                    },
                    {
                        "name": "Virginie Rondeau"
                    }
                ],
                "author_detail": {
                    "name": "Virginie Rondeau"
                },
                "author": "Virginie Rondeau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13488v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13488v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19104v1",
                "updated": "2025-09-23T14:49:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    49,
                    48,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:49:48Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    49,
                    48,
                    1,
                    266,
                    0
                ],
                "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast\n  and Efficient LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast\n  and Efficient LLM Alignment"
                },
                "summary": "Reinforcement learning with human feedback (RLHF) has become crucial for\naligning Large Language Models (LLMs) with human intent. However, existing\noffline RLHF approaches suffer from overoptimization, where models overfit to\nreward misspecification and drift from preferred behaviors observed during\ntraining. We introduce DRO-REBEL, a unified family of robust REBEL updates with\ntype-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality,\neach update reduces to a simple relative-reward regression, preserving\nscalability and avoiding PPO-style clipping or auxiliary value networks. Under\nstandard linear-reward and log-linear policy classes with a data-coverage\ncondition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants\nthan prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$\nrate via a localized Rademacher complexity analysis. The same analysis closes\nthe gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal\nparametric rates. We derive practical SGD algorithms for all three divergences:\ngradient regularization (Wasserstein), importance weighting (KL), and a fast\n1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale\nArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong\nworst-case robustness across unseen preference mixtures, model sizes, and data\nscales, with $\\chi^2$-REBEL showing consistently strong empirical performance.\nA controlled radius--coverage study validates a no-free-lunch trade-off: radii\nshrinking faster than empirical divergence concentration rates achieve\nminimax-optimal parametric rates but forfeit coverage, while\ncoverage-guaranteeing radii incur $O(n^{-1/4})$ rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with human feedback (RLHF) has become crucial for\naligning Large Language Models (LLMs) with human intent. However, existing\noffline RLHF approaches suffer from overoptimization, where models overfit to\nreward misspecification and drift from preferred behaviors observed during\ntraining. We introduce DRO-REBEL, a unified family of robust REBEL updates with\ntype-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality,\neach update reduces to a simple relative-reward regression, preserving\nscalability and avoiding PPO-style clipping or auxiliary value networks. Under\nstandard linear-reward and log-linear policy classes with a data-coverage\ncondition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants\nthan prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$\nrate via a localized Rademacher complexity analysis. The same analysis closes\nthe gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal\nparametric rates. We derive practical SGD algorithms for all three divergences:\ngradient regularization (Wasserstein), importance weighting (KL), and a fast\n1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale\nArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong\nworst-case robustness across unseen preference mixtures, model sizes, and data\nscales, with $\\chi^2$-REBEL showing consistently strong empirical performance.\nA controlled radius--coverage study validates a no-free-lunch trade-off: radii\nshrinking faster than empirical divergence concentration rates achieve\nminimax-optimal parametric rates but forfeit coverage, while\ncoverage-guaranteeing radii incur $O(n^{-1/4})$ rates."
                },
                "authors": [
                    {
                        "name": "Sharan Sahu"
                    },
                    {
                        "name": "Martin T. Wells"
                    }
                ],
                "author_detail": {
                    "name": "Martin T. Wells"
                },
                "author": "Martin T. Wells",
                "arxiv_comment": "70 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19105v1",
                "updated": "2025-09-23T14:49:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    49,
                    48,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:49:48Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    49,
                    48,
                    1,
                    266,
                    0
                ],
                "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation"
                },
                "summary": "Successful navigation in outdoor environments requires accurate prediction of\nthe physical interactions between the robot and the terrain. To this end,\nseveral methods rely on geometric or semantic labels to classify traversable\nsurfaces. However, such labels cannot distinguish visually similar surfaces\nthat differ in material properties. Spectral sensors enable inference of\nmaterial composition from surface reflectance measured across multiple\nwavelength bands. Although spectral sensing is gaining traction in robotics,\nwidespread deployment remains constrained by the need for custom hardware\nintegration, high sensor costs, and compute-intensive processing pipelines. In\nthis paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),\na deep neural network designed to bridge the gap between the accessibility of\nRGB sensing and the rich material information provided by spectral data. RS-Net\npredicts spectral signatures from RGB patches, which we map to terrain labels\nand friction coefficients. The resulting terrain classifications are integrated\ninto a sampling-based motion planner for a wheeled robot operating in outdoor\nenvironments. Likewise, the friction estimates are incorporated into a\ncontact-force-based MPC for a quadruped robot navigating slippery surfaces.\nThus, we introduce a framework that learns the task-relevant physical property\nonce during training and thereafter relies solely on RGB sensing at test time.\nThe code is available at https://github.com/prajapatisarvesh/RS-Net.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Successful navigation in outdoor environments requires accurate prediction of\nthe physical interactions between the robot and the terrain. To this end,\nseveral methods rely on geometric or semantic labels to classify traversable\nsurfaces. However, such labels cannot distinguish visually similar surfaces\nthat differ in material properties. Spectral sensors enable inference of\nmaterial composition from surface reflectance measured across multiple\nwavelength bands. Although spectral sensing is gaining traction in robotics,\nwidespread deployment remains constrained by the need for custom hardware\nintegration, high sensor costs, and compute-intensive processing pipelines. In\nthis paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),\na deep neural network designed to bridge the gap between the accessibility of\nRGB sensing and the rich material information provided by spectral data. RS-Net\npredicts spectral signatures from RGB patches, which we map to terrain labels\nand friction coefficients. The resulting terrain classifications are integrated\ninto a sampling-based motion planner for a wheeled robot operating in outdoor\nenvironments. Likewise, the friction estimates are incorporated into a\ncontact-force-based MPC for a quadruped robot navigating slippery surfaces.\nThus, we introduce a framework that learns the task-relevant physical property\nonce during training and thereafter relies solely on RGB sensing at test time.\nThe code is available at https://github.com/prajapatisarvesh/RS-Net."
                },
                "authors": [
                    {
                        "name": "Sarvesh Prajapati"
                    },
                    {
                        "name": "Ananya Trivedi"
                    },
                    {
                        "name": "Nathaniel Hanson"
                    },
                    {
                        "name": "Bruce Maxwell"
                    },
                    {
                        "name": "Taskin Padir"
                    }
                ],
                "author_detail": {
                    "name": "Taskin Padir"
                },
                "author": "Taskin Padir",
                "arxiv_comment": "8 pages, 10 figures, submitted to Robotic Computing & Communication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19100v1",
                "updated": "2025-09-23T14:48:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    48,
                    58,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:48:58Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    48,
                    58,
                    1,
                    266,
                    0
                ],
                "title": "Algorithms for Adversarially Robust Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithms for Adversarially Robust Deep Learning"
                },
                "summary": "Given the widespread use of deep learning models in safety-critical\napplications, ensuring that the decisions of such models are robust against\nadversarial exploitation is of fundamental importance. In this thesis, we\ndiscuss recent progress toward designing algorithms that exhibit desirable\nrobustness properties. First, we discuss the problem of adversarial examples in\ncomputer vision, for which we introduce new technical results, training\nparadigms, and certification algorithms. Next, we consider the problem of\ndomain generalization, wherein the task is to train neural networks to\ngeneralize from a family of training distributions to unseen test\ndistributions. We present new algorithms that achieve state-of-the-art\ngeneralization in medical imaging, molecular identification, and image\nclassification. Finally, we study the setting of jailbreaking large language\nmodels (LLMs), wherein an adversarial user attempts to design prompts that\nelicit objectionable content from an LLM. We propose new attacks and defenses,\nwhich represent the frontier of progress toward designing robust language-based\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the widespread use of deep learning models in safety-critical\napplications, ensuring that the decisions of such models are robust against\nadversarial exploitation is of fundamental importance. In this thesis, we\ndiscuss recent progress toward designing algorithms that exhibit desirable\nrobustness properties. First, we discuss the problem of adversarial examples in\ncomputer vision, for which we introduce new technical results, training\nparadigms, and certification algorithms. Next, we consider the problem of\ndomain generalization, wherein the task is to train neural networks to\ngeneralize from a family of training distributions to unseen test\ndistributions. We present new algorithms that achieve state-of-the-art\ngeneralization in medical imaging, molecular identification, and image\nclassification. Finally, we study the setting of jailbreaking large language\nmodels (LLMs), wherein an adversarial user attempts to design prompts that\nelicit objectionable content from an LLM. We propose new attacks and defenses,\nwhich represent the frontier of progress toward designing robust language-based\nagents."
                },
                "authors": [
                    {
                        "name": "Alexander Robey"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Robey"
                },
                "author": "Alexander Robey",
                "arxiv_comment": "PhD thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15587v2",
                "updated": "2025-09-23T14:48:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    48,
                    18,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-19T04:40:46Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    4,
                    40,
                    46,
                    4,
                    262,
                    0
                ],
                "title": "DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation\n  in Large Language Models"
                },
                "summary": "Logic reasoning in natural language has been recognized as an important\nmeasure of human intelligence for Large Language Models (LLMs). Popular\nbenchmarks may entangle multiple reasoning skills and thus provide unfaithful\nevaluations on the logic reasoning skill. Meanwhile, existing logic reasoning\nbenchmarks are limited in language diversity and their distributions are\ndeviated from the distribution of an ideal logic reasoning benchmark, which may\nlead to biased evaluation results. This paper thereby proposes a new classical\nlogic benchmark DivLogicEval, consisting of natural sentences composed of\ndiverse statements in a counterintuitive way. To ensure a more reliable\nevaluation, we also introduce a new evaluation metric that mitigates the\ninfluence of bias and randomness inherent in LLMs. Through experiments, we\ndemonstrate the extent to which logical reasoning is required to answer the\nquestions in DivLogicEval and compare the performance of different popular LLMs\nin conducting logical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic reasoning in natural language has been recognized as an important\nmeasure of human intelligence for Large Language Models (LLMs). Popular\nbenchmarks may entangle multiple reasoning skills and thus provide unfaithful\nevaluations on the logic reasoning skill. Meanwhile, existing logic reasoning\nbenchmarks are limited in language diversity and their distributions are\ndeviated from the distribution of an ideal logic reasoning benchmark, which may\nlead to biased evaluation results. This paper thereby proposes a new classical\nlogic benchmark DivLogicEval, consisting of natural sentences composed of\ndiverse statements in a counterintuitive way. To ensure a more reliable\nevaluation, we also introduce a new evaluation metric that mitigates the\ninfluence of bias and randomness inherent in LLMs. Through experiments, we\ndemonstrate the extent to which logical reasoning is required to answer the\nquestions in DivLogicEval and compare the performance of different popular LLMs\nin conducting logical reasoning."
                },
                "authors": [
                    {
                        "name": "Tsz Ting Chung"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Dit-Yan Yeung"
                },
                "author": "Dit-Yan Yeung",
                "arxiv_comment": "Accepted by EMNLP 2025. Project Page:\n  https://ttchungc.github.io/projects/divlogiceval/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10401v2",
                "updated": "2025-09-23T14:45:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    45,
                    53,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-12T16:51:15Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    51,
                    15,
                    4,
                    255,
                    0
                ],
                "title": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure\n  Attribution in Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure\n  Attribution in Multi-Agent Systems"
                },
                "summary": "Failure attribution in multi-agent systems -- pinpointing the exact step\nwhere a decisive error occurs -- is a critical yet unsolved challenge. Current\nmethods treat this as a pattern recognition task over long conversation logs,\nleading to critically low step-level accuracy (below 17\\%), which renders them\nimpractical for debugging complex systems. Their core weakness is a fundamental\ninability to perform robust counterfactual reasoning: to determine if\ncorrecting a single action would have actually averted the task failure. To\nbridge this \\emph{counterfactual inference gap}, we introduce\nAbduct-Act-Predict (A2P) Scaffolding, a novel agent framework that transforms\nfailure attribution from pattern recognition into a structured causal inference\ntask. A2P explicitly guides a large language model through a formal three-step\nreasoning process within a single inference pass: (1) Abduction, to infer the\nhidden root causes behind an agent's actions; (2) Action, to define a minimal\ncorrective intervention; and (3) Prediction, to simulate the subsequent\ntrajectory and verify if the intervention resolves the failure. This structured\napproach leverages the holistic context of the entire conversation while\nimposing a rigorous causal logic on the model's analysis. Our extensive\nexperiments on the Who\\&When benchmark demonstrate its efficacy. On the\nAlgorithm-Generated dataset, A2P achieves 47.46\\% step-level accuracy, a\n2.85$\\times$ improvement over the 16.67\\% of the baseline. On the more complex\nHand-Crafted dataset, it achieves 29.31\\% step accuracy, a 2.43$\\times$\nimprovement over the baseline's 12.07\\%. By reframing the problem through a\ncausal lens, A2P Scaffolding provides a robust, verifiable, and significantly\nmore accurate solution for automated failure attribution. Ours code are\nreleased at https://github.com/ResearAI/A2P.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure attribution in multi-agent systems -- pinpointing the exact step\nwhere a decisive error occurs -- is a critical yet unsolved challenge. Current\nmethods treat this as a pattern recognition task over long conversation logs,\nleading to critically low step-level accuracy (below 17\\%), which renders them\nimpractical for debugging complex systems. Their core weakness is a fundamental\ninability to perform robust counterfactual reasoning: to determine if\ncorrecting a single action would have actually averted the task failure. To\nbridge this \\emph{counterfactual inference gap}, we introduce\nAbduct-Act-Predict (A2P) Scaffolding, a novel agent framework that transforms\nfailure attribution from pattern recognition into a structured causal inference\ntask. A2P explicitly guides a large language model through a formal three-step\nreasoning process within a single inference pass: (1) Abduction, to infer the\nhidden root causes behind an agent's actions; (2) Action, to define a minimal\ncorrective intervention; and (3) Prediction, to simulate the subsequent\ntrajectory and verify if the intervention resolves the failure. This structured\napproach leverages the holistic context of the entire conversation while\nimposing a rigorous causal logic on the model's analysis. Our extensive\nexperiments on the Who\\&When benchmark demonstrate its efficacy. On the\nAlgorithm-Generated dataset, A2P achieves 47.46\\% step-level accuracy, a\n2.85$\\times$ improvement over the 16.67\\% of the baseline. On the more complex\nHand-Crafted dataset, it achieves 29.31\\% step accuracy, a 2.43$\\times$\nimprovement over the baseline's 12.07\\%. By reframing the problem through a\ncausal lens, A2P Scaffolding provides a robust, verifiable, and significantly\nmore accurate solution for automated failure attribution. Ours code are\nreleased at https://github.com/ResearAI/A2P."
                },
                "authors": [
                    {
                        "name": "Alva West"
                    },
                    {
                        "name": "Yixuan Weng"
                    },
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Zhiyuan Ning"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19094v1",
                "updated": "2025-09-23T14:44:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    44,
                    46,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:44:46Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    44,
                    46,
                    1,
                    266,
                    0
                ],
                "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form\n  Personalized Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathways of Thoughts: Multi-Directional Thinking for Long-form\n  Personalized Question Answering"
                },
                "summary": "Personalization is essential for adapting question answering (QA) systems to\nuser-specific information needs, thereby improving both accuracy and user\nsatisfaction. However, personalized QA remains relatively underexplored due to\nchallenges such as inferring preferences from long, noisy, and implicit\ncontexts, and generating responses that are simultaneously correct,\ncontextually appropriate, and aligned with user expectations and background\nknowledge. To address these challenges, we propose Pathways of Thoughts (PoT),\nan inference-stage method that applies to any large language model (LLM)\nwithout requiring task-specific fine-tuning. The approach models the reasoning\nof an LLM as an iterative decision process, where the model dynamically selects\namong cognitive operations such as reasoning, revision, personalization, and\nclarification. This enables exploration of multiple reasoning trajectories,\nproducing diverse candidate responses that capture different perspectives. PoT\nthen aggregates and reweights these candidates according to inferred user\npreferences, yielding a final personalized response that benefits from the\ncomplementary strengths of diverse reasoning paths. Experiments on the LaMP-QA\nbenchmark for personalized QA show that PoT consistently outperforms\ncompetitive baselines, achieving up to a 13.1% relative improvement. Human\nevaluation corroborates these results, with annotators preferring outputs from\nPoT in 66% of cases and reporting ties in only 15% of cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization is essential for adapting question answering (QA) systems to\nuser-specific information needs, thereby improving both accuracy and user\nsatisfaction. However, personalized QA remains relatively underexplored due to\nchallenges such as inferring preferences from long, noisy, and implicit\ncontexts, and generating responses that are simultaneously correct,\ncontextually appropriate, and aligned with user expectations and background\nknowledge. To address these challenges, we propose Pathways of Thoughts (PoT),\nan inference-stage method that applies to any large language model (LLM)\nwithout requiring task-specific fine-tuning. The approach models the reasoning\nof an LLM as an iterative decision process, where the model dynamically selects\namong cognitive operations such as reasoning, revision, personalization, and\nclarification. This enables exploration of multiple reasoning trajectories,\nproducing diverse candidate responses that capture different perspectives. PoT\nthen aggregates and reweights these candidates according to inferred user\npreferences, yielding a final personalized response that benefits from the\ncomplementary strengths of diverse reasoning paths. Experiments on the LaMP-QA\nbenchmark for personalized QA show that PoT consistently outperforms\ncompetitive baselines, achieving up to a 13.1% relative improvement. Human\nevaluation corroborates these results, with annotators preferring outputs from\nPoT in 66% of cases and reporting ties in only 15% of cases."
                },
                "authors": [
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Qiaozhu Mei"
                    },
                    {
                        "name": "Zhuowan Li"
                    },
                    {
                        "name": "Spurthi Amba Hombaiah"
                    },
                    {
                        "name": "Weize Kong"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Michael Bendersky"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bendersky"
                },
                "author": "Michael Bendersky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19090v2",
                "updated": "2025-09-24T08:19:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    8,
                    19,
                    50,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-23T14:42:31Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    42,
                    31,
                    1,
                    266,
                    0
                ],
                "title": "Citrus-V: Advancing Medical Foundation Models with Unified Medical Image\n  Grounding for Clinical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citrus-V: Advancing Medical Foundation Models with Unified Medical Image\n  Grounding for Clinical Reasoning"
                },
                "summary": "Medical imaging provides critical evidence for clinical diagnosis, treatment\nplanning, and surgical decisions, yet most existing imaging models are narrowly\nfocused and require multiple specialized networks, limiting their\ngeneralization. Although large-scale language and multimodal models exhibit\nstrong reasoning and multi-task capabilities, real-world clinical applications\ndemand precise visual grounding, multimodal integration, and chain-of-thought\nreasoning. We introduce Citrus-V, a multimodal medical foundation model that\ncombines image analysis with textual reasoning. The model integrates detection,\nsegmentation, and multimodal chain-of-thought reasoning, enabling pixel-level\nlesion localization, structured report generation, and physician-like\ndiagnostic inference in a single framework. We propose a novel multimodal\ntraining approach and release a curated open-source data suite covering\nreasoning, detection, segmentation, and document understanding tasks.\nEvaluations demonstrate that Citrus-V outperforms existing open-source medical\nmodels and expert-level imaging systems across multiple benchmarks, delivering\na unified pipeline from visual grounding to clinical reasoning and supporting\nprecise lesion quantification, automated reporting, and reliable second\nopinions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical imaging provides critical evidence for clinical diagnosis, treatment\nplanning, and surgical decisions, yet most existing imaging models are narrowly\nfocused and require multiple specialized networks, limiting their\ngeneralization. Although large-scale language and multimodal models exhibit\nstrong reasoning and multi-task capabilities, real-world clinical applications\ndemand precise visual grounding, multimodal integration, and chain-of-thought\nreasoning. We introduce Citrus-V, a multimodal medical foundation model that\ncombines image analysis with textual reasoning. The model integrates detection,\nsegmentation, and multimodal chain-of-thought reasoning, enabling pixel-level\nlesion localization, structured report generation, and physician-like\ndiagnostic inference in a single framework. We propose a novel multimodal\ntraining approach and release a curated open-source data suite covering\nreasoning, detection, segmentation, and document understanding tasks.\nEvaluations demonstrate that Citrus-V outperforms existing open-source medical\nmodels and expert-level imaging systems across multiple benchmarks, delivering\na unified pipeline from visual grounding to clinical reasoning and supporting\nprecise lesion quantification, automated reporting, and reliable second\nopinions."
                },
                "authors": [
                    {
                        "name": "Guoxin Wang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Xinyi Liu"
                    },
                    {
                        "name": "Yanbo Liu"
                    },
                    {
                        "name": "Xuyang Cao"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Zhuoyun Liu"
                    },
                    {
                        "name": "Qintian Sun"
                    },
                    {
                        "name": "Fangru Zhou"
                    },
                    {
                        "name": "Haoqiang Xing"
                    },
                    {
                        "name": "Zhenhong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhong Yang"
                },
                "author": "Zhenhong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19088v1",
                "updated": "2025-09-23T14:42:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    42,
                    14,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:42:14Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    42,
                    14,
                    1,
                    266,
                    0
                ],
                "title": "A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and\n  Opportunities for Further Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and\n  Opportunities for Further Improvement"
                },
                "summary": "Do \"digital twins\" capture individual responses in surveys and experiments?\nWe run 19 pre-registered studies on a national U.S. panel and their LLM-powered\ndigital twins (constructed based on previously-collected extensive\nindividual-level data) and compare twin and human answers across 164 outcomes.\nThe correlation between twin and human answers is modest (approximately 0.2 on\naverage) and twin responses are less variable than human responses. While\nconstructing digital twins based on rich individual-level data improves our\nability to capture heterogeneity across participants and predict relative\ndifferences between them, it does not substantially improve our ability to\npredict the exact answers given by specific participants or enhance predictions\nof population means. Twin performance varies by domain and is higher among more\neducated, higher-income, and ideologically moderate participants. These results\nsuggest current digital twins can capture some degree of relative differences\nbut are unreliable for individual-level predictions and sample mean and\nvariance estimation, underscoring the need for careful validation before use.\nOur data and code are publicly available for researchers and practitioners\ninterested in optimizing digital twin pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do \"digital twins\" capture individual responses in surveys and experiments?\nWe run 19 pre-registered studies on a national U.S. panel and their LLM-powered\ndigital twins (constructed based on previously-collected extensive\nindividual-level data) and compare twin and human answers across 164 outcomes.\nThe correlation between twin and human answers is modest (approximately 0.2 on\naverage) and twin responses are less variable than human responses. While\nconstructing digital twins based on rich individual-level data improves our\nability to capture heterogeneity across participants and predict relative\ndifferences between them, it does not substantially improve our ability to\npredict the exact answers given by specific participants or enhance predictions\nof population means. Twin performance varies by domain and is higher among more\neducated, higher-income, and ideologically moderate participants. These results\nsuggest current digital twins can capture some degree of relative differences\nbut are unreliable for individual-level predictions and sample mean and\nvariance estimation, underscoring the need for careful validation before use.\nOur data and code are publicly available for researchers and practitioners\ninterested in optimizing digital twin pipelines."
                },
                "authors": [
                    {
                        "name": "Tiany Peng"
                    },
                    {
                        "name": "George Gui"
                    },
                    {
                        "name": "Daniel J. Merlau"
                    },
                    {
                        "name": "Grace Jiarui Fan"
                    },
                    {
                        "name": "Malek Ben Sliman"
                    },
                    {
                        "name": "Melanie Brucks"
                    },
                    {
                        "name": "Eric J. Johnson"
                    },
                    {
                        "name": "Vicki Morwitz"
                    },
                    {
                        "name": "Abdullah Althenayyan"
                    },
                    {
                        "name": "Silvia Bellezza"
                    },
                    {
                        "name": "Dante Donati"
                    },
                    {
                        "name": "Hortense Fong"
                    },
                    {
                        "name": "Elizabeth Friedman"
                    },
                    {
                        "name": "Ariana Guevara"
                    },
                    {
                        "name": "Mohamed Hussein"
                    },
                    {
                        "name": "Kinshuk Jerath"
                    },
                    {
                        "name": "Bruce Kogut"
                    },
                    {
                        "name": "Kristen Lane"
                    },
                    {
                        "name": "Hannah Li"
                    },
                    {
                        "name": "Patryk Perkowski"
                    },
                    {
                        "name": "Oded Netzer"
                    },
                    {
                        "name": "Olivier Toubia"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Toubia"
                },
                "author": "Olivier Toubia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19082v1",
                "updated": "2025-09-23T14:38:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    38,
                    25,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:38:25Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    38,
                    25,
                    1,
                    266,
                    0
                ],
                "title": "3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA\n  Results with Consistent Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA\n  Results with Consistent Training and Inference"
                },
                "summary": "Sa2VA is a recent model for language-guided dense grounding in images and\nvideo that achieves state-of-the-art results on multiple segmentation\nbenchmarks and that has become widely popular. However, we found that Sa2VA\ndoes not perform according to its full potential for referring video object\nsegmentation tasks. We identify inconsistencies between training and inference\nprocedures as the key factor holding it back. To mitigate this issue, we\npropose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and\nimproves the results. In fact, Sa2VA-i sets a new state of the art for multiple\nvideo benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on\nRef-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA\ncheckpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the\noriginal Sa2VA-26B model on the MeViS benchmark. We hope that this work will\nshow the importance of seemingly trivial implementation details and that it\nwill provide valuable insights for the referring video segmentation field. We\nprovide the code and updated models at https://github.com/kumuji/sa2va-i",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sa2VA is a recent model for language-guided dense grounding in images and\nvideo that achieves state-of-the-art results on multiple segmentation\nbenchmarks and that has become widely popular. However, we found that Sa2VA\ndoes not perform according to its full potential for referring video object\nsegmentation tasks. We identify inconsistencies between training and inference\nprocedures as the key factor holding it back. To mitigate this issue, we\npropose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and\nimproves the results. In fact, Sa2VA-i sets a new state of the art for multiple\nvideo benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on\nRef-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA\ncheckpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the\noriginal Sa2VA-26B model on the MeViS benchmark. We hope that this work will\nshow the importance of seemingly trivial implementation details and that it\nwill provide valuable insights for the referring video segmentation field. We\nprovide the code and updated models at https://github.com/kumuji/sa2va-i"
                },
                "authors": [
                    {
                        "name": "Alexey Nekrasov"
                    },
                    {
                        "name": "Ali Athar"
                    },
                    {
                        "name": "Daan de Geus"
                    },
                    {
                        "name": "Alexander Hermans"
                    },
                    {
                        "name": "Bastian Leibe"
                    }
                ],
                "author_detail": {
                    "name": "Bastian Leibe"
                },
                "author": "Bastian Leibe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19078v1",
                "updated": "2025-09-23T14:36:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    36,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:36:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    36,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "Diffusion Bridge Variational Inference for Deep Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Bridge Variational Inference for Deep Gaussian Processes"
                },
                "summary": "Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian\nmodeling but pose substantial challenges for posterior inference, especially\nover inducing variables. Denoising diffusion variational inference (DDVI)\naddresses this by modeling the posterior as a time-reversed diffusion from a\nsimple Gaussian prior. However, DDVI's fixed unconditional starting\ndistribution remains far from the complex true posterior, resulting in\ninefficient inference trajectories and slow convergence. In this work, we\npropose Diffusion Bridge Variational Inference (DBVI), a principled extension\nof DDVI that initiates the reverse diffusion from a learnable, data-dependent\ninitial distribution. This initialization is parameterized via an amortized\nneural network and progressively adapted using gradients from the ELBO\nobjective, reducing the posterior gap and improving sample efficiency. To\nenable scalable amortization, we design the network to operate on the inducing\ninputs, which serve as structured, low-dimensional summaries of the dataset and\nnaturally align with the inducing variables' shape. DBVI retains the\nmathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time\nSDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We\nderive a tractable training objective under this formulation and implement DBVI\nfor scalable inference in large-scale DGPs. Across regression, classification,\nand image reconstruction tasks, DBVI consistently outperforms DDVI and other\nvariational baselines in predictive accuracy, convergence speed, and posterior\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian\nmodeling but pose substantial challenges for posterior inference, especially\nover inducing variables. Denoising diffusion variational inference (DDVI)\naddresses this by modeling the posterior as a time-reversed diffusion from a\nsimple Gaussian prior. However, DDVI's fixed unconditional starting\ndistribution remains far from the complex true posterior, resulting in\ninefficient inference trajectories and slow convergence. In this work, we\npropose Diffusion Bridge Variational Inference (DBVI), a principled extension\nof DDVI that initiates the reverse diffusion from a learnable, data-dependent\ninitial distribution. This initialization is parameterized via an amortized\nneural network and progressively adapted using gradients from the ELBO\nobjective, reducing the posterior gap and improving sample efficiency. To\nenable scalable amortization, we design the network to operate on the inducing\ninputs, which serve as structured, low-dimensional summaries of the dataset and\nnaturally align with the inducing variables' shape. DBVI retains the\nmathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time\nSDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We\nderive a tractable training objective under this formulation and implement DBVI\nfor scalable inference in large-scale DGPs. Across regression, classification,\nand image reconstruction tasks, DBVI consistently outperforms DDVI and other\nvariational baselines in predictive accuracy, convergence speed, and posterior\nquality."
                },
                "authors": [
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Qibin Zhao"
                    },
                    {
                        "name": "John Paisley"
                    },
                    {
                        "name": "Delu Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Delu Zeng"
                },
                "author": "Delu Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19077v1",
                "updated": "2025-09-23T14:36:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    36,
                    12,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:36:12Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    36,
                    12,
                    1,
                    266,
                    0
                ],
                "title": "Code Driven Planning with Domain-Adaptive Critic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Driven Planning with Domain-Adaptive Critic"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs."
                },
                "authors": [
                    {
                        "name": "Zikang Tian"
                    },
                    {
                        "name": "Shaohui Peng"
                    },
                    {
                        "name": "Du Huang"
                    },
                    {
                        "name": "Jiaming Guo"
                    },
                    {
                        "name": "Ruizhi Chen"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Xishan Zhang"
                    },
                    {
                        "name": "Yuxuan Guo"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Ling Li"
                    },
                    {
                        "name": "Yewen Pu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Yunji Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunji Chen"
                },
                "author": "Yunji Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19069v1",
                "updated": "2025-09-23T14:33:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    33,
                    2,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:33:02Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    33,
                    2,
                    1,
                    266,
                    0
                ],
                "title": "Linking Young Stellar Object Morphology to Evolutionary Stages with\n  Self-Organizing Maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linking Young Stellar Object Morphology to Evolutionary Stages with\n  Self-Organizing Maps"
                },
                "summary": "Studies in the past few decades have investigated young stellar object\nevolution based on their spectral energy distribution (SED). The SED is heavily\ninfluenced not only by evolutionary stage, but also the morphology of the young\nstar. This work is part of the NEMESIS project which is aiming to revisit star\nformation with the aid of machine learning techniques and provides the\nframework for this work. In a first effort towards a novel\nspectro-morphological classification we analyzed young stellar object\nmorphologies and linked them to the currently used observational classes.\nThereby we aim to lay the foundation for a spectro-morphological\nclassification, and apply the insights learned in this study in a future,\nrevisited classification scheme. We obtained archival high-resolution survey\nimages from VISTA for approximately 10,000 literature young stellar object\ncandidates towards the Orion star formation complex (OSFC). Utilizing a\nSelf-Organizing map (SOM) algorithm, an unsupervised machine learning method,\nwe created a grid of morphological prototypes from near- and mid-infrared\nimages. Furthermore, we determined which prototypes are most representative of\nthe different observational classes, derived from the infrared spectral index,\nvia Bayesian inference. We present our grids of morphological prototypes of\nyoung stellar objects in the near-infrared, which were created purely from\nobservational data. They are thus non-dependent on theoretical models. In\naddition, we show maps that indicate the probability for a prototype belonging\nto any of the observational classes. We find that SOMs created from\nnear-infrared images are a useful tool, with limitations, to identify\ncharacteristic morphologies of young stellar objects in different evolutionary\nstages. This first step lays the foundation for a spectro-morphological\nclassification of young stellar objects to be developed in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studies in the past few decades have investigated young stellar object\nevolution based on their spectral energy distribution (SED). The SED is heavily\ninfluenced not only by evolutionary stage, but also the morphology of the young\nstar. This work is part of the NEMESIS project which is aiming to revisit star\nformation with the aid of machine learning techniques and provides the\nframework for this work. In a first effort towards a novel\nspectro-morphological classification we analyzed young stellar object\nmorphologies and linked them to the currently used observational classes.\nThereby we aim to lay the foundation for a spectro-morphological\nclassification, and apply the insights learned in this study in a future,\nrevisited classification scheme. We obtained archival high-resolution survey\nimages from VISTA for approximately 10,000 literature young stellar object\ncandidates towards the Orion star formation complex (OSFC). Utilizing a\nSelf-Organizing map (SOM) algorithm, an unsupervised machine learning method,\nwe created a grid of morphological prototypes from near- and mid-infrared\nimages. Furthermore, we determined which prototypes are most representative of\nthe different observational classes, derived from the infrared spectral index,\nvia Bayesian inference. We present our grids of morphological prototypes of\nyoung stellar objects in the near-infrared, which were created purely from\nobservational data. They are thus non-dependent on theoretical models. In\naddition, we show maps that indicate the probability for a prototype belonging\nto any of the observational classes. We find that SOMs created from\nnear-infrared images are a useful tool, with limitations, to identify\ncharacteristic morphologies of young stellar objects in different evolutionary\nstages. This first step lays the foundation for a spectro-morphological\nclassification of young stellar objects to be developed in the future."
                },
                "authors": [
                    {
                        "name": "David Hernandez"
                    },
                    {
                        "name": "Odysseas Dionatos"
                    },
                    {
                        "name": "Marc Audard"
                    },
                    {
                        "name": "GÃ¡bor Marton"
                    },
                    {
                        "name": "Julia Roquette"
                    },
                    {
                        "name": "Ilknur Gezer"
                    },
                    {
                        "name": "MÃ¡tÃ© MadarÃ¡sz"
                    },
                    {
                        "name": "Kai L. Polsterer"
                    }
                ],
                "author_detail": {
                    "name": "Kai L. Polsterer"
                },
                "author": "Kai L. Polsterer",
                "arxiv_comment": "Accepted for publication in Astronomy & Astrophysics on September\n  22nd, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17544v2",
                "updated": "2025-09-23T14:32:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    32,
                    50,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T09:02:53Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    9,
                    2,
                    53,
                    0,
                    265,
                    0
                ],
                "title": "A Multimodal Conversational Assistant for the Characterization of\n  Agricultural Plots from Geospatial Open Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multimodal Conversational Assistant for the Characterization of\n  Agricultural Plots from Geospatial Open Data"
                },
                "summary": "The increasing availability of open Earth Observation (EO) and agricultural\ndatasets holds great potential for supporting sustainable land management.\nHowever, their high technical entry barrier limits accessibility for non-expert\nusers. This study presents an open-source conversational assistant that\nintegrates multimodal retrieval and large language models (LLMs) to enable\nnatural language interaction with heterogeneous agricultural and geospatial\ndata. The proposed architecture combines orthophotos, Sentinel-2 vegetation\nindices, and user-provided documents through retrieval-augmented generation\n(RAG), allowing the system to flexibly determine whether to rely on multimodal\nevidence, textual knowledge, or both in formulating an answer. To assess\nresponse quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a\nzero-shot, unsupervised setting, applying direct scoring in a multi-dimensional\nquantitative evaluation framework. Preliminary results show that the system is\ncapable of generating clear, relevant, and context-aware responses to\nagricultural queries, while remaining reproducible and scalable across\ngeographic regions. The primary contributions of this work include an\narchitecture for fusing multimodal EO and textual knowledge sources, a\ndemonstration of lowering the barrier to access specialized agricultural\ninformation through natural language interaction, and an open and reproducible\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of open Earth Observation (EO) and agricultural\ndatasets holds great potential for supporting sustainable land management.\nHowever, their high technical entry barrier limits accessibility for non-expert\nusers. This study presents an open-source conversational assistant that\nintegrates multimodal retrieval and large language models (LLMs) to enable\nnatural language interaction with heterogeneous agricultural and geospatial\ndata. The proposed architecture combines orthophotos, Sentinel-2 vegetation\nindices, and user-provided documents through retrieval-augmented generation\n(RAG), allowing the system to flexibly determine whether to rely on multimodal\nevidence, textual knowledge, or both in formulating an answer. To assess\nresponse quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a\nzero-shot, unsupervised setting, applying direct scoring in a multi-dimensional\nquantitative evaluation framework. Preliminary results show that the system is\ncapable of generating clear, relevant, and context-aware responses to\nagricultural queries, while remaining reproducible and scalable across\ngeographic regions. The primary contributions of this work include an\narchitecture for fusing multimodal EO and textual knowledge sources, a\ndemonstration of lowering the barrier to access specialized agricultural\ninformation through natural language interaction, and an open and reproducible\ndesign."
                },
                "authors": [
                    {
                        "name": "Juan CaÃ±ada"
                    },
                    {
                        "name": "RaÃºl Alonso"
                    },
                    {
                        "name": "Julio Molleda"
                    },
                    {
                        "name": "Fidel DÃ­ez"
                    }
                ],
                "author_detail": {
                    "name": "Fidel DÃ­ez"
                },
                "author": "Fidel DÃ­ez",
                "arxiv_comment": "Accepted at 2025 4th International Conference on Geographic\n  Information and Remote Sensing Technology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19068v1",
                "updated": "2025-09-23T14:32:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    32,
                    33,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:32:33Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    32,
                    33,
                    1,
                    266,
                    0
                ],
                "title": "Dwarf Galaxies in the MATLAS Survey: Hubble Space Telescope Observations\n  of Nuclear Star Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dwarf Galaxies in the MATLAS Survey: Hubble Space Telescope Observations\n  of Nuclear Star Clusters"
                },
                "summary": "In dwarf galaxies, nuclear star clusters (NSCs) are believed to primarily\nform from the migration and merger of globular clusters (GCs), with a possible\ncontribution from in-situ star-forming activity triggered by gas infall. We\npresent the study of NSCs in 41 MATLAS survey dwarf galaxies including\nultra-diffuse galaxies (UDGs), as part of a large follow-up imaging program\nwith the Hubble Space Telescope (HST) Advanced Camera for Surveys (ACS) using\nthe F606W and F814W filters. The sample is biased towards low-surface\nbrightness and large dwarfs, i.e., UDG-like galaxies, and includes two galaxies\nwith a double nucleus, 13 newly identified nucleated dwarfs thanks to HST's\nhigh spatial resolution, and five candidate ultra-compact dwarf progenitors. We\nmodeled the NSCs with a S\\'ersic profile and derived their structural\nproperties and photometry. We find the NSC S\\'ersic index to increase with the\nluminosity and stellar mass, while no obvious trend is seen on the effective\nradius and ellipticity. The faint NSCs tend to have a constant color profile,\nwhereas the bright ones have a bluer center, suggesting that the most massive\nNSCs in our sample might have experienced a mixed formation scenario, including\nin-situ star formation. A significant portion of our NSCs tend to be more\nmassive than for other galaxy samples of similar stellar mass, which could be\ndue to some dwarfs ongoing tidal disruption or an initial formation of massive\nNSCs from multiple GC mergers and in-situ star forming activity. More\nobservations of resolved NSC are needed to be able to infer their formation\nscenario from the structural properties and photometry in dwarfs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dwarf galaxies, nuclear star clusters (NSCs) are believed to primarily\nform from the migration and merger of globular clusters (GCs), with a possible\ncontribution from in-situ star-forming activity triggered by gas infall. We\npresent the study of NSCs in 41 MATLAS survey dwarf galaxies including\nultra-diffuse galaxies (UDGs), as part of a large follow-up imaging program\nwith the Hubble Space Telescope (HST) Advanced Camera for Surveys (ACS) using\nthe F606W and F814W filters. The sample is biased towards low-surface\nbrightness and large dwarfs, i.e., UDG-like galaxies, and includes two galaxies\nwith a double nucleus, 13 newly identified nucleated dwarfs thanks to HST's\nhigh spatial resolution, and five candidate ultra-compact dwarf progenitors. We\nmodeled the NSCs with a S\\'ersic profile and derived their structural\nproperties and photometry. We find the NSC S\\'ersic index to increase with the\nluminosity and stellar mass, while no obvious trend is seen on the effective\nradius and ellipticity. The faint NSCs tend to have a constant color profile,\nwhereas the bright ones have a bluer center, suggesting that the most massive\nNSCs in our sample might have experienced a mixed formation scenario, including\nin-situ star formation. A significant portion of our NSCs tend to be more\nmassive than for other galaxy samples of similar stellar mass, which could be\ndue to some dwarfs ongoing tidal disruption or an initial formation of massive\nNSCs from multiple GC mergers and in-situ star forming activity. More\nobservations of resolved NSC are needed to be able to infer their formation\nscenario from the structural properties and photometry in dwarfs."
                },
                "authors": [
                    {
                        "name": "MÃ©lina Poulain"
                    },
                    {
                        "name": "Francine R. Marleau"
                    },
                    {
                        "name": "Pierre-Alain Duc"
                    },
                    {
                        "name": "RubÃ©n SÃ¡nchez-Janssen"
                    },
                    {
                        "name": "Patrick R. Durrell"
                    },
                    {
                        "name": "Sanjaya Paudel"
                    },
                    {
                        "name": "Rebecca Habas"
                    },
                    {
                        "name": "Oliver MÃ¼ller"
                    },
                    {
                        "name": "Sungsoon Lim"
                    },
                    {
                        "name": "Nick Heesters"
                    },
                    {
                        "name": "JÃ©rÃ©my Fensch"
                    }
                ],
                "author_detail": {
                    "name": "JÃ©rÃ©my Fensch"
                },
                "author": "JÃ©rÃ©my Fensch",
                "arxiv_comment": "Accepted for publication in A&A. 18 pages, 10 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15173v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15173v3",
                "updated": "2025-09-23T14:29:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    29,
                    40,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-21T06:43:34Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    6,
                    43,
                    34,
                    2,
                    141,
                    0
                ],
                "title": "AvatarShield: Visual Reinforcement Learning for Human-Centric Synthetic\n  Video Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AvatarShield: Visual Reinforcement Learning for Human-Centric Synthetic\n  Video Detection"
                },
                "summary": "Recent advances in Artificial Intelligence Generated Content have led to\nhighly realistic synthetic videos, particularly in human-centric scenarios\ninvolving speech, gestures, and full-body motion, posing serious threats to\ninformation authenticity and public trust. Unlike DeepFake techniques that\nfocus on localized facial manipulation, human-centric video generation methods\ncan synthesize entire human bodies with controllable movements, enabling\ncomplex interactions with environments, objects, and even other people.\nHowever, existing detection methods largely overlook the growing risks posed by\nsuch full-body synthetic content. Meanwhile, a growing body of research has\nexplored leveraging LLMs for interpretable fake detection, aiming to explain\ndecisions in natural language. Yet these approaches heavily depend on\nsupervised fine-tuning, which introduces limitations such as annotation bias,\nhallucinated supervision, and weakened generalization. To address these\nchallenges, we propose AvatarShield, a novel multimodal human-centric synthetic\nvideo detection framework that eliminates the need for dense textual\nsupervision by adopting Group Relative Policy Optimization, enabling LLMs to\ndevelop reasoning capabilities from simple binary labels. Our architecture\ncombines a discrete vision tower for high-level semantic inconsistencies and a\nresidual extractor for fine-grained artifact analysis. We further introduce\nFakeHumanVid, a large-scale benchmark containing 15K real and synthetic videos\nacross nine state-of-the-art human generation methods driven by text, pose, or\naudio. Extensive experiments demonstrate that AvatarShield outperforms existing\nmethods in both in-domain and cross-domain settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Artificial Intelligence Generated Content have led to\nhighly realistic synthetic videos, particularly in human-centric scenarios\ninvolving speech, gestures, and full-body motion, posing serious threats to\ninformation authenticity and public trust. Unlike DeepFake techniques that\nfocus on localized facial manipulation, human-centric video generation methods\ncan synthesize entire human bodies with controllable movements, enabling\ncomplex interactions with environments, objects, and even other people.\nHowever, existing detection methods largely overlook the growing risks posed by\nsuch full-body synthetic content. Meanwhile, a growing body of research has\nexplored leveraging LLMs for interpretable fake detection, aiming to explain\ndecisions in natural language. Yet these approaches heavily depend on\nsupervised fine-tuning, which introduces limitations such as annotation bias,\nhallucinated supervision, and weakened generalization. To address these\nchallenges, we propose AvatarShield, a novel multimodal human-centric synthetic\nvideo detection framework that eliminates the need for dense textual\nsupervision by adopting Group Relative Policy Optimization, enabling LLMs to\ndevelop reasoning capabilities from simple binary labels. Our architecture\ncombines a discrete vision tower for high-level semantic inconsistencies and a\nresidual extractor for fine-grained artifact analysis. We further introduce\nFakeHumanVid, a large-scale benchmark containing 15K real and synthetic videos\nacross nine state-of-the-art human generation methods driven by text, pose, or\naudio. Extensive experiments demonstrate that AvatarShield outperforms existing\nmethods in both in-domain and cross-domain settings."
                },
                "authors": [
                    {
                        "name": "Zhipei Xu"
                    },
                    {
                        "name": "Xuanyu Zhang"
                    },
                    {
                        "name": "Qing Huang"
                    },
                    {
                        "name": "Xing Zhou"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15173v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15173v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17999v2",
                "updated": "2025-09-23T14:28:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    28,
                    10,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T16:39:22Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    39,
                    22,
                    0,
                    265,
                    0
                ],
                "title": "The Narcissus Hypothesis: Descending to the Rung of Illusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Narcissus Hypothesis: Descending to the Rung of Illusion"
                },
                "summary": "Modern foundational models increasingly reflect not just world knowledge, but\npatterns of human preference embedded in their training data. We hypothesize\nthat recursive alignment-via human feedback and model-generated corpora-induces\na social desirability bias, nudging models to favor agreeable or flattering\nresponses over objective reasoning. We refer to it as the Narcissus Hypothesis\nand test it across 31 models using standardized personality assessments and a\nnovel Social Desirability Bias score. Results reveal a significant drift toward\nsocially conforming traits, with profound implications for corpus integrity and\nthe reliability of downstream inferences. We then offer a novel epistemological\ninterpretation, tracing how recursive bias may collapse higher-order reasoning\ndown Pearl's Ladder of Causality, culminating in what we refer to as the Rung\nof Illusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern foundational models increasingly reflect not just world knowledge, but\npatterns of human preference embedded in their training data. We hypothesize\nthat recursive alignment-via human feedback and model-generated corpora-induces\na social desirability bias, nudging models to favor agreeable or flattering\nresponses over objective reasoning. We refer to it as the Narcissus Hypothesis\nand test it across 31 models using standardized personality assessments and a\nnovel Social Desirability Bias score. Results reveal a significant drift toward\nsocially conforming traits, with profound implications for corpus integrity and\nthe reliability of downstream inferences. We then offer a novel epistemological\ninterpretation, tracing how recursive bias may collapse higher-order reasoning\ndown Pearl's Ladder of Causality, culminating in what we refer to as the Rung\nof Illusion."
                },
                "authors": [
                    {
                        "name": "Riccardo Cadei"
                    },
                    {
                        "name": "Christian InternÃ²"
                    }
                ],
                "author_detail": {
                    "name": "Christian InternÃ²"
                },
                "author": "Christian InternÃ²",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07373v2",
                "updated": "2025-09-23T14:26:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    26,
                    51,
                    1,
                    266,
                    0
                ],
                "published": "2025-01-13T14:41:56Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    41,
                    56,
                    0,
                    13,
                    0
                ],
                "title": "Dynami-CAL GraphNet: A Physics-Informed Graph Neural Network Conserving\n  Linear and Angular Momentum for Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynami-CAL GraphNet: A Physics-Informed Graph Neural Network Conserving\n  Linear and Angular Momentum for Dynamical Systems"
                },
                "summary": "Accurate, interpretable, and real-time modeling of multi-body dynamical\nsystems is essential for predicting behaviors and inferring physical properties\nin natural and engineered environments. Traditional physics-based models face\nscalability challenges and are computationally demanding, while data-driven\napproaches like Graph Neural Networks (GNNs) often lack physical consistency,\ninterpretability, and generalization. In this paper, we propose Dynami-CAL\nGraphNet, a Physics-Informed Graph Neural Network that integrates the learning\ncapabilities of GNNs with physics-based inductive biases to address these\nlimitations. Dynami-CAL GraphNet enforces pairwise conservation of linear and\nangular momentum for interacting nodes using edge-local reference frames that\nare equivariant to rotational symmetries, invariant to translations, and\nequivariant to node permutations. This design ensures physically consistent\npredictions of node dynamics while offering interpretable, edge-wise linear and\nangular impulses resulting from pairwise interactions. Evaluated on a 3D\ngranular system with inelastic collisions, Dynami-CAL GraphNet demonstrates\nstable error accumulation over extended rollouts, effective extrapolations to\nunseen configurations, and robust handling of heterogeneous interactions and\nexternal forces. Dynami-CAL GraphNet offers significant advantages in fields\nrequiring accurate, interpretable, and real-time modeling of complex multi-body\ndynamical systems, such as robotics, aerospace engineering, and materials\nscience. By providing physically consistent and scalable predictions that\nadhere to fundamental conservation laws, it enables the inference of forces and\nmoments while efficiently handling heterogeneous interactions and external\nforces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate, interpretable, and real-time modeling of multi-body dynamical\nsystems is essential for predicting behaviors and inferring physical properties\nin natural and engineered environments. Traditional physics-based models face\nscalability challenges and are computationally demanding, while data-driven\napproaches like Graph Neural Networks (GNNs) often lack physical consistency,\ninterpretability, and generalization. In this paper, we propose Dynami-CAL\nGraphNet, a Physics-Informed Graph Neural Network that integrates the learning\ncapabilities of GNNs with physics-based inductive biases to address these\nlimitations. Dynami-CAL GraphNet enforces pairwise conservation of linear and\nangular momentum for interacting nodes using edge-local reference frames that\nare equivariant to rotational symmetries, invariant to translations, and\nequivariant to node permutations. This design ensures physically consistent\npredictions of node dynamics while offering interpretable, edge-wise linear and\nangular impulses resulting from pairwise interactions. Evaluated on a 3D\ngranular system with inelastic collisions, Dynami-CAL GraphNet demonstrates\nstable error accumulation over extended rollouts, effective extrapolations to\nunseen configurations, and robust handling of heterogeneous interactions and\nexternal forces. Dynami-CAL GraphNet offers significant advantages in fields\nrequiring accurate, interpretable, and real-time modeling of complex multi-body\ndynamical systems, such as robotics, aerospace engineering, and materials\nscience. By providing physically consistent and scalable predictions that\nadhere to fundamental conservation laws, it enables the inference of forces and\nmoments while efficiently handling heterogeneous interactions and external\nforces."
                },
                "authors": [
                    {
                        "name": "Vinay Sharma"
                    },
                    {
                        "name": "Olga Fink"
                    }
                ],
                "author_detail": {
                    "name": "Olga Fink"
                },
                "author": "Olga Fink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23577v2",
                "updated": "2025-09-23T14:22:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    22,
                    4,
                    1,
                    266,
                    0
                ],
                "published": "2025-07-31T14:08:04Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    8,
                    4,
                    3,
                    212,
                    0
                ],
                "title": "T-Detect: Tail-Aware Statistical Normalization for Robust Detection of\n  Adversarial Machine-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T-Detect: Tail-Aware Statistical Normalization for Robust Detection of\n  Adversarial Machine-Generated Text"
                },
                "summary": "Large language models (LLMs) have shown the capability to generate fluent and\nlogical content, presenting significant challenges to machine-generated text\ndetection, particularly text polished by adversarial perturbations such as\nparaphrasing. Current zero-shot detectors often employ Gaussian distributions\nas statistical measure for computing detection thresholds, which falters when\nconfronted with the heavy-tailed statistical artifacts characteristic of\nadversarial or non-native English texts. In this paper, we introduce T-Detect,\na novel detection method that fundamentally redesigns the curvature-based\ndetectors. Our primary innovation is the replacement of standard Gaussian\nnormalization with a heavy-tailed discrepancy score derived from the Student's\nt-distribution. This approach is theoretically grounded in the empirical\nobservation that adversarial texts exhibit significant leptokurtosis, rendering\ntraditional statistical assumptions inadequate. T-Detect computes a detection\nscore by normalizing the log-likelihood of a passage against the expected\nmoments of a t-distribution, providing superior resilience to statistical\noutliers. We validate our approach on the challenging RAID benchmark for\nadversarial text and the comprehensive HART dataset. Experiments show that\nT-Detect provides a consistent performance uplift over strong baselines,\nimproving AUROC by up to 3.9\\% in targeted domains. When integrated into a\ntwo-dimensional detection framework (CT), our method achieves state-of-the-art\nperformance, with an AUROC of 0.926 on the Books domain of RAID. Our\ncontributions are a new, theoretically-justified statistical foundation for\ntext detection, an ablation-validated method that demonstrates superior\nrobustness, and a comprehensive analysis of its performance under adversarial\nconditions. Ours code are released at https://github.com/ResearAI/t-detect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown the capability to generate fluent and\nlogical content, presenting significant challenges to machine-generated text\ndetection, particularly text polished by adversarial perturbations such as\nparaphrasing. Current zero-shot detectors often employ Gaussian distributions\nas statistical measure for computing detection thresholds, which falters when\nconfronted with the heavy-tailed statistical artifacts characteristic of\nadversarial or non-native English texts. In this paper, we introduce T-Detect,\na novel detection method that fundamentally redesigns the curvature-based\ndetectors. Our primary innovation is the replacement of standard Gaussian\nnormalization with a heavy-tailed discrepancy score derived from the Student's\nt-distribution. This approach is theoretically grounded in the empirical\nobservation that adversarial texts exhibit significant leptokurtosis, rendering\ntraditional statistical assumptions inadequate. T-Detect computes a detection\nscore by normalizing the log-likelihood of a passage against the expected\nmoments of a t-distribution, providing superior resilience to statistical\noutliers. We validate our approach on the challenging RAID benchmark for\nadversarial text and the comprehensive HART dataset. Experiments show that\nT-Detect provides a consistent performance uplift over strong baselines,\nimproving AUROC by up to 3.9\\% in targeted domains. When integrated into a\ntwo-dimensional detection framework (CT), our method achieves state-of-the-art\nperformance, with an AUROC of 0.926 on the Books domain of RAID. Our\ncontributions are a new, theoretically-justified statistical foundation for\ntext detection, an ablation-validated method that demonstrates superior\nrobustness, and a comprehensive analysis of its performance under adversarial\nconditions. Ours code are released at https://github.com/ResearAI/t-detect."
                },
                "authors": [
                    {
                        "name": "Alva West"
                    },
                    {
                        "name": "Luodan Zhang"
                    },
                    {
                        "name": "Liuliu Zhang"
                    },
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Yixuan Weng"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19057v1",
                "updated": "2025-09-23T14:21:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    21,
                    46,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:21:46Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    21,
                    46,
                    1,
                    266,
                    0
                ],
                "title": "RELATE: Relation Extraction in Biomedical Abstracts with LLMs and\n  Ontology Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RELATE: Relation Extraction in Biomedical Abstracts with LLMs and\n  Ontology Constraints"
                },
                "summary": "Biomedical knowledge graphs (KGs) are vital for drug discovery and clinical\ndecision support but remain incomplete. Large language models (LLMs) excel at\nextracting biomedical relations, yet their outputs lack standardization and\nalignment with ontologies, limiting KG integration. We introduce RELATE, a\nthree-stage pipeline that maps LLM-extracted relations to standardized ontology\npredicates using ChemProt and the Biolink Model. The pipeline includes: (1)\nontology preprocessing with predicate embeddings, (2) similarity-based\nretrieval enhanced with SapBERT, and (3) LLM-based reranking with explicit\nnegation handling. This approach transforms relation extraction from free-text\noutputs to structured, ontology-constrained representations. On the ChemProt\nbenchmark, RELATE achieves 52% exact match and 94% accuracy@10, and in 2,400\nHEAL Project abstracts, it effectively rejects irrelevant associations (0.4%)\nand identifies negated assertions. RELATE captures nuanced biomedical\nrelationships while ensuring quality for KG augmentation. By combining vector\nsearch with contextual LLM reasoning, RELATE provides a scalable, semantically\naccurate framework for converting unstructured biomedical literature into\nstandardized KGs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical knowledge graphs (KGs) are vital for drug discovery and clinical\ndecision support but remain incomplete. Large language models (LLMs) excel at\nextracting biomedical relations, yet their outputs lack standardization and\nalignment with ontologies, limiting KG integration. We introduce RELATE, a\nthree-stage pipeline that maps LLM-extracted relations to standardized ontology\npredicates using ChemProt and the Biolink Model. The pipeline includes: (1)\nontology preprocessing with predicate embeddings, (2) similarity-based\nretrieval enhanced with SapBERT, and (3) LLM-based reranking with explicit\nnegation handling. This approach transforms relation extraction from free-text\noutputs to structured, ontology-constrained representations. On the ChemProt\nbenchmark, RELATE achieves 52% exact match and 94% accuracy@10, and in 2,400\nHEAL Project abstracts, it effectively rejects irrelevant associations (0.4%)\nand identifies negated assertions. RELATE captures nuanced biomedical\nrelationships while ensuring quality for KG augmentation. By combining vector\nsearch with contextual LLM reasoning, RELATE provides a scalable, semantically\naccurate framework for converting unstructured biomedical literature into\nstandardized KGs."
                },
                "authors": [
                    {
                        "name": "Olawumi Olasunkanmi"
                    },
                    {
                        "name": "Mathew Satursky"
                    },
                    {
                        "name": "Hong Yi"
                    },
                    {
                        "name": "Chris Bizon"
                    },
                    {
                        "name": "Harlin Lee"
                    },
                    {
                        "name": "Stanley Ahalt"
                    }
                ],
                "author_detail": {
                    "name": "Stanley Ahalt"
                },
                "author": "Stanley Ahalt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19056v1",
                "updated": "2025-09-23T14:21:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    21,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:21:03Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    21,
                    3,
                    1,
                    266,
                    0
                ],
                "title": "Bayesian Convolutional Neural Networks for Prior Learning in Graph\n  Signal Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Convolutional Neural Networks for Prior Learning in Graph\n  Signal Recovery"
                },
                "summary": "Graph signal recovery (GSR) is a fundamental problem in graph signal\nprocessing, where the goal is to reconstruct a complete signal defined over a\ngraph from a subset of noisy or missing observations. A central challenge in\nGSR is that the underlying statistical model of the graph signal is often\nunknown or too complex to specify analytically. To address this, we propose a\nflexible, data-driven framework that learns the signal prior directly from\ntraining samples. We develop a Bayesian convolutional neural network (BCNN)\narchitecture that models the prior distribution of graph signals using\ngraph-aware filters based on Chebyshev polynomials. By interpreting the hidden\nlayers of the CNN as Gibbs distributions and employing Gaussian mixture model\n(GMM) nonlinearities, we obtain a closed-form and expressive prior. This prior\nis integrated into a variational Bayesian (VB) inference framework to estimate\nthe posterior distribution of the signal and noise precision. Extensive\nexperiments on synthetic and real-world graph datasets demonstrate that the\nproposed BCNN-GSR algorithm achieves accurate and robust recovery across a\nvariety of signal distributions. The method generalizes well to complex,\nnon-Gaussian signal models and remains computationally efficient, making it\nsuitable for practical large-scale graph recovery tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph signal recovery (GSR) is a fundamental problem in graph signal\nprocessing, where the goal is to reconstruct a complete signal defined over a\ngraph from a subset of noisy or missing observations. A central challenge in\nGSR is that the underlying statistical model of the graph signal is often\nunknown or too complex to specify analytically. To address this, we propose a\nflexible, data-driven framework that learns the signal prior directly from\ntraining samples. We develop a Bayesian convolutional neural network (BCNN)\narchitecture that models the prior distribution of graph signals using\ngraph-aware filters based on Chebyshev polynomials. By interpreting the hidden\nlayers of the CNN as Gibbs distributions and employing Gaussian mixture model\n(GMM) nonlinearities, we obtain a closed-form and expressive prior. This prior\nis integrated into a variational Bayesian (VB) inference framework to estimate\nthe posterior distribution of the signal and noise precision. Extensive\nexperiments on synthetic and real-world graph datasets demonstrate that the\nproposed BCNN-GSR algorithm achieves accurate and robust recovery across a\nvariety of signal distributions. The method generalizes well to complex,\nnon-Gaussian signal models and remains computationally efficient, making it\nsuitable for practical large-scale graph recovery tasks."
                },
                "authors": [
                    {
                        "name": "Razieh Torkamani"
                    },
                    {
                        "name": "Arash Amini"
                    },
                    {
                        "name": "Hadi Zayyani"
                    },
                    {
                        "name": "Mehdi Korki"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Korki"
                },
                "author": "Mehdi Korki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13232v2",
                "updated": "2025-09-23T14:19:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    19,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-16T16:39:11Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    39,
                    11,
                    1,
                    259,
                    0
                ],
                "title": "Single-stream Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-stream Policy Optimization"
                },
                "summary": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@$k$ across the evaluated $k$ values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@$k$ across the evaluated $k$ values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning."
                },
                "authors": [
                    {
                        "name": "Zhongwen Xu"
                    },
                    {
                        "name": "Zihan Ding"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Ding"
                },
                "author": "Zihan Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19040v1",
                "updated": "2025-09-23T14:09:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    9,
                    50,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:09:50Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    9,
                    50,
                    1,
                    266,
                    0
                ],
                "title": "Nonparametric efficient estimation of the longitudinal front-door\n  functional",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric efficient estimation of the longitudinal front-door\n  functional"
                },
                "summary": "The front-door criterion is an identification strategy for the\nintervention-specific mean outcome in settings where the standard back-door\ncriterion fails due to unmeasured exposure-outcome confounders, but an\nintermediate variable exists that completely mediates the effect of exposure on\nthe outcome and is not affected by unmeasured confounding. The front-door\ncriterion has been extended to the longitudinal setting, where exposure and\nmediator are measured repeatedly over time. However, to the best of our\nknowledge, applications of the longitudinal front-door criterion remain\nunexplored. This may reflect both limited awareness of the method and the\nabsence of suitable estimation techniques. In this report, we propose\nnonparametric efficient estimators of the longitudinal front-door functional.\nThe estimators are multiply robust and allow for the use of data-adaptive\n(machine learning) methods for nuisance estimation while providing valid\ninference. The theoretical properties of the estimators are showcased in a\nsimulation study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The front-door criterion is an identification strategy for the\nintervention-specific mean outcome in settings where the standard back-door\ncriterion fails due to unmeasured exposure-outcome confounders, but an\nintermediate variable exists that completely mediates the effect of exposure on\nthe outcome and is not affected by unmeasured confounding. The front-door\ncriterion has been extended to the longitudinal setting, where exposure and\nmediator are measured repeatedly over time. However, to the best of our\nknowledge, applications of the longitudinal front-door criterion remain\nunexplored. This may reflect both limited awareness of the method and the\nabsence of suitable estimation techniques. In this report, we propose\nnonparametric efficient estimators of the longitudinal front-door functional.\nThe estimators are multiply robust and allow for the use of data-adaptive\n(machine learning) methods for nuisance estimation while providing valid\ninference. The theoretical properties of the estimators are showcased in a\nsimulation study."
                },
                "authors": [
                    {
                        "name": "Marie S. Breum"
                    },
                    {
                        "name": "Helene C. W. Rytgaard"
                    },
                    {
                        "name": "Torben Martinussen"
                    },
                    {
                        "name": "Erin E. Gabriel"
                    }
                ],
                "author_detail": {
                    "name": "Erin E. Gabriel"
                },
                "author": "Erin E. Gabriel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19033v2",
                "updated": "2025-09-24T07:17:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    7,
                    17,
                    32,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-23T14:06:09Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    6,
                    9,
                    1,
                    266,
                    0
                ],
                "title": "Charting a Decade of Computational Linguistics in Italy: The CLiC-it\n  Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting a Decade of Computational Linguistics in Italy: The CLiC-it\n  Corpus"
                },
                "summary": "Over the past decade, Computational Linguistics (CL) and Natural Language\nProcessing (NLP) have evolved rapidly, especially with the advent of\nTransformer-based Large Language Models (LLMs). This shift has transformed\nresearch goals and priorities, from Lexical and Semantic Resources to Language\nModelling and Multimodality. In this study, we track the research trends of the\nItalian CL and NLP community through an analysis of the contributions to\nCLiC-it, arguably the leading Italian conference in the field. We compile the\nproceedings from the first 10 editions of the CLiC-it conference (from 2014 to\n2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its\nmetadata, including author provenance, gender, affiliations, and more, as well\nas the content of the papers themselves, which address various topics. Our goal\nis to provide the Italian and international research communities with valuable\ninsights into emerging trends and key developments over time, supporting\ninformed decisions and future directions in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past decade, Computational Linguistics (CL) and Natural Language\nProcessing (NLP) have evolved rapidly, especially with the advent of\nTransformer-based Large Language Models (LLMs). This shift has transformed\nresearch goals and priorities, from Lexical and Semantic Resources to Language\nModelling and Multimodality. In this study, we track the research trends of the\nItalian CL and NLP community through an analysis of the contributions to\nCLiC-it, arguably the leading Italian conference in the field. We compile the\nproceedings from the first 10 editions of the CLiC-it conference (from 2014 to\n2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its\nmetadata, including author provenance, gender, affiliations, and more, as well\nas the content of the papers themselves, which address various topics. Our goal\nis to provide the Italian and international research communities with valuable\ninsights into emerging trends and key developments over time, supporting\ninformed decisions and future directions in the field."
                },
                "authors": [
                    {
                        "name": "Chiara Alzetta"
                    },
                    {
                        "name": "Serena Auriemma"
                    },
                    {
                        "name": "Alessandro Bondielli"
                    },
                    {
                        "name": "Luca Dini"
                    },
                    {
                        "name": "Chiara Fazzone"
                    },
                    {
                        "name": "Alessio Miaschi"
                    },
                    {
                        "name": "Martina Miliani"
                    },
                    {
                        "name": "Marta Sartor"
                    }
                ],
                "author_detail": {
                    "name": "Marta Sartor"
                },
                "author": "Marta Sartor",
                "arxiv_comment": "Submitted to IJCoL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17078v2",
                "updated": "2025-09-23T14:02:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    2,
                    49,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-23T16:13:57Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    16,
                    13,
                    57,
                    5,
                    235,
                    0
                ],
                "title": "Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer\n  on Low-resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer\n  on Low-resource Languages"
                },
                "summary": "The current Large Language Models (LLMs) face significant challenges in\nimproving their performance on low-resource languages and urgently need\ndata-efficient methods without costly fine-tuning. From the perspective of\nlanguage-bridge, we propose a simple yet effective method, namely BridgeX-ICL,\nto improve the zero-shot Cross-lingual In-Context Learning (X-ICL) for\nlow-resource languages. Unlike existing works focusing on language-specific\nneurons, BridgeX-ICL explores whether sharing neurons can improve cross-lingual\nperformance in LLMs. We construct neuron probe data from the ground-truth MUSE\nbilingual dictionaries, and define a subset of language overlap neurons\naccordingly to ensure full activation of these anchored neurons. Subsequently,\nwe propose an HSIC-based metric to quantify LLMs' internal linguistic spectrum\nbased on overlapping neurons, guiding optimal bridge selection. The experiments\nconducted on 4 cross-lingual tasks and 15 language pairs from 7 diverse\nfamilies, covering both high-low and moderate-low pairs, validate the\neffectiveness of BridgeX-ICL and offer empirical insights into the underlying\nmultilingual mechanisms of LLMs. The code is publicly available at\nhttps://github.com/xuyuemei/BridgeX-ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Large Language Models (LLMs) face significant challenges in\nimproving their performance on low-resource languages and urgently need\ndata-efficient methods without costly fine-tuning. From the perspective of\nlanguage-bridge, we propose a simple yet effective method, namely BridgeX-ICL,\nto improve the zero-shot Cross-lingual In-Context Learning (X-ICL) for\nlow-resource languages. Unlike existing works focusing on language-specific\nneurons, BridgeX-ICL explores whether sharing neurons can improve cross-lingual\nperformance in LLMs. We construct neuron probe data from the ground-truth MUSE\nbilingual dictionaries, and define a subset of language overlap neurons\naccordingly to ensure full activation of these anchored neurons. Subsequently,\nwe propose an HSIC-based metric to quantify LLMs' internal linguistic spectrum\nbased on overlapping neurons, guiding optimal bridge selection. The experiments\nconducted on 4 cross-lingual tasks and 15 language pairs from 7 diverse\nfamilies, covering both high-low and moderate-low pairs, validate the\neffectiveness of BridgeX-ICL and offer empirical insights into the underlying\nmultilingual mechanisms of LLMs. The code is publicly available at\nhttps://github.com/xuyuemei/BridgeX-ICL."
                },
                "authors": [
                    {
                        "name": "Yuemei Xu"
                    },
                    {
                        "name": "Kexin Xu"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Ling Hu"
                    },
                    {
                        "name": "Lin Gui"
                    }
                ],
                "author_detail": {
                    "name": "Lin Gui"
                },
                "author": "Lin Gui",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01616v2",
                "updated": "2025-09-23T14:01:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    1,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-02T22:36:24Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    22,
                    36,
                    24,
                    4,
                    122,
                    0
                ],
                "title": "Phantora: Maximizing Code Reuse in Simulation-based Machine Learning\n  System Performance Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phantora: Maximizing Code Reuse in Simulation-based Machine Learning\n  System Performance Estimation"
                },
                "summary": "Modern machine learning (ML) training workloads place substantial demands on\nboth computational and communication resources. Consequently, accurate\nperformance estimation has become increasingly critical for guiding system\ndesign decisions, such as the selection of parallelization strategies, cluster\nconfigurations, and hardware provisioning. Existing simulation-based\nperformance estimation requires reimplementing the ML framework in a simulator,\nwhich demands significant manual effort and is hard to maintain as ML\nframeworks evolve rapidly.\n  This paper introduces Phantora, a hybrid GPU cluster simulator designed for\nperformance estimation of ML training workloads. Phantora executes unmodified\nML frameworks as is within a distributed, containerized environment. Each\ncontainer emulates the behavior of a GPU server in a large-scale cluster, while\nPhantora intercepts and simulates GPU- and communication-related operations to\nprovide high-fidelity performance estimation. We call this approach hybrid\nsimulation of ML systems, in contrast to traditional methods that simulate\nstatic workloads. The primary advantage of hybrid simulation is that it allows\ndirect reuse of ML framework source code in simulation, avoiding the need for\nreimplementation. Our evaluation shows that Phantora provides accuracy\ncomparable to static workload simulation while supporting three\nstate-of-the-art LLM training frameworks out-of-the-box. In addition, Phantora\noperates on a single GPU, eliminating the need for the resource-intensive trace\ncollection and workload extraction steps required by traditional trace-based\nsimulators. Phantora is open-sourced at https://github.com/QDelta/Phantora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern machine learning (ML) training workloads place substantial demands on\nboth computational and communication resources. Consequently, accurate\nperformance estimation has become increasingly critical for guiding system\ndesign decisions, such as the selection of parallelization strategies, cluster\nconfigurations, and hardware provisioning. Existing simulation-based\nperformance estimation requires reimplementing the ML framework in a simulator,\nwhich demands significant manual effort and is hard to maintain as ML\nframeworks evolve rapidly.\n  This paper introduces Phantora, a hybrid GPU cluster simulator designed for\nperformance estimation of ML training workloads. Phantora executes unmodified\nML frameworks as is within a distributed, containerized environment. Each\ncontainer emulates the behavior of a GPU server in a large-scale cluster, while\nPhantora intercepts and simulates GPU- and communication-related operations to\nprovide high-fidelity performance estimation. We call this approach hybrid\nsimulation of ML systems, in contrast to traditional methods that simulate\nstatic workloads. The primary advantage of hybrid simulation is that it allows\ndirect reuse of ML framework source code in simulation, avoiding the need for\nreimplementation. Our evaluation shows that Phantora provides accuracy\ncomparable to static workload simulation while supporting three\nstate-of-the-art LLM training frameworks out-of-the-box. In addition, Phantora\noperates on a single GPU, eliminating the need for the resource-intensive trace\ncollection and workload extraction steps required by traditional trace-based\nsimulators. Phantora is open-sourced at https://github.com/QDelta/Phantora."
                },
                "authors": [
                    {
                        "name": "Jianxing Qin"
                    },
                    {
                        "name": "Jingrong Chen"
                    },
                    {
                        "name": "Xinhao Kong"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Tianjun Yuan"
                    },
                    {
                        "name": "Liang Luo"
                    },
                    {
                        "name": "Zhaodong Wang"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Tingjun Chen"
                    },
                    {
                        "name": "Alvin R. Lebeck"
                    },
                    {
                        "name": "Danyang Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Danyang Zhuo"
                },
                "author": "Danyang Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03616v2",
                "updated": "2025-09-23T14:00:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    0,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-07-04T14:43:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    14,
                    43,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "EvoAgentX: An Automated Framework for Evolving Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoAgentX: An Automated Framework for Evolving Agentic Workflows"
                },
                "summary": "Multi-agent systems (MAS) have emerged as a powerful paradigm for\norchestrating large language models (LLMs) and specialized tools to\ncollaboratively address complex tasks. However, existing MAS frameworks often\nrequire manual workflow configuration and lack native support for dynamic\nevolution and performance optimization. In addition, many MAS optimization\nalgorithms are not integrated into a unified framework. In this paper, we\npresent EvoAgentX, an open-source platform that automates the generation,\nexecution, and evolutionary optimization of multi-agent workflows. EvoAgentX\nemploys a modular architecture consisting of five core layers: the basic\ncomponents, agent, workflow, evolving, and evaluation layers. Specifically,\nwithin the evolving layer, EvoAgentX integrates three MAS optimization\nalgorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts,\ntool configurations, and workflow topologies. We evaluate EvoAgentX on\nHotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and\nmathematical problem solving, respectively, and further assess it on real-world\ntasks using GAIA. Experimental results show that EvoAgentX consistently\nachieves significant performance improvements, including a 7.44% increase in\nHotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve\naccuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The\nsource code is available at: https://github.com/EvoAgentX/EvoAgentX",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) have emerged as a powerful paradigm for\norchestrating large language models (LLMs) and specialized tools to\ncollaboratively address complex tasks. However, existing MAS frameworks often\nrequire manual workflow configuration and lack native support for dynamic\nevolution and performance optimization. In addition, many MAS optimization\nalgorithms are not integrated into a unified framework. In this paper, we\npresent EvoAgentX, an open-source platform that automates the generation,\nexecution, and evolutionary optimization of multi-agent workflows. EvoAgentX\nemploys a modular architecture consisting of five core layers: the basic\ncomponents, agent, workflow, evolving, and evaluation layers. Specifically,\nwithin the evolving layer, EvoAgentX integrates three MAS optimization\nalgorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts,\ntool configurations, and workflow topologies. We evaluate EvoAgentX on\nHotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and\nmathematical problem solving, respectively, and further assess it on real-world\ntasks using GAIA. Experimental results show that EvoAgentX consistently\nachieves significant performance improvements, including a 7.44% increase in\nHotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve\naccuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The\nsource code is available at: https://github.com/EvoAgentX/EvoAgentX"
                },
                "authors": [
                    {
                        "name": "Yingxu Wang"
                    },
                    {
                        "name": "Siwei Liu"
                    },
                    {
                        "name": "Jinyuan Fang"
                    },
                    {
                        "name": "Zaiqiao Meng"
                    }
                ],
                "author_detail": {
                    "name": "Zaiqiao Meng"
                },
                "author": "Zaiqiao Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19020v1",
                "updated": "2025-09-23T13:58:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    58,
                    16,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:58:16Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    58,
                    16,
                    1,
                    266,
                    0
                ],
                "title": "Investigating Test-Time Scaling with Reranking for Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Test-Time Scaling with Reranking for Machine Translation"
                },
                "summary": "Scaling model parameters has become the de facto strategy for improving NLP\nsystems, but it comes with substantial computational costs. Test-Time Scaling\n(TTS) offers an alternative by allocating more computation at inference:\ngenerating multiple candidates and selecting the best. While effective in tasks\nsuch as mathematical reasoning, TTS has not been systematically explored for\nmachine translation (MT). In this paper, we present the first systematic study\nof TTS for MT, investigating a simple but practical best-of-N framework on\nWMT24 benchmarks. Our experiments cover six high-resource and one low-resource\nlanguage pairs, five model sizes (3B-72B), and various TTS compute budget (N up\nto 1024). Our results show that a) For high-resource languages, TTS generally\nimproves translation quality according to multiple neural MT evaluation\nmetrics, and our human evaluation confirms these gains; b) Augmenting smaller\nmodels with large $N$ can match or surpass larger models at $N{=}1$ with more\ncompute cost; c) Under fixed compute budgets, larger models are typically more\nefficient, and TTS can degrade quality due to metric blind spots in\nlow-resource cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling model parameters has become the de facto strategy for improving NLP\nsystems, but it comes with substantial computational costs. Test-Time Scaling\n(TTS) offers an alternative by allocating more computation at inference:\ngenerating multiple candidates and selecting the best. While effective in tasks\nsuch as mathematical reasoning, TTS has not been systematically explored for\nmachine translation (MT). In this paper, we present the first systematic study\nof TTS for MT, investigating a simple but practical best-of-N framework on\nWMT24 benchmarks. Our experiments cover six high-resource and one low-resource\nlanguage pairs, five model sizes (3B-72B), and various TTS compute budget (N up\nto 1024). Our results show that a) For high-resource languages, TTS generally\nimproves translation quality according to multiple neural MT evaluation\nmetrics, and our human evaluation confirms these gains; b) Augmenting smaller\nmodels with large $N$ can match or surpass larger models at $N{=}1$ with more\ncompute cost; c) Under fixed compute budgets, larger models are typically more\nefficient, and TTS can degrade quality due to metric blind spots in\nlow-resource cases."
                },
                "authors": [
                    {
                        "name": "Shaomu Tan"
                    },
                    {
                        "name": "Ryosuke Mitani"
                    },
                    {
                        "name": "Ritvik Choudhary"
                    },
                    {
                        "name": "Toshiyuki Sekiya"
                    }
                ],
                "author_detail": {
                    "name": "Toshiyuki Sekiya"
                },
                "author": "Toshiyuki Sekiya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19018v1",
                "updated": "2025-09-23T13:57:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    57,
                    55,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:57:55Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    57,
                    55,
                    1,
                    266,
                    0
                ],
                "title": "OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval\n  via Latent Space Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval\n  via Latent Space Alignment"
                },
                "summary": "Recent advances in multimodal large language models (LLMs) have led to\nsignificant progress in understanding, generation, and retrieval tasks.\nHowever, current solutions often treat these tasks in isolation or require\ntraining LLMs from scratch, resulting in high computational costs and limited\ngeneralization across modalities. In this work, we present OmniBridge, a\nunified and modular multimodal framework that supports vision-language\nunderstanding, generation, and retrieval within a unified architecture.\nOmniBridge adopts a language-centric design that reuses pretrained LLMs and\nintroduces a lightweight bidirectional latent alignment module. To address the\nchallenge of task interference, we propose a two-stage decoupled training\nstrategy: supervised fine-tuning and latent space alignment for aligning LLM\nbehavior with multimodal reasoning, and semantic-guided diffusion training to\nalign cross-modal latent spaces via learnable query embeddings. Extensive\nexperiments across a wide range of benchmarks demonstrate that OmniBridge\nachieves competitive or state-of-the-art performance in all three tasks.\nMoreover, our results highlight the effectiveness of latent space alignment for\nunifying multimodal modeling under a shared representation space. Code and\nmodels are released at https://github.com/xiao-xt/OmniBridge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal large language models (LLMs) have led to\nsignificant progress in understanding, generation, and retrieval tasks.\nHowever, current solutions often treat these tasks in isolation or require\ntraining LLMs from scratch, resulting in high computational costs and limited\ngeneralization across modalities. In this work, we present OmniBridge, a\nunified and modular multimodal framework that supports vision-language\nunderstanding, generation, and retrieval within a unified architecture.\nOmniBridge adopts a language-centric design that reuses pretrained LLMs and\nintroduces a lightweight bidirectional latent alignment module. To address the\nchallenge of task interference, we propose a two-stage decoupled training\nstrategy: supervised fine-tuning and latent space alignment for aligning LLM\nbehavior with multimodal reasoning, and semantic-guided diffusion training to\nalign cross-modal latent spaces via learnable query embeddings. Extensive\nexperiments across a wide range of benchmarks demonstrate that OmniBridge\nachieves competitive or state-of-the-art performance in all three tasks.\nMoreover, our results highlight the effectiveness of latent space alignment for\nunifying multimodal modeling under a shared representation space. Code and\nmodels are released at https://github.com/xiao-xt/OmniBridge."
                },
                "authors": [
                    {
                        "name": "Teng Xiao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lefei Zhang"
                },
                "author": "Lefei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18555v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18555v2",
                "updated": "2025-09-23T13:50:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    50,
                    21,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-24T06:45:45Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    45,
                    5,
                    144,
                    0
                ],
                "title": "Unraveling Misinformation Propagation in LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling Misinformation Propagation in LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning, positioning them as promising tools for supporting human\nproblem-solving. However, what happens when their performance is affected by\nmisinformation, i.e., incorrect inputs introduced by users due to oversights or\ngaps in knowledge? Such misinformation is prevalent in real-world interactions\nwith LLMs, yet how it propagates within LLMs' reasoning process remains\nunderexplored. Focusing on mathematical reasoning, we present a comprehensive\nanalysis of how misinformation affects intermediate reasoning steps and final\nanswers. We also examine how effectively LLMs can correct misinformation when\nexplicitly instructed to do so. Even with explicit instructions, LLMs succeed\nless than half the time in rectifying misinformation, despite possessing\ncorrect internal knowledge, leading to significant accuracy drops (10.02% -\n72.20%), and the degradation holds with thinking models (4.30% - 19.97%).\nFurther analysis shows that applying factual corrections early in the reasoning\nprocess most effectively reduces misinformation propagation, and fine-tuning on\nsynthesized data with early-stage corrections significantly improves reasoning\nfactuality. Our work offers a practical approach to mitigating misinformation\npropagation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning, positioning them as promising tools for supporting human\nproblem-solving. However, what happens when their performance is affected by\nmisinformation, i.e., incorrect inputs introduced by users due to oversights or\ngaps in knowledge? Such misinformation is prevalent in real-world interactions\nwith LLMs, yet how it propagates within LLMs' reasoning process remains\nunderexplored. Focusing on mathematical reasoning, we present a comprehensive\nanalysis of how misinformation affects intermediate reasoning steps and final\nanswers. We also examine how effectively LLMs can correct misinformation when\nexplicitly instructed to do so. Even with explicit instructions, LLMs succeed\nless than half the time in rectifying misinformation, despite possessing\ncorrect internal knowledge, leading to significant accuracy drops (10.02% -\n72.20%), and the degradation holds with thinking models (4.30% - 19.97%).\nFurther analysis shows that applying factual corrections early in the reasoning\nprocess most effectively reduces misinformation propagation, and fine-tuning on\nsynthesized data with early-stage corrections significantly improves reasoning\nfactuality. Our work offers a practical approach to mitigating misinformation\npropagation."
                },
                "authors": [
                    {
                        "name": "Yiyang Feng"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Shaobo Cui"
                    },
                    {
                        "name": "Boi Faltings"
                    },
                    {
                        "name": "Mina Lee"
                    },
                    {
                        "name": "Jiawei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhou"
                },
                "author": "Jiawei Zhou",
                "arxiv_comment": "Accepted to EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18555v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18555v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19003v1",
                "updated": "2025-09-23T13:47:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    47,
                    32,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:47:32Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    47,
                    32,
                    1,
                    266,
                    0
                ],
                "title": "Unveiling Chain of Step Reasoning for Vision-Language Models with\n  Fine-grained Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Chain of Step Reasoning for Vision-Language Models with\n  Fine-grained Rewards"
                },
                "summary": "Chain of thought reasoning has demonstrated remarkable success in large\nlanguage models, yet its adaptation to vision-language reasoning remains an\nopen challenge with unclear best practices. Existing attempts typically employ\nreasoning chains at a coarse-grained level, which struggles to perform\nfine-grained structured reasoning and, more importantly, are difficult to\nevaluate the reward and quality of intermediate reasoning. In this work, we\ndelve into chain of step reasoning for vision-language models, enabling\nassessing reasoning step quality accurately and leading to effective\nreinforcement learning and inference-time scaling with fine-grained rewards. We\npresent a simple, effective, and fully transparent framework, including the\nstep-level reasoning data, process reward model (PRM), and reinforcement\nlearning training. With the proposed approaches, our models set strong\nbaselines with consistent improvements on challenging vision-language\nbenchmarks. More importantly, we conduct a thorough empirical analysis and\nablation study, unveiling the impact of each component and several intriguing\nproperties of inference-time scaling. We believe this paper serves as a\nbaseline for vision-language models and offers insights into more complex\nmultimodal reasoning. Our dataset, PRM, and code will be available at\nhttps://github.com/baaivision/CoS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of thought reasoning has demonstrated remarkable success in large\nlanguage models, yet its adaptation to vision-language reasoning remains an\nopen challenge with unclear best practices. Existing attempts typically employ\nreasoning chains at a coarse-grained level, which struggles to perform\nfine-grained structured reasoning and, more importantly, are difficult to\nevaluate the reward and quality of intermediate reasoning. In this work, we\ndelve into chain of step reasoning for vision-language models, enabling\nassessing reasoning step quality accurately and leading to effective\nreinforcement learning and inference-time scaling with fine-grained rewards. We\npresent a simple, effective, and fully transparent framework, including the\nstep-level reasoning data, process reward model (PRM), and reinforcement\nlearning training. With the proposed approaches, our models set strong\nbaselines with consistent improvements on challenging vision-language\nbenchmarks. More importantly, we conduct a thorough empirical analysis and\nablation study, unveiling the impact of each component and several intriguing\nproperties of inference-time scaling. We believe this paper serves as a\nbaseline for vision-language models and offers insights into more complex\nmultimodal reasoning. Our dataset, PRM, and code will be available at\nhttps://github.com/baaivision/CoS."
                },
                "authors": [
                    {
                        "name": "Honghao Chen"
                    },
                    {
                        "name": "Xingzhou Lou"
                    },
                    {
                        "name": "Xiaokun Feng"
                    },
                    {
                        "name": "Kaiqi Huang"
                    },
                    {
                        "name": "Xinlong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinlong Wang"
                },
                "author": "Xinlong Wang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19001v1",
                "updated": "2025-09-23T13:45:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    45,
                    56,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:45:56Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    45,
                    56,
                    1,
                    266,
                    0
                ],
                "title": "HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens\n  for Instruction-based TTS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens\n  for Instruction-based TTS"
                },
                "summary": "Large Language Model (LLM)-based Text-to-Speech (TTS) models have already\nreached a high degree of naturalness. However, the precision control of TTS\ninference is still challenging. Although instruction-based Text-to-Speech\n(Instruct-TTS) models are proposed, these models still lack fine-grained\ncontrol due to the modality gap between single-level text instructions and\nmultilevel speech tokens. To address this limitation, we propose HD-PPT, a\nframework that transforms speech synthesis into a structured, hierarchical\ntask. To enable fine-grained control, we introduce a novel speech codec to\nextract distinct prompt-preference and content-preference tokens from the\ncomplex speech tokens, supervised by automatic speech recognition (ASR) and\ncross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality\ngap of these tokens, we propose a hierarchical decoding strategy, where the LLM\ngenerates tokens in a structured order: first semantic, then fine-grained\nstyle, and finally complete acoustic representation. Extensive experiments\ndemonstrate that this hierarchical paradigm significantly improves instruction\nadherence and achieves state-of-the-art naturalness, validating our approach\nfor precise and controllable speech synthesis. Audio samples are available at\nhttps://xxh333.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based Text-to-Speech (TTS) models have already\nreached a high degree of naturalness. However, the precision control of TTS\ninference is still challenging. Although instruction-based Text-to-Speech\n(Instruct-TTS) models are proposed, these models still lack fine-grained\ncontrol due to the modality gap between single-level text instructions and\nmultilevel speech tokens. To address this limitation, we propose HD-PPT, a\nframework that transforms speech synthesis into a structured, hierarchical\ntask. To enable fine-grained control, we introduce a novel speech codec to\nextract distinct prompt-preference and content-preference tokens from the\ncomplex speech tokens, supervised by automatic speech recognition (ASR) and\ncross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality\ngap of these tokens, we propose a hierarchical decoding strategy, where the LLM\ngenerates tokens in a structured order: first semantic, then fine-grained\nstyle, and finally complete acoustic representation. Extensive experiments\ndemonstrate that this hierarchical paradigm significantly improves instruction\nadherence and achieves state-of-the-art naturalness, validating our approach\nfor precise and controllable speech synthesis. Audio samples are available at\nhttps://xxh333.github.io/."
                },
                "authors": [
                    {
                        "name": "Sihang Nie"
                    },
                    {
                        "name": "Xiaofen Xing"
                    },
                    {
                        "name": "Jingyuan Xing"
                    },
                    {
                        "name": "Baiji Liu"
                    },
                    {
                        "name": "Xiangmin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangmin Xu"
                },
                "author": "Xiangmin Xu",
                "arxiv_comment": "5 pages, 2 figures, submitted to ICASSP2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18993v1",
                "updated": "2025-09-23T13:43:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    43,
                    2,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:43:02Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    43,
                    2,
                    1,
                    266,
                    0
                ],
                "title": "CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank\n  Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank\n  Structure"
                },
                "summary": "Low-rank architectures have become increasingly important for efficient large\nlanguage model (LLM) pre-training, providing substantial reductions in both\nparameter complexity and memory/computational demands. Despite these\nadvantages, current low-rank methods face three critical shortcomings: (1)\ncompromised model performance, (2) considerable computational overhead, and (3)\nlimited activation memory savings. To address these limitations, we propose\nCross-layer Low-Rank residual Network (CR-Net), an innovative\nparameter-efficient framework inspired by our discovery that inter-layer\nactivation residuals possess low-rank properties. CR-Net implements this\ninsight through a dual-path architecture that efficiently reconstructs layer\nactivations by combining previous-layer outputs with their low-rank\ndifferences, thereby maintaining high-rank information with minimal parameters.\nWe further develop a specialized activation recomputation strategy tailored for\nCR-Net that dramatically reduces memory requirements. Extensive pre-training\nexperiments across model scales from 60M to 7B parameters demonstrate that\nCR-Net consistently outperforms state-of-the-art low-rank frameworks while\nrequiring fewer computational resources and less memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank architectures have become increasingly important for efficient large\nlanguage model (LLM) pre-training, providing substantial reductions in both\nparameter complexity and memory/computational demands. Despite these\nadvantages, current low-rank methods face three critical shortcomings: (1)\ncompromised model performance, (2) considerable computational overhead, and (3)\nlimited activation memory savings. To address these limitations, we propose\nCross-layer Low-Rank residual Network (CR-Net), an innovative\nparameter-efficient framework inspired by our discovery that inter-layer\nactivation residuals possess low-rank properties. CR-Net implements this\ninsight through a dual-path architecture that efficiently reconstructs layer\nactivations by combining previous-layer outputs with their low-rank\ndifferences, thereby maintaining high-rank information with minimal parameters.\nWe further develop a specialized activation recomputation strategy tailored for\nCR-Net that dramatically reduces memory requirements. Extensive pre-training\nexperiments across model scales from 60M to 7B parameters demonstrate that\nCR-Net consistently outperforms state-of-the-art low-rank frameworks while\nrequiring fewer computational resources and less memory."
                },
                "authors": [
                    {
                        "name": "Boao Kong"
                    },
                    {
                        "name": "Junzhu Liang"
                    },
                    {
                        "name": "Yuxi Liu"
                    },
                    {
                        "name": "Renjia Deng"
                    },
                    {
                        "name": "Kun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yuan"
                },
                "author": "Kun Yuan",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18990v1",
                "updated": "2025-09-23T13:39:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    39,
                    11,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:39:11Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    39,
                    11,
                    1,
                    266,
                    0
                ],
                "title": "Learning From Simulators: A Theory of Simulation-Grounded Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning From Simulators: A Theory of Simulation-Grounded Learning"
                },
                "summary": "Simulation-Grounded Neural Networks (SGNNs) are predictive models trained\nentirely on synthetic data from mechanistic simulations. They have achieved\nstate-of-the-art performance in domains where real-world labels are limited or\nunobserved, but lack a formal underpinning.\n  We present the foundational theory of simulation-grounded learning. We show\nthat SGNNs implement amortized Bayesian inference under a simulation prior and\nconverge to the Bayes-optimal predictor. We derive generalization bounds under\nmodel misspecification and prove that SGNNs can learn unobservable scientific\nquantities that empirical methods provably cannot. We also formalize a novel\nform of mechanistic interpretability uniquely enabled by SGNNs: by attributing\npredictions to the simulated mechanisms that generated them, SGNNs yield\nposterior-consistent, scientifically grounded explanations.\n  We provide numerical experiments to validate all theoretical predictions.\nSGNNs recover latent parameters, remain robust under mismatch, and outperform\nclassical tools: in a model selection task, SGNNs achieve half the error of AIC\nin distinguishing mechanistic dynamics. These results establish SGNNs as a\nprincipled and practical framework for scientific prediction in data-limited\nregimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-Grounded Neural Networks (SGNNs) are predictive models trained\nentirely on synthetic data from mechanistic simulations. They have achieved\nstate-of-the-art performance in domains where real-world labels are limited or\nunobserved, but lack a formal underpinning.\n  We present the foundational theory of simulation-grounded learning. We show\nthat SGNNs implement amortized Bayesian inference under a simulation prior and\nconverge to the Bayes-optimal predictor. We derive generalization bounds under\nmodel misspecification and prove that SGNNs can learn unobservable scientific\nquantities that empirical methods provably cannot. We also formalize a novel\nform of mechanistic interpretability uniquely enabled by SGNNs: by attributing\npredictions to the simulated mechanisms that generated them, SGNNs yield\nposterior-consistent, scientifically grounded explanations.\n  We provide numerical experiments to validate all theoretical predictions.\nSGNNs recover latent parameters, remain robust under mismatch, and outperform\nclassical tools: in a model selection task, SGNNs achieve half the error of AIC\nin distinguishing mechanistic dynamics. These results establish SGNNs as a\nprincipled and practical framework for scientific prediction in data-limited\nregimes."
                },
                "authors": [
                    {
                        "name": "Carson Dudley"
                    },
                    {
                        "name": "Marisa Eisenberg"
                    }
                ],
                "author_detail": {
                    "name": "Marisa Eisenberg"
                },
                "author": "Marisa Eisenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18985v1",
                "updated": "2025-09-23T13:36:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    36,
                    48,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:36:48Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    36,
                    48,
                    1,
                    266,
                    0
                ],
                "title": "Simulating Online Social Media Conversations on Controversial Topics\n  Using AI Agents Calibrated on Real-World Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Online Social Media Conversations on Controversial Topics\n  Using AI Agents Calibrated on Real-World Data"
                },
                "summary": "Online social networks offer a valuable lens to analyze both individual and\ncollective phenomena. Researchers often use simulators to explore controlled\nscenarios, and the integration of Large Language Models (LLMs) makes these\nsimulations more realistic by enabling agents to understand and generate\nnatural language content. In this work, we investigate the behavior of\nLLM-based agents in a simulated microblogging social network. We initialize\nagents with realistic profiles calibrated on real-world online conversations\nfrom the 2022 Italian political election and extend an existing simulator by\nintroducing mechanisms for opinion modeling. We examine how LLM agents simulate\nonline conversations, interact with others, and evolve their opinions under\ndifferent scenarios. Our results show that LLM agents generate coherent\ncontent, form connections, and build a realistic social network structure.\nHowever, their generated content displays less heterogeneity in tone and\ntoxicity compared to real data. We also find that LLM-based opinion dynamics\nevolve over time in ways similar to traditional mathematical models. Varying\nparameter configurations produces no significant changes, indicating that\nsimulations require more careful cognitive modeling at initialization to\nreplicate human behavior more faithfully. Overall, we demonstrate the potential\nof LLMs for simulating user behavior in social environments, while also\nidentifying key challenges in capturing heterogeneity and complex dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online social networks offer a valuable lens to analyze both individual and\ncollective phenomena. Researchers often use simulators to explore controlled\nscenarios, and the integration of Large Language Models (LLMs) makes these\nsimulations more realistic by enabling agents to understand and generate\nnatural language content. In this work, we investigate the behavior of\nLLM-based agents in a simulated microblogging social network. We initialize\nagents with realistic profiles calibrated on real-world online conversations\nfrom the 2022 Italian political election and extend an existing simulator by\nintroducing mechanisms for opinion modeling. We examine how LLM agents simulate\nonline conversations, interact with others, and evolve their opinions under\ndifferent scenarios. Our results show that LLM agents generate coherent\ncontent, form connections, and build a realistic social network structure.\nHowever, their generated content displays less heterogeneity in tone and\ntoxicity compared to real data. We also find that LLM-based opinion dynamics\nevolve over time in ways similar to traditional mathematical models. Varying\nparameter configurations produces no significant changes, indicating that\nsimulations require more careful cognitive modeling at initialization to\nreplicate human behavior more faithfully. Overall, we demonstrate the potential\nof LLMs for simulating user behavior in social environments, while also\nidentifying key challenges in capturing heterogeneity and complex dynamics."
                },
                "authors": [
                    {
                        "name": "Elisa Composta"
                    },
                    {
                        "name": "Nicolo' Fontana"
                    },
                    {
                        "name": "Francesco Corso"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11079v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11079v2",
                "updated": "2025-09-23T13:32:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    32,
                    37,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-14T03:57:43Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    3,
                    57,
                    43,
                    6,
                    257,
                    0
                ],
                "title": "Difficulty-Aware Agent Orchestration in LLM-Powered Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Difficulty-Aware Agent Orchestration in LLM-Powered Workflows"
                },
                "summary": "Large Language Model (LLM)-based agentic systems have shown strong\ncapabilities across various tasks. However, existing multi-agent frameworks\noften rely on static or task-level workflows, which either over-process simple\nqueries or underperform on complex ones, while also neglecting the\nefficiency-performance trade-offs across heterogeneous LLMs. To address these\nlimitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a\ndynamic framework that adapts workflow depth, operator selection, and LLM\nassignment based on the difficulty of each input query. DAAO comprises three\ninterdependent modules: a variational autoencoder (VAE) for difficulty\nestimation, a modular operator allocator, and a cost- and performance-aware LLM\nrouter. By leveraging heterogeneous LLMs and dynamically tailoring workflows,\nDAAO enables fine-grained, query-specific reasoning strategies. DAAO\noutperforms prior multi-agent systems in both accuracy and inference efficiency\nacross six benchmarks. We will release our code and implementation details upon\npublication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agentic systems have shown strong\ncapabilities across various tasks. However, existing multi-agent frameworks\noften rely on static or task-level workflows, which either over-process simple\nqueries or underperform on complex ones, while also neglecting the\nefficiency-performance trade-offs across heterogeneous LLMs. To address these\nlimitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a\ndynamic framework that adapts workflow depth, operator selection, and LLM\nassignment based on the difficulty of each input query. DAAO comprises three\ninterdependent modules: a variational autoencoder (VAE) for difficulty\nestimation, a modular operator allocator, and a cost- and performance-aware LLM\nrouter. By leveraging heterogeneous LLMs and dynamically tailoring workflows,\nDAAO enables fine-grained, query-specific reasoning strategies. DAAO\noutperforms prior multi-agent systems in both accuracy and inference efficiency\nacross six benchmarks. We will release our code and implementation details upon\npublication."
                },
                "authors": [
                    {
                        "name": "Jinwei Su"
                    },
                    {
                        "name": "Yinghui Xia"
                    },
                    {
                        "name": "Qizhen Lan"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yang Jingsong"
                    },
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Tianyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Shi"
                },
                "author": "Tianyu Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11079v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11079v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13978v2",
                "updated": "2025-09-23T13:31:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    31,
                    18,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-17T13:51:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture\n  and Evaluation Methodology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agents for Interactive Workflow Provenance: Reference Architecture\n  and Evaluation Methodology"
                },
                "summary": "Modern scientific discovery increasingly relies on workflows that process\ndata across the Edge, Cloud, and High Performance Computing (HPC) continuum.\nComprehensive and in-depth analyses of these data are critical for hypothesis\nvalidation, anomaly detection, reproducibility, and impactful findings.\nAlthough workflow provenance techniques support such analyses, at large scale,\nthe provenance data become complex and difficult to analyze. Existing systems\ndepend on custom scripts, structured queries, or static dashboards, limiting\ndata interaction. In this work, we introduce an evaluation methodology,\nreference architecture, and open-source implementation that leverages\ninteractive Large Language Model (LLM) agents for runtime data analysis. Our\napproach uses a lightweight, metadata-driven design that translates natural\nlanguage into structured provenance queries. Evaluations across LLaMA, GPT,\nGemini, and Claude, covering diverse query classes and a real-world chemistry\nworkflow, show that modular design, prompt tuning, and Retrieval-Augmented\nGeneration (RAG) enable accurate and insightful LLM agent responses beyond\nrecorded provenance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern scientific discovery increasingly relies on workflows that process\ndata across the Edge, Cloud, and High Performance Computing (HPC) continuum.\nComprehensive and in-depth analyses of these data are critical for hypothesis\nvalidation, anomaly detection, reproducibility, and impactful findings.\nAlthough workflow provenance techniques support such analyses, at large scale,\nthe provenance data become complex and difficult to analyze. Existing systems\ndepend on custom scripts, structured queries, or static dashboards, limiting\ndata interaction. In this work, we introduce an evaluation methodology,\nreference architecture, and open-source implementation that leverages\ninteractive Large Language Model (LLM) agents for runtime data analysis. Our\napproach uses a lightweight, metadata-driven design that translates natural\nlanguage into structured provenance queries. Evaluations across LLaMA, GPT,\nGemini, and Claude, covering diverse query classes and a real-world chemistry\nworkflow, show that modular design, prompt tuning, and Retrieval-Augmented\nGeneration (RAG) enable accurate and insightful LLM agent responses beyond\nrecorded provenance."
                },
                "authors": [
                    {
                        "name": "Renan Souza"
                    },
                    {
                        "name": "Timothy Poteet"
                    },
                    {
                        "name": "Brian Etz"
                    },
                    {
                        "name": "Daniel Rosendo"
                    },
                    {
                        "name": "Amal Gueroudji"
                    },
                    {
                        "name": "Woong Shin"
                    },
                    {
                        "name": "Prasanna Balaprakash"
                    },
                    {
                        "name": "Rafael Ferreira da Silva"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Ferreira da Silva"
                },
                "author": "Rafael Ferreira da Silva",
                "arxiv_doi": "10.1145/3731599.3767582",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731599.3767582",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.13978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Paper accepted in the proceedings of the Supercomputing Conference\n  (SC). Cite it as Renan Souza, Timothy Poteet, Brian Etz, Daniel Rosendo, Amal\n  Gueroudji, Woong Shin, Prasanna Balaprakash, and Rafael Ferreira da Silva.\n  LLM Agents for Interactive Workflow Provenance: Reference Architecture and\n  Evaluation Methodology. In WORKS at the ACM/IEEE International Conference on\n  Supercomputing, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M14, 68M20, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; D.1.3; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18980v1",
                "updated": "2025-09-23T13:30:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    30,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:30:03Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    30,
                    3,
                    1,
                    266,
                    0
                ],
                "title": "From latent factors to language: a user study on LLM-generated\n  explanations for an inherently interpretable matrix-based recommender system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From latent factors to language: a user study on LLM-generated\n  explanations for an inherently interpretable matrix-based recommender system"
                },
                "summary": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves.To evaluate how different explanation strategies are\nperceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves.To evaluate how different explanation strategies are\nperceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results."
                },
                "authors": [
                    {
                        "name": "Maxime Manderlier"
                    },
                    {
                        "name": "Fabian Lecron"
                    },
                    {
                        "name": "Olivier Vu Thanh"
                    },
                    {
                        "name": "Nicolas Gillis"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Gillis"
                },
                "author": "Nicolas Gillis",
                "arxiv_journal_ref": "In Proceedings of the 12th Joint Workshop on Interfaces and Human\n  Decision Making for Recommender Systems (IntRS 2025) co-located with 19th ACM\n  Conference on Recommender Systems (RecSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18970v1",
                "updated": "2025-09-23T13:24:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    24,
                    48,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:24:48Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    24,
                    48,
                    1,
                    266,
                    0
                ],
                "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy,\n  Methods, and Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy,\n  Methods, and Directions"
                },
                "summary": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based\nagents have emerged as powerful intelligent systems capable of human-like\ncognition, reasoning, and interaction. These agents are increasingly being\ndeployed across diverse real-world applications, including student education,\nscientific research, and financial analysis. However, despite their remarkable\npotential, LLM-based agents remain vulnerable to hallucination issues, which\ncan result in erroneous task execution and undermine the reliability of the\noverall system design. Addressing this critical challenge requires a deep\nunderstanding and a systematic consolidation of recent advances on LLM-based\nagents. To this end, we present the first comprehensive survey of\nhallucinations in LLM-based agents. By carefully analyzing the complete\nworkflow of agents, we propose a new taxonomy that identifies different types\nof agent hallucinations occurring at different stages. Furthermore, we conduct\nan in-depth examination of eighteen triggering causes underlying the emergence\nof agent hallucinations. Through a detailed review of a large number of\nexisting studies, we summarize approaches for hallucination mitigation and\ndetection, and highlight promising directions for future research. We hope this\nsurvey will inspire further efforts toward addressing hallucinations in\nLLM-based agents, ultimately contributing to the development of more robust and\nreliable agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based\nagents have emerged as powerful intelligent systems capable of human-like\ncognition, reasoning, and interaction. These agents are increasingly being\ndeployed across diverse real-world applications, including student education,\nscientific research, and financial analysis. However, despite their remarkable\npotential, LLM-based agents remain vulnerable to hallucination issues, which\ncan result in erroneous task execution and undermine the reliability of the\noverall system design. Addressing this critical challenge requires a deep\nunderstanding and a systematic consolidation of recent advances on LLM-based\nagents. To this end, we present the first comprehensive survey of\nhallucinations in LLM-based agents. By carefully analyzing the complete\nworkflow of agents, we propose a new taxonomy that identifies different types\nof agent hallucinations occurring at different stages. Furthermore, we conduct\nan in-depth examination of eighteen triggering causes underlying the emergence\nof agent hallucinations. Through a detailed review of a large number of\nexisting studies, we summarize approaches for hallucination mitigation and\ndetection, and highlight promising directions for future research. We hope this\nsurvey will inspire further efforts toward addressing hallucinations in\nLLM-based agents, ultimately contributing to the development of more robust and\nreliable agent systems."
                },
                "authors": [
                    {
                        "name": "Xixun Lin"
                    },
                    {
                        "name": "Yucheng Ning"
                    },
                    {
                        "name": "Jingwen Zhang"
                    },
                    {
                        "name": "Yan Dong"
                    },
                    {
                        "name": "Yilong Liu"
                    },
                    {
                        "name": "Yongxuan Wu"
                    },
                    {
                        "name": "Xiaohua Qi"
                    },
                    {
                        "name": "Nan Sun"
                    },
                    {
                        "name": "Yanmin Shang"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Lixin Zou"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Chuan Zhou"
                    },
                    {
                        "name": "Jia Wu"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Yanan Cao"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Songlin Hu"
                    },
                    {
                        "name": "Li Guo"
                    }
                ],
                "author_detail": {
                    "name": "Li Guo"
                },
                "author": "Li Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18968v1",
                "updated": "2025-09-23T13:23:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    23,
                    48,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:23:48Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    23,
                    48,
                    1,
                    266,
                    0
                ],
                "title": "Otters: An Energy-Efficient SpikingTransformer via Optical\n  Time-to-First-Spike Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Otters: An Energy-Efficient SpikingTransformer via Optical\n  Time-to-First-Spike Encoding"
                },
                "summary": "Spiking neural networks (SNNs) promise high energy efficiency, particularly\nwith time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting\nat most one spike per neuron. However, such energy advantage is often\nunrealized because inference requires evaluating a temporal decay function and\nsubsequent multiplication with the synaptic weights. This paper challenges this\ncostly approach by repurposing a physical hardware `bug', namely, the natural\nsignal decay in optoelectronic devices, as the core computation of TTFS. We\nfabricated a custom indium oxide optoelectronic synapse, showing how its\nnatural physical decay directly implements the required temporal function. By\ntreating the device's analog output as the fused product of the synaptic weight\nand temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates\nthese expensive digital operations. To use the Otters paradigm in complex\narchitectures like the transformer, which are challenging to train directly due\nto the sparsity issue, we introduce a novel quantized neural network-to-SNN\nconversion algorithm. This complete hardware-software co-design enables our\nmodel to achieve state-of-the-art accuracy across seven GLUE benchmark datasets\nand demonstrates a 1.77$\\times$ improvement in energy efficiency over previous\nleading SNNs, based on a comprehensive analysis of compute, data movement, and\nmemory access costs using energy measurements from a commercial 22nm process.\nOur work thus establishes a new paradigm for energy-efficient SNNs, translating\nfundamental device physics directly into powerful computational primitives. All\ncodes and data are open source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking neural networks (SNNs) promise high energy efficiency, particularly\nwith time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting\nat most one spike per neuron. However, such energy advantage is often\nunrealized because inference requires evaluating a temporal decay function and\nsubsequent multiplication with the synaptic weights. This paper challenges this\ncostly approach by repurposing a physical hardware `bug', namely, the natural\nsignal decay in optoelectronic devices, as the core computation of TTFS. We\nfabricated a custom indium oxide optoelectronic synapse, showing how its\nnatural physical decay directly implements the required temporal function. By\ntreating the device's analog output as the fused product of the synaptic weight\nand temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates\nthese expensive digital operations. To use the Otters paradigm in complex\narchitectures like the transformer, which are challenging to train directly due\nto the sparsity issue, we introduce a novel quantized neural network-to-SNN\nconversion algorithm. This complete hardware-software co-design enables our\nmodel to achieve state-of-the-art accuracy across seven GLUE benchmark datasets\nand demonstrates a 1.77$\\times$ improvement in energy efficiency over previous\nleading SNNs, based on a comprehensive analysis of compute, data movement, and\nmemory access costs using energy measurements from a commercial 22nm process.\nOur work thus establishes a new paradigm for energy-efficient SNNs, translating\nfundamental device physics directly into powerful computational primitives. All\ncodes and data are open source."
                },
                "authors": [
                    {
                        "name": "Zhanglu Yan"
                    },
                    {
                        "name": "Jiayi Mao"
                    },
                    {
                        "name": "Qianhui Liu"
                    },
                    {
                        "name": "Fanfan Li"
                    },
                    {
                        "name": "Gang Pan"
                    },
                    {
                        "name": "Tao Luo"
                    },
                    {
                        "name": "Bowen Zhu"
                    },
                    {
                        "name": "Weng-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Weng-Fai Wong"
                },
                "author": "Weng-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18965v1",
                "updated": "2025-09-23T13:17:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    17,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:17:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    17,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "Benchmarking PDF Accessibility Evaluation A Dataset and Framework for\n  Assessing Automated and LLM-Based Approaches for Accessibility Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking PDF Accessibility Evaluation A Dataset and Framework for\n  Assessing Automated and LLM-Based Approaches for Accessibility Testing"
                },
                "summary": "PDFs remain the dominant format for scholarly communication, despite\nsignificant accessibility challenges for blind and low-vision users. While\nvarious tools attempt to evaluate PDF accessibility, there is no standardized\nmethodology to evaluate how different accessibility assessment approaches\nperform. Our work addresses this critical gap by introducing a novel benchmark\ndataset of scholarly PDFs with expert-validated accessibility annotations\nacross seven criteria (alternative text quality, logical reading order,\nsemantic tagging, table structure, functional hyperlinks, color contrast, and\nfont readability), and a four-category evaluation framework with standardized\nlabels (Passed, Failed, Not Present, Cannot Tell) to systematically assess\naccessibility evaluation approaches. Using our evaluation framework, we explore\nwhether large language models (LLMs) are capable of supporting automated\naccessibility evaluation. We benchmark five LLMs, which demonstrate varying\ncapabilities in correctly assessing different accessibility criteria, with\nGPT-4-Turbo achieving the highest overall accuracy (0.85). However, all models\nstruggled in correctly categorizing documents with Not Present and Cannot Tell\naccessibility labels, particularly for alt text quality assessment. Our\nqualitative comparison with standard automated checkers reveals complementary\nstrengths: rule-based tools excel at technical verification, while LLMs better\nevaluate semantic appropriateness and contextual relevance. Based on our\nfindings, we propose a hybrid approach that would combine automated checkers,\nLLM evaluation, and human assessment as a future strategy for PDF accessibility\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDFs remain the dominant format for scholarly communication, despite\nsignificant accessibility challenges for blind and low-vision users. While\nvarious tools attempt to evaluate PDF accessibility, there is no standardized\nmethodology to evaluate how different accessibility assessment approaches\nperform. Our work addresses this critical gap by introducing a novel benchmark\ndataset of scholarly PDFs with expert-validated accessibility annotations\nacross seven criteria (alternative text quality, logical reading order,\nsemantic tagging, table structure, functional hyperlinks, color contrast, and\nfont readability), and a four-category evaluation framework with standardized\nlabels (Passed, Failed, Not Present, Cannot Tell) to systematically assess\naccessibility evaluation approaches. Using our evaluation framework, we explore\nwhether large language models (LLMs) are capable of supporting automated\naccessibility evaluation. We benchmark five LLMs, which demonstrate varying\ncapabilities in correctly assessing different accessibility criteria, with\nGPT-4-Turbo achieving the highest overall accuracy (0.85). However, all models\nstruggled in correctly categorizing documents with Not Present and Cannot Tell\naccessibility labels, particularly for alt text quality assessment. Our\nqualitative comparison with standard automated checkers reveals complementary\nstrengths: rule-based tools excel at technical verification, while LLMs better\nevaluate semantic appropriateness and contextual relevance. Based on our\nfindings, we propose a hybrid approach that would combine automated checkers,\nLLM evaluation, and human assessment as a future strategy for PDF accessibility\nevaluation."
                },
                "authors": [
                    {
                        "name": "Anukriti Kumar"
                    },
                    {
                        "name": "Tanushree Padath"
                    },
                    {
                        "name": "Lucy Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lucy Lu Wang"
                },
                "author": "Lucy Lu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18949v1",
                "updated": "2025-09-23T12:58:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    58,
                    32,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:58:32Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    58,
                    32,
                    1,
                    266,
                    0
                ],
                "title": "Towards Privacy-Aware Bayesian Networks: A Credal Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Privacy-Aware Bayesian Networks: A Credal Approach"
                },
                "summary": "Bayesian networks (BN) are probabilistic graphical models that enable\nefficient knowledge representation and inference. These have proven effective\nacross diverse domains, including healthcare, bioinformatics and economics. The\nstructure and parameters of a BN can be obtained by domain experts or directly\nlearned from available data. However, as privacy concerns escalate, it becomes\nincreasingly critical for publicly released models to safeguard sensitive\ninformation in training data. Typically, released models do not prioritize\nprivacy by design. In particular, tracing attacks from adversaries can combine\nthe released BN with auxiliary data to determine whether specific individuals\nbelong to the data from which the BN was learned. State-of-the-art protection\ntecniques involve introducing noise into the learned parameters. While this\noffers robust protection against tracing attacks, it significantly impacts the\nmodel's utility, in terms of both the significance and accuracy of the\nresulting inferences. Hence, high privacy may be attained at the cost of\nreleasing a possibly ineffective model. This paper introduces credal networks\n(CN) as a novel solution for balancing the model's privacy and utility. After\nadapting the notion of tracing attacks, we demonstrate that a CN enables the\nmasking of the learned BN, thereby reducing the probability of successful\nattacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve\nmeaningful inferences while safeguarding privacy. Moreover, we identify key\nlearning information that must be concealed to prevent attackers from\nrecovering the underlying BN. Finally, we conduct a set of numerical\nexperiments to analyze how privacy gains can be modulated by tuning the CN\nhyperparameters. Our results confirm that CNs provide a principled, practical,\nand effective approach towards the development of privacy-aware probabilistic\ngraphical models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian networks (BN) are probabilistic graphical models that enable\nefficient knowledge representation and inference. These have proven effective\nacross diverse domains, including healthcare, bioinformatics and economics. The\nstructure and parameters of a BN can be obtained by domain experts or directly\nlearned from available data. However, as privacy concerns escalate, it becomes\nincreasingly critical for publicly released models to safeguard sensitive\ninformation in training data. Typically, released models do not prioritize\nprivacy by design. In particular, tracing attacks from adversaries can combine\nthe released BN with auxiliary data to determine whether specific individuals\nbelong to the data from which the BN was learned. State-of-the-art protection\ntecniques involve introducing noise into the learned parameters. While this\noffers robust protection against tracing attacks, it significantly impacts the\nmodel's utility, in terms of both the significance and accuracy of the\nresulting inferences. Hence, high privacy may be attained at the cost of\nreleasing a possibly ineffective model. This paper introduces credal networks\n(CN) as a novel solution for balancing the model's privacy and utility. After\nadapting the notion of tracing attacks, we demonstrate that a CN enables the\nmasking of the learned BN, thereby reducing the probability of successful\nattacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve\nmeaningful inferences while safeguarding privacy. Moreover, we identify key\nlearning information that must be concealed to prevent attackers from\nrecovering the underlying BN. Finally, we conduct a set of numerical\nexperiments to analyze how privacy gains can be modulated by tuning the CN\nhyperparameters. Our results confirm that CNs provide a principled, practical,\nand effective approach towards the development of privacy-aware probabilistic\ngraphical models."
                },
                "authors": [
                    {
                        "name": "NiccolÃ² Rocchi"
                    },
                    {
                        "name": "Fabio Stella"
                    },
                    {
                        "name": "Cassio de Campos"
                    }
                ],
                "author_detail": {
                    "name": "Cassio de Campos"
                },
                "author": "Cassio de Campos",
                "arxiv_comment": "Accepted at ECAI2025 conference, 20 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18948v1",
                "updated": "2025-09-23T12:58:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    58,
                    15,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:58:15Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    58,
                    15,
                    1,
                    266,
                    0
                ],
                "title": "One-shot Embroidery Customization via Contrastive LoRA Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-shot Embroidery Customization via Contrastive LoRA Modulation"
                },
                "summary": "Diffusion models have significantly advanced image manipulation techniques,\nand their ability to generate photorealistic images is beginning to transform\nretail workflows, particularly in presale visualization. Beyond artistic style\ntransfer, the capability to perform fine-grained visual feature transfer is\nbecoming increasingly important. Embroidery is a textile art form characterized\nby intricate interplay of diverse stitch patterns and material properties,\nwhich poses unique challenges for existing style transfer methods. To explore\nthe customization for such fine-grained features, we propose a novel\ncontrastive learning framework that disentangles fine-grained style and content\nfeatures with a single reference image, building on the classic concept of\nimage analogy. We first construct an image pair to define the target style, and\nthen adopt a similarity metric based on the decoupled representations of\npretrained diffusion models for style-content separation. Subsequently, we\npropose a two-stage contrastive LoRA modulation technique to capture\nfine-grained style features. In the first stage, we iteratively update the\nwhole LoRA and the selected style blocks to initially separate style from\ncontent. In the second stage, we design a contrastive learning strategy to\nfurther decouple style and content through self-knowledge distillation.\nFinally, we build an inference pipeline to handle image or text inputs with\nonly the style blocks. To evaluate our method on fine-grained style transfer,\nwe build a benchmark for embroidery customization. Our approach surpasses prior\nmethods on this task and further demonstrates strong generalization to three\nadditional domains: artistic style transfer, sketch colorization, and\nappearance transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have significantly advanced image manipulation techniques,\nand their ability to generate photorealistic images is beginning to transform\nretail workflows, particularly in presale visualization. Beyond artistic style\ntransfer, the capability to perform fine-grained visual feature transfer is\nbecoming increasingly important. Embroidery is a textile art form characterized\nby intricate interplay of diverse stitch patterns and material properties,\nwhich poses unique challenges for existing style transfer methods. To explore\nthe customization for such fine-grained features, we propose a novel\ncontrastive learning framework that disentangles fine-grained style and content\nfeatures with a single reference image, building on the classic concept of\nimage analogy. We first construct an image pair to define the target style, and\nthen adopt a similarity metric based on the decoupled representations of\npretrained diffusion models for style-content separation. Subsequently, we\npropose a two-stage contrastive LoRA modulation technique to capture\nfine-grained style features. In the first stage, we iteratively update the\nwhole LoRA and the selected style blocks to initially separate style from\ncontent. In the second stage, we design a contrastive learning strategy to\nfurther decouple style and content through self-knowledge distillation.\nFinally, we build an inference pipeline to handle image or text inputs with\nonly the style blocks. To evaluate our method on fine-grained style transfer,\nwe build a benchmark for embroidery customization. Our approach surpasses prior\nmethods on this task and further demonstrates strong generalization to three\nadditional domains: artistic style transfer, sketch colorization, and\nappearance transfer."
                },
                "authors": [
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Qian He"
                    },
                    {
                        "name": "Gaofeng He"
                    },
                    {
                        "name": "Huang Chen"
                    },
                    {
                        "name": "Chen Liu"
                    },
                    {
                        "name": "Xiaogang Jin"
                    },
                    {
                        "name": "Huamin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Wang"
                },
                "author": "Huamin Wang",
                "arxiv_comment": "Accepted to ACM Transactions on Graphics (TOG), SIGGRAPH Asia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.20197v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20197v4",
                "updated": "2025-09-23T17:57:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    57,
                    16,
                    1,
                    266,
                    0
                ],
                "published": "2025-03-26T03:44:03Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    3,
                    44,
                    3,
                    2,
                    85,
                    0
                ],
                "title": "A Preliminary Study on the Robustness of Code Generation by Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Preliminary Study on the Robustness of Code Generation by Large\n  Language Models"
                },
                "summary": "Robustness is a critical factor for reliable code generation by large\nlanguage models, yet most evaluations focus on correctness and overlook key\nissues such as missing input validation and inadequate error handling. In this\nwork, we present the first empirical study of LLM-generated code robustness\nusing the CoderEval benchmark. Evaluating four state-of-the-art code LLMs, we\nfind that 35.2% of their outputs are less robust than human-written code, with\nover 90% of deficiencies caused by missing conditional checks-70% of which\noccur in the first line. Interestingly, in 63% of cases where a conditional\nstatement is needed but absent, the \"if\" token still ranks among the top three\npredictions, suggesting implicit recognition of control flow.\n  To address these issues, we propose RobGen, a model-agnostic framework that\nimproves robustness without retraining. RobGen combines a line-level\nintervention checker, which decides whether to adjust logits for each generated\nline, with token-level conditional logit adjustments to promote essential\ncontrol structures. Experiments show that RobGen reduces the proportion of less\nrobust code by 10%, achieves the highest average Pass@1 (43.57), and adds\nminimal overhead (+33.4%). As a lightweight and adaptable solution, RobGen\neffectively enhances the reliability of LLM-generated code across diverse\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness is a critical factor for reliable code generation by large\nlanguage models, yet most evaluations focus on correctness and overlook key\nissues such as missing input validation and inadequate error handling. In this\nwork, we present the first empirical study of LLM-generated code robustness\nusing the CoderEval benchmark. Evaluating four state-of-the-art code LLMs, we\nfind that 35.2% of their outputs are less robust than human-written code, with\nover 90% of deficiencies caused by missing conditional checks-70% of which\noccur in the first line. Interestingly, in 63% of cases where a conditional\nstatement is needed but absent, the \"if\" token still ranks among the top three\npredictions, suggesting implicit recognition of control flow.\n  To address these issues, we propose RobGen, a model-agnostic framework that\nimproves robustness without retraining. RobGen combines a line-level\nintervention checker, which decides whether to adjust logits for each generated\nline, with token-level conditional logit adjustments to promote essential\ncontrol structures. Experiments show that RobGen reduces the proportion of less\nrobust code by 10%, achieves the highest average Pass@1 (43.57), and adds\nminimal overhead (+33.4%). As a lightweight and adaptable solution, RobGen\neffectively enhances the reliability of LLM-generated code across diverse\ntasks."
                },
                "authors": [
                    {
                        "name": "Zike Li"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Anji Li"
                    },
                    {
                        "name": "Kaifeng He"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20197v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20197v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10571v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10571v3",
                "updated": "2025-09-23T17:47:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    47,
                    4,
                    1,
                    266,
                    0
                ],
                "published": "2025-04-30T16:18:39Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    18,
                    39,
                    2,
                    120,
                    0
                ],
                "title": "Language Models Do Not Have Human-Like Working Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Do Not Have Human-Like Working Memory"
                },
                "summary": "While Large Language Models (LLMs) exhibit remarkable reasoning abilities, we\ndemonstrate that they lack a fundamental aspect of human cognition: working\nmemory. Human working memory is an active cognitive system that enables not\nonly the temporary storage of information but also its processing and\nutilization, enabling coherent reasoning and decision-making. Without working\nmemory, individuals may produce unrealistic responses, exhibit\nself-contradictions, and struggle with tasks that require mental reasoning.\nExisting evaluations using N-back or context-dependent tasks fall short as they\nallow LLMs to exploit external context rather than retaining the reasoning\nprocess in the latent space. We introduce three novel tasks: (1) Number\nGuessing, (2) Yes-No Deduction, and (3) Math Magic, designed to isolate\ninternal representation from external context. Across seventeen frontier models\nspanning four major model families, we consistently observe irrational or\ncontradictory behaviors, indicating LLMs' inability to retain and manipulate\nlatent information. Our work establishes a new benchmark for evaluating working\nmemory in LLMs and highlights this limitation as a key bottleneck for advancing\nreliable reasoning systems. Code and prompts for the experiments are available\nat https://github.com/penguinnnnn/LLM-Working-Memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) exhibit remarkable reasoning abilities, we\ndemonstrate that they lack a fundamental aspect of human cognition: working\nmemory. Human working memory is an active cognitive system that enables not\nonly the temporary storage of information but also its processing and\nutilization, enabling coherent reasoning and decision-making. Without working\nmemory, individuals may produce unrealistic responses, exhibit\nself-contradictions, and struggle with tasks that require mental reasoning.\nExisting evaluations using N-back or context-dependent tasks fall short as they\nallow LLMs to exploit external context rather than retaining the reasoning\nprocess in the latent space. We introduce three novel tasks: (1) Number\nGuessing, (2) Yes-No Deduction, and (3) Math Magic, designed to isolate\ninternal representation from external context. Across seventeen frontier models\nspanning four major model families, we consistently observe irrational or\ncontradictory behaviors, indicating LLMs' inability to retain and manipulate\nlatent information. Our work establishes a new benchmark for evaluating working\nmemory in LLMs and highlights this limitation as a key bottleneck for advancing\nreliable reasoning systems. Code and prompts for the experiments are available\nat https://github.com/penguinnnnn/LLM-Working-Memory."
                },
                "authors": [
                    {
                        "name": "Jen-tse Huang"
                    },
                    {
                        "name": "Kaiser Sun"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10571v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10571v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18057v2",
                "updated": "2025-09-23T17:34:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    34,
                    30,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T17:30:33Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    33,
                    0,
                    265,
                    0
                ],
                "title": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory"
                },
                "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve on known limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs."
                },
                "authors": [
                    {
                        "name": "Ansh Nagda"
                    },
                    {
                        "name": "Prabhakar Raghavan"
                    },
                    {
                        "name": "Abhradeep Thakurta"
                    }
                ],
                "author_detail": {
                    "name": "Abhradeep Thakurta"
                },
                "author": "Abhradeep Thakurta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18058v2",
                "updated": "2025-09-23T17:34:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    34,
                    27,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T17:30:56Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    56,
                    0,
                    265,
                    0
                ],
                "title": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier\n  LLMs"
                },
                "summary": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are crafted to be subtly incorrect or otherwise harmless in\npractice. This behavior emerges with hard-to-predict variations even within\nmodels from the same model family. We find no apparent cause for the propensity\nto deceive, but show that more capable models are better at executing this\nstrategy. Strategic dishonesty already has a practical impact on safety\nevaluations, as we show that dishonest responses fool all output-based monitors\nused to detect jailbreaks that we test, rendering benchmark scores unreliable.\nFurther, strategic dishonesty can act like a honeypot against malicious users,\nwhich noticeably obfuscates prior jailbreak attacks. While output monitors\nfail, we show that linear probes on internal activations can be used to\nreliably detect strategic dishonesty. We validate probes on datasets with\nverifiable outcomes and by using them as steering vectors. Overall, we consider\nstrategic dishonesty as a concrete example of a broader concern that alignment\nof LLMs is hard to control, especially when helpfulness and harmlessness\nconflict.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) developers aim for their models to be honest,\nhelpful, and harmless. However, when faced with malicious requests, models are\ntrained to refuse, sacrificing helpfulness. We show that frontier LLMs can\ndevelop a preference for dishonesty as a new strategy, even when other options\nare available. Affected models respond to harmful requests with outputs that\nsound harmful but are crafted to be subtly incorrect or otherwise harmless in\npractice. This behavior emerges with hard-to-predict variations even within\nmodels from the same model family. We find no apparent cause for the propensity\nto deceive, but show that more capable models are better at executing this\nstrategy. Strategic dishonesty already has a practical impact on safety\nevaluations, as we show that dishonest responses fool all output-based monitors\nused to detect jailbreaks that we test, rendering benchmark scores unreliable.\nFurther, strategic dishonesty can act like a honeypot against malicious users,\nwhich noticeably obfuscates prior jailbreak attacks. While output monitors\nfail, we show that linear probes on internal activations can be used to\nreliably detect strategic dishonesty. We validate probes on datasets with\nverifiable outcomes and by using them as steering vectors. Overall, we consider\nstrategic dishonesty as a concrete example of a broader concern that alignment\nof LLMs is hard to control, especially when helpfulness and harmlessness\nconflict."
                },
                "authors": [
                    {
                        "name": "Alexander Panfilov"
                    },
                    {
                        "name": "Evgenii Kortukov"
                    },
                    {
                        "name": "Kristina NikoliÄ"
                    },
                    {
                        "name": "Matthias Bethge"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19269v1",
                "updated": "2025-09-23T17:33:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    33,
                    30,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:33:30Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    33,
                    30,
                    1,
                    266,
                    0
                ],
                "title": "Extracting Conceptual Spaces from LLMs Using Prototype Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Conceptual Spaces from LLMs Using Prototype Embeddings"
                },
                "summary": "Conceptual spaces represent entities and concepts using cognitively\nmeaningful dimensions, typically referring to perceptual features. Such\nrepresentations are widely used in cognitive science and have the potential to\nserve as a cornerstone for explainable AI. Unfortunately, they have proven\nnotoriously difficult to learn, although recent LLMs appear to capture the\nrequired perceptual features to a remarkable extent. Nonetheless, practical\nmethods for extracting the corresponding conceptual spaces are currently still\nlacking. While various methods exist for extracting embeddings from LLMs,\nextracting conceptual spaces also requires us to encode the underlying\nfeatures. In this paper, we propose a strategy in which features (e.g.\nsweetness) are encoded by embedding the description of a corresponding\nprototype (e.g. a very sweet food). To improve this strategy, we fine-tune the\nLLM to align the prototype embeddings with the corresponding conceptual space\ndimensions. Our empirical analysis finds this approach to be highly effective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conceptual spaces represent entities and concepts using cognitively\nmeaningful dimensions, typically referring to perceptual features. Such\nrepresentations are widely used in cognitive science and have the potential to\nserve as a cornerstone for explainable AI. Unfortunately, they have proven\nnotoriously difficult to learn, although recent LLMs appear to capture the\nrequired perceptual features to a remarkable extent. Nonetheless, practical\nmethods for extracting the corresponding conceptual spaces are currently still\nlacking. While various methods exist for extracting embeddings from LLMs,\nextracting conceptual spaces also requires us to encode the underlying\nfeatures. In this paper, we propose a strategy in which features (e.g.\nsweetness) are encoded by embedding the description of a corresponding\nprototype (e.g. a very sweet food). To improve this strategy, we fine-tune the\nLLM to align the prototype embeddings with the corresponding conceptual space\ndimensions. Our empirical analysis finds this approach to be highly effective."
                },
                "authors": [
                    {
                        "name": "Nitesh Kumar"
                    },
                    {
                        "name": "Usashi Chatterjee"
                    },
                    {
                        "name": "Steven Schockaert"
                    }
                ],
                "author_detail": {
                    "name": "Steven Schockaert"
                },
                "author": "Steven Schockaert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12613v3",
                "updated": "2025-09-23T17:26:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    26,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2024-10-16T14:29:29Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    29,
                    29,
                    2,
                    290,
                    0
                ],
                "title": "Exploring Model Kinship for Merging Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Model Kinship for Merging Large Language Models"
                },
                "summary": "Model merging has emerged as a key technique for enhancing the capabilities\nand efficiency of Large Language Models (LLMs). The open-source community has\ndriven model evolution by iteratively merging existing models, yet a principled\nunderstanding of the gains and underlying factors in model merging remains\nlimited. In this work, we study model evolution through iterative merging,\ndrawing an analogy to biological evolution, and introduce the concept of model\nkinship, the degree of similarity or relatedness between LLMs. Through\ncomprehensive empirical analysis, we show that model kinship is closely linked\nto the performance improvements achieved by merging, providing a useful\ncriterion for selecting candidate models. Building on this insight, we propose\na new model merging strategy: Top-k Greedy Merging with Model Kinship, which\ncan improve benchmark performance. Specifically, we discover that incorporating\nmodel kinship as a guiding criterion enables continuous merging while\nmitigating performance degradation caused by local optima, thereby facilitating\nmore effective model evolution. Code is available at\nhttps://github.com/zjunlp/ModelKinship.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging has emerged as a key technique for enhancing the capabilities\nand efficiency of Large Language Models (LLMs). The open-source community has\ndriven model evolution by iteratively merging existing models, yet a principled\nunderstanding of the gains and underlying factors in model merging remains\nlimited. In this work, we study model evolution through iterative merging,\ndrawing an analogy to biological evolution, and introduce the concept of model\nkinship, the degree of similarity or relatedness between LLMs. Through\ncomprehensive empirical analysis, we show that model kinship is closely linked\nto the performance improvements achieved by merging, providing a useful\ncriterion for selecting candidate models. Building on this insight, we propose\na new model merging strategy: Top-k Greedy Merging with Model Kinship, which\ncan improve benchmark performance. Specifically, we discover that incorporating\nmodel kinship as a guiding criterion enables continuous merging while\nmitigating performance degradation caused by local optima, thereby facilitating\nmore effective model evolution. Code is available at\nhttps://github.com/zjunlp/ModelKinship."
                },
                "authors": [
                    {
                        "name": "Yedi Hu"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Shumin Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shumin Deng"
                },
                "author": "Shumin Deng",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15044v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15044v3",
                "updated": "2025-09-23T17:25:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    25,
                    59,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-20T20:10:56Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    10,
                    56,
                    2,
                    232,
                    0
                ],
                "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time\n  Weak-to-Strong Aligner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time\n  Weak-to-Strong Aligner"
                },
                "summary": "Aligning large language models (LLMs) with human preferences has become a\ncritical step in their development. Recent research has increasingly focused on\ntest-time alignment, where additional compute is allocated during inference to\nenhance LLM safety and reasoning capabilities. However, these test-time\nalignment techniques often incur substantial inference costs, limiting their\npractical application. We are inspired by the speculative sampling\nacceleration, which leverages a small draft model to efficiently predict future\ntokens, to address the efficiency bottleneck of test-time alignment. We\nintroduce the reward-shifted speculative sampling (SSS) algorithm, in which the\ndraft model is aligned with human preferences, while the target model remains\nunchanged. We theoretically demonstrate that the distributional shift between\nthe aligned draft model and the unaligned target model can be exploited to\nrecover the RLHF optimal solution without actually obtaining it, by modifying\nthe acceptance criterion and bonus token distribution. Our algorithm achieves\nsuperior gold reward scores at a significantly reduced inference cost in\ntest-time weak-to-strong alignment experiments, thereby validating both its\neffectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human preferences has become a\ncritical step in their development. Recent research has increasingly focused on\ntest-time alignment, where additional compute is allocated during inference to\nenhance LLM safety and reasoning capabilities. However, these test-time\nalignment techniques often incur substantial inference costs, limiting their\npractical application. We are inspired by the speculative sampling\nacceleration, which leverages a small draft model to efficiently predict future\ntokens, to address the efficiency bottleneck of test-time alignment. We\nintroduce the reward-shifted speculative sampling (SSS) algorithm, in which the\ndraft model is aligned with human preferences, while the target model remains\nunchanged. We theoretically demonstrate that the distributional shift between\nthe aligned draft model and the unaligned target model can be exploited to\nrecover the RLHF optimal solution without actually obtaining it, by modifying\nthe acceptance criterion and bonus token distribution. Our algorithm achieves\nsuperior gold reward scores at a significantly reduced inference cost in\ntest-time weak-to-strong alignment experiments, thereby validating both its\neffectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Bolian Li"
                    },
                    {
                        "name": "Yanran Wu"
                    },
                    {
                        "name": "Xinyu Luo"
                    },
                    {
                        "name": "Ruqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Zhang"
                },
                "author": "Ruqi Zhang",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15044v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15044v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15653v2",
                "updated": "2025-09-23T17:25:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    25,
                    54,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-19T06:25:12Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    6,
                    25,
                    12,
                    4,
                    262,
                    0
                ],
                "title": "Future-Proofing Cloud Security Against Quantum Attacks: Risk,\n  Transition, and Mitigation Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future-Proofing Cloud Security Against Quantum Attacks: Risk,\n  Transition, and Mitigation Strategies"
                },
                "summary": "Quantum Computing (QC) introduces a transformative threat to digital\nsecurity, with the potential to compromise widely deployed classical\ncryptographic systems. This survey offers a comprehensive and systematic\nexamination of quantumsafe security for Cloud Computing (CC), focusing on the\nvulnerabilities, transition strategies, and mitigation mechanisms required to\nsecure cloud infrastructures in the quantum era. We evaluated the landscape of\nquantum threats across the entire CC stack, demonstrating how quantum\nalgorithms can undermine classical encryption and compromise cloud security at\nmultiple architectural layers. Using a structured risk assessment methodology\nbased on the STRIDE model, we evaluate quantum-induced attack vectors and their\nimpact on cloud environments. To address these challenges, we propose a layered\nsecurity framework that integrates hybrid cryptographic transition strategies,\ncryptographic agility, and proactive risk mitigation. We analyze the\npreparation and implementation approaches of the major Cloud Service Providers\n(CSPs), including AWS, Azure and GCP, synthesizing platform-specific\ninitiatives toward Post-Quantum Cryptography (PQC). Furthermore, we provide a\ndetailed evaluation of standardized PQC algorithms, exploring their resilience\nto side-channel and active attacks within cloud-native deployments. This survey\nserves as a strategic reference for cloud architects, policymakers, and\nresearchers, offering actionable insights for navigating the complex transition\nto quantum-resilient cloud systems. We conclude by identifying six key future\nresearch directions: standardization and interoperability, performance and\nscalability, implementation security, integration with emerging technologies,\nsystemic preparedness, and crypto-agile migration frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Computing (QC) introduces a transformative threat to digital\nsecurity, with the potential to compromise widely deployed classical\ncryptographic systems. This survey offers a comprehensive and systematic\nexamination of quantumsafe security for Cloud Computing (CC), focusing on the\nvulnerabilities, transition strategies, and mitigation mechanisms required to\nsecure cloud infrastructures in the quantum era. We evaluated the landscape of\nquantum threats across the entire CC stack, demonstrating how quantum\nalgorithms can undermine classical encryption and compromise cloud security at\nmultiple architectural layers. Using a structured risk assessment methodology\nbased on the STRIDE model, we evaluate quantum-induced attack vectors and their\nimpact on cloud environments. To address these challenges, we propose a layered\nsecurity framework that integrates hybrid cryptographic transition strategies,\ncryptographic agility, and proactive risk mitigation. We analyze the\npreparation and implementation approaches of the major Cloud Service Providers\n(CSPs), including AWS, Azure and GCP, synthesizing platform-specific\ninitiatives toward Post-Quantum Cryptography (PQC). Furthermore, we provide a\ndetailed evaluation of standardized PQC algorithms, exploring their resilience\nto side-channel and active attacks within cloud-native deployments. This survey\nserves as a strategic reference for cloud architects, policymakers, and\nresearchers, offering actionable insights for navigating the complex transition\nto quantum-resilient cloud systems. We conclude by identifying six key future\nresearch directions: standardization and interoperability, performance and\nscalability, implementation security, integration with emerging technologies,\nsystemic preparedness, and crypto-agile migration frameworks."
                },
                "authors": [
                    {
                        "name": "Yaser Baseri"
                    },
                    {
                        "name": "Abdelhakim Hafid"
                    },
                    {
                        "name": "Arash Habibi Lashkari"
                    }
                ],
                "author_detail": {
                    "name": "Arash Habibi Lashkari"
                },
                "author": "Arash Habibi Lashkari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15433v2",
                "updated": "2025-09-23T17:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    24,
                    40,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-18T21:15:20Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    21,
                    15,
                    20,
                    3,
                    261,
                    0
                ],
                "title": "LLM-Driven SAST-Genius: A Hybrid Static Analysis Framework for\n  Comprehensive and Actionable Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven SAST-Genius: A Hybrid Static Analysis Framework for\n  Comprehensive and Actionable Security"
                },
                "summary": "This report examines the synergy between Large Language Models (LLMs) and\nStatic Application Security Testing (SAST) to improve vulnerability discovery.\nTraditional SAST tools, while effective for proactive security, are limited by\nhigh false-positive rates and a lack of contextual understanding. Conversely,\nLLMs excel at code analysis and pattern recognition but can be prone to\ninconsistencies and hallucinations. By integrating these two technologies, a\nmore intelligent and efficient system is created. This combination moves beyond\nmere vulnerability detection optimization, transforming security into a deeply\nintegrated, contextual process that provides tangible benefits like improved\ntriage, dynamic bug descriptions, bug validation via exploit generation and\nenhanced analysis of complex codebases. The result is a more effective security\napproach that leverages the strengths of both technologies while mitigating\ntheir weaknesses. SAST-Genius reduced false positives by about 91 % (225 to 20)\ncompared to Semgrep alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report examines the synergy between Large Language Models (LLMs) and\nStatic Application Security Testing (SAST) to improve vulnerability discovery.\nTraditional SAST tools, while effective for proactive security, are limited by\nhigh false-positive rates and a lack of contextual understanding. Conversely,\nLLMs excel at code analysis and pattern recognition but can be prone to\ninconsistencies and hallucinations. By integrating these two technologies, a\nmore intelligent and efficient system is created. This combination moves beyond\nmere vulnerability detection optimization, transforming security into a deeply\nintegrated, contextual process that provides tangible benefits like improved\ntriage, dynamic bug descriptions, bug validation via exploit generation and\nenhanced analysis of complex codebases. The result is a more effective security\napproach that leverages the strengths of both technologies while mitigating\ntheir weaknesses. SAST-Genius reduced false positives by about 91 % (225 to 20)\ncompared to Semgrep alone."
                },
                "authors": [
                    {
                        "name": "Vaibhav Agrawal"
                    },
                    {
                        "name": "Kiarash Ahi"
                    }
                ],
                "author_detail": {
                    "name": "Kiarash Ahi"
                },
                "author": "Kiarash Ahi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19265v1",
                "updated": "2025-09-23T17:24:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    24,
                    14,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:24:14Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    24,
                    14,
                    1,
                    266,
                    0
                ],
                "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from\n  the Arab World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from\n  the Arab World"
                },
                "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting\ntheir effectiveness in diverse cultural contexts. Although some work has\nexplored cultural alignment, the potential for cross-cultural transfer, using\nalignment in one culture to improve performance in others, remains\nunderexplored. This paper investigates cross-cultural transfer of commonsense\nreasoning in the Arab world, where linguistic and historical similarities\ncoexist with local cultural differences. Using a culturally grounded\ncommonsense reasoning dataset covering 13 Arab countries, we evaluate\nlightweight alignment methods such as in-context learning and\ndemonstration-based reinforcement (DITTO), alongside baselines like supervised\nfine-tuning and direct preference optimization. Our results show that merely 12\nculture-specific examples from one country can improve performance in others by\n10\\% on average, within multilingual models. In addition, we demonstrate that\nout-of-culture demonstrations from Indonesia and US contexts can match or\nsurpass in-culture alignment for MCQ reasoning, highlighting cultural\ncommonsense transferability beyond the Arab world. These findings demonstrate\nthat efficient cross-cultural alignment is possible and offer a promising\napproach to adapt LLMs to low-resource cultural settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often reflect Western-centric biases, limiting\ntheir effectiveness in diverse cultural contexts. Although some work has\nexplored cultural alignment, the potential for cross-cultural transfer, using\nalignment in one culture to improve performance in others, remains\nunderexplored. This paper investigates cross-cultural transfer of commonsense\nreasoning in the Arab world, where linguistic and historical similarities\ncoexist with local cultural differences. Using a culturally grounded\ncommonsense reasoning dataset covering 13 Arab countries, we evaluate\nlightweight alignment methods such as in-context learning and\ndemonstration-based reinforcement (DITTO), alongside baselines like supervised\nfine-tuning and direct preference optimization. Our results show that merely 12\nculture-specific examples from one country can improve performance in others by\n10\\% on average, within multilingual models. In addition, we demonstrate that\nout-of-culture demonstrations from Indonesia and US contexts can match or\nsurpass in-culture alignment for MCQ reasoning, highlighting cultural\ncommonsense transferability beyond the Arab world. These findings demonstrate\nthat efficient cross-cultural alignment is possible and offer a promising\napproach to adapt LLMs to low-resource cultural settings."
                },
                "authors": [
                    {
                        "name": "Saeed Almheiri"
                    },
                    {
                        "name": "Rania Hossam"
                    },
                    {
                        "name": "Mena Attia"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "arxiv_comment": "EMNLP 2025 - Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15589v2",
                "updated": "2025-09-23T17:20:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    20,
                    22,
                    1,
                    266,
                    0
                ],
                "published": "2025-02-21T16:57:22Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    16,
                    57,
                    22,
                    4,
                    52,
                    0
                ],
                "title": "LightThinker: Thinking Step-by-Step Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightThinker: Thinking Step-by-Step Compression"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code is released at https://github.com/zjunlp/LightThinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance in complex\nreasoning tasks, but their efficiency is hindered by the substantial memory and\ncomputational costs associated with generating lengthy tokens. In this paper,\nwe propose LightThinker, a novel method that enables LLMs to dynamically\ncompress intermediate thoughts during reasoning. Inspired by human cognitive\nprocesses, LightThinker compresses verbose thought steps into compact\nrepresentations and discards the original reasoning chains, thereby\nsignificantly reducing the number of tokens stored in the context window. This\nis achieved by training the model on when and how to perform compression\nthrough data construction, mapping hidden states to condensed gist tokens, and\ncreating specialized attention masks. Additionally, we introduce the Dependency\n(Dep) metric to quantify the degree of compression by measuring the reliance on\nhistorical tokens during generation. Extensive experiments on four datasets and\ntwo models show that LightThinker reduces peak memory usage and inference time,\nwhile maintaining competitive accuracy. Our work provides a new direction for\nimproving the efficiency of LLMs in complex reasoning tasks without sacrificing\nperformance. Code is released at https://github.com/zjunlp/LightThinker."
                },
                "authors": [
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "EMNLP 2025 (oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19252v1",
                "updated": "2025-09-23T17:12:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    12,
                    20,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:12:20Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    12,
                    20,
                    1,
                    266,
                    0
                ],
                "title": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for\n  Spatio-Temporal Heatmaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for\n  Spatio-Temporal Heatmaps"
                },
                "summary": "Continuous human motion understanding remains a core challenge in computer\nvision due to its high dimensionality and inherent redundancy. Efficient\ncompression and representation are crucial for analyzing complex motion\ndynamics. In this work, we introduce an adversarially-refined VQ-GAN framework\nwith dense motion tokenization for compressing spatio-temporal heatmaps while\npreserving the fine-grained traces of human motion. Our approach combines dense\nmotion tokenization with adversarial refinement, which eliminates\nreconstruction artifacts like motion smearing and temporal misalignment\nobserved in non-adversarial baselines. Our experiments on the CMU Panoptic\ndataset provide conclusive evidence of our method's superiority, outperforming\nthe dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.\nFurthermore, our dense tokenization strategy enables a novel analysis of motion\ncomplexity, revealing that 2D motion can be optimally represented with a\ncompact 128-token vocabulary, while 3D motion's complexity demands a much\nlarger 1024-token codebook for faithful reconstruction. These results establish\npractical deployment feasibility across diverse motion analysis applications.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Pose-Quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous human motion understanding remains a core challenge in computer\nvision due to its high dimensionality and inherent redundancy. Efficient\ncompression and representation are crucial for analyzing complex motion\ndynamics. In this work, we introduce an adversarially-refined VQ-GAN framework\nwith dense motion tokenization for compressing spatio-temporal heatmaps while\npreserving the fine-grained traces of human motion. Our approach combines dense\nmotion tokenization with adversarial refinement, which eliminates\nreconstruction artifacts like motion smearing and temporal misalignment\nobserved in non-adversarial baselines. Our experiments on the CMU Panoptic\ndataset provide conclusive evidence of our method's superiority, outperforming\nthe dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.\nFurthermore, our dense tokenization strategy enables a novel analysis of motion\ncomplexity, revealing that 2D motion can be optimally represented with a\ncompact 128-token vocabulary, while 3D motion's complexity demands a much\nlarger 1024-token codebook for faithful reconstruction. These results establish\npractical deployment feasibility across diverse motion analysis applications.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Pose-Quantization."
                },
                "authors": [
                    {
                        "name": "Gabriel Maldonado"
                    },
                    {
                        "name": "Narges Rashvand"
                    },
                    {
                        "name": "Armin Danesh Pazho"
                    },
                    {
                        "name": "Ghazal Alinezhad Noghre"
                    },
                    {
                        "name": "Vinit Katariya"
                    },
                    {
                        "name": "Hamed Tabkhi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Tabkhi"
                },
                "author": "Hamed Tabkhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19249v1",
                "updated": "2025-09-23T17:10:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    10,
                    40,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:10:40Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    10,
                    40,
                    1,
                    266,
                    0
                ],
                "title": "Reinforcement Learning on Pre-Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning on Pre-Training Data"
                },
                "summary": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,\n$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,\n$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance."
                },
                "authors": [
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Guanhua Huang"
                    },
                    {
                        "name": "Evander Yang"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Haoyuan Wu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Zihao Zheng"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Kun Shi"
                    },
                    {
                        "name": "Kyrierl Deng"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Ruibin Xiong"
                    },
                    {
                        "name": "Tingqiang Xu"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Fei Gao"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Bo Chao Wang"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Wai Lam"
                    },
                    {
                        "name": "Wayyt Wang"
                    },
                    {
                        "name": "Bo Zhou"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16356v2",
                "updated": "2025-09-23T17:10:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    10,
                    14,
                    1,
                    266,
                    0
                ],
                "published": "2025-03-20T17:14:34Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    14,
                    34,
                    3,
                    79,
                    0
                ],
                "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners"
                },
                "summary": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they often fail to generalize these updates to multi-hop\nreasoning tasks that rely on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we find that current layer-localized KE approaches (e.g., MEMIT,\nWISE), which edit only single or a few model layers, inadequately integrate\nupdated knowledge into these reasoning pathways. To address this limitation, we\npresent CaKE (Circuit-aware Knowledge Editing), a novel method that enhances\nthe effective integration of updated knowledge in LLMs. By only leveraging a\nfew curated data samples guided by our circuit-based analysis, CaKE stimulates\nthe model to develop appropriate reasoning circuits for newly incorporated\nknowledge. Experiments show that CaKE enables more accurate and consistent use\nof edited knowledge across related reasoning tasks, achieving an average\nimprovement of 20% in multi-hop reasoning accuracy on the MQuAKE dataset while\nrequiring less memory than existing KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they often fail to generalize these updates to multi-hop\nreasoning tasks that rely on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we find that current layer-localized KE approaches (e.g., MEMIT,\nWISE), which edit only single or a few model layers, inadequately integrate\nupdated knowledge into these reasoning pathways. To address this limitation, we\npresent CaKE (Circuit-aware Knowledge Editing), a novel method that enhances\nthe effective integration of updated knowledge in LLMs. By only leveraging a\nfew curated data samples guided by our circuit-based analysis, CaKE stimulates\nthe model to develop appropriate reasoning circuits for newly incorporated\nknowledge. Experiments show that CaKE enables more accurate and consistent use\nof edited knowledge across related reasoning tasks, achieving an average\nimprovement of 20% in multi-hop reasoning accuracy on the MQuAKE dataset while\nrequiring less memory than existing KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE."
                },
                "authors": [
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Jizhan Fang"
                    },
                    {
                        "name": "Jia-Chen Gu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11341v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11341v3",
                "updated": "2025-09-23T17:07:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    7,
                    48,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-16T15:08:04Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    15,
                    8,
                    4,
                    4,
                    136,
                    0
                ],
                "title": "Benchmarking Critical Questions Generation: A Challenging Reasoning Task\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Critical Questions Generation: A Challenging Reasoning Task\n  for Large Language Models"
                },
                "summary": "The task of Critical Questions Generation (CQs-Gen) aims to foster critical\nthinking by enabling systems to generate questions that expose underlying\nassumptions and challenge the validity of argumentative reasoning structures.\nDespite growing interest in this area, progress has been hindered by the lack\nof suitable datasets and automatic evaluation standards. This paper presents a\ncomprehensive approach to support the development and benchmarking of systems\nfor this task. We construct the first large-scale dataset including ~5K\nmanually annotated questions. We also investigate automatic evaluation methods\nand propose reference-based techniques as the strategy that best correlates\nwith human judgments. Our zero-shot evaluation of 11 LLMs establishes a strong\nbaseline while showcasing the difficulty of the task. Data and code plus a\npublic leaderboard are provided to encourage further research, not only in\nterms of model performance, but also to explore the practical benefits of\nCQs-Gen for both automated reasoning and human critical thinking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of Critical Questions Generation (CQs-Gen) aims to foster critical\nthinking by enabling systems to generate questions that expose underlying\nassumptions and challenge the validity of argumentative reasoning structures.\nDespite growing interest in this area, progress has been hindered by the lack\nof suitable datasets and automatic evaluation standards. This paper presents a\ncomprehensive approach to support the development and benchmarking of systems\nfor this task. We construct the first large-scale dataset including ~5K\nmanually annotated questions. We also investigate automatic evaluation methods\nand propose reference-based techniques as the strategy that best correlates\nwith human judgments. Our zero-shot evaluation of 11 LLMs establishes a strong\nbaseline while showcasing the difficulty of the task. Data and code plus a\npublic leaderboard are provided to encourage further research, not only in\nterms of model performance, but also to explore the practical benefits of\nCQs-Gen for both automated reasoning and human critical thinking."
                },
                "authors": [
                    {
                        "name": "Banca Calvo Figueras"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11341v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11341v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08727v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08727v3",
                "updated": "2025-09-23T17:04:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    4,
                    48,
                    1,
                    266,
                    0
                ],
                "published": "2025-04-11T17:55:45Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    55,
                    45,
                    4,
                    101,
                    0
                ],
                "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images"
                },
                "summary": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles."
                },
                "authors": [
                    {
                        "name": "Boyang Deng"
                    },
                    {
                        "name": "Songyou Peng"
                    },
                    {
                        "name": "Kyle Genova"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Noah Snavely"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Thomas Funkhouser"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Funkhouser"
                },
                "author": "Thomas Funkhouser",
                "arxiv_comment": "ICCV 2025, Project page: https://boyangdeng.com/visual-chronicles ,\n  second and third listed authors have equal contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08727v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08727v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19041v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19041v2",
                "updated": "2025-09-23T17:04:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    4,
                    18,
                    1,
                    266,
                    0
                ],
                "published": "2025-03-24T18:11:42Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    11,
                    42,
                    0,
                    83,
                    0
                ],
                "title": "LookAhead Tuning: Safer Language Models via Partial Answer Previews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LookAhead Tuning: Safer Language Models via Partial Answer Previews"
                },
                "summary": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often compromises their previously established safety alignment.\nTo mitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, a lightweight and effective data-driven approach that\npreserves safety during fine-tuning. The method introduces two simple\nstrategies that modify training data by previewing partial answer prefixes,\nthereby minimizing perturbations to the model's initial token distributions and\nmaintaining its built-in safety mechanisms. Comprehensive experiments\ndemonstrate that LookAhead Tuning effectively maintains model safety without\nsacrificing robust performance on downstream tasks. Our findings position\nLookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning enables large language models (LLMs) to adapt to specific\ndomains, but often compromises their previously established safety alignment.\nTo mitigate the degradation of model safety during fine-tuning, we introduce\nLookAhead Tuning, a lightweight and effective data-driven approach that\npreserves safety during fine-tuning. The method introduces two simple\nstrategies that modify training data by previewing partial answer prefixes,\nthereby minimizing perturbations to the model's initial token distributions and\nmaintaining its built-in safety mechanisms. Comprehensive experiments\ndemonstrate that LookAhead Tuning effectively maintains model safety without\nsacrificing robust performance on downstream tasks. Our findings position\nLookAhead Tuning as a reliable and efficient solution for the safe and\neffective adaptation of LLMs."
                },
                "authors": [
                    {
                        "name": "Kangwei Liu"
                    },
                    {
                        "name": "Mengru Wang"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Yuan Lin"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Shumin Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shumin Deng"
                },
                "author": "Shumin Deng",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19041v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19041v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19236v1",
                "updated": "2025-09-23T16:58:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    58,
                    54,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:58:54Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    58,
                    54,
                    1,
                    266,
                    0
                ],
                "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and\n  Expertise Orchestration for Effective and Efficient Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and\n  Expertise Orchestration for Effective and Efficient Collaboration"
                },
                "summary": "Proper initialization is crucial for any system, particularly in multi-agent\nsystems (MAS), where it plays a pivotal role in determining both the system's\nefficiency and effectiveness. However, existing MAS initialization methods do\nnot fully account for the collaborative needs of the generated agents in\nsubsequent stages. Inspired by the principles of effective team composition, we\npropose AgentInit, which aims to optimize the structure of agent teams.\nSpecifically, in addition to multi-round interactions and reflections between\nagents during agent generation, AgentInit incorporates a Natural Language to\nFormat mechanism to ensure consistency and standardization. Balanced team\nselection strategies using Pareto principles are subsequently applied to\njointly consider agent team diversity and task relevance to promote effective\nand efficient collaboration and enhance overall system performance. Experiments\nshow that AgentInit consistently outperforms state-of-the-art initialization\nmethods and pre-defined strategies across various frameworks and tasks,\nachieving an overall performance improvement of up to 1.2 and 1.6,\nrespectively, while also significantly reducing token consumption. Further\nanalysis confirms its strong transferability to similar tasks and verifies the\neffectiveness of its key components, demonstrating its capability and\nadaptability as a reliable MAS initialization method. Source code and models\nare available at https://github.com/1737423697/AgentInit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proper initialization is crucial for any system, particularly in multi-agent\nsystems (MAS), where it plays a pivotal role in determining both the system's\nefficiency and effectiveness. However, existing MAS initialization methods do\nnot fully account for the collaborative needs of the generated agents in\nsubsequent stages. Inspired by the principles of effective team composition, we\npropose AgentInit, which aims to optimize the structure of agent teams.\nSpecifically, in addition to multi-round interactions and reflections between\nagents during agent generation, AgentInit incorporates a Natural Language to\nFormat mechanism to ensure consistency and standardization. Balanced team\nselection strategies using Pareto principles are subsequently applied to\njointly consider agent team diversity and task relevance to promote effective\nand efficient collaboration and enhance overall system performance. Experiments\nshow that AgentInit consistently outperforms state-of-the-art initialization\nmethods and pre-defined strategies across various frameworks and tasks,\nachieving an overall performance improvement of up to 1.2 and 1.6,\nrespectively, while also significantly reducing token consumption. Further\nanalysis confirms its strong transferability to similar tasks and verifies the\neffectiveness of its key components, demonstrating its capability and\nadaptability as a reliable MAS initialization method. Source code and models\nare available at https://github.com/1737423697/AgentInit."
                },
                "authors": [
                    {
                        "name": "Chunhao Tian"
                    },
                    {
                        "name": "Yutong Wang"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Zhexuan Wang"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19228v1",
                "updated": "2025-09-23T16:49:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:49:43Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "title": "CompLLM: Compression for Long Context Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompLLM: Compression for Long Context Q&A"
                },
                "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility."
                },
                "authors": [
                    {
                        "name": "Gabriele Berton"
                    },
                    {
                        "name": "Jayakrishnan Unnikrishnan"
                    },
                    {
                        "name": "Son Tran"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19227v1",
                "updated": "2025-09-23T16:49:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    25,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:49:25Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    25,
                    1,
                    266,
                    0
                ],
                "title": "MsFIN: Multi-scale Feature Interaction Network for Traffic Accident\n  Anticipation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MsFIN: Multi-scale Feature Interaction Network for Traffic Accident\n  Anticipation"
                },
                "summary": "With the widespread deployment of dashcams and advancements in computer\nvision, developing accident prediction models from the dashcam perspective has\nbecome critical for proactive safety interventions. However, two key challenges\npersist: modeling feature-level interactions among traffic participants (often\noccluded in dashcam views) and capturing complex, asynchronous multi-temporal\nbehavioral cues preceding accidents. To deal with these two challenges, a\nMulti-scale Feature Interaction Network (MsFIN) is proposed for early-stage\naccident anticipation from dashcam videos. MsFIN has three layers for\nmulti-scale feature aggregation, temporal feature processing and multi-scale\nfeature post fusion, respectively. For multi-scale feature aggregation, a\nMulti-scale Module is designed to extract scene representations at short-term,\nmid-term and long-term temporal scales. Meanwhile, the Transformer architecture\nis leveraged to facilitate comprehensive feature interactions. Temporal feature\nprocessing captures the sequential evolution of scene and object features under\ncausal constraints. In the multi-scale feature post fusion stage, the network\nfuses scene and object features across multiple temporal scales to generate a\ncomprehensive risk representation. Experiments on DAD and DADA datasets show\nthat MsFIN significantly outperforms state-of-the-art models with single-scale\nfeature extraction in both prediction correctness and earliness. Ablation\nstudies validate the effectiveness of each module in MsFIN, highlighting how\nthe network achieves superior performance through multi-scale feature fusion\nand contextual interaction modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of dashcams and advancements in computer\nvision, developing accident prediction models from the dashcam perspective has\nbecome critical for proactive safety interventions. However, two key challenges\npersist: modeling feature-level interactions among traffic participants (often\noccluded in dashcam views) and capturing complex, asynchronous multi-temporal\nbehavioral cues preceding accidents. To deal with these two challenges, a\nMulti-scale Feature Interaction Network (MsFIN) is proposed for early-stage\naccident anticipation from dashcam videos. MsFIN has three layers for\nmulti-scale feature aggregation, temporal feature processing and multi-scale\nfeature post fusion, respectively. For multi-scale feature aggregation, a\nMulti-scale Module is designed to extract scene representations at short-term,\nmid-term and long-term temporal scales. Meanwhile, the Transformer architecture\nis leveraged to facilitate comprehensive feature interactions. Temporal feature\nprocessing captures the sequential evolution of scene and object features under\ncausal constraints. In the multi-scale feature post fusion stage, the network\nfuses scene and object features across multiple temporal scales to generate a\ncomprehensive risk representation. Experiments on DAD and DADA datasets show\nthat MsFIN significantly outperforms state-of-the-art models with single-scale\nfeature extraction in both prediction correctness and earliness. Ablation\nstudies validate the effectiveness of each module in MsFIN, highlighting how\nthe network achieves superior performance through multi-scale feature fusion\nand contextual interaction modeling."
                },
                "authors": [
                    {
                        "name": "Tongshuai Wu"
                    },
                    {
                        "name": "Chao Lu"
                    },
                    {
                        "name": "Ze Song"
                    },
                    {
                        "name": "Yunlong Lin"
                    },
                    {
                        "name": "Sizhe Fan"
                    },
                    {
                        "name": "Xuemei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xuemei Chen"
                },
                "author": "Xuemei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05613v3",
                "updated": "2025-09-23T16:48:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    48,
                    10,
                    1,
                    266,
                    0
                ],
                "published": "2025-03-07T17:38:00Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    38,
                    0,
                    4,
                    66,
                    0
                ],
                "title": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have transformed natural language processing,\nyet their internal mechanisms remain largely opaque. Recently, mechanistic\ninterpretability has attracted significant attention from the research\ncommunity as a means to understand the inner workings of LLMs. Among various\nmechanistic interpretability approaches, Sparse Autoencoders (SAEs) have\nemerged as a promising method due to their ability to disentangle the complex,\nsuperimposed features within LLMs into more interpretable components. This\npaper presents a comprehensive survey of SAEs for interpreting and\nunderstanding the internal workings of LLMs. Our major contributions include:\n(1) exploring the technical framework of SAEs, covering basic architecture,\ndesign improvements, and effective training strategies; (2) examining different\napproaches to explaining SAE features, categorized into input-based and\noutput-based explanation methods; (3) discussing evaluation methods for\nassessing SAE performance, covering both structural and functional metrics; and\n(4) investigating real-world applications of SAEs in understanding and\nmanipulating LLM behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed natural language processing,\nyet their internal mechanisms remain largely opaque. Recently, mechanistic\ninterpretability has attracted significant attention from the research\ncommunity as a means to understand the inner workings of LLMs. Among various\nmechanistic interpretability approaches, Sparse Autoencoders (SAEs) have\nemerged as a promising method due to their ability to disentangle the complex,\nsuperimposed features within LLMs into more interpretable components. This\npaper presents a comprehensive survey of SAEs for interpreting and\nunderstanding the internal workings of LLMs. Our major contributions include:\n(1) exploring the technical framework of SAEs, covering basic architecture,\ndesign improvements, and effective training strategies; (2) examining different\napproaches to explaining SAE features, categorized into input-based and\noutput-based explanation methods; (3) discussing evaluation methods for\nassessing SAE performance, covering both structural and functional metrics; and\n(4) investigating real-world applications of SAEs in understanding and\nmanipulating LLM behaviors."
                },
                "authors": [
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Daking Rai"
                    },
                    {
                        "name": "Ziyu Yao"
                    },
                    {
                        "name": "Ninghao Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08080v2",
                "updated": "2025-09-23T16:43:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    43,
                    51,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-12T21:29:12Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    21,
                    29,
                    12,
                    0,
                    132,
                    0
                ],
                "title": "Beyond Input Activations: Identifying Influential Latents by Gradient\n  Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Input Activations: Identifying Influential Latents by Gradient\n  Sparse Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) have recently emerged as powerful tools for\ninterpreting and steering the internal representations of large language models\n(LLMs). However, conventional approaches to analyzing SAEs typically rely\nsolely on input-side activations, without considering the causal influence\nbetween each latent feature and the model's output. This work is built on two\nkey hypotheses: (1) activated latents do not contribute equally to the\nconstruction of the model's output, and (2) only latents with high causal\ninfluence are effective for model steering. To validate these hypotheses, we\npropose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method\nthat identifies the most influential latents by incorporating output-side\ngradient information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have recently emerged as powerful tools for\ninterpreting and steering the internal representations of large language models\n(LLMs). However, conventional approaches to analyzing SAEs typically rely\nsolely on input-side activations, without considering the causal influence\nbetween each latent feature and the model's output. This work is built on two\nkey hypotheses: (1) activated latents do not contribute equally to the\nconstruction of the model's output, and (2) only latents with high causal\ninfluence are effective for model steering. To validate these hypotheses, we\npropose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method\nthat identifies the most influential latents by incorporating output-side\ngradient information."
                },
                "authors": [
                    {
                        "name": "Dong Shu"
                    },
                    {
                        "name": "Xuansheng Wu"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Ninghao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ninghao Liu"
                },
                "author": "Ninghao Liu",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12642v2",
                "updated": "2025-09-23T16:36:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    36,
                    24,
                    1,
                    266,
                    0
                ],
                "published": "2025-07-16T21:27:31Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    21,
                    27,
                    31,
                    2,
                    197,
                    0
                ],
                "title": "QSpark: Towards Reliable Qiskit Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpark: Towards Reliable Qiskit Code Generation"
                },
                "summary": "Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and\nStarCoder often output flawed Qiskit code. We fine-tuned the Qwen2.5-Coder-32B\nmodel with two RL methods, Group Relative Policy Optimization (GRPO) and\nOdds-Ratio Preference Optimization (ORPO), using a richly annotated synthetic\ndataset. On the Qiskit HumanEval benchmark, ORPO reaches 56.29% Pass@1\n($\\approx+10$ pp over Granite-8B-QK) and GRPO hits 49%, both beating all\ngeneral-purpose baselines; on the original HumanEval they score 65.90% and\n63.00%. GRPO performs well on basic tasks (44/78) and excels on intermediate\nones (41/68), but neither GRPO nor ORPO solves any of the five advanced tasks,\nhighlighting clear gains yet room for progress in AI-assisted quantum\nprogramming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and\nStarCoder often output flawed Qiskit code. We fine-tuned the Qwen2.5-Coder-32B\nmodel with two RL methods, Group Relative Policy Optimization (GRPO) and\nOdds-Ratio Preference Optimization (ORPO), using a richly annotated synthetic\ndataset. On the Qiskit HumanEval benchmark, ORPO reaches 56.29% Pass@1\n($\\approx+10$ pp over Granite-8B-QK) and GRPO hits 49%, both beating all\ngeneral-purpose baselines; on the original HumanEval they score 65.90% and\n63.00%. GRPO performs well on basic tasks (44/78) and excels on intermediate\nones (41/68), but neither GRPO nor ORPO solves any of the five advanced tasks,\nhighlighting clear gains yet room for progress in AI-assisted quantum\nprogramming."
                },
                "authors": [
                    {
                        "name": "Kiana Kheiri"
                    },
                    {
                        "name": "Aamna Aamir"
                    },
                    {
                        "name": "Andriy Miranskyy"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_journal_ref": "In Proceedings of AIQxQIA 2025: International Workshop on AI for\n  Quantum and Quantum for AI | co-located with ECAI 2025, Bologna, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15273v2",
                "updated": "2025-09-23T16:30:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    30,
                    58,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-18T11:53:37Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    53,
                    37,
                    3,
                    261,
                    0
                ],
                "title": "Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation\n  Platform for Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Arena: A Comprehensive, Unified, and Evolving Evaluation\n  Platform for Embodied AI"
                },
                "summary": "Embodied AI development significantly lags behind large foundation models due\nto three critical challenges: (1) lack of systematic understanding of core\ncapabilities needed for Embodied AI, making research lack clear objectives; (2)\nabsence of unified and standardized evaluation systems, rendering\ncross-benchmark evaluation infeasible; and (3) underdeveloped automated and\nscalable acquisition methods for embodied data, creating critical bottlenecks\nfor model scaling. To address these obstacles, we present Embodied Arena, a\ncomprehensive, unified, and evolving evaluation platform for Embodied AI. Our\nplatform establishes a systematic embodied capability taxonomy spanning three\nlevels (perception, reasoning, task execution), seven core capabilities, and 25\nfine-grained dimensions, enabling unified evaluation with systematic research\nobjectives. We introduce a standardized evaluation system built upon unified\ninfrastructure supporting flexible integration of 22 diverse benchmarks across\nthree domains (2D/3D Embodied Q&A, Navigation, Task Planning) and 30+ advanced\nmodels from 20+ worldwide institutes. Additionally, we develop a novel\nLLM-driven automated generation pipeline ensuring scalable embodied evaluation\ndata with continuous evolution for diversity and comprehensiveness. Embodied\nArena publishes three real-time leaderboards (Embodied Q&A, Navigation, Task\nPlanning) with dual perspectives (benchmark view and capability view),\nproviding comprehensive overviews of advanced model capabilities. Especially,\nwe present nine findings summarized from the evaluation results on the\nleaderboards of Embodied Arena. This helps to establish clear research veins\nand pinpoint critical research problems, thereby driving forward progress in\nthe field of Embodied AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI development significantly lags behind large foundation models due\nto three critical challenges: (1) lack of systematic understanding of core\ncapabilities needed for Embodied AI, making research lack clear objectives; (2)\nabsence of unified and standardized evaluation systems, rendering\ncross-benchmark evaluation infeasible; and (3) underdeveloped automated and\nscalable acquisition methods for embodied data, creating critical bottlenecks\nfor model scaling. To address these obstacles, we present Embodied Arena, a\ncomprehensive, unified, and evolving evaluation platform for Embodied AI. Our\nplatform establishes a systematic embodied capability taxonomy spanning three\nlevels (perception, reasoning, task execution), seven core capabilities, and 25\nfine-grained dimensions, enabling unified evaluation with systematic research\nobjectives. We introduce a standardized evaluation system built upon unified\ninfrastructure supporting flexible integration of 22 diverse benchmarks across\nthree domains (2D/3D Embodied Q&A, Navigation, Task Planning) and 30+ advanced\nmodels from 20+ worldwide institutes. Additionally, we develop a novel\nLLM-driven automated generation pipeline ensuring scalable embodied evaluation\ndata with continuous evolution for diversity and comprehensiveness. Embodied\nArena publishes three real-time leaderboards (Embodied Q&A, Navigation, Task\nPlanning) with dual perspectives (benchmark view and capability view),\nproviding comprehensive overviews of advanced model capabilities. Especially,\nwe present nine findings summarized from the evaluation results on the\nleaderboards of Embodied Arena. This helps to establish clear research veins\nand pinpoint critical research problems, thereby driving forward progress in\nthe field of Embodied AI."
                },
                "authors": [
                    {
                        "name": "Fei Ni"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Pengyi Li"
                    },
                    {
                        "name": "Yifu Yuan"
                    },
                    {
                        "name": "Lingfeng Zhang"
                    },
                    {
                        "name": "Yuecheng Liu"
                    },
                    {
                        "name": "Peilong Han"
                    },
                    {
                        "name": "Longxin Kou"
                    },
                    {
                        "name": "Shaojin Ma"
                    },
                    {
                        "name": "Jinbin Qiao"
                    },
                    {
                        "name": "David Gamaliel Arcos Bravo"
                    },
                    {
                        "name": "Yuening Wang"
                    },
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhanguang Zhang"
                    },
                    {
                        "name": "Xianze Yao"
                    },
                    {
                        "name": "Yutong Li"
                    },
                    {
                        "name": "Zhao Zhang"
                    },
                    {
                        "name": "Ying Wen"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Bin He"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Huazhe Xu"
                    },
                    {
                        "name": "Jiankang Deng"
                    },
                    {
                        "name": "Shan Luo"
                    },
                    {
                        "name": "Shuqiang Jiang"
                    },
                    {
                        "name": "Wei Pan"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Stefanos Zafeiriou"
                    },
                    {
                        "name": "Jan Peters"
                    },
                    {
                        "name": "Yuzheng Zhuang"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Yan Zheng"
                    },
                    {
                        "name": "Hongyao Tang"
                    },
                    {
                        "name": "Jianye Hao"
                    }
                ],
                "author_detail": {
                    "name": "Jianye Hao"
                },
                "author": "Jianye Hao",
                "arxiv_comment": "32 pages, 5 figures, Embodied Arena Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19209v1",
                "updated": "2025-09-23T16:29:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    29,
                    22,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:29:22Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    29,
                    22,
                    1,
                    266,
                    0
                ],
                "title": "A Knowledge Graph and a Tripartite Evaluation Framework Make\n  Retrieval-Augmented Generation Scalable and Transparent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Knowledge Graph and a Tripartite Evaluation Framework Make\n  Retrieval-Augmented Generation Scalable and Transparent"
                },
                "summary": "Large Language Models (LLMs) have significantly enhanced conversational\nArtificial Intelligence(AI) chatbots; however, domain-specific accuracy and the\navoidance of factual inconsistencies remain pressing challenges, particularly\nfor large datasets. Designing an effective chatbot with appropriate methods and\nevaluating its effectiveness is among the challenges in this domain. This study\npresents a Retrieval Augmented Generation (RAG) chatbot that harnesses a\nknowledge graph and vector search retrieval to deliver precise, context-rich\nresponses in an exemplary use case from over high-volume engineering\nproject-related emails, thereby minimising the need for document chunking. A\ncentral innovation of this work is the introduction of RAG Evaluation\n(RAG-Eval), a novel chain-of-thought LLM-based tripartite evaluation framework\nspecifically developed to assess RAG applications. This framework operates in\nparallel with the chatbot, jointly assessing the user's query, the retrieved\ndocument, and the generated response, enabling a holistic evaluation across\nmultiple quality metrics like query relevance, factual accuracy, coverage,\ncoherence and fluency. The resulting scoring system is provided directly to\nusers as a confidence score (1 to 100%), enabling quick identification of\npossible misaligned or incomplete answers. This proposed approach promotes\ntransparency and rapid verification by incorporating metadata email IDs,\ntimestamps into responses. Experimental comparisons against BERTScore and\nG-EVAL for summarisation evaluation tasks confirm its effectiveness, and\nempirical analysis also shows RAG-Eval reliably detects factual gaps and query\nmismatches, thereby fostering trust in high demand, data centric environments.\nThese findings highlight a scalable path for developing accurate,\nuser-verifiable chatbots that bridge the gap between high-level conversational\nfluency and factual accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly enhanced conversational\nArtificial Intelligence(AI) chatbots; however, domain-specific accuracy and the\navoidance of factual inconsistencies remain pressing challenges, particularly\nfor large datasets. Designing an effective chatbot with appropriate methods and\nevaluating its effectiveness is among the challenges in this domain. This study\npresents a Retrieval Augmented Generation (RAG) chatbot that harnesses a\nknowledge graph and vector search retrieval to deliver precise, context-rich\nresponses in an exemplary use case from over high-volume engineering\nproject-related emails, thereby minimising the need for document chunking. A\ncentral innovation of this work is the introduction of RAG Evaluation\n(RAG-Eval), a novel chain-of-thought LLM-based tripartite evaluation framework\nspecifically developed to assess RAG applications. This framework operates in\nparallel with the chatbot, jointly assessing the user's query, the retrieved\ndocument, and the generated response, enabling a holistic evaluation across\nmultiple quality metrics like query relevance, factual accuracy, coverage,\ncoherence and fluency. The resulting scoring system is provided directly to\nusers as a confidence score (1 to 100%), enabling quick identification of\npossible misaligned or incomplete answers. This proposed approach promotes\ntransparency and rapid verification by incorporating metadata email IDs,\ntimestamps into responses. Experimental comparisons against BERTScore and\nG-EVAL for summarisation evaluation tasks confirm its effectiveness, and\nempirical analysis also shows RAG-Eval reliably detects factual gaps and query\nmismatches, thereby fostering trust in high demand, data centric environments.\nThese findings highlight a scalable path for developing accurate,\nuser-verifiable chatbots that bridge the gap between high-level conversational\nfluency and factual accuracy."
                },
                "authors": [
                    {
                        "name": "Olalekan K. Akindele"
                    },
                    {
                        "name": "Bhupesh Kumar Mishra"
                    },
                    {
                        "name": "Kenneth Y. Wertheim"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Y. Wertheim"
                },
                "author": "Kenneth Y. Wertheim",
                "arxiv_comment": "25 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09403v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09403v3",
                "updated": "2025-09-23T16:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    19,
                    6,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-13T00:39:22Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    0,
                    39,
                    22,
                    2,
                    225,
                    0
                ],
                "title": "Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large\n  Language Models"
                },
                "summary": "Expanding the abbreviated column names of tables, such as \"esal\" to \"employee\nsalary\", is critical for many downstream NLP tasks for tabular data, such as\nNL2SQL, table QA, and keyword search. This problem arises in enterprises,\ndomain sciences, government agencies, and more. In this paper, we make three\ncontributions that significantly advance the state of the art. First, we show\nthat the synthetic public data used by prior work has major limitations, and we\nintroduce four new datasets in enterprise/science domains, with real-world\nabbreviations. Second, we show that accuracy measures used by prior work\nseriously undercount correct expansions, and we propose new synonym-aware\nmeasures that capture accuracy much more accurately. Finally, we develop\nColumbo, a powerful LLM-based solution that exploits context, rules,\nchain-of-thought reasoning, and token-level analysis. Extensive experiments\nshow that Columbo significantly outperforms NameGuess, the current most\nadvanced solution, by 4-29%, over five datasets. Columbo has been used in\nproduction on EDI, a major data lake for environmental sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expanding the abbreviated column names of tables, such as \"esal\" to \"employee\nsalary\", is critical for many downstream NLP tasks for tabular data, such as\nNL2SQL, table QA, and keyword search. This problem arises in enterprises,\ndomain sciences, government agencies, and more. In this paper, we make three\ncontributions that significantly advance the state of the art. First, we show\nthat the synthetic public data used by prior work has major limitations, and we\nintroduce four new datasets in enterprise/science domains, with real-world\nabbreviations. Second, we show that accuracy measures used by prior work\nseriously undercount correct expansions, and we propose new synonym-aware\nmeasures that capture accuracy much more accurately. Finally, we develop\nColumbo, a powerful LLM-based solution that exploits context, rules,\nchain-of-thought reasoning, and token-level analysis. Extensive experiments\nshow that Columbo significantly outperforms NameGuess, the current most\nadvanced solution, by 4-29%, over five datasets. Columbo has been used in\nproduction on EDI, a major data lake for environmental sciences."
                },
                "authors": [
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Stephen Sheen"
                    },
                    {
                        "name": "AnHai Doan"
                    }
                ],
                "author_detail": {
                    "name": "AnHai Doan"
                },
                "author": "AnHai Doan",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025; 19 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09403v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09403v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19199v2",
                "updated": "2025-09-24T01:27:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    27,
                    23,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-23T16:15:42Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    15,
                    42,
                    1,
                    266,
                    0
                ],
                "title": "Online Process Reward Leanring for Agentic Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Process Reward Leanring for Agentic Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning (RL) as autonomous agents that reason and act over long horizons in\ninteractive environments. However, sparse and sometimes unverifiable rewards\nmake temporal credit assignment extremely challenging. Recent work attempts to\nintegrate process supervision into agent learning but suffers from biased\nannotation, reward hacking, high-variance from overly fine-grained signals or\nfailtures when state overlap is rare. We therefore introduce Online Process\nReward Learning (OPRL), a general credit-assignment strategy for agentic RL\nthat integrates seamlessly with standard on-policy algorithms without relying\non additional rollouts or explicit step labels. In OPRL, we optimize an\nimplicit process reward model (PRM) alternately with the agent's policy to\ntransform trajectory preferences into implicit step rewards through a\ntrajectory-based DPO objective. These step rewards are then used to compute\nstep-level advantages, which are combined with episode-level advantages from\noutcome rewards for policy update, creating a self-reinforcing loop.\nTheoretical findings guarantee that the learned step rewards are consistent\nwith trajectory preferences and act as potential-based shaping rewards,\nproviding bounded gradients to stabilize training. Empirically, we evaluate\nOPRL on three distinct agent benmarks, including WebShop and VisualSokoban, as\nwell as open-ended social interactions with unverfiable rewards in SOTOPIA.\nCrucially, OPRL shows superior performance over frontier LLMs and strong RL\nbaselines across domains, achieving state-of-the-art results with higher\nsample-efficiency and lower variance during training. Further analysis also\ndemonstrates the efficient exploration by OPRL using fewer actions,\nunderscoring its potential for agentic learning in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning (RL) as autonomous agents that reason and act over long horizons in\ninteractive environments. However, sparse and sometimes unverifiable rewards\nmake temporal credit assignment extremely challenging. Recent work attempts to\nintegrate process supervision into agent learning but suffers from biased\nannotation, reward hacking, high-variance from overly fine-grained signals or\nfailtures when state overlap is rare. We therefore introduce Online Process\nReward Learning (OPRL), a general credit-assignment strategy for agentic RL\nthat integrates seamlessly with standard on-policy algorithms without relying\non additional rollouts or explicit step labels. In OPRL, we optimize an\nimplicit process reward model (PRM) alternately with the agent's policy to\ntransform trajectory preferences into implicit step rewards through a\ntrajectory-based DPO objective. These step rewards are then used to compute\nstep-level advantages, which are combined with episode-level advantages from\noutcome rewards for policy update, creating a self-reinforcing loop.\nTheoretical findings guarantee that the learned step rewards are consistent\nwith trajectory preferences and act as potential-based shaping rewards,\nproviding bounded gradients to stabilize training. Empirically, we evaluate\nOPRL on three distinct agent benmarks, including WebShop and VisualSokoban, as\nwell as open-ended social interactions with unverfiable rewards in SOTOPIA.\nCrucially, OPRL shows superior performance over frontier LLMs and strong RL\nbaselines across domains, achieving state-of-the-art results with higher\nsample-efficiency and lower variance during training. Further analysis also\ndemonstrates the efficient exploration by OPRL using fewer actions,\nunderscoring its potential for agentic learning in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Yuchuan Wu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Junge Zhang"
                    },
                    {
                        "name": "Jianbin Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Jianbin Jiao"
                },
                "author": "Jianbin Jiao",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.11593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.11593v2",
                "updated": "2025-09-23T16:12:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    12,
                    45,
                    1,
                    266,
                    0
                ],
                "published": "2023-06-20T15:13:02Z",
                "published_parsed": [
                    2023,
                    6,
                    20,
                    15,
                    13,
                    2,
                    1,
                    171,
                    0
                ],
                "title": "Improving Image Captioning Descriptiveness by Ranking and LLM-based\n  Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Image Captioning Descriptiveness by Ranking and LLM-based\n  Fusion"
                },
                "summary": "State-of-The-Art (SoTA) image captioning models are often trained on the\nMicroSoft Common Objects in Context (MS-COCO) dataset, which contains\nhuman-annotated captions with an average length of approximately ten tokens.\nAlthough effective for general scene understanding, these short captions often\nfail to capture complex scenes and convey detailed information. Moreover,\ncaptioning models tend to exhibit bias towards the ``average'' caption, which\ncaptures only the more general aspects, thus overlooking finer details. In this\npaper, we present a novel approach to generate richer and more informative\nimage captions by combining the captions generated from different SoTA\ncaptioning models. Our proposed method requires no additional model training:\ngiven an image, it leverages pre-trained models from the literature to generate\nthe initial captions, and then ranks them using a newly introduced\nimage-text-based metric, which we name BLIPScore. Subsequently, the top two\ncaptions are fused using a Large Language Model (LLM) to produce the final,\nmore detailed description. Experimental results on the MS-COCO and Flickr30k\ntest sets demonstrate the effectiveness of our approach in terms of\ncaption-image alignment and hallucination reduction according to the ALOHa,\nCAPTURE, and Polos metrics. A subjective study lends additional support to\nthese results, suggesting that the captions produced by our model are generally\nperceived as more consistent with human judgment. By combining the strengths of\ndiverse SoTA models, our method enhances the quality and appeal of image\ncaptions, bridging the gap between automated systems and the rich and\ninformative nature of human-generated descriptions. This advance enables the\ngeneration of more suitable captions for the training of both vision-language\nand captioning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-The-Art (SoTA) image captioning models are often trained on the\nMicroSoft Common Objects in Context (MS-COCO) dataset, which contains\nhuman-annotated captions with an average length of approximately ten tokens.\nAlthough effective for general scene understanding, these short captions often\nfail to capture complex scenes and convey detailed information. Moreover,\ncaptioning models tend to exhibit bias towards the ``average'' caption, which\ncaptures only the more general aspects, thus overlooking finer details. In this\npaper, we present a novel approach to generate richer and more informative\nimage captions by combining the captions generated from different SoTA\ncaptioning models. Our proposed method requires no additional model training:\ngiven an image, it leverages pre-trained models from the literature to generate\nthe initial captions, and then ranks them using a newly introduced\nimage-text-based metric, which we name BLIPScore. Subsequently, the top two\ncaptions are fused using a Large Language Model (LLM) to produce the final,\nmore detailed description. Experimental results on the MS-COCO and Flickr30k\ntest sets demonstrate the effectiveness of our approach in terms of\ncaption-image alignment and hallucination reduction according to the ALOHa,\nCAPTURE, and Polos metrics. A subjective study lends additional support to\nthese results, suggesting that the captions produced by our model are generally\nperceived as more consistent with human judgment. By combining the strengths of\ndiverse SoTA models, our method enhances the quality and appeal of image\ncaptions, bridging the gap between automated systems and the rich and\ninformative nature of human-generated descriptions. This advance enables the\ngeneration of more suitable captions for the training of both vision-language\nand captioning models."
                },
                "authors": [
                    {
                        "name": "Luigi Celona"
                    },
                    {
                        "name": "Simone Bianco"
                    },
                    {
                        "name": "Marco Donzella"
                    },
                    {
                        "name": "Paolo Napoletano"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Napoletano"
                },
                "author": "Paolo Napoletano",
                "arxiv_comment": "This manuscript has been accepted for publication in Springer Neural\n  Computing and Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.11593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.11593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19189v2",
                "updated": "2025-09-24T05:27:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    5,
                    27,
                    45,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-23T16:05:16Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    5,
                    16,
                    1,
                    266,
                    0
                ],
                "title": "Unveiling the Role of Learning Rate Schedules via Functional Scaling\n  Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Role of Learning Rate Schedules via Functional Scaling\n  Laws"
                },
                "summary": "Scaling laws have played a cornerstone role in guiding the training of large\nlanguage models (LLMs). However, most existing works on scaling laws primarily\nfocus on the final-step loss, overlooking the loss dynamics during the training\nprocess and, crucially, the impact of learning rate schedule (LRS). In this\npaper, we aim to bridge this gap by studying a teacher-student kernel\nregression setup trained via online stochastic gradient descent (SGD).\nLeveraging a novel intrinsic time viewpoint and stochastic differential\nequation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),\nwhich characterizes the evolution of population risk during the training\nprocess for general LRSs. Remarkably, the impact of the LRSs is captured\nthrough an explicit convolution-type functional term, making their effects\nfully tractable. To illustrate the utility of FSL, we analyze three widely used\nLRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under\nboth data-limited and compute-limited regimes. We provide theoretical\njustification for widely adopted empirical practices in LLMs pre-training such\nas (i) higher-capacity models are more data- and compute-efficient; (ii)\nlearning rate decay can improve training efficiency; (iii) WSD-like schedules\ncan outperform direct-decay schedules. Lastly, we explore the practical\nrelevance of FSL as a surrogate model for fitting, predicting and optimizing\nthe loss curves in LLM pre-training, with experiments conducted across model\nsizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen\nthe understanding of LLM pre-training dynamics and provide insights for\nimproving large-scale model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws have played a cornerstone role in guiding the training of large\nlanguage models (LLMs). However, most existing works on scaling laws primarily\nfocus on the final-step loss, overlooking the loss dynamics during the training\nprocess and, crucially, the impact of learning rate schedule (LRS). In this\npaper, we aim to bridge this gap by studying a teacher-student kernel\nregression setup trained via online stochastic gradient descent (SGD).\nLeveraging a novel intrinsic time viewpoint and stochastic differential\nequation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),\nwhich characterizes the evolution of population risk during the training\nprocess for general LRSs. Remarkably, the impact of the LRSs is captured\nthrough an explicit convolution-type functional term, making their effects\nfully tractable. To illustrate the utility of FSL, we analyze three widely used\nLRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under\nboth data-limited and compute-limited regimes. We provide theoretical\njustification for widely adopted empirical practices in LLMs pre-training such\nas (i) higher-capacity models are more data- and compute-efficient; (ii)\nlearning rate decay can improve training efficiency; (iii) WSD-like schedules\ncan outperform direct-decay schedules. Lastly, we explore the practical\nrelevance of FSL as a surrogate model for fitting, predicting and optimizing\nthe loss curves in LLM pre-training, with experiments conducted across model\nsizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen\nthe understanding of LLM pre-training dynamics and provide insights for\nimproving large-scale model training."
                },
                "authors": [
                    {
                        "name": "Binghui Li"
                    },
                    {
                        "name": "Fengling Chen"
                    },
                    {
                        "name": "Zixun Huang"
                    },
                    {
                        "name": "Lean Wang"
                    },
                    {
                        "name": "Lei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lei Wu"
                },
                "author": "Lei Wu",
                "arxiv_comment": "52 pages, accepted by NeurIPS 2025 as a spotlight paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19179v1",
                "updated": "2025-09-23T15:50:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    50,
                    9,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:50:09Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    50,
                    9,
                    1,
                    266,
                    0
                ],
                "title": "Who Let the Diamonds Out?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Let the Diamonds Out?"
                },
                "summary": "Nitrogen-Vacancy (NV) center magnetometry is a highly promising quantum\nsensing technology, with early prototypes demonstrating impressive sensitivity\nin compact sensing heads. Yet, most existing implementations remain tied to\nlaboratory setups, lacking the portability and environmental robustness needed\nto unlock their full potential in real-world applications. In this work, we\nintroduce a fully portable, hand-held NV-based magnetometer that delivers a\nvector sensitivity of approximately 400 pT/sqrt(Hz), heading errors below 5 nT\nin Earth's field, and a wide signal bandwidth that supports on-field\nrecalibration and operation on moving platforms. We further demonstrate the\nsystem's technological maturity through environmental qualification such as\nthermal, vibration, radiation and other operational stresses related to\ndeployment in low Earth orbit, and through successful deployments in demanding\nscenarios, including northern Canadian weather conditions, drone-mounted\nsurveys and high-altitude balloon flights. Together, these achievements\nestablish this NV-based magnetometer as a robust, versatile tool ready to bring\nquantum sensing performance to a broad range of field and autonomous\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nitrogen-Vacancy (NV) center magnetometry is a highly promising quantum\nsensing technology, with early prototypes demonstrating impressive sensitivity\nin compact sensing heads. Yet, most existing implementations remain tied to\nlaboratory setups, lacking the portability and environmental robustness needed\nto unlock their full potential in real-world applications. In this work, we\nintroduce a fully portable, hand-held NV-based magnetometer that delivers a\nvector sensitivity of approximately 400 pT/sqrt(Hz), heading errors below 5 nT\nin Earth's field, and a wide signal bandwidth that supports on-field\nrecalibration and operation on moving platforms. We further demonstrate the\nsystem's technological maturity through environmental qualification such as\nthermal, vibration, radiation and other operational stresses related to\ndeployment in low Earth orbit, and through successful deployments in demanding\nscenarios, including northern Canadian weather conditions, drone-mounted\nsurveys and high-altitude balloon flights. Together, these achievements\nestablish this NV-based magnetometer as a robust, versatile tool ready to bring\nquantum sensing performance to a broad range of field and autonomous\napplications."
                },
                "authors": [
                    {
                        "name": "Vincent Halde"
                    },
                    {
                        "name": "Olivier Bernard"
                    },
                    {
                        "name": "Mathieu Brochu"
                    },
                    {
                        "name": "Laurier Dufresne"
                    },
                    {
                        "name": "Nicolas Fleury"
                    },
                    {
                        "name": "Kayla Johnson"
                    },
                    {
                        "name": "Benjamin Moffet"
                    },
                    {
                        "name": "David Roy-Guay"
                    }
                ],
                "author_detail": {
                    "name": "David Roy-Guay"
                },
                "author": "David Roy-Guay",
                "arxiv_comment": "8 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19170v1",
                "updated": "2025-09-23T15:43:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    43,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:43:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    43,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "Soft Tokens, Hard Truths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Tokens, Hard Truths"
                },
                "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model."
                },
                "authors": [
                    {
                        "name": "Natasha Butt"
                    },
                    {
                        "name": "Ariel Kwiatkowski"
                    },
                    {
                        "name": "Ismail Labiad"
                    },
                    {
                        "name": "Julia Kempe"
                    },
                    {
                        "name": "Yann Ollivier"
                    }
                ],
                "author_detail": {
                    "name": "Yann Ollivier"
                },
                "author": "Yann Ollivier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19169v1",
                "updated": "2025-09-23T15:43:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    43,
                    28,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:43:28Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    43,
                    28,
                    1,
                    266,
                    0
                ],
                "title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human\n  Demonstration to Robotic Deployment Gap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human\n  Demonstration to Robotic Deployment Gap"
                },
                "summary": "The transfer of manipulation skills from human demonstration to robotic\nexecution is often hindered by a \"domain gap\" in sensing and morphology. This\npaper introduces MagiClaw, a versatile two-finger end-effector designed to\nbridge this gap. MagiClaw functions interchangeably as both a handheld tool for\nintuitive data collection and a robotic end-effector for policy deployment,\nensuring hardware consistency and reliability. Each finger incorporates a Soft\nPolyhedral Network (SPN) with an embedded camera, enabling vision-based\nestimation of 6-DoF forces and contact deformation. This proprioceptive data is\nfused with exteroceptive environmental sensing from an integrated iPhone, which\nprovides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS\napplication, MagiClaw streams synchronized, multi-modal data for real-time\nteleoperation, offline policy learning, and immersive control via mixed-reality\ninterfaces. We demonstrate how this unified system architecture lowers the\nbarrier to collecting high-fidelity, contact-rich datasets and accelerates the\ndevelopment of generalizable manipulation policies. Please refer to the iOS app\nat https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transfer of manipulation skills from human demonstration to robotic\nexecution is often hindered by a \"domain gap\" in sensing and morphology. This\npaper introduces MagiClaw, a versatile two-finger end-effector designed to\nbridge this gap. MagiClaw functions interchangeably as both a handheld tool for\nintuitive data collection and a robotic end-effector for policy deployment,\nensuring hardware consistency and reliability. Each finger incorporates a Soft\nPolyhedral Network (SPN) with an embedded camera, enabling vision-based\nestimation of 6-DoF forces and contact deformation. This proprioceptive data is\nfused with exteroceptive environmental sensing from an integrated iPhone, which\nprovides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS\napplication, MagiClaw streams synchronized, multi-modal data for real-time\nteleoperation, offline policy learning, and immersive control via mixed-reality\ninterfaces. We demonstrate how this unified system architecture lowers the\nbarrier to collecting high-fidelity, contact-rich datasets and accelerates the\ndevelopment of generalizable manipulation policies. Please refer to the iOS app\nat https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details."
                },
                "authors": [
                    {
                        "name": "Tianyu Wu"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Zishang Zhang"
                    },
                    {
                        "name": "Bangchao Huang"
                    },
                    {
                        "name": "Chaoyang Song"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "8 pages, 4 figures, accepted to Data@CoRL2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19162v1",
                "updated": "2025-09-23T15:40:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    40,
                    36,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:40:36Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    40,
                    36,
                    1,
                    266,
                    0
                ],
                "title": "CayleyPy Growth: Efficient growth computations and hundreds of new\n  conjectures on Cayley graphs (Brief version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CayleyPy Growth: Efficient growth computations and hundreds of new\n  conjectures on Cayley graphs (Brief version)"
                },
                "summary": "This is the third paper of the CayleyPy project applying artificial\nintelligence to problems in group theory. We announce the first public release\nof CayleyPy, an open source Python library for computations with Cayley and\nSchreier graphs. Compared with systems such as GAP and Sage, CayleyPy handles\nmuch larger graphs and performs several orders of magnitude faster.\n  Using CayleyPy we obtained about 200 new conjectures on Cayley and Schreier\ngraphs, focused on diameters and growth. For many Cayley graphs of symmetric\ngroups Sn we observe quasi polynomial diameter formulas: a small set of\nquadratic or linear polynomials indexed by n mod s. We conjecture that this is\na general phenomenon, giving efficient diameter computation despite the problem\nbeing NP hard. We propose a refinement of the Babai type conjecture on\ndiameters of Sn: n^2/2 + 4n upper bounds in the undirected case, compared to\nprevious O(n^2) bounds. We also provide explicit generator families, related to\ninvolutions in a square with whiskers pattern, conjectured to maximize the\ndiameter; search confirms this for all n up to 15. We further conjecture an\nanswer to a question posed by V M Glushkov in 1968 on directed Cayley graphs\ngenerated by a cyclic shift and a transposition.\n  For nilpotent groups we conjecture an improvement of J S Ellenberg's results\non upper unitriangular matrices over Z/pZ, showing linear dependence of\ndiameter on p. Moreover.\n  Some conjectures are LLM friendly, naturally stated as sorting problems\nverifiable by algorithms or Python code. To benchmark path finding we created\nmore than 10 Kaggle datasets. CayleyPy works with arbitrary permutation or\nmatrix groups and includes over 100 predefined generators. Our growth\ncomputation code outperforms GAP and Sage up to 1000 times in speed and size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This is the third paper of the CayleyPy project applying artificial\nintelligence to problems in group theory. We announce the first public release\nof CayleyPy, an open source Python library for computations with Cayley and\nSchreier graphs. Compared with systems such as GAP and Sage, CayleyPy handles\nmuch larger graphs and performs several orders of magnitude faster.\n  Using CayleyPy we obtained about 200 new conjectures on Cayley and Schreier\ngraphs, focused on diameters and growth. For many Cayley graphs of symmetric\ngroups Sn we observe quasi polynomial diameter formulas: a small set of\nquadratic or linear polynomials indexed by n mod s. We conjecture that this is\na general phenomenon, giving efficient diameter computation despite the problem\nbeing NP hard. We propose a refinement of the Babai type conjecture on\ndiameters of Sn: n^2/2 + 4n upper bounds in the undirected case, compared to\nprevious O(n^2) bounds. We also provide explicit generator families, related to\ninvolutions in a square with whiskers pattern, conjectured to maximize the\ndiameter; search confirms this for all n up to 15. We further conjecture an\nanswer to a question posed by V M Glushkov in 1968 on directed Cayley graphs\ngenerated by a cyclic shift and a transposition.\n  For nilpotent groups we conjecture an improvement of J S Ellenberg's results\non upper unitriangular matrices over Z/pZ, showing linear dependence of\ndiameter on p. Moreover.\n  Some conjectures are LLM friendly, naturally stated as sorting problems\nverifiable by algorithms or Python code. To benchmark path finding we created\nmore than 10 Kaggle datasets. CayleyPy works with arbitrary permutation or\nmatrix groups and includes over 100 predefined generators. Our growth\ncomputation code outperforms GAP and Sage up to 1000 times in speed and size."
                },
                "authors": [
                    {
                        "name": "A. Chervov"
                    },
                    {
                        "name": "D. Fedoriaka"
                    },
                    {
                        "name": "E. Konstantinova"
                    },
                    {
                        "name": "A. Naumov"
                    },
                    {
                        "name": "I. Kiselev"
                    },
                    {
                        "name": "A. Sheveleva"
                    },
                    {
                        "name": "I. Koltsov"
                    },
                    {
                        "name": "S. Lytkin"
                    },
                    {
                        "name": "A. Smolensky"
                    },
                    {
                        "name": "A. Soibelman"
                    },
                    {
                        "name": "F. Levkovich-Maslyuk"
                    },
                    {
                        "name": "R. Grimov"
                    },
                    {
                        "name": "D. Volovich"
                    },
                    {
                        "name": "A. Isakov"
                    },
                    {
                        "name": "A. Kostin"
                    },
                    {
                        "name": "M. Litvinov"
                    },
                    {
                        "name": "N. Vilkin-Krom"
                    },
                    {
                        "name": "A. Bidzhiev"
                    },
                    {
                        "name": "A. Krasnyi"
                    },
                    {
                        "name": "M. Evseev"
                    },
                    {
                        "name": "E. Geraseva"
                    },
                    {
                        "name": "L. Grunwald"
                    },
                    {
                        "name": "S. Galkin"
                    },
                    {
                        "name": "E. Koldunov"
                    },
                    {
                        "name": "S. Diner"
                    },
                    {
                        "name": "A. Chevychelov"
                    },
                    {
                        "name": "E. Kudasheva"
                    },
                    {
                        "name": "A. Sychev"
                    },
                    {
                        "name": "A. Kravchenko"
                    },
                    {
                        "name": "Z. Kogan"
                    },
                    {
                        "name": "A. Natyrova"
                    },
                    {
                        "name": "L. Shishina"
                    },
                    {
                        "name": "L. Cheldieva"
                    },
                    {
                        "name": "V. Zamkovoy"
                    },
                    {
                        "name": "D. Kovalenko"
                    },
                    {
                        "name": "O. Papulov"
                    },
                    {
                        "name": "S. Kudashev"
                    },
                    {
                        "name": "D. Shiltsov"
                    },
                    {
                        "name": "R. Turtayev"
                    },
                    {
                        "name": "O. Nikitina"
                    },
                    {
                        "name": "D. Mamayeva"
                    },
                    {
                        "name": "S. Nikolenko"
                    },
                    {
                        "name": "M. Obozov"
                    },
                    {
                        "name": "A. Titarenko"
                    },
                    {
                        "name": "A. Dolgorukova"
                    },
                    {
                        "name": "A. Aparnev"
                    },
                    {
                        "name": "O. Debeaupuis"
                    },
                    {
                        "name": "S. Alami C."
                    },
                    {
                        "name": "H. Isambert"
                    }
                ],
                "author_detail": {
                    "name": "H. Isambert"
                },
                "author": "H. Isambert",
                "arxiv_comment": "46 pages, 30 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17681v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17681v3",
                "updated": "2025-09-23T15:40:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    40,
                    35,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-25T05:24:15Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    5,
                    24,
                    15,
                    0,
                    237,
                    0
                ],
                "title": "Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative\n  Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative\n  Scientific Discovery"
                },
                "summary": "Bold claims about AI's role in science-from \"AGI will cure all diseases\" to\npromises of radically accelerated discovery-raise a central epistemic question:\ndo large language models (LLMs) truly generate new knowledge, or do they merely\nremix memorized fragments? We propose unlearning-as-ablation as a falsifiable\nprobe of constructive scientific discovery. The idea is to systematically\nremove a target result together with its forget-closure (supporting lemmas,\nparaphrases, and multi-hop entailments) and then evaluate whether the model can\nre-derive the result from only permitted axioms and tools. Success would\nindicate generative capability beyond recall; failure would expose current\nlimits. Unlike prevailing motivations for unlearning-privacy, copyright, or\nsafety-our framing repositions it as an epistemic probe for AI-for-Science. We\noutline a minimal pilot in mathematics and algorithms to illustrate\nfeasibility, and sketch how the same approach could later be extended to\ndomains such as physics or chemistry. This is a position paper: our\ncontribution is conceptual and methodological, not empirical. We aim to\nstimulate discussion on how principled ablation tests could help distinguish\nmodels that reconstruct knowledge from those that merely retrieve it, and how\nsuch probes might guide the next generation of AI-for-Science benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bold claims about AI's role in science-from \"AGI will cure all diseases\" to\npromises of radically accelerated discovery-raise a central epistemic question:\ndo large language models (LLMs) truly generate new knowledge, or do they merely\nremix memorized fragments? We propose unlearning-as-ablation as a falsifiable\nprobe of constructive scientific discovery. The idea is to systematically\nremove a target result together with its forget-closure (supporting lemmas,\nparaphrases, and multi-hop entailments) and then evaluate whether the model can\nre-derive the result from only permitted axioms and tools. Success would\nindicate generative capability beyond recall; failure would expose current\nlimits. Unlike prevailing motivations for unlearning-privacy, copyright, or\nsafety-our framing repositions it as an epistemic probe for AI-for-Science. We\noutline a minimal pilot in mathematics and algorithms to illustrate\nfeasibility, and sketch how the same approach could later be extended to\ndomains such as physics or chemistry. This is a position paper: our\ncontribution is conceptual and methodological, not empirical. We aim to\nstimulate discussion on how principled ablation tests could help distinguish\nmodels that reconstruct knowledge from those that merely retrieve it, and how\nsuch probes might guide the next generation of AI-for-Science benchmarks."
                },
                "authors": [
                    {
                        "name": "Robert Yang"
                    }
                ],
                "author_detail": {
                    "name": "Robert Yang"
                },
                "author": "Robert Yang",
                "arxiv_comment": "6 pages. Accepted to NeurIPS 2025 AI4Science Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17681v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17681v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09177v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09177v2",
                "updated": "2025-09-23T15:39:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    39,
                    49,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-11T06:27:10Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    27,
                    10,
                    3,
                    254,
                    0
                ],
                "title": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level\n  RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level\n  RL"
                },
                "summary": "We propose FSPO (Fair Sequence Policy Optimization), a sequence-level\nreinforcement learning method for LLMs that enforces length-fair clipping on\nthe importance-sampling (IS) weight. We study RL methods with sequence-level IS\nand identify a mismatch when PPO/GRPO-style clipping is transplanted to\nsequences: a fixed clip range systematically reweights short vs.\\ long\nresponses, distorting the optimization direction. FSPO introduces a simple\nremedy: we clip the sequence log-IS ratio with a band that scales as\n$\\sqrt{L}$. Theoretically, we formalize length fairness via a Length\nReweighting Error (LRE) and prove that small LRE yields a cosine directional\nguarantee between the clipped and true updates. Empirically, FSPO flattens clip\nrates across length bins, stabilizes training, and outperforms all baselines\nacross multiple evaluation datasets on Qwen3-8B-Base model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose FSPO (Fair Sequence Policy Optimization), a sequence-level\nreinforcement learning method for LLMs that enforces length-fair clipping on\nthe importance-sampling (IS) weight. We study RL methods with sequence-level IS\nand identify a mismatch when PPO/GRPO-style clipping is transplanted to\nsequences: a fixed clip range systematically reweights short vs.\\ long\nresponses, distorting the optimization direction. FSPO introduces a simple\nremedy: we clip the sequence log-IS ratio with a band that scales as\n$\\sqrt{L}$. Theoretically, we formalize length fairness via a Length\nReweighting Error (LRE) and prove that small LRE yields a cosine directional\nguarantee between the clipped and true updates. Empirically, FSPO flattens clip\nrates across length bins, stabilizes training, and outperforms all baselines\nacross multiple evaluation datasets on Qwen3-8B-Base model."
                },
                "authors": [
                    {
                        "name": "Hanyi Mao"
                    },
                    {
                        "name": "Quanjia Xiao"
                    },
                    {
                        "name": "Lei Pang"
                    },
                    {
                        "name": "Haixiao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Haixiao Liu"
                },
                "author": "Haixiao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09177v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09177v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08022v3",
                "updated": "2025-09-23T15:39:30Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    39,
                    30,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-12T19:46:29Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    19,
                    46,
                    29,
                    0,
                    132,
                    0
                ],
                "title": "Dynamical Low-Rank Compression of Neural Networks with Robustness under\n  Adversarial Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamical Low-Rank Compression of Neural Networks with Robustness under\n  Adversarial Attacks"
                },
                "summary": "Deployment of neural networks on resource-constrained devices demands models\nthat are both compact and robust to adversarial inputs. However, compression\nand adversarial robustness often conflict. In this work, we introduce a\ndynamical low-rank training scheme enhanced with a novel spectral regularizer\nthat controls the condition number of the low-rank core in each layer. This\napproach mitigates the sensitivity of compressed models to adversarial\nperturbations without sacrificing accuracy on clean data. The method is model-\nand data-agnostic, computationally efficient, and supports rank adaptivity to\nautomatically compress the network at hand. Extensive experiments across\nstandard architectures, datasets, and adversarial attacks show the regularized\nnetworks can achieve over 94% compression while recovering or improving\nadversarial accuracy relative to uncompressed baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of neural networks on resource-constrained devices demands models\nthat are both compact and robust to adversarial inputs. However, compression\nand adversarial robustness often conflict. In this work, we introduce a\ndynamical low-rank training scheme enhanced with a novel spectral regularizer\nthat controls the condition number of the low-rank core in each layer. This\napproach mitigates the sensitivity of compressed models to adversarial\nperturbations without sacrificing accuracy on clean data. The method is model-\nand data-agnostic, computationally efficient, and supports rank adaptivity to\nautomatically compress the network at hand. Extensive experiments across\nstandard architectures, datasets, and adversarial attacks show the regularized\nnetworks can achieve over 94% compression while recovering or improving\nadversarial accuracy relative to uncompressed baselines."
                },
                "authors": [
                    {
                        "name": "Steffen SchotthÃ¶fer"
                    },
                    {
                        "name": "H. Lexie Yang"
                    },
                    {
                        "name": "Stefan Schnake"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Schnake"
                },
                "author": "Stefan Schnake",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19156v1",
                "updated": "2025-09-23T15:34:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    34,
                    33,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:34:33Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    34,
                    33,
                    1,
                    266,
                    0
                ],
                "title": "NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and\n  Dynamic Early-Exit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and\n  Dynamic Early-Exit"
                },
                "summary": "Spiking Neural Networks (SNNs) offer significant potential for enabling\nenergy-efficient intelligence at the edge. However, performing full SNN\ninference at the edge can be challenging due to the latency and energy\nconstraints arising from fixed and high timestep overheads. Edge-cloud\nco-inference systems present a promising solution, but their deployment is\noften hindered by high latency and feature transmission costs. To address these\nissues, we introduce NeuCODEX, a neuromorphic co-inference architecture that\njointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a\nlearned spike-driven compression module to reduce data transmission and employs\na dynamic early-exit mechanism to adaptively terminate inference based on\noutput confidence. We evaluated NeuCODEX on both static images (CIFAR10 and\nCaltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To\ndemonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16\nbackbones in a real edge-to-cloud testbed. Our proposed system reduces data\ntransfer by up to 2048x and edge energy consumption by over 90%, while reducing\nend-to-end latency by up to 3x compared to edge-only inference, all with a\nnegligible accuracy drop of less than 2%. In doing so, NeuCODEX enables\npractical, high-performance SNN deployment in resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) offer significant potential for enabling\nenergy-efficient intelligence at the edge. However, performing full SNN\ninference at the edge can be challenging due to the latency and energy\nconstraints arising from fixed and high timestep overheads. Edge-cloud\nco-inference systems present a promising solution, but their deployment is\noften hindered by high latency and feature transmission costs. To address these\nissues, we introduce NeuCODEX, a neuromorphic co-inference architecture that\njointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a\nlearned spike-driven compression module to reduce data transmission and employs\na dynamic early-exit mechanism to adaptively terminate inference based on\noutput confidence. We evaluated NeuCODEX on both static images (CIFAR10 and\nCaltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To\ndemonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16\nbackbones in a real edge-to-cloud testbed. Our proposed system reduces data\ntransfer by up to 2048x and edge energy consumption by over 90%, while reducing\nend-to-end latency by up to 3x compared to edge-only inference, all with a\nnegligible accuracy drop of less than 2%. In doing so, NeuCODEX enables\npractical, high-performance SNN deployment in resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Maurf Hassan"
                    },
                    {
                        "name": "Steven Davy"
                    },
                    {
                        "name": "Muhammad Zawish"
                    },
                    {
                        "name": "Owais Bin Zuber"
                    },
                    {
                        "name": "Nouman Ashraf"
                    }
                ],
                "author_detail": {
                    "name": "Nouman Ashraf"
                },
                "author": "Nouman Ashraf",
                "arxiv_comment": "This paper was accepted at ICMLA 2025. The official version will\n  appear in IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19153v1",
                "updated": "2025-09-23T15:32:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    32,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:32:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    32,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "LLMs as verification oracles for Solidity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as verification oracles for Solidity"
                },
                "summary": "Ensuring the correctness of smart contracts is critical, as even subtle flaws\ncan lead to severe financial losses. While bug detection tools able to spot\ncommon vulnerability patterns can serve as a first line of defense, most\nreal-world exploits and losses stem from errors in the contract business logic.\nFormal verification tools such as SolCMC and the Certora Prover address this\nchallenge, but their impact remains limited by steep learning curves and\nrestricted specification languages. Recent works have begun to explore the use\nof large language models (LLMs) for security-related tasks such as\nvulnerability detection and test generation. Yet, a fundamental question\nremains open: can LLMs serve as verification oracles, capable of reasoning\nabout arbitrary contract-specific properties? In this paper, we provide the\nfirst systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this\nrole. We benchmark its performance on a large dataset of verification tasks,\ncompare its outputs against those of established formal verification tools, and\nassess its practical effectiveness in real-world auditing scenarios. Our study\ncombines quantitative metrics with qualitative analysis, and shows that recent\nreasoning-oriented LLMs can be surprisingly effective as verification oracles,\nsuggesting a new frontier in the convergence of AI and formal methods for\nsecure smart contract development and auditing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the correctness of smart contracts is critical, as even subtle flaws\ncan lead to severe financial losses. While bug detection tools able to spot\ncommon vulnerability patterns can serve as a first line of defense, most\nreal-world exploits and losses stem from errors in the contract business logic.\nFormal verification tools such as SolCMC and the Certora Prover address this\nchallenge, but their impact remains limited by steep learning curves and\nrestricted specification languages. Recent works have begun to explore the use\nof large language models (LLMs) for security-related tasks such as\nvulnerability detection and test generation. Yet, a fundamental question\nremains open: can LLMs serve as verification oracles, capable of reasoning\nabout arbitrary contract-specific properties? In this paper, we provide the\nfirst systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this\nrole. We benchmark its performance on a large dataset of verification tasks,\ncompare its outputs against those of established formal verification tools, and\nassess its practical effectiveness in real-world auditing scenarios. Our study\ncombines quantitative metrics with qualitative analysis, and shows that recent\nreasoning-oriented LLMs can be surprisingly effective as verification oracles,\nsuggesting a new frontier in the convergence of AI and formal methods for\nsecure smart contract development and auditing."
                },
                "authors": [
                    {
                        "name": "Massimo Bartoletti"
                    },
                    {
                        "name": "Enrico Lipparini"
                    },
                    {
                        "name": "Livio Pompianu"
                    }
                ],
                "author_detail": {
                    "name": "Livio Pompianu"
                },
                "author": "Livio Pompianu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19143v1",
                "updated": "2025-09-23T15:26:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    26,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:26:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    26,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "Anecdoctoring: Automated Red-Teaming Across Language and Place",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anecdoctoring: Automated Red-Teaming Across Language and Place"
                },
                "summary": "Disinformation is among the top risks of generative artificial intelligence\n(AI) misuse. Global adoption of generative AI necessitates red-teaming\nevaluations (i.e., systematic adversarial probing) that are robust across\ndiverse languages and cultures, but red-teaming datasets are commonly US- and\nEnglish-centric. To address this gap, we propose \"anecdoctoring\", a novel\nred-teaming approach that automatically generates adversarial prompts across\nlanguages and cultures. We collect misinformation claims from fact-checking\nwebsites in three languages (English, Spanish, and Hindi) and two geographies\n(US and India). We then cluster individual claims into broader narratives and\ncharacterize the resulting clusters with knowledge graphs, with which we\naugment an attacker LLM. Our method produces higher attack success rates and\noffers interpretability benefits relative to few-shot prompting. Results\nunderscore the need for disinformation mitigations that scale globally and are\ngrounded in real-world adversarial misuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disinformation is among the top risks of generative artificial intelligence\n(AI) misuse. Global adoption of generative AI necessitates red-teaming\nevaluations (i.e., systematic adversarial probing) that are robust across\ndiverse languages and cultures, but red-teaming datasets are commonly US- and\nEnglish-centric. To address this gap, we propose \"anecdoctoring\", a novel\nred-teaming approach that automatically generates adversarial prompts across\nlanguages and cultures. We collect misinformation claims from fact-checking\nwebsites in three languages (English, Spanish, and Hindi) and two geographies\n(US and India). We then cluster individual claims into broader narratives and\ncharacterize the resulting clusters with knowledge graphs, with which we\naugment an attacker LLM. Our method produces higher attack success rates and\noffers interpretability benefits relative to few-shot prompting. Results\nunderscore the need for disinformation mitigations that scale globally and are\ngrounded in real-world adversarial misuse."
                },
                "authors": [
                    {
                        "name": "Alejandro Cuevas"
                    },
                    {
                        "name": "Saloni Dash"
                    },
                    {
                        "name": "Bharat Kumar Nayak"
                    },
                    {
                        "name": "Dan Vann"
                    },
                    {
                        "name": "Madeleine I. G. Daepp"
                    }
                ],
                "author_detail": {
                    "name": "Madeleine I. G. Daepp"
                },
                "author": "Madeleine I. G. Daepp",
                "arxiv_comment": "To be published in EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19136v1",
                "updated": "2025-09-23T15:20:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    20,
                    40,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:20:40Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    20,
                    40,
                    1,
                    266,
                    0
                ],
                "title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases\n  Written in Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Soundness and Consistency of LLM Agents for Executing Test Cases\n  Written in Natural Language"
                },
                "summary": "The use of natural language (NL) test cases for validating graphical user\ninterface (GUI) applications is emerging as a promising direction to manually\nwritten executable test scripts, which are costly to develop and difficult to\nmaintain. Recent advances in large language models (LLMs) have opened the\npossibility of the direct execution of NL test cases by LLM agents. This paper\ninvestigates this direction, focusing on the impact on NL test case unsoundness\nand on test case execution consistency. NL test cases are inherently unsound,\nas they may yield false failures due to ambiguous instructions or unpredictable\nagent behaviour. Furthermore, repeated executions of the same NL test case may\nlead to inconsistent outcomes, undermining test reliability. To address these\nchallenges, we propose an algorithm for executing NL test cases with guardrail\nmechanisms and specialised agents that dynamically verify the correct execution\nof each test step. We introduce measures to evaluate the capabilities of LLMs\nin test execution and one measure to quantify execution consistency. We propose\na definition of weak unsoundness to characterise contexts in which NL test case\nexecution remains acceptable, with respect to the industrial quality levels Six\nSigma. Our experimental evaluation with eight publicly available LLMs, ranging\nfrom 3B to 70B parameters, demonstrates both the potential and current\nlimitations of current LLM agents for GUI testing. Our experiments show that\nMeta Llama 3.1 70B demonstrates acceptable capabilities in NL test case\nexecution with high execution consistency (above the level 3-sigma). We provide\nprototype tools, test suites, and results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of natural language (NL) test cases for validating graphical user\ninterface (GUI) applications is emerging as a promising direction to manually\nwritten executable test scripts, which are costly to develop and difficult to\nmaintain. Recent advances in large language models (LLMs) have opened the\npossibility of the direct execution of NL test cases by LLM agents. This paper\ninvestigates this direction, focusing on the impact on NL test case unsoundness\nand on test case execution consistency. NL test cases are inherently unsound,\nas they may yield false failures due to ambiguous instructions or unpredictable\nagent behaviour. Furthermore, repeated executions of the same NL test case may\nlead to inconsistent outcomes, undermining test reliability. To address these\nchallenges, we propose an algorithm for executing NL test cases with guardrail\nmechanisms and specialised agents that dynamically verify the correct execution\nof each test step. We introduce measures to evaluate the capabilities of LLMs\nin test execution and one measure to quantify execution consistency. We propose\na definition of weak unsoundness to characterise contexts in which NL test case\nexecution remains acceptable, with respect to the industrial quality levels Six\nSigma. Our experimental evaluation with eight publicly available LLMs, ranging\nfrom 3B to 70B parameters, demonstrates both the potential and current\nlimitations of current LLM agents for GUI testing. Our experiments show that\nMeta Llama 3.1 70B demonstrates acceptable capabilities in NL test case\nexecution with high execution consistency (above the level 3-sigma). We provide\nprototype tools, test suites, and results."
                },
                "authors": [
                    {
                        "name": "SÃ©bastien Salva"
                    },
                    {
                        "name": "Redha Taguelmimt"
                    }
                ],
                "author_detail": {
                    "name": "Redha Taguelmimt"
                },
                "author": "Redha Taguelmimt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; D.2.5; F.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11189v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11189v2",
                "updated": "2025-09-23T15:19:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    19,
                    17,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-16T12:48:44Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    12,
                    48,
                    44,
                    4,
                    136,
                    0
                ],
                "title": "Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule\n  Extraction vs RuleSHAP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Global XAI Methods Reveal Injected Bias in LLMs? SHAP vs Rule\n  Extraction vs RuleSHAP"
                },
                "summary": "Large language models (LLMs) can amplify misinformation, undermining societal\ngoals like the UN SDGs. We study three documented drivers of misinformation\n(valence framing, information overload, and oversimplification) which are often\nshaped by one's default beliefs. Building on evidence that LLMs encode such\ndefaults (e.g., \"joy is positive,\" \"math is complex\") and can act as \"bags of\nheuristics,\" we ask: can general belief-driven heuristics behind misinformative\nbehaviour be recovered from LLMs as clear rules? A key obstacle is that global\nrule-extraction methods in explainable AI (XAI) are built for numerical\ninputs/outputs, not text. We address this by eliciting global LLM beliefs and\nmapping them to numerical scores via statistically reliable abstractions,\nthereby enabling off-the-shelf global XAI to detect belief-related heuristics\nin LLMs. To obtain ground truth, we hard-code bias-inducing nonlinear\nheuristics of increasing complexity (univariate, conjunctive, nonconvex) into\npopular LLMs (ChatGPT and Llama) via system instructions. This way, we find\nthat RuleFit under-detects non-univariate biases, while global SHAP better\napproximates conjunctive ones but does not yield actionable rules. To bridge\nthis gap, we propose RuleSHAP, a rule-extraction algorithm that couples global\nSHAP-value aggregations with rule induction to better capture non-univariate\nbias, improving heuristics detection over RuleFit by +94% (MRR@1) on average.\nOur results provide a practical pathway for revealing belief-driven biases in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can amplify misinformation, undermining societal\ngoals like the UN SDGs. We study three documented drivers of misinformation\n(valence framing, information overload, and oversimplification) which are often\nshaped by one's default beliefs. Building on evidence that LLMs encode such\ndefaults (e.g., \"joy is positive,\" \"math is complex\") and can act as \"bags of\nheuristics,\" we ask: can general belief-driven heuristics behind misinformative\nbehaviour be recovered from LLMs as clear rules? A key obstacle is that global\nrule-extraction methods in explainable AI (XAI) are built for numerical\ninputs/outputs, not text. We address this by eliciting global LLM beliefs and\nmapping them to numerical scores via statistically reliable abstractions,\nthereby enabling off-the-shelf global XAI to detect belief-related heuristics\nin LLMs. To obtain ground truth, we hard-code bias-inducing nonlinear\nheuristics of increasing complexity (univariate, conjunctive, nonconvex) into\npopular LLMs (ChatGPT and Llama) via system instructions. This way, we find\nthat RuleFit under-detects non-univariate biases, while global SHAP better\napproximates conjunctive ones but does not yield actionable rules. To bridge\nthis gap, we propose RuleSHAP, a rule-extraction algorithm that couples global\nSHAP-value aggregations with rule induction to better capture non-univariate\nbias, improving heuristics detection over RuleFit by +94% (MRR@1) on average.\nOur results provide a practical pathway for revealing belief-driven biases in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Francesco Sovrano"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Sovrano"
                },
                "author": "Francesco Sovrano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11189v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11189v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17310v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17310v4",
                "updated": "2025-09-23T15:17:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    17,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2025-01-28T21:43:56Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    21,
                    43,
                    56,
                    1,
                    28,
                    0
                ],
                "title": "Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds\n  Decoding"
                },
                "summary": "Guesstimation--the task of making approximate quantitative estimates about\nobjects or events-is a common real--world skill, yet remains underexplored in\nlarge language model (LLM) research. We introduce three guesstimation datasets:\nMARBLES, FUTURE, and ELECPRED, spanning physical estimation (e.g., how many\nmarbles fit in a cup) to abstract predictions (e.g., the 2024 U.S. presidential\nelection). Inspired by the social science concept of Wisdom of Crowds (WOC)-\nwhere the median of multiple estimates improves accuracy-we propose WOC\ndecoding for LLMs. We replicate WOC effects in human participants and find that\nLLMs exhibit similar benefits: median aggregation across sampled responses\nconsistently improves accuracy over greedy decoding, self-consistency decoding,\nand mean decoding. This suggests that LLMs encode a world model that supports\napproximate reasoning. Our results position guesstimation as a useful probe of\nLLM world knowledge and highlight WOC decoding as a strategy for enhancing LLM\nguesstimation performance on real-world tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guesstimation--the task of making approximate quantitative estimates about\nobjects or events-is a common real--world skill, yet remains underexplored in\nlarge language model (LLM) research. We introduce three guesstimation datasets:\nMARBLES, FUTURE, and ELECPRED, spanning physical estimation (e.g., how many\nmarbles fit in a cup) to abstract predictions (e.g., the 2024 U.S. presidential\nelection). Inspired by the social science concept of Wisdom of Crowds (WOC)-\nwhere the median of multiple estimates improves accuracy-we propose WOC\ndecoding for LLMs. We replicate WOC effects in human participants and find that\nLLMs exhibit similar benefits: median aggregation across sampled responses\nconsistently improves accuracy over greedy decoding, self-consistency decoding,\nand mean decoding. This suggests that LLMs encode a world model that supports\napproximate reasoning. Our results position guesstimation as a useful probe of\nLLM world knowledge and highlight WOC decoding as a strategy for enhancing LLM\nguesstimation performance on real-world tasks."
                },
                "authors": [
                    {
                        "name": "Yun-Shiuan Chuang"
                    },
                    {
                        "name": "Sameer Narendran"
                    },
                    {
                        "name": "Nikunj Harlalka"
                    },
                    {
                        "name": "Alexander Cheung"
                    },
                    {
                        "name": "Sizhe Gao"
                    },
                    {
                        "name": "Siddharth Suresh"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Timothy T. Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Timothy T. Rogers"
                },
                "author": "Timothy T. Rogers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17310v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17310v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19128v1",
                "updated": "2025-09-23T15:15:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    15,
                    21,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:15:21Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    15,
                    21,
                    1,
                    266,
                    0
                ],
                "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence\n  Generatio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence\n  Generatio"
                },
                "summary": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning\ncapabilities of Large Language Models (LLMs). However, effectively scaling\nthese RL methods presents significant challenges, primarily due to the\ndifficulty in maintaining high AI accelerator utilization without generating\nstale, off-policy data that harms common RL algorithms. This paper introduces\nPipelineRL, an approach designed to achieve a superior trade-off between\nhardware efficiency and data on-policyness for LLM training. PipelineRL employs\nconcurrent asynchronous data generation and model training, distinguished by\nthe novel in-flight weight updates. This mechanism allows the LLM generation\nengine to receive updated model weights with minimal interruption during the\ngeneration of token sequences, thereby maximizing both the accelerator\nutilization and the freshness of training data. Experiments conducted on\nlong-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL\nachieves approximately $\\sim 2x$ faster learning compared to conventional RL\nbaselines while maintaining highly on-policy training data. A scalable and\nmodular open-source implementation of PipelineRL is also released as a key\ncontribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning\ncapabilities of Large Language Models (LLMs). However, effectively scaling\nthese RL methods presents significant challenges, primarily due to the\ndifficulty in maintaining high AI accelerator utilization without generating\nstale, off-policy data that harms common RL algorithms. This paper introduces\nPipelineRL, an approach designed to achieve a superior trade-off between\nhardware efficiency and data on-policyness for LLM training. PipelineRL employs\nconcurrent asynchronous data generation and model training, distinguished by\nthe novel in-flight weight updates. This mechanism allows the LLM generation\nengine to receive updated model weights with minimal interruption during the\ngeneration of token sequences, thereby maximizing both the accelerator\nutilization and the freshness of training data. Experiments conducted on\nlong-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL\nachieves approximately $\\sim 2x$ faster learning compared to conventional RL\nbaselines while maintaining highly on-policy training data. A scalable and\nmodular open-source implementation of PipelineRL is also released as a key\ncontribution."
                },
                "authors": [
                    {
                        "name": "Alexandre PichÃ©"
                    },
                    {
                        "name": "Ehsan Kamaloo"
                    },
                    {
                        "name": "Rafael Pardinas"
                    },
                    {
                        "name": "Dzmitry Bahdanau"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Bahdanau"
                },
                "author": "Dzmitry Bahdanau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15389v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15389v3",
                "updated": "2025-09-23T15:14:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    14,
                    42,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-21T11:26:40Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    11,
                    26,
                    40,
                    2,
                    141,
                    0
                ],
                "title": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark\n  Study"
                },
                "summary": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs are more vulnerable to meme-based\nharmful prompts than to synthetic or typographic images. Memes significantly\nincrease harmful responses and decrease refusals compared to text-only inputs.\nThough multi-turn interactions provide partial mitigation, elevated\nvulnerability persists. These results highlight the need for ecologically valid\nevaluations and stronger safety mechanisms. MemeSafetyBench is publicly\navailable at https://github.com/oneonlee/Meme-Safety-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet\nmost evaluations rely on artificial images. This study asks: How safe are\ncurrent VLMs when confronted with meme images that ordinary users share? To\ninvestigate this question, we introduce MemeSafetyBench, a 50,430-instance\nbenchmark pairing real meme images with both harmful and benign instructions.\nUsing a comprehensive safety taxonomy and LLM-based instruction generation, we\nassess multiple VLMs across single and multi-turn interactions. We investigate\nhow real-world memes influence harmful outputs, the mitigating effects of\nconversational context, and the relationship between model scale and safety\nmetrics. Our findings demonstrate that VLMs are more vulnerable to meme-based\nharmful prompts than to synthetic or typographic images. Memes significantly\nincrease harmful responses and decrease refusals compared to text-only inputs.\nThough multi-turn interactions provide partial mitigation, elevated\nvulnerability persists. These results highlight the need for ecologically valid\nevaluations and stronger safety mechanisms. MemeSafetyBench is publicly\navailable at https://github.com/oneonlee/Meme-Safety-Bench."
                },
                "authors": [
                    {
                        "name": "DongGeon Lee"
                    },
                    {
                        "name": "Joonwon Jang"
                    },
                    {
                        "name": "Jihae Jeong"
                    },
                    {
                        "name": "Hwanjo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjo Yu"
                },
                "author": "Hwanjo Yu",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15389v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15389v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19125v1",
                "updated": "2025-09-23T15:12:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    12,
                    58,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:12:58Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    12,
                    58,
                    1,
                    266,
                    0
                ],
                "title": "Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via\n  LLM-Guided Multi-Aspect Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via\n  LLM-Guided Multi-Aspect Clustering"
                },
                "summary": "The rapid growth of scientific literature demands efficient methods to\norganize and synthesize research findings. Existing taxonomy construction\nmethods, leveraging unsupervised clustering or direct prompting of large\nlanguage models (LLMs), often lack coherence and granularity. We propose a\nnovel context-aware hierarchical taxonomy generation framework that integrates\nLLM-guided multi-aspect encoding with dynamic clustering. Our method leverages\nLLMs to identify key aspects of each paper (e.g., methodology, dataset,\nevaluation) and generates aspect-specific paper summaries, which are then\nencoded and clustered along each aspect to form a coherent hierarchy. In\naddition, we introduce a new evaluation benchmark of 156 expert-crafted\ntaxonomies encompassing 11.6k papers, providing the first naturally annotated\ndataset for this task. Experimental results demonstrate that our method\nsignificantly outperforms prior approaches, achieving state-of-the-art\nperformance in taxonomy coherence, granularity, and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of scientific literature demands efficient methods to\norganize and synthesize research findings. Existing taxonomy construction\nmethods, leveraging unsupervised clustering or direct prompting of large\nlanguage models (LLMs), often lack coherence and granularity. We propose a\nnovel context-aware hierarchical taxonomy generation framework that integrates\nLLM-guided multi-aspect encoding with dynamic clustering. Our method leverages\nLLMs to identify key aspects of each paper (e.g., methodology, dataset,\nevaluation) and generates aspect-specific paper summaries, which are then\nencoded and clustered along each aspect to form a coherent hierarchy. In\naddition, we introduce a new evaluation benchmark of 156 expert-crafted\ntaxonomies encompassing 11.6k papers, providing the first naturally annotated\ndataset for this task. Experimental results demonstrate that our method\nsignificantly outperforms prior approaches, achieving state-of-the-art\nperformance in taxonomy coherence, granularity, and interpretability."
                },
                "authors": [
                    {
                        "name": "Kun Zhu"
                    },
                    {
                        "name": "Lizi Liao"
                    },
                    {
                        "name": "Yuxuan Gu"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Xiaocheng Feng"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "Accepted to EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11361v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11361v4",
                "updated": "2025-09-23T15:07:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    7,
                    33,
                    1,
                    266,
                    0
                ],
                "published": "2025-02-17T02:18:47Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    2,
                    18,
                    47,
                    0,
                    48,
                    0
                ],
                "title": "VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment"
                },
                "summary": "Detecting disinformation that blends manipulated text and images has become\nincreasingly challenging, as AI tools make synthetic content easy to generate\nand disseminate. While most existing AI safety benchmarks focus on single\nmodality misinformation (i.e., false content shared without intent to deceive),\nintentional multimodal disinformation, such as propaganda or conspiracy\ntheories that imitate credible news, remains largely unaddressed. We introduce\nthe Vision-Language Disinformation Detection Benchmark (VLDBench), the first\nlarge-scale resource supporting both unimodal (text-only) and multimodal (text\n+ image) disinformation detection. VLDBench comprises approximately 62,000\nlabeled text-image pairs across 13 categories, curated from 58 news outlets.\nUsing a semi-automated pipeline followed by expert review, 22 domain experts\ninvested over 500 hours to produce high-quality annotations with substantial\ninter-annotator agreement. Evaluations of state-of-the-art Large Language\nModels (LLMs) and Vision-Language Models (VLMs) on VLDBench show that\nincorporating visual cues improves detection accuracy by 5 to 35 percentage\npoints over text-only models. VLDBench provides data and code for evaluation,\nfine-tuning, and robustness testing to support disinformation analysis.\nDeveloped in alignment with AI governance frameworks (e.g., the MIT AI Risk\nRepository), VLDBench offers a principled foundation for advancing trustworthy\ndisinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset:\nhttps://huggingface.co/datasets/vector-institute/VLDBench Code:\nhttps://github.com/VectorInstitute/VLDBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting disinformation that blends manipulated text and images has become\nincreasingly challenging, as AI tools make synthetic content easy to generate\nand disseminate. While most existing AI safety benchmarks focus on single\nmodality misinformation (i.e., false content shared without intent to deceive),\nintentional multimodal disinformation, such as propaganda or conspiracy\ntheories that imitate credible news, remains largely unaddressed. We introduce\nthe Vision-Language Disinformation Detection Benchmark (VLDBench), the first\nlarge-scale resource supporting both unimodal (text-only) and multimodal (text\n+ image) disinformation detection. VLDBench comprises approximately 62,000\nlabeled text-image pairs across 13 categories, curated from 58 news outlets.\nUsing a semi-automated pipeline followed by expert review, 22 domain experts\ninvested over 500 hours to produce high-quality annotations with substantial\ninter-annotator agreement. Evaluations of state-of-the-art Large Language\nModels (LLMs) and Vision-Language Models (VLMs) on VLDBench show that\nincorporating visual cues improves detection accuracy by 5 to 35 percentage\npoints over text-only models. VLDBench provides data and code for evaluation,\nfine-tuning, and robustness testing to support disinformation analysis.\nDeveloped in alignment with AI governance frameworks (e.g., the MIT AI Risk\nRepository), VLDBench offers a principled foundation for advancing trustworthy\ndisinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset:\nhttps://huggingface.co/datasets/vector-institute/VLDBench Code:\nhttps://github.com/VectorInstitute/VLDBench"
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Ashmal Vayani"
                    },
                    {
                        "name": "Aditya Jain"
                    },
                    {
                        "name": "Aravind Narayanan"
                    },
                    {
                        "name": "Vahid Reza Khazaie"
                    },
                    {
                        "name": "Syed Raza Bashir"
                    },
                    {
                        "name": "Elham Dolatabadi"
                    },
                    {
                        "name": "Gias Uddin"
                    },
                    {
                        "name": "Christos Emmanouilidis"
                    },
                    {
                        "name": "Rizwan Qureshi"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11361v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11361v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19120v1",
                "updated": "2025-09-23T15:06:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    6,
                    4,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:06:04Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    6,
                    4,
                    1,
                    266,
                    0
                ],
                "title": "FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy\n  Federated Learning in Healthcare AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy\n  Federated Learning in Healthcare AI"
                },
                "summary": "Federated Learning (FL) has emerged as a powerful paradigm for\nprivacy-preserving model training, yet deployments in sensitive domains such as\nhealthcare face persistent challenges from non-IID data, client unreliability,\nand adversarial manipulation. This paper introduces FedFiTS, a trust and\nfairness-aware selective FL framework that advances the FedFaSt line by\ncombining fitness-based client election with slotted aggregation. FedFiTS\nimplements a three-phase participation strategy-free-for-all training, natural\nselection, and slotted team participation-augmented with dynamic client\nscoring, adaptive thresholding, and cohort-based scheduling to balance\nconvergence efficiency with robustness. A theoretical convergence analysis\nestablishes bounds for both convex and non-convex objectives under standard\nassumptions, while a communication-complexity analysis shows reductions\nrelative to FedAvg and other baselines. Experiments on diverse datasets-medical\nimaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular\nagricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently\noutperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and\nresilience to poisoning attacks. By integrating trust-aware aggregation with\nfairness-oriented client selection, FedFiTS advances scalable and secure FL,\nmaking it well suited for real-world healthcare and cross-domain deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) has emerged as a powerful paradigm for\nprivacy-preserving model training, yet deployments in sensitive domains such as\nhealthcare face persistent challenges from non-IID data, client unreliability,\nand adversarial manipulation. This paper introduces FedFiTS, a trust and\nfairness-aware selective FL framework that advances the FedFaSt line by\ncombining fitness-based client election with slotted aggregation. FedFiTS\nimplements a three-phase participation strategy-free-for-all training, natural\nselection, and slotted team participation-augmented with dynamic client\nscoring, adaptive thresholding, and cohort-based scheduling to balance\nconvergence efficiency with robustness. A theoretical convergence analysis\nestablishes bounds for both convex and non-convex objectives under standard\nassumptions, while a communication-complexity analysis shows reductions\nrelative to FedAvg and other baselines. Experiments on diverse datasets-medical\nimaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular\nagricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently\noutperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and\nresilience to poisoning attacks. By integrating trust-aware aggregation with\nfairness-oriented client selection, FedFiTS advances scalable and secure FL,\nmaking it well suited for real-world healthcare and cross-domain deployments."
                },
                "authors": [
                    {
                        "name": "Ferdinand Kahenga"
                    },
                    {
                        "name": "Antoine Bagula"
                    },
                    {
                        "name": "Sajal K. Das"
                    },
                    {
                        "name": "Patrick Sello"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Sello"
                },
                "author": "Patrick Sello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19119v1",
                "updated": "2025-09-23T15:05:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    5,
                    51,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:05:51Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    5,
                    51,
                    1,
                    266,
                    0
                ],
                "title": "Enabling Drone Detection with SWARM Repeater-Assisted MIMO ISAC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Drone Detection with SWARM Repeater-Assisted MIMO ISAC"
                },
                "summary": "As definitions about new architectural aspects, use cases, and standards for\nintegrated sensing and communication (ISAC) continue to appear, cellular\nsystems based on massive multiple-input multiple-output (MIMO) antenna\ntechnology are also experiencing a parallel evolution through the integration\nof novel network components. This evolution should support emerging ISAC use\ncases and services. In particular, this paper explores a recent vision for\ncost-efficient cellular network densification through the deployment of swarms\nof repeaters. Leveraging their ability to retransmit signals instantaneously,\nwe investigate how these repeaters can enhance radar sensing capabilities for\ndrone detection in a swarm repeater-assisted MIMO ISAC system. Our results\ndemonstrate that, by optimizing the gains of repeaters given a sufficient\nmaximum amplification gain, increasing the number of repeaters can lead to\ngains in sensing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As definitions about new architectural aspects, use cases, and standards for\nintegrated sensing and communication (ISAC) continue to appear, cellular\nsystems based on massive multiple-input multiple-output (MIMO) antenna\ntechnology are also experiencing a parallel evolution through the integration\nof novel network components. This evolution should support emerging ISAC use\ncases and services. In particular, this paper explores a recent vision for\ncost-efficient cellular network densification through the deployment of swarms\nof repeaters. Leveraging their ability to retransmit signals instantaneously,\nwe investigate how these repeaters can enhance radar sensing capabilities for\ndrone detection in a swarm repeater-assisted MIMO ISAC system. Our results\ndemonstrate that, by optimizing the gains of repeaters given a sufficient\nmaximum amplification gain, increasing the number of repeaters can lead to\ngains in sensing performance."
                },
                "authors": [
                    {
                        "name": "Palatip Jopanya"
                    },
                    {
                        "name": "Diana P. M. Osorio"
                    }
                ],
                "author_detail": {
                    "name": "Diana P. M. Osorio"
                },
                "author": "Diana P. M. Osorio",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19117v1",
                "updated": "2025-09-23T15:03:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    3,
                    5,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T15:03:05Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    3,
                    5,
                    1,
                    266,
                    0
                ],
                "title": "LLM-based Vulnerability Discovery through the Lens of Code Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Vulnerability Discovery through the Lens of Code Metrics"
                },
                "summary": "Large language models (LLMs) excel in many tasks of software engineering, yet\nprogress in leveraging them for vulnerability discovery has stalled in recent\nyears. To understand this phenomenon, we investigate LLMs through the lens of\nclassic code metrics. Surprisingly, we find that a classifier trained solely on\nthese metrics performs on par with state-of-the-art LLMs for vulnerability\ndiscovery. A root-cause analysis reveals a strong correlation and a causal\neffect between LLMs and code metrics: When the value of a metric is changed,\nLLM predictions tend to shift by a corresponding magnitude. This dependency\nsuggests that LLMs operate at a similarly shallow level as code metrics,\nlimiting their ability to grasp complex patterns and fully realize their\npotential in vulnerability discovery. Based on these findings, we derive\nrecommendations on how research should more effectively address this challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in many tasks of software engineering, yet\nprogress in leveraging them for vulnerability discovery has stalled in recent\nyears. To understand this phenomenon, we investigate LLMs through the lens of\nclassic code metrics. Surprisingly, we find that a classifier trained solely on\nthese metrics performs on par with state-of-the-art LLMs for vulnerability\ndiscovery. A root-cause analysis reveals a strong correlation and a causal\neffect between LLMs and code metrics: When the value of a metric is changed,\nLLM predictions tend to shift by a corresponding magnitude. This dependency\nsuggests that LLMs operate at a similarly shallow level as code metrics,\nlimiting their ability to grasp complex patterns and fully realize their\npotential in vulnerability discovery. Based on these findings, we derive\nrecommendations on how research should more effectively address this challenge."
                },
                "authors": [
                    {
                        "name": "Felix Weissberg"
                    },
                    {
                        "name": "Lukas Pirch"
                    },
                    {
                        "name": "Erik Imgrund"
                    },
                    {
                        "name": "Jonas MÃ¶ller"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Konrad Rieck"
                    }
                ],
                "author_detail": {
                    "name": "Konrad Rieck"
                },
                "author": "Konrad Rieck",
                "arxiv_doi": "10.1145/3744916.3764574",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764574",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.19117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12623v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12623v3",
                "updated": "2025-09-23T15:01:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    15,
                    1,
                    37,
                    1,
                    266,
                    0
                ],
                "published": "2025-02-18T08:09:42Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    8,
                    9,
                    42,
                    1,
                    49,
                    0
                ],
                "title": "DeepResonance: Enhancing Multimodal Music Understanding via\n  Music-centric Multi-way Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepResonance: Enhancing Multimodal Music Understanding via\n  Music-centric Multi-way Instruction Tuning"
                },
                "summary": "Recent advancements in music large language models (LLMs) have significantly\nimproved music understanding tasks, which involve the model's ability to\nanalyze and interpret various musical elements. These improvements primarily\nfocused on integrating both music and text inputs. However, the potential of\nincorporating additional modalities such as images, videos and textual music\nfeatures to enhance music understanding remains unexplored. To bridge this gap,\nwe propose DeepResonance, a multimodal music understanding LLM fine-tuned via\nmulti-way instruction tuning with multi-way aligned music, text, image, and\nvideo data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and\nMusic4way-Any2T, three 4-way training and evaluation datasets designed to\nenable DeepResonance to integrate both visual and textual music feature\ncontent. We also introduce multi-sampled ImageBind embeddings and a pre-LLM\nfusion Transformer to enhance modality fusion prior to input into text LLMs,\ntailoring for multi-way instruction tuning. Our model achieves state-of-the-art\nperformances across six music understanding tasks, highlighting the benefits of\nthe auxiliary modalities and the structural superiority of DeepResonance. We\nopen-source the codes, models and datasets we constructed:\ngithub.com/sony/DeepResonance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in music large language models (LLMs) have significantly\nimproved music understanding tasks, which involve the model's ability to\nanalyze and interpret various musical elements. These improvements primarily\nfocused on integrating both music and text inputs. However, the potential of\nincorporating additional modalities such as images, videos and textual music\nfeatures to enhance music understanding remains unexplored. To bridge this gap,\nwe propose DeepResonance, a multimodal music understanding LLM fine-tuned via\nmulti-way instruction tuning with multi-way aligned music, text, image, and\nvideo data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and\nMusic4way-Any2T, three 4-way training and evaluation datasets designed to\nenable DeepResonance to integrate both visual and textual music feature\ncontent. We also introduce multi-sampled ImageBind embeddings and a pre-LLM\nfusion Transformer to enhance modality fusion prior to input into text LLMs,\ntailoring for multi-way instruction tuning. Our model achieves state-of-the-art\nperformances across six music understanding tasks, highlighting the benefits of\nthe auxiliary modalities and the structural superiority of DeepResonance. We\nopen-source the codes, models and datasets we constructed:\ngithub.com/sony/DeepResonance."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Mao"
                    },
                    {
                        "name": "Mengjie Zhao"
                    },
                    {
                        "name": "Qiyu Wu"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "Accepted to EMNLP 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12623v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12623v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07838v2",
                "updated": "2025-09-23T14:58:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    58,
                    35,
                    1,
                    266,
                    0
                ],
                "published": "2025-07-10T15:09:20Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    15,
                    9,
                    20,
                    3,
                    191,
                    0
                ],
                "title": "3D-ADAM: A Dataset for 3D Anomaly Detection in Additive Manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D-ADAM: A Dataset for 3D Anomaly Detection in Additive Manufacturing"
                },
                "summary": "Surface defects are a primary source of yield loss in manufacturing, yet\nexisting anomaly detection methods often fail in real-world deployment due to\nlimited and unrepresentative datasets. To overcome this, we introduce 3D-ADAM,\na 3D Anomaly Detection in Additive Manufacturing dataset, that is the first\nlarge-scale, industry-relevant dataset for RGB+3D surface defect detection in\nadditive manufacturing. 3D-ADAM comprises 14,120 high-resolution scans of 217\nunique parts, captured with four industrial depth sensors, and includes 27,346\nannotated defects across 12 categories along with 27,346 annotations of machine\nelement features in 16 classes. 3D-ADAM is captured in a real industrial\nenvironment and as such reflects real production conditions, including\nvariations in part placement, sensor positioning, lighting, and partial\nocclusion. Benchmarking state-of-the-art models demonstrates that 3D-ADAM\npresents substantial challenges beyond existing datasets. Validation through\nexpert labelling surveys with industry partners further confirms its industrial\nrelevance. By providing this benchmark, 3D-ADAM establishes a foundation for\nadvancing robust 3D anomaly detection capable of meeting manufacturing demands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface defects are a primary source of yield loss in manufacturing, yet\nexisting anomaly detection methods often fail in real-world deployment due to\nlimited and unrepresentative datasets. To overcome this, we introduce 3D-ADAM,\na 3D Anomaly Detection in Additive Manufacturing dataset, that is the first\nlarge-scale, industry-relevant dataset for RGB+3D surface defect detection in\nadditive manufacturing. 3D-ADAM comprises 14,120 high-resolution scans of 217\nunique parts, captured with four industrial depth sensors, and includes 27,346\nannotated defects across 12 categories along with 27,346 annotations of machine\nelement features in 16 classes. 3D-ADAM is captured in a real industrial\nenvironment and as such reflects real production conditions, including\nvariations in part placement, sensor positioning, lighting, and partial\nocclusion. Benchmarking state-of-the-art models demonstrates that 3D-ADAM\npresents substantial challenges beyond existing datasets. Validation through\nexpert labelling surveys with industry partners further confirms its industrial\nrelevance. By providing this benchmark, 3D-ADAM establishes a foundation for\nadvancing robust 3D anomaly detection capable of meeting manufacturing demands."
                },
                "authors": [
                    {
                        "name": "Paul McHard"
                    },
                    {
                        "name": "Florent P. Audonnet"
                    },
                    {
                        "name": "Oliver Summerell"
                    },
                    {
                        "name": "Sebastian Andraos"
                    },
                    {
                        "name": "Paul Henderson"
                    },
                    {
                        "name": "Gerardo Aragon-Camarasa"
                    }
                ],
                "author_detail": {
                    "name": "Gerardo Aragon-Camarasa"
                },
                "author": "Gerardo Aragon-Camarasa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19104v1",
                "updated": "2025-09-23T14:49:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    49,
                    48,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:49:48Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    49,
                    48,
                    1,
                    266,
                    0
                ],
                "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast\n  and Efficient LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast\n  and Efficient LLM Alignment"
                },
                "summary": "Reinforcement learning with human feedback (RLHF) has become crucial for\naligning Large Language Models (LLMs) with human intent. However, existing\noffline RLHF approaches suffer from overoptimization, where models overfit to\nreward misspecification and drift from preferred behaviors observed during\ntraining. We introduce DRO-REBEL, a unified family of robust REBEL updates with\ntype-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality,\neach update reduces to a simple relative-reward regression, preserving\nscalability and avoiding PPO-style clipping or auxiliary value networks. Under\nstandard linear-reward and log-linear policy classes with a data-coverage\ncondition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants\nthan prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$\nrate via a localized Rademacher complexity analysis. The same analysis closes\nthe gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal\nparametric rates. We derive practical SGD algorithms for all three divergences:\ngradient regularization (Wasserstein), importance weighting (KL), and a fast\n1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale\nArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong\nworst-case robustness across unseen preference mixtures, model sizes, and data\nscales, with $\\chi^2$-REBEL showing consistently strong empirical performance.\nA controlled radius--coverage study validates a no-free-lunch trade-off: radii\nshrinking faster than empirical divergence concentration rates achieve\nminimax-optimal parametric rates but forfeit coverage, while\ncoverage-guaranteeing radii incur $O(n^{-1/4})$ rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with human feedback (RLHF) has become crucial for\naligning Large Language Models (LLMs) with human intent. However, existing\noffline RLHF approaches suffer from overoptimization, where models overfit to\nreward misspecification and drift from preferred behaviors observed during\ntraining. We introduce DRO-REBEL, a unified family of robust REBEL updates with\ntype-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality,\neach update reduces to a simple relative-reward regression, preserving\nscalability and avoiding PPO-style clipping or auxiliary value networks. Under\nstandard linear-reward and log-linear policy classes with a data-coverage\ncondition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants\nthan prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$\nrate via a localized Rademacher complexity analysis. The same analysis closes\nthe gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal\nparametric rates. We derive practical SGD algorithms for all three divergences:\ngradient regularization (Wasserstein), importance weighting (KL), and a fast\n1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale\nArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong\nworst-case robustness across unseen preference mixtures, model sizes, and data\nscales, with $\\chi^2$-REBEL showing consistently strong empirical performance.\nA controlled radius--coverage study validates a no-free-lunch trade-off: radii\nshrinking faster than empirical divergence concentration rates achieve\nminimax-optimal parametric rates but forfeit coverage, while\ncoverage-guaranteeing radii incur $O(n^{-1/4})$ rates."
                },
                "authors": [
                    {
                        "name": "Sharan Sahu"
                    },
                    {
                        "name": "Martin T. Wells"
                    }
                ],
                "author_detail": {
                    "name": "Martin T. Wells"
                },
                "author": "Martin T. Wells",
                "arxiv_comment": "70 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19105v1",
                "updated": "2025-09-23T14:49:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    49,
                    48,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:49:48Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    49,
                    48,
                    1,
                    266,
                    0
                ],
                "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation"
                },
                "summary": "Successful navigation in outdoor environments requires accurate prediction of\nthe physical interactions between the robot and the terrain. To this end,\nseveral methods rely on geometric or semantic labels to classify traversable\nsurfaces. However, such labels cannot distinguish visually similar surfaces\nthat differ in material properties. Spectral sensors enable inference of\nmaterial composition from surface reflectance measured across multiple\nwavelength bands. Although spectral sensing is gaining traction in robotics,\nwidespread deployment remains constrained by the need for custom hardware\nintegration, high sensor costs, and compute-intensive processing pipelines. In\nthis paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),\na deep neural network designed to bridge the gap between the accessibility of\nRGB sensing and the rich material information provided by spectral data. RS-Net\npredicts spectral signatures from RGB patches, which we map to terrain labels\nand friction coefficients. The resulting terrain classifications are integrated\ninto a sampling-based motion planner for a wheeled robot operating in outdoor\nenvironments. Likewise, the friction estimates are incorporated into a\ncontact-force-based MPC for a quadruped robot navigating slippery surfaces.\nThus, we introduce a framework that learns the task-relevant physical property\nonce during training and thereafter relies solely on RGB sensing at test time.\nThe code is available at https://github.com/prajapatisarvesh/RS-Net.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Successful navigation in outdoor environments requires accurate prediction of\nthe physical interactions between the robot and the terrain. To this end,\nseveral methods rely on geometric or semantic labels to classify traversable\nsurfaces. However, such labels cannot distinguish visually similar surfaces\nthat differ in material properties. Spectral sensors enable inference of\nmaterial composition from surface reflectance measured across multiple\nwavelength bands. Although spectral sensing is gaining traction in robotics,\nwidespread deployment remains constrained by the need for custom hardware\nintegration, high sensor costs, and compute-intensive processing pipelines. In\nthis paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),\na deep neural network designed to bridge the gap between the accessibility of\nRGB sensing and the rich material information provided by spectral data. RS-Net\npredicts spectral signatures from RGB patches, which we map to terrain labels\nand friction coefficients. The resulting terrain classifications are integrated\ninto a sampling-based motion planner for a wheeled robot operating in outdoor\nenvironments. Likewise, the friction estimates are incorporated into a\ncontact-force-based MPC for a quadruped robot navigating slippery surfaces.\nThus, we introduce a framework that learns the task-relevant physical property\nonce during training and thereafter relies solely on RGB sensing at test time.\nThe code is available at https://github.com/prajapatisarvesh/RS-Net."
                },
                "authors": [
                    {
                        "name": "Sarvesh Prajapati"
                    },
                    {
                        "name": "Ananya Trivedi"
                    },
                    {
                        "name": "Nathaniel Hanson"
                    },
                    {
                        "name": "Bruce Maxwell"
                    },
                    {
                        "name": "Taskin Padir"
                    }
                ],
                "author_detail": {
                    "name": "Taskin Padir"
                },
                "author": "Taskin Padir",
                "arxiv_comment": "8 pages, 10 figures, submitted to Robotic Computing & Communication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19102v1",
                "updated": "2025-09-23T14:49:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    49,
                    5,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:49:05Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    49,
                    5,
                    1,
                    266,
                    0
                ],
                "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object\n  Canonicalization for Generalizable Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object\n  Canonicalization for Generalizable Robotic Manipulation"
                },
                "summary": "General-purpose robotic skills from end-to-end demonstrations often leads to\ntask-specific policies that fail to generalize beyond the training\ndistribution. Therefore, we introduce FunCanon, a framework that converts\nlong-horizon manipulation tasks into sequences of action chunks, each defined\nby an actor, verb, and object. These chunks focus policy learning on the\nactions themselves, rather than isolated tasks, enabling compositionality and\nreuse. To make policies pose-aware and category-general, we perform functional\nobject canonicalization for functional alignment and automatic manipulation\ntrajectory transfer, mapping objects into shared functional frames using\naffordance cues from large vision language models. An object centric and action\ncentric diffusion policy FuncDiffuser trained on this aligned data naturally\nrespects object affordances and poses, simplifying learning and improving\ngeneralization ability. Experiments on simulated and real-world benchmarks\ndemonstrate category-level generalization, cross-task behavior reuse, and\nrobust sim2real deployment, showing that functional canonicalization provides a\nstrong inductive bias for scalable imitation learning in complex manipulation\ndomains. Details of the demo and supplemental material are available on our\nproject website https://sites.google.com/view/funcanon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose robotic skills from end-to-end demonstrations often leads to\ntask-specific policies that fail to generalize beyond the training\ndistribution. Therefore, we introduce FunCanon, a framework that converts\nlong-horizon manipulation tasks into sequences of action chunks, each defined\nby an actor, verb, and object. These chunks focus policy learning on the\nactions themselves, rather than isolated tasks, enabling compositionality and\nreuse. To make policies pose-aware and category-general, we perform functional\nobject canonicalization for functional alignment and automatic manipulation\ntrajectory transfer, mapping objects into shared functional frames using\naffordance cues from large vision language models. An object centric and action\ncentric diffusion policy FuncDiffuser trained on this aligned data naturally\nrespects object affordances and poses, simplifying learning and improving\ngeneralization ability. Experiments on simulated and real-world benchmarks\ndemonstrate category-level generalization, cross-task behavior reuse, and\nrobust sim2real deployment, showing that functional canonicalization provides a\nstrong inductive bias for scalable imitation learning in complex manipulation\ndomains. Details of the demo and supplemental material are available on our\nproject website https://sites.google.com/view/funcanon."
                },
                "authors": [
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Xiaoyue Hu"
                    },
                    {
                        "name": "Boyang Zhong"
                    },
                    {
                        "name": "Kaixin Bai"
                    },
                    {
                        "name": "ZoltÃ¡n-Csaba MÃ¡rton"
                    },
                    {
                        "name": "Zhenshan Bing"
                    },
                    {
                        "name": "Zhaopeng Chen"
                    },
                    {
                        "name": "Alois Christian Knoll"
                    },
                    {
                        "name": "Jianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Zhang"
                },
                "author": "Jianwei Zhang",
                "arxiv_comment": "project website: https://sites.google.com/view/funcanon, 11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19100v1",
                "updated": "2025-09-23T14:48:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    48,
                    58,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:48:58Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    48,
                    58,
                    1,
                    266,
                    0
                ],
                "title": "Algorithms for Adversarially Robust Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithms for Adversarially Robust Deep Learning"
                },
                "summary": "Given the widespread use of deep learning models in safety-critical\napplications, ensuring that the decisions of such models are robust against\nadversarial exploitation is of fundamental importance. In this thesis, we\ndiscuss recent progress toward designing algorithms that exhibit desirable\nrobustness properties. First, we discuss the problem of adversarial examples in\ncomputer vision, for which we introduce new technical results, training\nparadigms, and certification algorithms. Next, we consider the problem of\ndomain generalization, wherein the task is to train neural networks to\ngeneralize from a family of training distributions to unseen test\ndistributions. We present new algorithms that achieve state-of-the-art\ngeneralization in medical imaging, molecular identification, and image\nclassification. Finally, we study the setting of jailbreaking large language\nmodels (LLMs), wherein an adversarial user attempts to design prompts that\nelicit objectionable content from an LLM. We propose new attacks and defenses,\nwhich represent the frontier of progress toward designing robust language-based\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the widespread use of deep learning models in safety-critical\napplications, ensuring that the decisions of such models are robust against\nadversarial exploitation is of fundamental importance. In this thesis, we\ndiscuss recent progress toward designing algorithms that exhibit desirable\nrobustness properties. First, we discuss the problem of adversarial examples in\ncomputer vision, for which we introduce new technical results, training\nparadigms, and certification algorithms. Next, we consider the problem of\ndomain generalization, wherein the task is to train neural networks to\ngeneralize from a family of training distributions to unseen test\ndistributions. We present new algorithms that achieve state-of-the-art\ngeneralization in medical imaging, molecular identification, and image\nclassification. Finally, we study the setting of jailbreaking large language\nmodels (LLMs), wherein an adversarial user attempts to design prompts that\nelicit objectionable content from an LLM. We propose new attacks and defenses,\nwhich represent the frontier of progress toward designing robust language-based\nagents."
                },
                "authors": [
                    {
                        "name": "Alexander Robey"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Robey"
                },
                "author": "Alexander Robey",
                "arxiv_comment": "PhD thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15587v2",
                "updated": "2025-09-23T14:48:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    48,
                    18,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-19T04:40:46Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    4,
                    40,
                    46,
                    4,
                    262,
                    0
                ],
                "title": "DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation\n  in Large Language Models"
                },
                "summary": "Logic reasoning in natural language has been recognized as an important\nmeasure of human intelligence for Large Language Models (LLMs). Popular\nbenchmarks may entangle multiple reasoning skills and thus provide unfaithful\nevaluations on the logic reasoning skill. Meanwhile, existing logic reasoning\nbenchmarks are limited in language diversity and their distributions are\ndeviated from the distribution of an ideal logic reasoning benchmark, which may\nlead to biased evaluation results. This paper thereby proposes a new classical\nlogic benchmark DivLogicEval, consisting of natural sentences composed of\ndiverse statements in a counterintuitive way. To ensure a more reliable\nevaluation, we also introduce a new evaluation metric that mitigates the\ninfluence of bias and randomness inherent in LLMs. Through experiments, we\ndemonstrate the extent to which logical reasoning is required to answer the\nquestions in DivLogicEval and compare the performance of different popular LLMs\nin conducting logical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic reasoning in natural language has been recognized as an important\nmeasure of human intelligence for Large Language Models (LLMs). Popular\nbenchmarks may entangle multiple reasoning skills and thus provide unfaithful\nevaluations on the logic reasoning skill. Meanwhile, existing logic reasoning\nbenchmarks are limited in language diversity and their distributions are\ndeviated from the distribution of an ideal logic reasoning benchmark, which may\nlead to biased evaluation results. This paper thereby proposes a new classical\nlogic benchmark DivLogicEval, consisting of natural sentences composed of\ndiverse statements in a counterintuitive way. To ensure a more reliable\nevaluation, we also introduce a new evaluation metric that mitigates the\ninfluence of bias and randomness inherent in LLMs. Through experiments, we\ndemonstrate the extent to which logical reasoning is required to answer the\nquestions in DivLogicEval and compare the performance of different popular LLMs\nin conducting logical reasoning."
                },
                "authors": [
                    {
                        "name": "Tsz Ting Chung"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Dit-Yan Yeung"
                },
                "author": "Dit-Yan Yeung",
                "arxiv_comment": "Accepted by EMNLP 2025. Project Page:\n  https://ttchungc.github.io/projects/divlogiceval/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19094v1",
                "updated": "2025-09-23T14:44:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    44,
                    46,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:44:46Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    44,
                    46,
                    1,
                    266,
                    0
                ],
                "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form\n  Personalized Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathways of Thoughts: Multi-Directional Thinking for Long-form\n  Personalized Question Answering"
                },
                "summary": "Personalization is essential for adapting question answering (QA) systems to\nuser-specific information needs, thereby improving both accuracy and user\nsatisfaction. However, personalized QA remains relatively underexplored due to\nchallenges such as inferring preferences from long, noisy, and implicit\ncontexts, and generating responses that are simultaneously correct,\ncontextually appropriate, and aligned with user expectations and background\nknowledge. To address these challenges, we propose Pathways of Thoughts (PoT),\nan inference-stage method that applies to any large language model (LLM)\nwithout requiring task-specific fine-tuning. The approach models the reasoning\nof an LLM as an iterative decision process, where the model dynamically selects\namong cognitive operations such as reasoning, revision, personalization, and\nclarification. This enables exploration of multiple reasoning trajectories,\nproducing diverse candidate responses that capture different perspectives. PoT\nthen aggregates and reweights these candidates according to inferred user\npreferences, yielding a final personalized response that benefits from the\ncomplementary strengths of diverse reasoning paths. Experiments on the LaMP-QA\nbenchmark for personalized QA show that PoT consistently outperforms\ncompetitive baselines, achieving up to a 13.1% relative improvement. Human\nevaluation corroborates these results, with annotators preferring outputs from\nPoT in 66% of cases and reporting ties in only 15% of cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization is essential for adapting question answering (QA) systems to\nuser-specific information needs, thereby improving both accuracy and user\nsatisfaction. However, personalized QA remains relatively underexplored due to\nchallenges such as inferring preferences from long, noisy, and implicit\ncontexts, and generating responses that are simultaneously correct,\ncontextually appropriate, and aligned with user expectations and background\nknowledge. To address these challenges, we propose Pathways of Thoughts (PoT),\nan inference-stage method that applies to any large language model (LLM)\nwithout requiring task-specific fine-tuning. The approach models the reasoning\nof an LLM as an iterative decision process, where the model dynamically selects\namong cognitive operations such as reasoning, revision, personalization, and\nclarification. This enables exploration of multiple reasoning trajectories,\nproducing diverse candidate responses that capture different perspectives. PoT\nthen aggregates and reweights these candidates according to inferred user\npreferences, yielding a final personalized response that benefits from the\ncomplementary strengths of diverse reasoning paths. Experiments on the LaMP-QA\nbenchmark for personalized QA show that PoT consistently outperforms\ncompetitive baselines, achieving up to a 13.1% relative improvement. Human\nevaluation corroborates these results, with annotators preferring outputs from\nPoT in 66% of cases and reporting ties in only 15% of cases."
                },
                "authors": [
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Qiaozhu Mei"
                    },
                    {
                        "name": "Zhuowan Li"
                    },
                    {
                        "name": "Spurthi Amba Hombaiah"
                    },
                    {
                        "name": "Weize Kong"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Michael Bendersky"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bendersky"
                },
                "author": "Michael Bendersky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19088v1",
                "updated": "2025-09-23T14:42:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    42,
                    14,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:42:14Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    42,
                    14,
                    1,
                    266,
                    0
                ],
                "title": "A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and\n  Opportunities for Further Improvement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and\n  Opportunities for Further Improvement"
                },
                "summary": "Do \"digital twins\" capture individual responses in surveys and experiments?\nWe run 19 pre-registered studies on a national U.S. panel and their LLM-powered\ndigital twins (constructed based on previously-collected extensive\nindividual-level data) and compare twin and human answers across 164 outcomes.\nThe correlation between twin and human answers is modest (approximately 0.2 on\naverage) and twin responses are less variable than human responses. While\nconstructing digital twins based on rich individual-level data improves our\nability to capture heterogeneity across participants and predict relative\ndifferences between them, it does not substantially improve our ability to\npredict the exact answers given by specific participants or enhance predictions\nof population means. Twin performance varies by domain and is higher among more\neducated, higher-income, and ideologically moderate participants. These results\nsuggest current digital twins can capture some degree of relative differences\nbut are unreliable for individual-level predictions and sample mean and\nvariance estimation, underscoring the need for careful validation before use.\nOur data and code are publicly available for researchers and practitioners\ninterested in optimizing digital twin pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do \"digital twins\" capture individual responses in surveys and experiments?\nWe run 19 pre-registered studies on a national U.S. panel and their LLM-powered\ndigital twins (constructed based on previously-collected extensive\nindividual-level data) and compare twin and human answers across 164 outcomes.\nThe correlation between twin and human answers is modest (approximately 0.2 on\naverage) and twin responses are less variable than human responses. While\nconstructing digital twins based on rich individual-level data improves our\nability to capture heterogeneity across participants and predict relative\ndifferences between them, it does not substantially improve our ability to\npredict the exact answers given by specific participants or enhance predictions\nof population means. Twin performance varies by domain and is higher among more\neducated, higher-income, and ideologically moderate participants. These results\nsuggest current digital twins can capture some degree of relative differences\nbut are unreliable for individual-level predictions and sample mean and\nvariance estimation, underscoring the need for careful validation before use.\nOur data and code are publicly available for researchers and practitioners\ninterested in optimizing digital twin pipelines."
                },
                "authors": [
                    {
                        "name": "Tiany Peng"
                    },
                    {
                        "name": "George Gui"
                    },
                    {
                        "name": "Daniel J. Merlau"
                    },
                    {
                        "name": "Grace Jiarui Fan"
                    },
                    {
                        "name": "Malek Ben Sliman"
                    },
                    {
                        "name": "Melanie Brucks"
                    },
                    {
                        "name": "Eric J. Johnson"
                    },
                    {
                        "name": "Vicki Morwitz"
                    },
                    {
                        "name": "Abdullah Althenayyan"
                    },
                    {
                        "name": "Silvia Bellezza"
                    },
                    {
                        "name": "Dante Donati"
                    },
                    {
                        "name": "Hortense Fong"
                    },
                    {
                        "name": "Elizabeth Friedman"
                    },
                    {
                        "name": "Ariana Guevara"
                    },
                    {
                        "name": "Mohamed Hussein"
                    },
                    {
                        "name": "Kinshuk Jerath"
                    },
                    {
                        "name": "Bruce Kogut"
                    },
                    {
                        "name": "Kristen Lane"
                    },
                    {
                        "name": "Hannah Li"
                    },
                    {
                        "name": "Patryk Perkowski"
                    },
                    {
                        "name": "Oded Netzer"
                    },
                    {
                        "name": "Olivier Toubia"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Toubia"
                },
                "author": "Olivier Toubia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19077v1",
                "updated": "2025-09-23T14:36:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    36,
                    12,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:36:12Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    36,
                    12,
                    1,
                    266,
                    0
                ],
                "title": "Code Driven Planning with Domain-Adaptive Critic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Driven Planning with Domain-Adaptive Critic"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs."
                },
                "authors": [
                    {
                        "name": "Zikang Tian"
                    },
                    {
                        "name": "Shaohui Peng"
                    },
                    {
                        "name": "Du Huang"
                    },
                    {
                        "name": "Jiaming Guo"
                    },
                    {
                        "name": "Ruizhi Chen"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Xishan Zhang"
                    },
                    {
                        "name": "Yuxuan Guo"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Ling Li"
                    },
                    {
                        "name": "Yewen Pu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Yunji Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yunji Chen"
                },
                "author": "Yunji Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17544v2",
                "updated": "2025-09-23T14:32:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    32,
                    50,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T09:02:53Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    9,
                    2,
                    53,
                    0,
                    265,
                    0
                ],
                "title": "A Multimodal Conversational Assistant for the Characterization of\n  Agricultural Plots from Geospatial Open Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multimodal Conversational Assistant for the Characterization of\n  Agricultural Plots from Geospatial Open Data"
                },
                "summary": "The increasing availability of open Earth Observation (EO) and agricultural\ndatasets holds great potential for supporting sustainable land management.\nHowever, their high technical entry barrier limits accessibility for non-expert\nusers. This study presents an open-source conversational assistant that\nintegrates multimodal retrieval and large language models (LLMs) to enable\nnatural language interaction with heterogeneous agricultural and geospatial\ndata. The proposed architecture combines orthophotos, Sentinel-2 vegetation\nindices, and user-provided documents through retrieval-augmented generation\n(RAG), allowing the system to flexibly determine whether to rely on multimodal\nevidence, textual knowledge, or both in formulating an answer. To assess\nresponse quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a\nzero-shot, unsupervised setting, applying direct scoring in a multi-dimensional\nquantitative evaluation framework. Preliminary results show that the system is\ncapable of generating clear, relevant, and context-aware responses to\nagricultural queries, while remaining reproducible and scalable across\ngeographic regions. The primary contributions of this work include an\narchitecture for fusing multimodal EO and textual knowledge sources, a\ndemonstration of lowering the barrier to access specialized agricultural\ninformation through natural language interaction, and an open and reproducible\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of open Earth Observation (EO) and agricultural\ndatasets holds great potential for supporting sustainable land management.\nHowever, their high technical entry barrier limits accessibility for non-expert\nusers. This study presents an open-source conversational assistant that\nintegrates multimodal retrieval and large language models (LLMs) to enable\nnatural language interaction with heterogeneous agricultural and geospatial\ndata. The proposed architecture combines orthophotos, Sentinel-2 vegetation\nindices, and user-provided documents through retrieval-augmented generation\n(RAG), allowing the system to flexibly determine whether to rely on multimodal\nevidence, textual knowledge, or both in formulating an answer. To assess\nresponse quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a\nzero-shot, unsupervised setting, applying direct scoring in a multi-dimensional\nquantitative evaluation framework. Preliminary results show that the system is\ncapable of generating clear, relevant, and context-aware responses to\nagricultural queries, while remaining reproducible and scalable across\ngeographic regions. The primary contributions of this work include an\narchitecture for fusing multimodal EO and textual knowledge sources, a\ndemonstration of lowering the barrier to access specialized agricultural\ninformation through natural language interaction, and an open and reproducible\ndesign."
                },
                "authors": [
                    {
                        "name": "Juan CaÃ±ada"
                    },
                    {
                        "name": "RaÃºl Alonso"
                    },
                    {
                        "name": "Julio Molleda"
                    },
                    {
                        "name": "Fidel DÃ­ez"
                    }
                ],
                "author_detail": {
                    "name": "Fidel DÃ­ez"
                },
                "author": "Fidel DÃ­ez",
                "arxiv_comment": "Accepted at 2025 4th International Conference on Geographic\n  Information and Remote Sensing Technology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15173v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15173v3",
                "updated": "2025-09-23T14:29:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    29,
                    40,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-21T06:43:34Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    6,
                    43,
                    34,
                    2,
                    141,
                    0
                ],
                "title": "AvatarShield: Visual Reinforcement Learning for Human-Centric Synthetic\n  Video Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AvatarShield: Visual Reinforcement Learning for Human-Centric Synthetic\n  Video Detection"
                },
                "summary": "Recent advances in Artificial Intelligence Generated Content have led to\nhighly realistic synthetic videos, particularly in human-centric scenarios\ninvolving speech, gestures, and full-body motion, posing serious threats to\ninformation authenticity and public trust. Unlike DeepFake techniques that\nfocus on localized facial manipulation, human-centric video generation methods\ncan synthesize entire human bodies with controllable movements, enabling\ncomplex interactions with environments, objects, and even other people.\nHowever, existing detection methods largely overlook the growing risks posed by\nsuch full-body synthetic content. Meanwhile, a growing body of research has\nexplored leveraging LLMs for interpretable fake detection, aiming to explain\ndecisions in natural language. Yet these approaches heavily depend on\nsupervised fine-tuning, which introduces limitations such as annotation bias,\nhallucinated supervision, and weakened generalization. To address these\nchallenges, we propose AvatarShield, a novel multimodal human-centric synthetic\nvideo detection framework that eliminates the need for dense textual\nsupervision by adopting Group Relative Policy Optimization, enabling LLMs to\ndevelop reasoning capabilities from simple binary labels. Our architecture\ncombines a discrete vision tower for high-level semantic inconsistencies and a\nresidual extractor for fine-grained artifact analysis. We further introduce\nFakeHumanVid, a large-scale benchmark containing 15K real and synthetic videos\nacross nine state-of-the-art human generation methods driven by text, pose, or\naudio. Extensive experiments demonstrate that AvatarShield outperforms existing\nmethods in both in-domain and cross-domain settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Artificial Intelligence Generated Content have led to\nhighly realistic synthetic videos, particularly in human-centric scenarios\ninvolving speech, gestures, and full-body motion, posing serious threats to\ninformation authenticity and public trust. Unlike DeepFake techniques that\nfocus on localized facial manipulation, human-centric video generation methods\ncan synthesize entire human bodies with controllable movements, enabling\ncomplex interactions with environments, objects, and even other people.\nHowever, existing detection methods largely overlook the growing risks posed by\nsuch full-body synthetic content. Meanwhile, a growing body of research has\nexplored leveraging LLMs for interpretable fake detection, aiming to explain\ndecisions in natural language. Yet these approaches heavily depend on\nsupervised fine-tuning, which introduces limitations such as annotation bias,\nhallucinated supervision, and weakened generalization. To address these\nchallenges, we propose AvatarShield, a novel multimodal human-centric synthetic\nvideo detection framework that eliminates the need for dense textual\nsupervision by adopting Group Relative Policy Optimization, enabling LLMs to\ndevelop reasoning capabilities from simple binary labels. Our architecture\ncombines a discrete vision tower for high-level semantic inconsistencies and a\nresidual extractor for fine-grained artifact analysis. We further introduce\nFakeHumanVid, a large-scale benchmark containing 15K real and synthetic videos\nacross nine state-of-the-art human generation methods driven by text, pose, or\naudio. Extensive experiments demonstrate that AvatarShield outperforms existing\nmethods in both in-domain and cross-domain settings."
                },
                "authors": [
                    {
                        "name": "Zhipei Xu"
                    },
                    {
                        "name": "Xuanyu Zhang"
                    },
                    {
                        "name": "Qing Huang"
                    },
                    {
                        "name": "Xing Zhou"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15173v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15173v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23577v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23577v2",
                "updated": "2025-09-23T14:22:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    22,
                    4,
                    1,
                    266,
                    0
                ],
                "published": "2025-07-31T14:08:04Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    14,
                    8,
                    4,
                    3,
                    212,
                    0
                ],
                "title": "T-Detect: Tail-Aware Statistical Normalization for Robust Detection of\n  Adversarial Machine-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T-Detect: Tail-Aware Statistical Normalization for Robust Detection of\n  Adversarial Machine-Generated Text"
                },
                "summary": "Large language models (LLMs) have shown the capability to generate fluent and\nlogical content, presenting significant challenges to machine-generated text\ndetection, particularly text polished by adversarial perturbations such as\nparaphrasing. Current zero-shot detectors often employ Gaussian distributions\nas statistical measure for computing detection thresholds, which falters when\nconfronted with the heavy-tailed statistical artifacts characteristic of\nadversarial or non-native English texts. In this paper, we introduce T-Detect,\na novel detection method that fundamentally redesigns the curvature-based\ndetectors. Our primary innovation is the replacement of standard Gaussian\nnormalization with a heavy-tailed discrepancy score derived from the Student's\nt-distribution. This approach is theoretically grounded in the empirical\nobservation that adversarial texts exhibit significant leptokurtosis, rendering\ntraditional statistical assumptions inadequate. T-Detect computes a detection\nscore by normalizing the log-likelihood of a passage against the expected\nmoments of a t-distribution, providing superior resilience to statistical\noutliers. We validate our approach on the challenging RAID benchmark for\nadversarial text and the comprehensive HART dataset. Experiments show that\nT-Detect provides a consistent performance uplift over strong baselines,\nimproving AUROC by up to 3.9\\% in targeted domains. When integrated into a\ntwo-dimensional detection framework (CT), our method achieves state-of-the-art\nperformance, with an AUROC of 0.926 on the Books domain of RAID. Our\ncontributions are a new, theoretically-justified statistical foundation for\ntext detection, an ablation-validated method that demonstrates superior\nrobustness, and a comprehensive analysis of its performance under adversarial\nconditions. Ours code are released at https://github.com/ResearAI/t-detect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown the capability to generate fluent and\nlogical content, presenting significant challenges to machine-generated text\ndetection, particularly text polished by adversarial perturbations such as\nparaphrasing. Current zero-shot detectors often employ Gaussian distributions\nas statistical measure for computing detection thresholds, which falters when\nconfronted with the heavy-tailed statistical artifacts characteristic of\nadversarial or non-native English texts. In this paper, we introduce T-Detect,\na novel detection method that fundamentally redesigns the curvature-based\ndetectors. Our primary innovation is the replacement of standard Gaussian\nnormalization with a heavy-tailed discrepancy score derived from the Student's\nt-distribution. This approach is theoretically grounded in the empirical\nobservation that adversarial texts exhibit significant leptokurtosis, rendering\ntraditional statistical assumptions inadequate. T-Detect computes a detection\nscore by normalizing the log-likelihood of a passage against the expected\nmoments of a t-distribution, providing superior resilience to statistical\noutliers. We validate our approach on the challenging RAID benchmark for\nadversarial text and the comprehensive HART dataset. Experiments show that\nT-Detect provides a consistent performance uplift over strong baselines,\nimproving AUROC by up to 3.9\\% in targeted domains. When integrated into a\ntwo-dimensional detection framework (CT), our method achieves state-of-the-art\nperformance, with an AUROC of 0.926 on the Books domain of RAID. Our\ncontributions are a new, theoretically-justified statistical foundation for\ntext detection, an ablation-validated method that demonstrates superior\nrobustness, and a comprehensive analysis of its performance under adversarial\nconditions. Ours code are released at https://github.com/ResearAI/t-detect."
                },
                "authors": [
                    {
                        "name": "Alva West"
                    },
                    {
                        "name": "Luodan Zhang"
                    },
                    {
                        "name": "Liuliu Zhang"
                    },
                    {
                        "name": "Minjun Zhu"
                    },
                    {
                        "name": "Yixuan Weng"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23577v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23577v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19057v1",
                "updated": "2025-09-23T14:21:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    21,
                    46,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:21:46Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    21,
                    46,
                    1,
                    266,
                    0
                ],
                "title": "RELATE: Relation Extraction in Biomedical Abstracts with LLMs and\n  Ontology Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RELATE: Relation Extraction in Biomedical Abstracts with LLMs and\n  Ontology Constraints"
                },
                "summary": "Biomedical knowledge graphs (KGs) are vital for drug discovery and clinical\ndecision support but remain incomplete. Large language models (LLMs) excel at\nextracting biomedical relations, yet their outputs lack standardization and\nalignment with ontologies, limiting KG integration. We introduce RELATE, a\nthree-stage pipeline that maps LLM-extracted relations to standardized ontology\npredicates using ChemProt and the Biolink Model. The pipeline includes: (1)\nontology preprocessing with predicate embeddings, (2) similarity-based\nretrieval enhanced with SapBERT, and (3) LLM-based reranking with explicit\nnegation handling. This approach transforms relation extraction from free-text\noutputs to structured, ontology-constrained representations. On the ChemProt\nbenchmark, RELATE achieves 52% exact match and 94% accuracy@10, and in 2,400\nHEAL Project abstracts, it effectively rejects irrelevant associations (0.4%)\nand identifies negated assertions. RELATE captures nuanced biomedical\nrelationships while ensuring quality for KG augmentation. By combining vector\nsearch with contextual LLM reasoning, RELATE provides a scalable, semantically\naccurate framework for converting unstructured biomedical literature into\nstandardized KGs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical knowledge graphs (KGs) are vital for drug discovery and clinical\ndecision support but remain incomplete. Large language models (LLMs) excel at\nextracting biomedical relations, yet their outputs lack standardization and\nalignment with ontologies, limiting KG integration. We introduce RELATE, a\nthree-stage pipeline that maps LLM-extracted relations to standardized ontology\npredicates using ChemProt and the Biolink Model. The pipeline includes: (1)\nontology preprocessing with predicate embeddings, (2) similarity-based\nretrieval enhanced with SapBERT, and (3) LLM-based reranking with explicit\nnegation handling. This approach transforms relation extraction from free-text\noutputs to structured, ontology-constrained representations. On the ChemProt\nbenchmark, RELATE achieves 52% exact match and 94% accuracy@10, and in 2,400\nHEAL Project abstracts, it effectively rejects irrelevant associations (0.4%)\nand identifies negated assertions. RELATE captures nuanced biomedical\nrelationships while ensuring quality for KG augmentation. By combining vector\nsearch with contextual LLM reasoning, RELATE provides a scalable, semantically\naccurate framework for converting unstructured biomedical literature into\nstandardized KGs."
                },
                "authors": [
                    {
                        "name": "Olawumi Olasunkanmi"
                    },
                    {
                        "name": "Mathew Satursky"
                    },
                    {
                        "name": "Hong Yi"
                    },
                    {
                        "name": "Chris Bizon"
                    },
                    {
                        "name": "Harlin Lee"
                    },
                    {
                        "name": "Stanley Ahalt"
                    }
                ],
                "author_detail": {
                    "name": "Stanley Ahalt"
                },
                "author": "Stanley Ahalt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13232v2",
                "updated": "2025-09-23T14:19:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    19,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-16T16:39:11Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    16,
                    39,
                    11,
                    1,
                    259,
                    0
                ],
                "title": "Single-stream Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-stream Policy Optimization"
                },
                "summary": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@$k$ across the evaluated $k$ values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@$k$ across the evaluated $k$ values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning."
                },
                "authors": [
                    {
                        "name": "Zhongwen Xu"
                    },
                    {
                        "name": "Zihan Ding"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Ding"
                },
                "author": "Zihan Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19033v2",
                "updated": "2025-09-24T07:17:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    7,
                    17,
                    32,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-23T14:06:09Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    6,
                    9,
                    1,
                    266,
                    0
                ],
                "title": "Charting a Decade of Computational Linguistics in Italy: The CLiC-it\n  Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting a Decade of Computational Linguistics in Italy: The CLiC-it\n  Corpus"
                },
                "summary": "Over the past decade, Computational Linguistics (CL) and Natural Language\nProcessing (NLP) have evolved rapidly, especially with the advent of\nTransformer-based Large Language Models (LLMs). This shift has transformed\nresearch goals and priorities, from Lexical and Semantic Resources to Language\nModelling and Multimodality. In this study, we track the research trends of the\nItalian CL and NLP community through an analysis of the contributions to\nCLiC-it, arguably the leading Italian conference in the field. We compile the\nproceedings from the first 10 editions of the CLiC-it conference (from 2014 to\n2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its\nmetadata, including author provenance, gender, affiliations, and more, as well\nas the content of the papers themselves, which address various topics. Our goal\nis to provide the Italian and international research communities with valuable\ninsights into emerging trends and key developments over time, supporting\ninformed decisions and future directions in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past decade, Computational Linguistics (CL) and Natural Language\nProcessing (NLP) have evolved rapidly, especially with the advent of\nTransformer-based Large Language Models (LLMs). This shift has transformed\nresearch goals and priorities, from Lexical and Semantic Resources to Language\nModelling and Multimodality. In this study, we track the research trends of the\nItalian CL and NLP community through an analysis of the contributions to\nCLiC-it, arguably the leading Italian conference in the field. We compile the\nproceedings from the first 10 editions of the CLiC-it conference (from 2014 to\n2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its\nmetadata, including author provenance, gender, affiliations, and more, as well\nas the content of the papers themselves, which address various topics. Our goal\nis to provide the Italian and international research communities with valuable\ninsights into emerging trends and key developments over time, supporting\ninformed decisions and future directions in the field."
                },
                "authors": [
                    {
                        "name": "Chiara Alzetta"
                    },
                    {
                        "name": "Serena Auriemma"
                    },
                    {
                        "name": "Alessandro Bondielli"
                    },
                    {
                        "name": "Luca Dini"
                    },
                    {
                        "name": "Chiara Fazzone"
                    },
                    {
                        "name": "Alessio Miaschi"
                    },
                    {
                        "name": "Martina Miliani"
                    },
                    {
                        "name": "Marta Sartor"
                    }
                ],
                "author_detail": {
                    "name": "Marta Sartor"
                },
                "author": "Marta Sartor",
                "arxiv_comment": "Submitted to IJCoL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17078v2",
                "updated": "2025-09-23T14:02:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    2,
                    49,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-23T16:13:57Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    16,
                    13,
                    57,
                    5,
                    235,
                    0
                ],
                "title": "Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer\n  on Low-resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer\n  on Low-resource Languages"
                },
                "summary": "The current Large Language Models (LLMs) face significant challenges in\nimproving their performance on low-resource languages and urgently need\ndata-efficient methods without costly fine-tuning. From the perspective of\nlanguage-bridge, we propose a simple yet effective method, namely BridgeX-ICL,\nto improve the zero-shot Cross-lingual In-Context Learning (X-ICL) for\nlow-resource languages. Unlike existing works focusing on language-specific\nneurons, BridgeX-ICL explores whether sharing neurons can improve cross-lingual\nperformance in LLMs. We construct neuron probe data from the ground-truth MUSE\nbilingual dictionaries, and define a subset of language overlap neurons\naccordingly to ensure full activation of these anchored neurons. Subsequently,\nwe propose an HSIC-based metric to quantify LLMs' internal linguistic spectrum\nbased on overlapping neurons, guiding optimal bridge selection. The experiments\nconducted on 4 cross-lingual tasks and 15 language pairs from 7 diverse\nfamilies, covering both high-low and moderate-low pairs, validate the\neffectiveness of BridgeX-ICL and offer empirical insights into the underlying\nmultilingual mechanisms of LLMs. The code is publicly available at\nhttps://github.com/xuyuemei/BridgeX-ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Large Language Models (LLMs) face significant challenges in\nimproving their performance on low-resource languages and urgently need\ndata-efficient methods without costly fine-tuning. From the perspective of\nlanguage-bridge, we propose a simple yet effective method, namely BridgeX-ICL,\nto improve the zero-shot Cross-lingual In-Context Learning (X-ICL) for\nlow-resource languages. Unlike existing works focusing on language-specific\nneurons, BridgeX-ICL explores whether sharing neurons can improve cross-lingual\nperformance in LLMs. We construct neuron probe data from the ground-truth MUSE\nbilingual dictionaries, and define a subset of language overlap neurons\naccordingly to ensure full activation of these anchored neurons. Subsequently,\nwe propose an HSIC-based metric to quantify LLMs' internal linguistic spectrum\nbased on overlapping neurons, guiding optimal bridge selection. The experiments\nconducted on 4 cross-lingual tasks and 15 language pairs from 7 diverse\nfamilies, covering both high-low and moderate-low pairs, validate the\neffectiveness of BridgeX-ICL and offer empirical insights into the underlying\nmultilingual mechanisms of LLMs. The code is publicly available at\nhttps://github.com/xuyuemei/BridgeX-ICL."
                },
                "authors": [
                    {
                        "name": "Yuemei Xu"
                    },
                    {
                        "name": "Kexin Xu"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Ling Hu"
                    },
                    {
                        "name": "Lin Gui"
                    }
                ],
                "author_detail": {
                    "name": "Lin Gui"
                },
                "author": "Lin Gui",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01616v2",
                "updated": "2025-09-23T14:01:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    1,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-02T22:36:24Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    22,
                    36,
                    24,
                    4,
                    122,
                    0
                ],
                "title": "Phantora: Maximizing Code Reuse in Simulation-based Machine Learning\n  System Performance Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phantora: Maximizing Code Reuse in Simulation-based Machine Learning\n  System Performance Estimation"
                },
                "summary": "Modern machine learning (ML) training workloads place substantial demands on\nboth computational and communication resources. Consequently, accurate\nperformance estimation has become increasingly critical for guiding system\ndesign decisions, such as the selection of parallelization strategies, cluster\nconfigurations, and hardware provisioning. Existing simulation-based\nperformance estimation requires reimplementing the ML framework in a simulator,\nwhich demands significant manual effort and is hard to maintain as ML\nframeworks evolve rapidly.\n  This paper introduces Phantora, a hybrid GPU cluster simulator designed for\nperformance estimation of ML training workloads. Phantora executes unmodified\nML frameworks as is within a distributed, containerized environment. Each\ncontainer emulates the behavior of a GPU server in a large-scale cluster, while\nPhantora intercepts and simulates GPU- and communication-related operations to\nprovide high-fidelity performance estimation. We call this approach hybrid\nsimulation of ML systems, in contrast to traditional methods that simulate\nstatic workloads. The primary advantage of hybrid simulation is that it allows\ndirect reuse of ML framework source code in simulation, avoiding the need for\nreimplementation. Our evaluation shows that Phantora provides accuracy\ncomparable to static workload simulation while supporting three\nstate-of-the-art LLM training frameworks out-of-the-box. In addition, Phantora\noperates on a single GPU, eliminating the need for the resource-intensive trace\ncollection and workload extraction steps required by traditional trace-based\nsimulators. Phantora is open-sourced at https://github.com/QDelta/Phantora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern machine learning (ML) training workloads place substantial demands on\nboth computational and communication resources. Consequently, accurate\nperformance estimation has become increasingly critical for guiding system\ndesign decisions, such as the selection of parallelization strategies, cluster\nconfigurations, and hardware provisioning. Existing simulation-based\nperformance estimation requires reimplementing the ML framework in a simulator,\nwhich demands significant manual effort and is hard to maintain as ML\nframeworks evolve rapidly.\n  This paper introduces Phantora, a hybrid GPU cluster simulator designed for\nperformance estimation of ML training workloads. Phantora executes unmodified\nML frameworks as is within a distributed, containerized environment. Each\ncontainer emulates the behavior of a GPU server in a large-scale cluster, while\nPhantora intercepts and simulates GPU- and communication-related operations to\nprovide high-fidelity performance estimation. We call this approach hybrid\nsimulation of ML systems, in contrast to traditional methods that simulate\nstatic workloads. The primary advantage of hybrid simulation is that it allows\ndirect reuse of ML framework source code in simulation, avoiding the need for\nreimplementation. Our evaluation shows that Phantora provides accuracy\ncomparable to static workload simulation while supporting three\nstate-of-the-art LLM training frameworks out-of-the-box. In addition, Phantora\noperates on a single GPU, eliminating the need for the resource-intensive trace\ncollection and workload extraction steps required by traditional trace-based\nsimulators. Phantora is open-sourced at https://github.com/QDelta/Phantora."
                },
                "authors": [
                    {
                        "name": "Jianxing Qin"
                    },
                    {
                        "name": "Jingrong Chen"
                    },
                    {
                        "name": "Xinhao Kong"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Tianjun Yuan"
                    },
                    {
                        "name": "Liang Luo"
                    },
                    {
                        "name": "Zhaodong Wang"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Tingjun Chen"
                    },
                    {
                        "name": "Alvin R. Lebeck"
                    },
                    {
                        "name": "Danyang Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Danyang Zhuo"
                },
                "author": "Danyang Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03616v2",
                "updated": "2025-09-23T14:00:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    0,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-07-04T14:43:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    14,
                    43,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "EvoAgentX: An Automated Framework for Evolving Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoAgentX: An Automated Framework for Evolving Agentic Workflows"
                },
                "summary": "Multi-agent systems (MAS) have emerged as a powerful paradigm for\norchestrating large language models (LLMs) and specialized tools to\ncollaboratively address complex tasks. However, existing MAS frameworks often\nrequire manual workflow configuration and lack native support for dynamic\nevolution and performance optimization. In addition, many MAS optimization\nalgorithms are not integrated into a unified framework. In this paper, we\npresent EvoAgentX, an open-source platform that automates the generation,\nexecution, and evolutionary optimization of multi-agent workflows. EvoAgentX\nemploys a modular architecture consisting of five core layers: the basic\ncomponents, agent, workflow, evolving, and evaluation layers. Specifically,\nwithin the evolving layer, EvoAgentX integrates three MAS optimization\nalgorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts,\ntool configurations, and workflow topologies. We evaluate EvoAgentX on\nHotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and\nmathematical problem solving, respectively, and further assess it on real-world\ntasks using GAIA. Experimental results show that EvoAgentX consistently\nachieves significant performance improvements, including a 7.44% increase in\nHotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve\naccuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The\nsource code is available at: https://github.com/EvoAgentX/EvoAgentX",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) have emerged as a powerful paradigm for\norchestrating large language models (LLMs) and specialized tools to\ncollaboratively address complex tasks. However, existing MAS frameworks often\nrequire manual workflow configuration and lack native support for dynamic\nevolution and performance optimization. In addition, many MAS optimization\nalgorithms are not integrated into a unified framework. In this paper, we\npresent EvoAgentX, an open-source platform that automates the generation,\nexecution, and evolutionary optimization of multi-agent workflows. EvoAgentX\nemploys a modular architecture consisting of five core layers: the basic\ncomponents, agent, workflow, evolving, and evaluation layers. Specifically,\nwithin the evolving layer, EvoAgentX integrates three MAS optimization\nalgorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts,\ntool configurations, and workflow topologies. We evaluate EvoAgentX on\nHotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and\nmathematical problem solving, respectively, and further assess it on real-world\ntasks using GAIA. Experimental results show that EvoAgentX consistently\nachieves significant performance improvements, including a 7.44% increase in\nHotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve\naccuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The\nsource code is available at: https://github.com/EvoAgentX/EvoAgentX"
                },
                "authors": [
                    {
                        "name": "Yingxu Wang"
                    },
                    {
                        "name": "Siwei Liu"
                    },
                    {
                        "name": "Jinyuan Fang"
                    },
                    {
                        "name": "Zaiqiao Meng"
                    }
                ],
                "author_detail": {
                    "name": "Zaiqiao Meng"
                },
                "author": "Zaiqiao Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19018v1",
                "updated": "2025-09-23T13:57:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    57,
                    55,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:57:55Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    57,
                    55,
                    1,
                    266,
                    0
                ],
                "title": "OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval\n  via Latent Space Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval\n  via Latent Space Alignment"
                },
                "summary": "Recent advances in multimodal large language models (LLMs) have led to\nsignificant progress in understanding, generation, and retrieval tasks.\nHowever, current solutions often treat these tasks in isolation or require\ntraining LLMs from scratch, resulting in high computational costs and limited\ngeneralization across modalities. In this work, we present OmniBridge, a\nunified and modular multimodal framework that supports vision-language\nunderstanding, generation, and retrieval within a unified architecture.\nOmniBridge adopts a language-centric design that reuses pretrained LLMs and\nintroduces a lightweight bidirectional latent alignment module. To address the\nchallenge of task interference, we propose a two-stage decoupled training\nstrategy: supervised fine-tuning and latent space alignment for aligning LLM\nbehavior with multimodal reasoning, and semantic-guided diffusion training to\nalign cross-modal latent spaces via learnable query embeddings. Extensive\nexperiments across a wide range of benchmarks demonstrate that OmniBridge\nachieves competitive or state-of-the-art performance in all three tasks.\nMoreover, our results highlight the effectiveness of latent space alignment for\nunifying multimodal modeling under a shared representation space. Code and\nmodels are released at https://github.com/xiao-xt/OmniBridge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal large language models (LLMs) have led to\nsignificant progress in understanding, generation, and retrieval tasks.\nHowever, current solutions often treat these tasks in isolation or require\ntraining LLMs from scratch, resulting in high computational costs and limited\ngeneralization across modalities. In this work, we present OmniBridge, a\nunified and modular multimodal framework that supports vision-language\nunderstanding, generation, and retrieval within a unified architecture.\nOmniBridge adopts a language-centric design that reuses pretrained LLMs and\nintroduces a lightweight bidirectional latent alignment module. To address the\nchallenge of task interference, we propose a two-stage decoupled training\nstrategy: supervised fine-tuning and latent space alignment for aligning LLM\nbehavior with multimodal reasoning, and semantic-guided diffusion training to\nalign cross-modal latent spaces via learnable query embeddings. Extensive\nexperiments across a wide range of benchmarks demonstrate that OmniBridge\nachieves competitive or state-of-the-art performance in all three tasks.\nMoreover, our results highlight the effectiveness of latent space alignment for\nunifying multimodal modeling under a shared representation space. Code and\nmodels are released at https://github.com/xiao-xt/OmniBridge."
                },
                "authors": [
                    {
                        "name": "Teng Xiao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lefei Zhang"
                },
                "author": "Lefei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18555v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18555v2",
                "updated": "2025-09-23T13:50:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    50,
                    21,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-24T06:45:45Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    6,
                    45,
                    45,
                    5,
                    144,
                    0
                ],
                "title": "Unraveling Misinformation Propagation in LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling Misinformation Propagation in LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning, positioning them as promising tools for supporting human\nproblem-solving. However, what happens when their performance is affected by\nmisinformation, i.e., incorrect inputs introduced by users due to oversights or\ngaps in knowledge? Such misinformation is prevalent in real-world interactions\nwith LLMs, yet how it propagates within LLMs' reasoning process remains\nunderexplored. Focusing on mathematical reasoning, we present a comprehensive\nanalysis of how misinformation affects intermediate reasoning steps and final\nanswers. We also examine how effectively LLMs can correct misinformation when\nexplicitly instructed to do so. Even with explicit instructions, LLMs succeed\nless than half the time in rectifying misinformation, despite possessing\ncorrect internal knowledge, leading to significant accuracy drops (10.02% -\n72.20%), and the degradation holds with thinking models (4.30% - 19.97%).\nFurther analysis shows that applying factual corrections early in the reasoning\nprocess most effectively reduces misinformation propagation, and fine-tuning on\nsynthesized data with early-stage corrections significantly improves reasoning\nfactuality. Our work offers a practical approach to mitigating misinformation\npropagation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning, positioning them as promising tools for supporting human\nproblem-solving. However, what happens when their performance is affected by\nmisinformation, i.e., incorrect inputs introduced by users due to oversights or\ngaps in knowledge? Such misinformation is prevalent in real-world interactions\nwith LLMs, yet how it propagates within LLMs' reasoning process remains\nunderexplored. Focusing on mathematical reasoning, we present a comprehensive\nanalysis of how misinformation affects intermediate reasoning steps and final\nanswers. We also examine how effectively LLMs can correct misinformation when\nexplicitly instructed to do so. Even with explicit instructions, LLMs succeed\nless than half the time in rectifying misinformation, despite possessing\ncorrect internal knowledge, leading to significant accuracy drops (10.02% -\n72.20%), and the degradation holds with thinking models (4.30% - 19.97%).\nFurther analysis shows that applying factual corrections early in the reasoning\nprocess most effectively reduces misinformation propagation, and fine-tuning on\nsynthesized data with early-stage corrections significantly improves reasoning\nfactuality. Our work offers a practical approach to mitigating misinformation\npropagation."
                },
                "authors": [
                    {
                        "name": "Yiyang Feng"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Shaobo Cui"
                    },
                    {
                        "name": "Boi Faltings"
                    },
                    {
                        "name": "Mina Lee"
                    },
                    {
                        "name": "Jiawei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhou"
                },
                "author": "Jiawei Zhou",
                "arxiv_comment": "Accepted to EMNLP 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18555v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18555v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19001v1",
                "updated": "2025-09-23T13:45:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    45,
                    56,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:45:56Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    45,
                    56,
                    1,
                    266,
                    0
                ],
                "title": "HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens\n  for Instruction-based TTS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens\n  for Instruction-based TTS"
                },
                "summary": "Large Language Model (LLM)-based Text-to-Speech (TTS) models have already\nreached a high degree of naturalness. However, the precision control of TTS\ninference is still challenging. Although instruction-based Text-to-Speech\n(Instruct-TTS) models are proposed, these models still lack fine-grained\ncontrol due to the modality gap between single-level text instructions and\nmultilevel speech tokens. To address this limitation, we propose HD-PPT, a\nframework that transforms speech synthesis into a structured, hierarchical\ntask. To enable fine-grained control, we introduce a novel speech codec to\nextract distinct prompt-preference and content-preference tokens from the\ncomplex speech tokens, supervised by automatic speech recognition (ASR) and\ncross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality\ngap of these tokens, we propose a hierarchical decoding strategy, where the LLM\ngenerates tokens in a structured order: first semantic, then fine-grained\nstyle, and finally complete acoustic representation. Extensive experiments\ndemonstrate that this hierarchical paradigm significantly improves instruction\nadherence and achieves state-of-the-art naturalness, validating our approach\nfor precise and controllable speech synthesis. Audio samples are available at\nhttps://xxh333.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based Text-to-Speech (TTS) models have already\nreached a high degree of naturalness. However, the precision control of TTS\ninference is still challenging. Although instruction-based Text-to-Speech\n(Instruct-TTS) models are proposed, these models still lack fine-grained\ncontrol due to the modality gap between single-level text instructions and\nmultilevel speech tokens. To address this limitation, we propose HD-PPT, a\nframework that transforms speech synthesis into a structured, hierarchical\ntask. To enable fine-grained control, we introduce a novel speech codec to\nextract distinct prompt-preference and content-preference tokens from the\ncomplex speech tokens, supervised by automatic speech recognition (ASR) and\ncross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality\ngap of these tokens, we propose a hierarchical decoding strategy, where the LLM\ngenerates tokens in a structured order: first semantic, then fine-grained\nstyle, and finally complete acoustic representation. Extensive experiments\ndemonstrate that this hierarchical paradigm significantly improves instruction\nadherence and achieves state-of-the-art naturalness, validating our approach\nfor precise and controllable speech synthesis. Audio samples are available at\nhttps://xxh333.github.io/."
                },
                "authors": [
                    {
                        "name": "Sihang Nie"
                    },
                    {
                        "name": "Xiaofen Xing"
                    },
                    {
                        "name": "Jingyuan Xing"
                    },
                    {
                        "name": "Baiji Liu"
                    },
                    {
                        "name": "Xiangmin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangmin Xu"
                },
                "author": "Xiangmin Xu",
                "arxiv_comment": "5 pages, 2 figures, submitted to ICASSP2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18993v1",
                "updated": "2025-09-23T13:43:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    43,
                    2,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:43:02Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    43,
                    2,
                    1,
                    266,
                    0
                ],
                "title": "CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank\n  Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank\n  Structure"
                },
                "summary": "Low-rank architectures have become increasingly important for efficient large\nlanguage model (LLM) pre-training, providing substantial reductions in both\nparameter complexity and memory/computational demands. Despite these\nadvantages, current low-rank methods face three critical shortcomings: (1)\ncompromised model performance, (2) considerable computational overhead, and (3)\nlimited activation memory savings. To address these limitations, we propose\nCross-layer Low-Rank residual Network (CR-Net), an innovative\nparameter-efficient framework inspired by our discovery that inter-layer\nactivation residuals possess low-rank properties. CR-Net implements this\ninsight through a dual-path architecture that efficiently reconstructs layer\nactivations by combining previous-layer outputs with their low-rank\ndifferences, thereby maintaining high-rank information with minimal parameters.\nWe further develop a specialized activation recomputation strategy tailored for\nCR-Net that dramatically reduces memory requirements. Extensive pre-training\nexperiments across model scales from 60M to 7B parameters demonstrate that\nCR-Net consistently outperforms state-of-the-art low-rank frameworks while\nrequiring fewer computational resources and less memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank architectures have become increasingly important for efficient large\nlanguage model (LLM) pre-training, providing substantial reductions in both\nparameter complexity and memory/computational demands. Despite these\nadvantages, current low-rank methods face three critical shortcomings: (1)\ncompromised model performance, (2) considerable computational overhead, and (3)\nlimited activation memory savings. To address these limitations, we propose\nCross-layer Low-Rank residual Network (CR-Net), an innovative\nparameter-efficient framework inspired by our discovery that inter-layer\nactivation residuals possess low-rank properties. CR-Net implements this\ninsight through a dual-path architecture that efficiently reconstructs layer\nactivations by combining previous-layer outputs with their low-rank\ndifferences, thereby maintaining high-rank information with minimal parameters.\nWe further develop a specialized activation recomputation strategy tailored for\nCR-Net that dramatically reduces memory requirements. Extensive pre-training\nexperiments across model scales from 60M to 7B parameters demonstrate that\nCR-Net consistently outperforms state-of-the-art low-rank frameworks while\nrequiring fewer computational resources and less memory."
                },
                "authors": [
                    {
                        "name": "Boao Kong"
                    },
                    {
                        "name": "Junzhu Liang"
                    },
                    {
                        "name": "Yuxi Liu"
                    },
                    {
                        "name": "Renjia Deng"
                    },
                    {
                        "name": "Kun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yuan"
                },
                "author": "Kun Yuan",
                "arxiv_comment": "32 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18985v1",
                "updated": "2025-09-23T13:36:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    36,
                    48,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:36:48Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    36,
                    48,
                    1,
                    266,
                    0
                ],
                "title": "Simulating Online Social Media Conversations on Controversial Topics\n  Using AI Agents Calibrated on Real-World Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Online Social Media Conversations on Controversial Topics\n  Using AI Agents Calibrated on Real-World Data"
                },
                "summary": "Online social networks offer a valuable lens to analyze both individual and\ncollective phenomena. Researchers often use simulators to explore controlled\nscenarios, and the integration of Large Language Models (LLMs) makes these\nsimulations more realistic by enabling agents to understand and generate\nnatural language content. In this work, we investigate the behavior of\nLLM-based agents in a simulated microblogging social network. We initialize\nagents with realistic profiles calibrated on real-world online conversations\nfrom the 2022 Italian political election and extend an existing simulator by\nintroducing mechanisms for opinion modeling. We examine how LLM agents simulate\nonline conversations, interact with others, and evolve their opinions under\ndifferent scenarios. Our results show that LLM agents generate coherent\ncontent, form connections, and build a realistic social network structure.\nHowever, their generated content displays less heterogeneity in tone and\ntoxicity compared to real data. We also find that LLM-based opinion dynamics\nevolve over time in ways similar to traditional mathematical models. Varying\nparameter configurations produces no significant changes, indicating that\nsimulations require more careful cognitive modeling at initialization to\nreplicate human behavior more faithfully. Overall, we demonstrate the potential\nof LLMs for simulating user behavior in social environments, while also\nidentifying key challenges in capturing heterogeneity and complex dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online social networks offer a valuable lens to analyze both individual and\ncollective phenomena. Researchers often use simulators to explore controlled\nscenarios, and the integration of Large Language Models (LLMs) makes these\nsimulations more realistic by enabling agents to understand and generate\nnatural language content. In this work, we investigate the behavior of\nLLM-based agents in a simulated microblogging social network. We initialize\nagents with realistic profiles calibrated on real-world online conversations\nfrom the 2022 Italian political election and extend an existing simulator by\nintroducing mechanisms for opinion modeling. We examine how LLM agents simulate\nonline conversations, interact with others, and evolve their opinions under\ndifferent scenarios. Our results show that LLM agents generate coherent\ncontent, form connections, and build a realistic social network structure.\nHowever, their generated content displays less heterogeneity in tone and\ntoxicity compared to real data. We also find that LLM-based opinion dynamics\nevolve over time in ways similar to traditional mathematical models. Varying\nparameter configurations produces no significant changes, indicating that\nsimulations require more careful cognitive modeling at initialization to\nreplicate human behavior more faithfully. Overall, we demonstrate the potential\nof LLMs for simulating user behavior in social environments, while also\nidentifying key challenges in capturing heterogeneity and complex dynamics."
                },
                "authors": [
                    {
                        "name": "Elisa Composta"
                    },
                    {
                        "name": "Nicolo' Fontana"
                    },
                    {
                        "name": "Francesco Corso"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11079v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11079v2",
                "updated": "2025-09-23T13:32:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    32,
                    37,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-14T03:57:43Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    3,
                    57,
                    43,
                    6,
                    257,
                    0
                ],
                "title": "Difficulty-Aware Agent Orchestration in LLM-Powered Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Difficulty-Aware Agent Orchestration in LLM-Powered Workflows"
                },
                "summary": "Large Language Model (LLM)-based agentic systems have shown strong\ncapabilities across various tasks. However, existing multi-agent frameworks\noften rely on static or task-level workflows, which either over-process simple\nqueries or underperform on complex ones, while also neglecting the\nefficiency-performance trade-offs across heterogeneous LLMs. To address these\nlimitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a\ndynamic framework that adapts workflow depth, operator selection, and LLM\nassignment based on the difficulty of each input query. DAAO comprises three\ninterdependent modules: a variational autoencoder (VAE) for difficulty\nestimation, a modular operator allocator, and a cost- and performance-aware LLM\nrouter. By leveraging heterogeneous LLMs and dynamically tailoring workflows,\nDAAO enables fine-grained, query-specific reasoning strategies. DAAO\noutperforms prior multi-agent systems in both accuracy and inference efficiency\nacross six benchmarks. We will release our code and implementation details upon\npublication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agentic systems have shown strong\ncapabilities across various tasks. However, existing multi-agent frameworks\noften rely on static or task-level workflows, which either over-process simple\nqueries or underperform on complex ones, while also neglecting the\nefficiency-performance trade-offs across heterogeneous LLMs. To address these\nlimitations, we propose Difficulty-Aware Agentic Orchestration (DAAO), a\ndynamic framework that adapts workflow depth, operator selection, and LLM\nassignment based on the difficulty of each input query. DAAO comprises three\ninterdependent modules: a variational autoencoder (VAE) for difficulty\nestimation, a modular operator allocator, and a cost- and performance-aware LLM\nrouter. By leveraging heterogeneous LLMs and dynamically tailoring workflows,\nDAAO enables fine-grained, query-specific reasoning strategies. DAAO\noutperforms prior multi-agent systems in both accuracy and inference efficiency\nacross six benchmarks. We will release our code and implementation details upon\npublication."
                },
                "authors": [
                    {
                        "name": "Jinwei Su"
                    },
                    {
                        "name": "Yinghui Xia"
                    },
                    {
                        "name": "Qizhen Lan"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yang Jingsong"
                    },
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Tianyu Shi"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Shi"
                },
                "author": "Tianyu Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11079v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11079v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13978v2",
                "updated": "2025-09-23T13:31:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    31,
                    18,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-17T13:51:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture\n  and Evaluation Methodology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agents for Interactive Workflow Provenance: Reference Architecture\n  and Evaluation Methodology"
                },
                "summary": "Modern scientific discovery increasingly relies on workflows that process\ndata across the Edge, Cloud, and High Performance Computing (HPC) continuum.\nComprehensive and in-depth analyses of these data are critical for hypothesis\nvalidation, anomaly detection, reproducibility, and impactful findings.\nAlthough workflow provenance techniques support such analyses, at large scale,\nthe provenance data become complex and difficult to analyze. Existing systems\ndepend on custom scripts, structured queries, or static dashboards, limiting\ndata interaction. In this work, we introduce an evaluation methodology,\nreference architecture, and open-source implementation that leverages\ninteractive Large Language Model (LLM) agents for runtime data analysis. Our\napproach uses a lightweight, metadata-driven design that translates natural\nlanguage into structured provenance queries. Evaluations across LLaMA, GPT,\nGemini, and Claude, covering diverse query classes and a real-world chemistry\nworkflow, show that modular design, prompt tuning, and Retrieval-Augmented\nGeneration (RAG) enable accurate and insightful LLM agent responses beyond\nrecorded provenance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern scientific discovery increasingly relies on workflows that process\ndata across the Edge, Cloud, and High Performance Computing (HPC) continuum.\nComprehensive and in-depth analyses of these data are critical for hypothesis\nvalidation, anomaly detection, reproducibility, and impactful findings.\nAlthough workflow provenance techniques support such analyses, at large scale,\nthe provenance data become complex and difficult to analyze. Existing systems\ndepend on custom scripts, structured queries, or static dashboards, limiting\ndata interaction. In this work, we introduce an evaluation methodology,\nreference architecture, and open-source implementation that leverages\ninteractive Large Language Model (LLM) agents for runtime data analysis. Our\napproach uses a lightweight, metadata-driven design that translates natural\nlanguage into structured provenance queries. Evaluations across LLaMA, GPT,\nGemini, and Claude, covering diverse query classes and a real-world chemistry\nworkflow, show that modular design, prompt tuning, and Retrieval-Augmented\nGeneration (RAG) enable accurate and insightful LLM agent responses beyond\nrecorded provenance."
                },
                "authors": [
                    {
                        "name": "Renan Souza"
                    },
                    {
                        "name": "Timothy Poteet"
                    },
                    {
                        "name": "Brian Etz"
                    },
                    {
                        "name": "Daniel Rosendo"
                    },
                    {
                        "name": "Amal Gueroudji"
                    },
                    {
                        "name": "Woong Shin"
                    },
                    {
                        "name": "Prasanna Balaprakash"
                    },
                    {
                        "name": "Rafael Ferreira da Silva"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Ferreira da Silva"
                },
                "author": "Rafael Ferreira da Silva",
                "arxiv_doi": "10.1145/3731599.3767582",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731599.3767582",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.13978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Paper accepted in the proceedings of the Supercomputing Conference\n  (SC). Cite it as Renan Souza, Timothy Poteet, Brian Etz, Daniel Rosendo, Amal\n  Gueroudji, Woong Shin, Prasanna Balaprakash, and Rafael Ferreira da Silva.\n  LLM Agents for Interactive Workflow Provenance: Reference Architecture and\n  Evaluation Methodology. In WORKS at the ACM/IEEE International Conference on\n  Supercomputing, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M14, 68M20, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; D.1.3; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18980v1",
                "updated": "2025-09-23T13:30:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    30,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:30:03Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    30,
                    3,
                    1,
                    266,
                    0
                ],
                "title": "From latent factors to language: a user study on LLM-generated\n  explanations for an inherently interpretable matrix-based recommender system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From latent factors to language: a user study on LLM-generated\n  explanations for an inherently interpretable matrix-based recommender system"
                },
                "summary": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves.To evaluate how different explanation strategies are\nperceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves.To evaluate how different explanation strategies are\nperceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results."
                },
                "authors": [
                    {
                        "name": "Maxime Manderlier"
                    },
                    {
                        "name": "Fabian Lecron"
                    },
                    {
                        "name": "Olivier Vu Thanh"
                    },
                    {
                        "name": "Nicolas Gillis"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Gillis"
                },
                "author": "Nicolas Gillis",
                "arxiv_journal_ref": "In Proceedings of the 12th Joint Workshop on Interfaces and Human\n  Decision Making for Recommender Systems (IntRS 2025) co-located with 19th ACM\n  Conference on Recommender Systems (RecSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18970v1",
                "updated": "2025-09-23T13:24:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    24,
                    48,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:24:48Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    24,
                    48,
                    1,
                    266,
                    0
                ],
                "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy,\n  Methods, and Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy,\n  Methods, and Directions"
                },
                "summary": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based\nagents have emerged as powerful intelligent systems capable of human-like\ncognition, reasoning, and interaction. These agents are increasingly being\ndeployed across diverse real-world applications, including student education,\nscientific research, and financial analysis. However, despite their remarkable\npotential, LLM-based agents remain vulnerable to hallucination issues, which\ncan result in erroneous task execution and undermine the reliability of the\noverall system design. Addressing this critical challenge requires a deep\nunderstanding and a systematic consolidation of recent advances on LLM-based\nagents. To this end, we present the first comprehensive survey of\nhallucinations in LLM-based agents. By carefully analyzing the complete\nworkflow of agents, we propose a new taxonomy that identifies different types\nof agent hallucinations occurring at different stages. Furthermore, we conduct\nan in-depth examination of eighteen triggering causes underlying the emergence\nof agent hallucinations. Through a detailed review of a large number of\nexisting studies, we summarize approaches for hallucination mitigation and\ndetection, and highlight promising directions for future research. We hope this\nsurvey will inspire further efforts toward addressing hallucinations in\nLLM-based agents, ultimately contributing to the development of more robust and\nreliable agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based\nagents have emerged as powerful intelligent systems capable of human-like\ncognition, reasoning, and interaction. These agents are increasingly being\ndeployed across diverse real-world applications, including student education,\nscientific research, and financial analysis. However, despite their remarkable\npotential, LLM-based agents remain vulnerable to hallucination issues, which\ncan result in erroneous task execution and undermine the reliability of the\noverall system design. Addressing this critical challenge requires a deep\nunderstanding and a systematic consolidation of recent advances on LLM-based\nagents. To this end, we present the first comprehensive survey of\nhallucinations in LLM-based agents. By carefully analyzing the complete\nworkflow of agents, we propose a new taxonomy that identifies different types\nof agent hallucinations occurring at different stages. Furthermore, we conduct\nan in-depth examination of eighteen triggering causes underlying the emergence\nof agent hallucinations. Through a detailed review of a large number of\nexisting studies, we summarize approaches for hallucination mitigation and\ndetection, and highlight promising directions for future research. We hope this\nsurvey will inspire further efforts toward addressing hallucinations in\nLLM-based agents, ultimately contributing to the development of more robust and\nreliable agent systems."
                },
                "authors": [
                    {
                        "name": "Xixun Lin"
                    },
                    {
                        "name": "Yucheng Ning"
                    },
                    {
                        "name": "Jingwen Zhang"
                    },
                    {
                        "name": "Yan Dong"
                    },
                    {
                        "name": "Yilong Liu"
                    },
                    {
                        "name": "Yongxuan Wu"
                    },
                    {
                        "name": "Xiaohua Qi"
                    },
                    {
                        "name": "Nan Sun"
                    },
                    {
                        "name": "Yanmin Shang"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Lixin Zou"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Chuan Zhou"
                    },
                    {
                        "name": "Jia Wu"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Yanan Cao"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Songlin Hu"
                    },
                    {
                        "name": "Li Guo"
                    }
                ],
                "author_detail": {
                    "name": "Li Guo"
                },
                "author": "Li Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18965v1",
                "updated": "2025-09-23T13:17:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    17,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:17:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    17,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "Benchmarking PDF Accessibility Evaluation A Dataset and Framework for\n  Assessing Automated and LLM-Based Approaches for Accessibility Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking PDF Accessibility Evaluation A Dataset and Framework for\n  Assessing Automated and LLM-Based Approaches for Accessibility Testing"
                },
                "summary": "PDFs remain the dominant format for scholarly communication, despite\nsignificant accessibility challenges for blind and low-vision users. While\nvarious tools attempt to evaluate PDF accessibility, there is no standardized\nmethodology to evaluate how different accessibility assessment approaches\nperform. Our work addresses this critical gap by introducing a novel benchmark\ndataset of scholarly PDFs with expert-validated accessibility annotations\nacross seven criteria (alternative text quality, logical reading order,\nsemantic tagging, table structure, functional hyperlinks, color contrast, and\nfont readability), and a four-category evaluation framework with standardized\nlabels (Passed, Failed, Not Present, Cannot Tell) to systematically assess\naccessibility evaluation approaches. Using our evaluation framework, we explore\nwhether large language models (LLMs) are capable of supporting automated\naccessibility evaluation. We benchmark five LLMs, which demonstrate varying\ncapabilities in correctly assessing different accessibility criteria, with\nGPT-4-Turbo achieving the highest overall accuracy (0.85). However, all models\nstruggled in correctly categorizing documents with Not Present and Cannot Tell\naccessibility labels, particularly for alt text quality assessment. Our\nqualitative comparison with standard automated checkers reveals complementary\nstrengths: rule-based tools excel at technical verification, while LLMs better\nevaluate semantic appropriateness and contextual relevance. Based on our\nfindings, we propose a hybrid approach that would combine automated checkers,\nLLM evaluation, and human assessment as a future strategy for PDF accessibility\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDFs remain the dominant format for scholarly communication, despite\nsignificant accessibility challenges for blind and low-vision users. While\nvarious tools attempt to evaluate PDF accessibility, there is no standardized\nmethodology to evaluate how different accessibility assessment approaches\nperform. Our work addresses this critical gap by introducing a novel benchmark\ndataset of scholarly PDFs with expert-validated accessibility annotations\nacross seven criteria (alternative text quality, logical reading order,\nsemantic tagging, table structure, functional hyperlinks, color contrast, and\nfont readability), and a four-category evaluation framework with standardized\nlabels (Passed, Failed, Not Present, Cannot Tell) to systematically assess\naccessibility evaluation approaches. Using our evaluation framework, we explore\nwhether large language models (LLMs) are capable of supporting automated\naccessibility evaluation. We benchmark five LLMs, which demonstrate varying\ncapabilities in correctly assessing different accessibility criteria, with\nGPT-4-Turbo achieving the highest overall accuracy (0.85). However, all models\nstruggled in correctly categorizing documents with Not Present and Cannot Tell\naccessibility labels, particularly for alt text quality assessment. Our\nqualitative comparison with standard automated checkers reveals complementary\nstrengths: rule-based tools excel at technical verification, while LLMs better\nevaluate semantic appropriateness and contextual relevance. Based on our\nfindings, we propose a hybrid approach that would combine automated checkers,\nLLM evaluation, and human assessment as a future strategy for PDF accessibility\nevaluation."
                },
                "authors": [
                    {
                        "name": "Anukriti Kumar"
                    },
                    {
                        "name": "Tanushree Padath"
                    },
                    {
                        "name": "Lucy Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lucy Lu Wang"
                },
                "author": "Lucy Lu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18953v1",
                "updated": "2025-09-23T13:02:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    2,
                    23,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T13:02:23Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    13,
                    2,
                    23,
                    1,
                    266,
                    0
                ],
                "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under\n  Real-World Physical Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under\n  Real-World Physical Variations"
                },
                "summary": "Vision-Language-Action (VLA) models have emerged as promising solutions for\nrobotic manipulation, yet their robustness to real-world physical variations\nremains critically underexplored. To bridge this gap, we propose Eva-VLA, the\nfirst unified framework that systematically evaluates the robustness of VLA\nmodels by transforming discrete physical variations into continuous\noptimization problems. However, comprehensively assessing VLA robustness\npresents two key challenges: (1) how to systematically characterize diverse\nphysical variations encountered in real-world deployments while maintaining\nevaluation reproducibility, and (2) how to discover worst-case scenarios\nwithout prohibitive real-world data collection costs efficiently. To address\nthe first challenge, we decompose real-world variations into three critical\ndomains: object 3D transformations that affect spatial reasoning, illumination\nvariations that challenge visual perception, and adversarial patches that\ndisrupt scene understanding. For the second challenge, we introduce a\ncontinuous black-box optimization framework that transforms discrete physical\nvariations into parameter optimization, enabling systematic exploration of\nworst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models\nacross multiple benchmarks reveal alarming vulnerabilities: all variation types\ntrigger failure rates exceeding 60%, with object transformations causing up to\n97.8% failure in long-horizon tasks. Our findings expose critical gaps between\ncontrolled laboratory success and unpredictable deployment readiness, while the\nEva-VLA framework provides a practical pathway for hardening VLA-based robotic\nmanipulation models against real-world deployment challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have emerged as promising solutions for\nrobotic manipulation, yet their robustness to real-world physical variations\nremains critically underexplored. To bridge this gap, we propose Eva-VLA, the\nfirst unified framework that systematically evaluates the robustness of VLA\nmodels by transforming discrete physical variations into continuous\noptimization problems. However, comprehensively assessing VLA robustness\npresents two key challenges: (1) how to systematically characterize diverse\nphysical variations encountered in real-world deployments while maintaining\nevaluation reproducibility, and (2) how to discover worst-case scenarios\nwithout prohibitive real-world data collection costs efficiently. To address\nthe first challenge, we decompose real-world variations into three critical\ndomains: object 3D transformations that affect spatial reasoning, illumination\nvariations that challenge visual perception, and adversarial patches that\ndisrupt scene understanding. For the second challenge, we introduce a\ncontinuous black-box optimization framework that transforms discrete physical\nvariations into parameter optimization, enabling systematic exploration of\nworst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models\nacross multiple benchmarks reveal alarming vulnerabilities: all variation types\ntrigger failure rates exceeding 60%, with object transformations causing up to\n97.8% failure in long-horizon tasks. Our findings expose critical gaps between\ncontrolled laboratory success and unpredictable deployment readiness, while the\nEva-VLA framework provides a practical pathway for hardening VLA-based robotic\nmanipulation models against real-world deployment challenges."
                },
                "authors": [
                    {
                        "name": "Hanqing Liu"
                    },
                    {
                        "name": "Jiahuan Long"
                    },
                    {
                        "name": "Junqi Wu"
                    },
                    {
                        "name": "Jiacheng Hou"
                    },
                    {
                        "name": "Huili Tang"
                    },
                    {
                        "name": "Tingsong Jiang"
                    },
                    {
                        "name": "Weien Zhou"
                    },
                    {
                        "name": "Wen Yao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Yao"
                },
                "author": "Wen Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17998v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17998v2",
                "updated": "2025-09-23T12:57:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    57,
                    8,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-22T16:39:12Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    39,
                    12,
                    0,
                    265,
                    0
                ],
                "title": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with\n  LLMs"
                },
                "summary": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of\nthe Gaussian process (GP) kernel, which plays a central role in balancing\nexploration and exploitation under limited evaluation budgets. Traditional BO\nmethods often rely on fixed or heuristic kernel selection strategies, which can\nresult in slow convergence or suboptimal solutions when the chosen kernel is\npoorly suited to the underlying objective function. To address this limitation,\nwe propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO\nwith large language models (LLMs). Concretely, CAKE leverages LLMs as the\ncrossover and mutation operators to adaptively generate and refine GP kernels\nbased on the observed data throughout the optimization process. To maximize the\npower of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to\nselect the most effective kernel through balancing the model fit measured by\nthe Bayesian information criterion (BIC) with the expected improvement at each\niteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO\nmethod consistently outperforms established baselines across a range of\nreal-world tasks, including hyperparameter optimization, controller tuning, and\nphotonic chip design. Our code is publicly available at\nhttps://github.com/richardcsuwandi/cake.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of\nthe Gaussian process (GP) kernel, which plays a central role in balancing\nexploration and exploitation under limited evaluation budgets. Traditional BO\nmethods often rely on fixed or heuristic kernel selection strategies, which can\nresult in slow convergence or suboptimal solutions when the chosen kernel is\npoorly suited to the underlying objective function. To address this limitation,\nwe propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO\nwith large language models (LLMs). Concretely, CAKE leverages LLMs as the\ncrossover and mutation operators to adaptively generate and refine GP kernels\nbased on the observed data throughout the optimization process. To maximize the\npower of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to\nselect the most effective kernel through balancing the model fit measured by\nthe Bayesian information criterion (BIC) with the expected improvement at each\niteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO\nmethod consistently outperforms established baselines across a range of\nreal-world tasks, including hyperparameter optimization, controller tuning, and\nphotonic chip design. Our code is publicly available at\nhttps://github.com/richardcsuwandi/cake."
                },
                "authors": [
                    {
                        "name": "Richard Cornelius Suwandi"
                    },
                    {
                        "name": "Feng Yin"
                    },
                    {
                        "name": "Juntao Wang"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Tsung-Hui Chang"
                    },
                    {
                        "name": "Sergios Theodoridis"
                    }
                ],
                "author_detail": {
                    "name": "Sergios Theodoridis"
                },
                "author": "Sergios Theodoridis",
                "arxiv_comment": "Accepted as Poster at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17998v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17998v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18942v1",
                "updated": "2025-09-23T12:55:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    55,
                    57,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:55:57Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    55,
                    57,
                    1,
                    266,
                    0
                ],
                "title": "Data Efficient Adaptation in Large Language Models via Continuous\n  Low-Rank Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Efficient Adaptation in Large Language Models via Continuous\n  Low-Rank Fine-Tuning"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have emphasized the\ncritical role of fine-tuning (FT) techniques in adapting LLMs to specific\ntasks, especially when retraining from scratch is computationally infeasible.\nFine-tuning enables LLMs to leverage task- or domain-specific data, producing\nmodels that more effectively meet the requirements of targeted applications.\nHowever, con- ventional FT approaches often suffer from catastrophic forgetting\nand suboptimal data efficiency, limiting their real-world applicability. To\naddress these challenges, this paper proposes DEAL, a novel framework that\nintegrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.\nBy incorporating knowledge retention and adaptive parameter update modules, the\nframework mitigates the lim- itations of existing FT methods while maintaining\nefficiency in privacy-preserving settings. Experiments on 15 diverse datasets\nshow that DEAL consistently outper- forms baseline methods, yielding\nsubstantial gains in task accuracy and resource efficiency. These findings\ndemonstrate the potential of our approach to advance continual adaptation in\nLLMs by enhancing task performance while improving resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have emphasized the\ncritical role of fine-tuning (FT) techniques in adapting LLMs to specific\ntasks, especially when retraining from scratch is computationally infeasible.\nFine-tuning enables LLMs to leverage task- or domain-specific data, producing\nmodels that more effectively meet the requirements of targeted applications.\nHowever, con- ventional FT approaches often suffer from catastrophic forgetting\nand suboptimal data efficiency, limiting their real-world applicability. To\naddress these challenges, this paper proposes DEAL, a novel framework that\nintegrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.\nBy incorporating knowledge retention and adaptive parameter update modules, the\nframework mitigates the lim- itations of existing FT methods while maintaining\nefficiency in privacy-preserving settings. Experiments on 15 diverse datasets\nshow that DEAL consistently outper- forms baseline methods, yielding\nsubstantial gains in task accuracy and resource efficiency. These findings\ndemonstrate the potential of our approach to advance continual adaptation in\nLLMs by enhancing task performance while improving resource efficiency."
                },
                "authors": [
                    {
                        "name": "Xiao Han"
                    },
                    {
                        "name": "Zimo Zhao"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Maolin Wang"
                    },
                    {
                        "name": "Zitao Liu"
                    },
                    {
                        "name": "Yi Chang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19191v2",
                "updated": "2025-09-23T12:55:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    55,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2024-12-26T12:12:23Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    12,
                    12,
                    23,
                    3,
                    361,
                    0
                ],
                "title": "Biology-Instructions: A Dataset and Benchmark for Multi-Omics Sequence\n  Understanding Capability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biology-Instructions: A Dataset and Benchmark for Multi-Omics Sequence\n  Understanding Capability of Large Language Models"
                },
                "summary": "Large language models (LLMs) have shown remarkable capabilities in general\ndomains, but their application to multi-omics biology remains underexplored. To\naddress this gap, we introduce Biology-Instructions, the first large-scale\ninstruction-tuning dataset for multi-omics biological sequences, including DNA,\nRNA, proteins, and multi-molecules. This dataset bridges LLMs and complex\nbiological sequence-related tasks, enhancing their versatility and reasoning\nwhile maintaining conversational fluency. We also highlight significant\nlimitations of current state-of-the-art LLMs on multi-omics tasks without\nspecialized training. To overcome this, we propose ChatMultiOmics, a strong\nbaseline with a novel three-stage training pipeline, demonstrating superior\nbiological understanding through Biology-Instructions. Both resources are\npublicly available, paving the way for better integration of LLMs in\nmulti-omics analysis. The Biology-Instructions is publicly available at:\nhttps://github.com/hhnqqq/Biology-Instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable capabilities in general\ndomains, but their application to multi-omics biology remains underexplored. To\naddress this gap, we introduce Biology-Instructions, the first large-scale\ninstruction-tuning dataset for multi-omics biological sequences, including DNA,\nRNA, proteins, and multi-molecules. This dataset bridges LLMs and complex\nbiological sequence-related tasks, enhancing their versatility and reasoning\nwhile maintaining conversational fluency. We also highlight significant\nlimitations of current state-of-the-art LLMs on multi-omics tasks without\nspecialized training. To overcome this, we propose ChatMultiOmics, a strong\nbaseline with a novel three-stage training pipeline, demonstrating superior\nbiological understanding through Biology-Instructions. Both resources are\npublicly available, paving the way for better integration of LLMs in\nmulti-omics analysis. The Biology-Instructions is publicly available at:\nhttps://github.com/hhnqqq/Biology-Instructions."
                },
                "authors": [
                    {
                        "name": "Haonan He"
                    },
                    {
                        "name": "Yuchen Ren"
                    },
                    {
                        "name": "Yining Tang"
                    },
                    {
                        "name": "Ziyang Xu"
                    },
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Minghao Yang"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Dong Yuan"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Nanqing Dong"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Peng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Peng Ye"
                },
                "author": "Peng Ye",
                "arxiv_comment": "EMNLP 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18937v1",
                "updated": "2025-09-23T12:54:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    54,
                    52,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:54:52Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    54,
                    52,
                    1,
                    266,
                    0
                ],
                "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands"
                },
                "summary": "Designing robotic hand morphologies for diverse manipulation tasks requires\nbalancing dexterity, manufacturability, and task-specific functionality. While\nopen-source frameworks and parametric tools support reproducible design, they\nstill rely on expert heuristics and manual tuning. Automated methods using\noptimization are often compute-intensive, simulation-dependent, and rarely\ntarget dexterous hands. Large language models (LLMs), with their broad\nknowledge of human-object interactions and strong generative capabilities,\noffer a promising alternative for zero-shot design reasoning. In this paper, we\npresent Lang2Morph, a language-driven pipeline for robotic hand design. It uses\nLLMs to translate natural-language task descriptions into symbolic structures\nand OPH-compatible parameters, enabling 3D-printable task-specific\nmorphologies. The pipeline consists of: (i) Morphology Design, which maps tasks\ninto semantic tags, structural grammars, and OPH-compatible parameters; and\n(ii) Selection and Refinement, which evaluates design candidates based on\nsemantic alignment and size compatibility, and optionally applies LLM-guided\nrefinement when needed. We evaluate Lang2Morph across varied tasks, and results\nshow that our approach can generate diverse, task-relevant morphologies. To our\nknowledge, this is the first attempt to develop an LLM-based framework for\ntask-conditioned robotic hand design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing robotic hand morphologies for diverse manipulation tasks requires\nbalancing dexterity, manufacturability, and task-specific functionality. While\nopen-source frameworks and parametric tools support reproducible design, they\nstill rely on expert heuristics and manual tuning. Automated methods using\noptimization are often compute-intensive, simulation-dependent, and rarely\ntarget dexterous hands. Large language models (LLMs), with their broad\nknowledge of human-object interactions and strong generative capabilities,\noffer a promising alternative for zero-shot design reasoning. In this paper, we\npresent Lang2Morph, a language-driven pipeline for robotic hand design. It uses\nLLMs to translate natural-language task descriptions into symbolic structures\nand OPH-compatible parameters, enabling 3D-printable task-specific\nmorphologies. The pipeline consists of: (i) Morphology Design, which maps tasks\ninto semantic tags, structural grammars, and OPH-compatible parameters; and\n(ii) Selection and Refinement, which evaluates design candidates based on\nsemantic alignment and size compatibility, and optionally applies LLM-guided\nrefinement when needed. We evaluate Lang2Morph across varied tasks, and results\nshow that our approach can generate diverse, task-relevant morphologies. To our\nknowledge, this is the first attempt to develop an LLM-based framework for\ntask-conditioned robotic hand design."
                },
                "authors": [
                    {
                        "name": "Yanyuan Qiao"
                    },
                    {
                        "name": "Kieran Gilday"
                    },
                    {
                        "name": "Yutong Xie"
                    },
                    {
                        "name": "Josie Hughes"
                    }
                ],
                "author_detail": {
                    "name": "Josie Hughes"
                },
                "author": "Josie Hughes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18934v1",
                "updated": "2025-09-23T12:52:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    52,
                    5,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:52:05Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    52,
                    5,
                    1,
                    266,
                    0
                ],
                "title": "Generic Adversarial Smart Contract Detection with Semantics and\n  Uncertainty-Aware LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generic Adversarial Smart Contract Detection with Semantics and\n  Uncertainty-Aware LLM"
                },
                "summary": "Adversarial smart contracts, mostly on EVM-compatible chains like Ethereum\nand BSC, are deployed as EVM bytecode to exploit vulnerable smart contracts\ntypically for financial gains. Detecting such malicious contracts at the time\nof deployment is an important proactive strategy preventing loss from victim\ncontracts. It offers a better cost-benefit than detecting vulnerabilities on\ndiverse potential victims. However, existing works are not generic with limited\ndetection types and effectiveness due to imbalanced samples, while the emerging\nLLM technologies, which show its potentials in generalization, have two key\nproblems impeding its application in this task: hard digestion of compiled-code\ninputs, especially those with task-specific logic, and hard assessment of LLMs'\ncertainty in their binary answers, i.e., yes-or-no answers. Therefore, we\npropose a generic adversarial smart contracts detection framework FinDet, which\nleverages LLMs with two enhancements addressing above two problems. FinDet\ntakes as input only the EVM-bytecode contracts and identifies adversarial ones\namong them with high balanced accuracy. The first enhancement extracts concise\nsemantic intentions and high-level behavioral logic from the low-level bytecode\ninputs, unleashing the LLM reasoning capability restricted by the task input.\nThe second enhancement probes and measures the LLM uncertainty to its\nmulti-round answering to the same query, improving the LLM answering robustness\nfor binary classifications required by the task output. Our comprehensive\nevaluation shows that FinDet achieves a BAC of 0.9223 and a TPR of 0.8950,\nsignificantly outperforming existing baselines. It remains robust under\nchallenging conditions including unseen attack patterns, low-data settings, and\nfeature obfuscation. FinDet detects all 5 public and 20+ unreported adversarial\ncontracts in a 10-day real-world test, confirmed manually.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial smart contracts, mostly on EVM-compatible chains like Ethereum\nand BSC, are deployed as EVM bytecode to exploit vulnerable smart contracts\ntypically for financial gains. Detecting such malicious contracts at the time\nof deployment is an important proactive strategy preventing loss from victim\ncontracts. It offers a better cost-benefit than detecting vulnerabilities on\ndiverse potential victims. However, existing works are not generic with limited\ndetection types and effectiveness due to imbalanced samples, while the emerging\nLLM technologies, which show its potentials in generalization, have two key\nproblems impeding its application in this task: hard digestion of compiled-code\ninputs, especially those with task-specific logic, and hard assessment of LLMs'\ncertainty in their binary answers, i.e., yes-or-no answers. Therefore, we\npropose a generic adversarial smart contracts detection framework FinDet, which\nleverages LLMs with two enhancements addressing above two problems. FinDet\ntakes as input only the EVM-bytecode contracts and identifies adversarial ones\namong them with high balanced accuracy. The first enhancement extracts concise\nsemantic intentions and high-level behavioral logic from the low-level bytecode\ninputs, unleashing the LLM reasoning capability restricted by the task input.\nThe second enhancement probes and measures the LLM uncertainty to its\nmulti-round answering to the same query, improving the LLM answering robustness\nfor binary classifications required by the task output. Our comprehensive\nevaluation shows that FinDet achieves a BAC of 0.9223 and a TPR of 0.8950,\nsignificantly outperforming existing baselines. It remains robust under\nchallenging conditions including unseen attack patterns, low-data settings, and\nfeature obfuscation. FinDet detects all 5 public and 20+ unreported adversarial\ncontracts in a 10-day real-world test, confirmed manually."
                },
                "authors": [
                    {
                        "name": "Yating Liu"
                    },
                    {
                        "name": "Xing Su"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Sijin Li"
                    },
                    {
                        "name": "Yuxi Cheng"
                    },
                    {
                        "name": "Fengyuan Xu"
                    },
                    {
                        "name": "Sheng Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhong"
                },
                "author": "Sheng Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18933v1",
                "updated": "2025-09-23T12:52:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    52,
                    1,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:52:01Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    52,
                    1,
                    1,
                    266,
                    0
                ],
                "title": "Accurate and Efficient Prediction of Wi-Fi Link Quality Based on Machine\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and Efficient Prediction of Wi-Fi Link Quality Based on Machine\n  Learning"
                },
                "summary": "Wireless communications are characterized by their unpredictability, posing\nchallenges for maintaining consistent communication quality. This paper\npresents a comprehensive analysis of various prediction models, with a focus on\nachieving accurate and efficient Wi-Fi link quality forecasts using machine\nlearning techniques. Specifically, the paper evaluates the performance of\ndata-driven models based on the linear combination of exponential moving\naverages, which are designed for low-complexity implementations and are then\nsuitable for hardware platforms with limited processing resources. Accuracy of\nthe proposed approaches was assessed using experimental data from a real-world\nWi-Fi testbed, considering both channel-dependent and channel-independent\ntraining data. Remarkably, channel-independent models, which allow for\ngeneralized training by equipment manufacturers, demonstrated competitive\nperformance. Overall, this study provides insights into the practical\ndeployment of machine learning-based prediction models for enhancing Wi-Fi\ndependability in industrial environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communications are characterized by their unpredictability, posing\nchallenges for maintaining consistent communication quality. This paper\npresents a comprehensive analysis of various prediction models, with a focus on\nachieving accurate and efficient Wi-Fi link quality forecasts using machine\nlearning techniques. Specifically, the paper evaluates the performance of\ndata-driven models based on the linear combination of exponential moving\naverages, which are designed for low-complexity implementations and are then\nsuitable for hardware platforms with limited processing resources. Accuracy of\nthe proposed approaches was assessed using experimental data from a real-world\nWi-Fi testbed, considering both channel-dependent and channel-independent\ntraining data. Remarkably, channel-independent models, which allow for\ngeneralized training by equipment manufacturers, demonstrated competitive\nperformance. Overall, this study provides insights into the practical\ndeployment of machine learning-based prediction models for enhancing Wi-Fi\ndependability in industrial environments."
                },
                "authors": [
                    {
                        "name": "Gabriele Formis"
                    },
                    {
                        "name": "Gianluca Cena"
                    },
                    {
                        "name": "Lukasz Wisniewski"
                    },
                    {
                        "name": "Stefano Scanzio"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Scanzio"
                },
                "author": "Stefano Scanzio",
                "arxiv_doi": "10.1109/TII.2025.3609224",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TII.2025.3609224",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.18933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted version in IEEE Transactions on Industrial Informatics, 12\n  pages, 2025",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22974v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22974v2",
                "updated": "2025-09-23T12:45:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    45,
                    55,
                    1,
                    266,
                    0
                ],
                "published": "2025-05-29T01:26:30Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    1,
                    26,
                    30,
                    3,
                    149,
                    0
                ],
                "title": "Learning coordinated badminton skills for legged manipulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning coordinated badminton skills for legged manipulators"
                },
                "summary": "Coordinating the motion between lower and upper limbs and aligning limb\ncontrol with perception are substantial challenges in robotics, particularly in\ndynamic environments. To this end, we introduce an approach for enabling legged\nmobile manipulators to play badminton, a task that requires precise\ncoordination of perception, locomotion, and arm swinging. We propose a unified\nreinforcement learning-based control policy for whole-body visuomotor skills\ninvolving all degrees of freedom to achieve effective shuttlecock tracking and\nstriking. This policy is informed by a perception noise model that utilizes\nreal-world camera data, allowing for consistent perception error levels between\nsimulation and deployment and encouraging learned active perception behaviors.\nOur method includes a shuttlecock prediction model, constrained reinforcement\nlearning for robust motion control, and integrated system identification\ntechniques to enhance deployment readiness. Extensive experimental results in a\nvariety of environments validate the robot's capability to predict shuttlecock\ntrajectories, navigate the service area effectively, and execute precise\nstrikes against human players, demonstrating the feasibility of using legged\nmobile manipulators in complex and dynamic sports scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coordinating the motion between lower and upper limbs and aligning limb\ncontrol with perception are substantial challenges in robotics, particularly in\ndynamic environments. To this end, we introduce an approach for enabling legged\nmobile manipulators to play badminton, a task that requires precise\ncoordination of perception, locomotion, and arm swinging. We propose a unified\nreinforcement learning-based control policy for whole-body visuomotor skills\ninvolving all degrees of freedom to achieve effective shuttlecock tracking and\nstriking. This policy is informed by a perception noise model that utilizes\nreal-world camera data, allowing for consistent perception error levels between\nsimulation and deployment and encouraging learned active perception behaviors.\nOur method includes a shuttlecock prediction model, constrained reinforcement\nlearning for robust motion control, and integrated system identification\ntechniques to enhance deployment readiness. Extensive experimental results in a\nvariety of environments validate the robot's capability to predict shuttlecock\ntrajectories, navigate the service area effectively, and execute precise\nstrikes against human players, demonstrating the feasibility of using legged\nmobile manipulators in complex and dynamic sports scenarios."
                },
                "authors": [
                    {
                        "name": "Yuntao Ma"
                    },
                    {
                        "name": "Andrei Cramariuc"
                    },
                    {
                        "name": "Farbod Farshidian"
                    },
                    {
                        "name": "Marco Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Marco Hutter"
                },
                "author": "Marco Hutter",
                "arxiv_doi": "10.1126/scirobotics.adu3922",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1126/scirobotics.adu3922",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.22974v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22974v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Science Robotics DOI: 10.1126/scirobotics.adu3922",
                "arxiv_journal_ref": "Sci. Robot.10,eadu3922(2025)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40, 93C85",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.6; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11381v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11381v4",
                "updated": "2025-09-23T12:39:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    39,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-02-17T02:53:08Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    2,
                    53,
                    8,
                    0,
                    48,
                    0
                ],
                "title": "Without Paired Labeled Data: End-to-End Self-Supervised Learning for\n  Drone-view Geo-Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Without Paired Labeled Data: End-to-End Self-Supervised Learning for\n  Drone-view Geo-Localization"
                },
                "summary": "Drone-view Geo-Localization (DVGL) aims to achieve accurate localization of\ndrones by retrieving the most relevant GPS-tagged satellite images. However,\nmost existing methods heavily rely on strictly pre-paired drone-satellite\nimages for supervised learning. When the target region shifts, new paired\nsamples are typically required to adapt to the distribution changes. The high\ncost of annotation and the limited transferability of these methods\nsignificantly hinder the practical deployment of DVGL in open-world scenarios.\nTo address these limitations, we propose a novel end-to-end self-supervised\nlearning method with a shallow backbone network, called the dynamic\nmemory-driven and neighborhood information learning (DMNIL) method. It employs\na clustering algorithm to generate pseudo-labels and adopts a dual-path\ncontrastive learning framework to learn discriminative intra-view\nrepresentations. Furthermore, DMNIL incorporates two core modules, including\nthe dynamic hierarchical memory learning (DHML) module and the information\nconsistency evolution learning (ICEL) module. The DHML module combines\nshort-term and long-term memory to enhance intra-view feature consistency and\ndiscriminability. Meanwhile, the ICEL module utilizes a neighborhood-driven\ndynamic constraint mechanism to systematically capture implicit cross-view\nsemantic correlations, consequently improving cross-view feature alignment. To\nfurther stabilize and strengthen the self-supervised training process, a\npseudo-label enhancement strategy is introduced to enhance the quality of\npseudo supervision. Extensive experiments on three public benchmark datasets\ndemonstrate that the proposed method consistently outperforms existing\nself-supervised methods and even surpasses several state-of-the-art supervised\nmethods. Our code is available at https://github.com/ISChenawei/DMNIL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drone-view Geo-Localization (DVGL) aims to achieve accurate localization of\ndrones by retrieving the most relevant GPS-tagged satellite images. However,\nmost existing methods heavily rely on strictly pre-paired drone-satellite\nimages for supervised learning. When the target region shifts, new paired\nsamples are typically required to adapt to the distribution changes. The high\ncost of annotation and the limited transferability of these methods\nsignificantly hinder the practical deployment of DVGL in open-world scenarios.\nTo address these limitations, we propose a novel end-to-end self-supervised\nlearning method with a shallow backbone network, called the dynamic\nmemory-driven and neighborhood information learning (DMNIL) method. It employs\na clustering algorithm to generate pseudo-labels and adopts a dual-path\ncontrastive learning framework to learn discriminative intra-view\nrepresentations. Furthermore, DMNIL incorporates two core modules, including\nthe dynamic hierarchical memory learning (DHML) module and the information\nconsistency evolution learning (ICEL) module. The DHML module combines\nshort-term and long-term memory to enhance intra-view feature consistency and\ndiscriminability. Meanwhile, the ICEL module utilizes a neighborhood-driven\ndynamic constraint mechanism to systematically capture implicit cross-view\nsemantic correlations, consequently improving cross-view feature alignment. To\nfurther stabilize and strengthen the self-supervised training process, a\npseudo-label enhancement strategy is introduced to enhance the quality of\npseudo supervision. Extensive experiments on three public benchmark datasets\ndemonstrate that the proposed method consistently outperforms existing\nself-supervised methods and even surpasses several state-of-the-art supervised\nmethods. Our code is available at https://github.com/ISChenawei/DMNIL."
                },
                "authors": [
                    {
                        "name": "Zhongwei Chen"
                    },
                    {
                        "name": "Zhao-Xu Yang"
                    },
                    {
                        "name": "Hai-Jun Rong"
                    },
                    {
                        "name": "Guoqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoqi Li"
                },
                "author": "Guoqi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11381v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11381v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15335v2",
                "updated": "2025-09-23T11:42:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    11,
                    42,
                    25,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-18T18:26:53Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    18,
                    26,
                    53,
                    3,
                    261,
                    0
                ],
                "title": "PolBiX: Detecting LLMs' Political Bias in Fact-Checking through\n  X-phemisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolBiX: Detecting LLMs' Political Bias in Fact-Checking through\n  X-phemisms"
                },
                "summary": "Large Language Models are increasingly used in applications requiring\nobjective assessment, which could be compromised by political bias. Many\nstudies found preferences for left-leaning positions in LLMs, but downstream\neffects on tasks like fact-checking remain underexplored. In this study, we\nsystematically investigate political bias through exchanging words with\neuphemisms or dysphemisms in German claims. We construct minimal pairs of\nfactually equivalent claims that differ in political connotation, to assess the\nconsistency of LLMs in classifying them as true or false. We evaluate six LLMs\nand find that, more than political leaning, the presence of judgmental words\nsignificantly influences truthfulness assessment. While a few models show\ntendencies of political bias, this is not mitigated by explicitly calling for\nobjectivism in prompts. Warning: This paper contains content that may be\noffensive or upsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly used in applications requiring\nobjective assessment, which could be compromised by political bias. Many\nstudies found preferences for left-leaning positions in LLMs, but downstream\neffects on tasks like fact-checking remain underexplored. In this study, we\nsystematically investigate political bias through exchanging words with\neuphemisms or dysphemisms in German claims. We construct minimal pairs of\nfactually equivalent claims that differ in political connotation, to assess the\nconsistency of LLMs in classifying them as true or false. We evaluate six LLMs\nand find that, more than political leaning, the presence of judgmental words\nsignificantly influences truthfulness assessment. While a few models show\ntendencies of political bias, this is not mitigated by explicitly calling for\nobjectivism in prompts. Warning: This paper contains content that may be\noffensive or upsetting."
                },
                "authors": [
                    {
                        "name": "Charlott Jakob"
                    },
                    {
                        "name": "David Harbecke"
                    },
                    {
                        "name": "Patrick Parschan"
                    },
                    {
                        "name": "Pia Wenzel Neves"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "Accepted at Findings of EMNLP 2025, camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18901v1",
                "updated": "2025-09-23T11:30:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    11,
                    30,
                    42,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T11:30:42Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    11,
                    30,
                    42,
                    1,
                    266,
                    0
                ],
                "title": "Extractive Fact Decomposition for Interpretable Natural Language\n  Inference in one Forward Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extractive Fact Decomposition for Interpretable Natural Language\n  Inference in one Forward Pass"
                },
                "summary": "Recent works in Natural Language Inference (NLI) and related tasks, such as\nautomated fact-checking, employ atomic fact decomposition to enhance\ninterpretability and robustness. For this, existing methods rely on\nresource-intensive generative large language models (LLMs) to perform\ndecomposition. We propose JEDI, an encoder-only architecture that jointly\nperforms extractive atomic fact decomposition and interpretable inference\nwithout requiring generative models during inference. To facilitate training,\nwe produce a large corpus of synthetic rationales covering multiple NLI\nbenchmarks. Experimental results demonstrate that JEDI achieves competitive\naccuracy in distribution and significantly improves robustness out of\ndistribution and in adversarial settings over models based solely on extractive\nrationale supervision. Our findings show that interpretability and robust\ngeneralization in NLI can be realized using encoder-only architectures and\nsynthetic rationales. Code and data available at https://jedi.nicpopovic.com",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works in Natural Language Inference (NLI) and related tasks, such as\nautomated fact-checking, employ atomic fact decomposition to enhance\ninterpretability and robustness. For this, existing methods rely on\nresource-intensive generative large language models (LLMs) to perform\ndecomposition. We propose JEDI, an encoder-only architecture that jointly\nperforms extractive atomic fact decomposition and interpretable inference\nwithout requiring generative models during inference. To facilitate training,\nwe produce a large corpus of synthetic rationales covering multiple NLI\nbenchmarks. Experimental results demonstrate that JEDI achieves competitive\naccuracy in distribution and significantly improves robustness out of\ndistribution and in adversarial settings over models based solely on extractive\nrationale supervision. Our findings show that interpretability and robust\ngeneralization in NLI can be realized using encoder-only architectures and\nsynthetic rationales. Code and data available at https://jedi.nicpopovic.com"
                },
                "authors": [
                    {
                        "name": "Nicholas PopoviÄ"
                    },
                    {
                        "name": "Michael FÃ¤rber"
                    }
                ],
                "author_detail": {
                    "name": "Michael FÃ¤rber"
                },
                "author": "Michael FÃ¤rber",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12734v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12734v2",
                "updated": "2025-09-23T11:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    11,
                    15,
                    44,
                    1,
                    266,
                    0
                ],
                "published": "2025-04-17T08:18:09Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    8,
                    18,
                    9,
                    3,
                    107,
                    0
                ],
                "title": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning\n  Across Diverse Structured Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning\n  Across Diverse Structured Knowledge"
                },
                "summary": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods."
                },
                "authors": [
                    {
                        "name": "Yongrui Chen"
                    },
                    {
                        "name": "Junhao He"
                    },
                    {
                        "name": "Linbo Fu"
                    },
                    {
                        "name": "Shenyu Zhang"
                    },
                    {
                        "name": "Rihui Jin"
                    },
                    {
                        "name": "Xinbang Dai"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Dehai Min"
                    },
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Yi Huang"
                    },
                    {
                        "name": "Tongtong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongtong Wu"
                },
                "author": "Tongtong Wu",
                "arxiv_comment": "New version is arXiv:2508.17905",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12734v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12734v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18894v1",
                "updated": "2025-09-23T11:07:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    11,
                    7,
                    18,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T11:07:18Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    11,
                    7,
                    18,
                    1,
                    266,
                    0
                ],
                "title": "SmartWilds: Multimodal Wildlife Monitoring Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartWilds: Multimodal Wildlife Monitoring Dataset"
                },
                "summary": "We present the first release of SmartWilds, a multimodal wildlife monitoring\ndataset. SmartWilds is a synchronized collection of drone imagery, camera trap\nphotographs and videos, and bioacoustic recordings collected during summer 2025\nat The Wilds safari park in Ohio. This dataset supports multimodal AI research\nfor comprehensive environmental monitoring, addressing critical needs in\nendangered species research, conservation ecology, and habitat management. Our\npilot deployment captured four days of synchronized monitoring across three\nmodalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,\nPrzewalski's horses, as well as species native to Ohio, including bald eagles,\nwhite-tailed deer, and coyotes. We provide a comparative analysis of sensor\nmodality performance, demonstrating complementary strengths for landuse\npatterns, species detection, behavioral analysis, and habitat monitoring. This\nwork establishes reproducible protocols for multimodal wildlife monitoring\nwhile contributing open datasets to advance conservation computer vision\nresearch. Future releases will include synchronized GPS tracking data from\ntagged individuals, citizen science data, and expanded temporal coverage across\nmultiple seasons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first release of SmartWilds, a multimodal wildlife monitoring\ndataset. SmartWilds is a synchronized collection of drone imagery, camera trap\nphotographs and videos, and bioacoustic recordings collected during summer 2025\nat The Wilds safari park in Ohio. This dataset supports multimodal AI research\nfor comprehensive environmental monitoring, addressing critical needs in\nendangered species research, conservation ecology, and habitat management. Our\npilot deployment captured four days of synchronized monitoring across three\nmodalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,\nPrzewalski's horses, as well as species native to Ohio, including bald eagles,\nwhite-tailed deer, and coyotes. We provide a comparative analysis of sensor\nmodality performance, demonstrating complementary strengths for landuse\npatterns, species detection, behavioral analysis, and habitat monitoring. This\nwork establishes reproducible protocols for multimodal wildlife monitoring\nwhile contributing open datasets to advance conservation computer vision\nresearch. Future releases will include synchronized GPS tracking data from\ntagged individuals, citizen science data, and expanded temporal coverage across\nmultiple seasons."
                },
                "authors": [
                    {
                        "name": "Jenna Kline"
                    },
                    {
                        "name": "Anirudh Potlapally"
                    },
                    {
                        "name": "Bharath Pillai"
                    },
                    {
                        "name": "Tanishka Wani"
                    },
                    {
                        "name": "Rugved Katole"
                    },
                    {
                        "name": "Vedant Patil"
                    },
                    {
                        "name": "Penelope Covey"
                    },
                    {
                        "name": "Hari Subramoni"
                    },
                    {
                        "name": "Tanya Berger-Wolf"
                    },
                    {
                        "name": "Christopher Stewart"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Stewart"
                },
                "author": "Christopher Stewart",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18886v1",
                "updated": "2025-09-23T10:36:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    10,
                    36,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T10:36:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    10,
                    36,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "Confidential LLM Inference: Performance and Cost Across CPU and GPU TEEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidential LLM Inference: Performance and Cost Across CPU and GPU TEEs"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed on converged Cloud and\nHigh-Performance Computing (HPC) infrastructure. However, as LLMs handle\nconfidential inputs and are fine-tuned on costly, proprietary datasets, their\nheightened security requirements slow adoption in privacy-sensitive sectors\nsuch as healthcare and finance. We investigate methods to address this gap and\npropose Trusted Execution Environments (TEEs) as a solution for securing\nend-to-end LLM inference. We validate their practicality by evaluating these\ncompute-intensive workloads entirely within CPU and GPU TEEs. On the CPU side,\nwe conduct an in-depth study running full Llama2 inference pipelines (7B, 13B,\n70B) inside Intel's TDX and SGX, accelerated by Advanced Matrix Extensions\n(AMX). We derive 12 insights, including that across various data types, batch\nsizes, and input lengths, CPU TEEs impose under 10% throughput and 20% latency\noverheads, further reduced by AMX. We run LLM inference on NVIDIA H100\nConfidential Compute GPUs, contextualizing our CPU findings and observing\nthroughput penalties of 4-8% that diminish as batch and input sizes grow. By\ncomparing performance, cost, and security trade-offs, we show how CPU TEEs can\nbe more cost-effective or secure than their GPU counterparts. To our knowledge,\nour work is the first to comprehensively demonstrate the performance and\npracticality of modern TEEs across both CPUs and GPUs for enabling confidential\nLLMs (cLLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed on converged Cloud and\nHigh-Performance Computing (HPC) infrastructure. However, as LLMs handle\nconfidential inputs and are fine-tuned on costly, proprietary datasets, their\nheightened security requirements slow adoption in privacy-sensitive sectors\nsuch as healthcare and finance. We investigate methods to address this gap and\npropose Trusted Execution Environments (TEEs) as a solution for securing\nend-to-end LLM inference. We validate their practicality by evaluating these\ncompute-intensive workloads entirely within CPU and GPU TEEs. On the CPU side,\nwe conduct an in-depth study running full Llama2 inference pipelines (7B, 13B,\n70B) inside Intel's TDX and SGX, accelerated by Advanced Matrix Extensions\n(AMX). We derive 12 insights, including that across various data types, batch\nsizes, and input lengths, CPU TEEs impose under 10% throughput and 20% latency\noverheads, further reduced by AMX. We run LLM inference on NVIDIA H100\nConfidential Compute GPUs, contextualizing our CPU findings and observing\nthroughput penalties of 4-8% that diminish as batch and input sizes grow. By\ncomparing performance, cost, and security trade-offs, we show how CPU TEEs can\nbe more cost-effective or secure than their GPU counterparts. To our knowledge,\nour work is the first to comprehensively demonstrate the performance and\npracticality of modern TEEs across both CPUs and GPUs for enabling confidential\nLLMs (cLLMs)."
                },
                "authors": [
                    {
                        "name": "Marcin Chrapek"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Etienne Mettaz"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18880v1",
                "updated": "2025-09-23T10:21:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    10,
                    21,
                    22,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T10:21:22Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    10,
                    21,
                    22,
                    1,
                    266,
                    0
                ],
                "title": "Diversity Boosts AI-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity Boosts AI-Generated Text Detection"
                },
                "summary": "Detecting AI-generated text is an increasing necessity to combat misuse of\nLLMs in education, business compliance, journalism, and social media, where\nsynthetic fluency can mask misinformation or deception. While prior detectors\noften rely on token-level likelihoods or opaque black-box classifiers, these\napproaches struggle against high-quality generations and offer little\ninterpretability. In this work, we propose DivEye, a novel detection framework\nthat captures how unpredictability fluctuates across a text using\nsurprisal-based features. Motivated by the observation that human-authored text\nexhibits richer variability in lexical and structural unpredictability than LLM\noutputs, DivEye captures this signal through a set of interpretable statistical\nfeatures. Our method outperforms existing zero-shot detectors by up to 33.2%\nand achieves competitive performance with fine-tuned baselines across multiple\nbenchmarks. DivEye is robust to paraphrasing and adversarial attacks,\ngeneralizes well across domains and models, and improves the performance of\nexisting detectors by up to 18.7% when used as an auxiliary signal. Beyond\ndetection, DivEye provides interpretable insights into why a text is flagged,\npointing to rhythmic unpredictability as a powerful and underexplored signal\nfor LLM detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting AI-generated text is an increasing necessity to combat misuse of\nLLMs in education, business compliance, journalism, and social media, where\nsynthetic fluency can mask misinformation or deception. While prior detectors\noften rely on token-level likelihoods or opaque black-box classifiers, these\napproaches struggle against high-quality generations and offer little\ninterpretability. In this work, we propose DivEye, a novel detection framework\nthat captures how unpredictability fluctuates across a text using\nsurprisal-based features. Motivated by the observation that human-authored text\nexhibits richer variability in lexical and structural unpredictability than LLM\noutputs, DivEye captures this signal through a set of interpretable statistical\nfeatures. Our method outperforms existing zero-shot detectors by up to 33.2%\nand achieves competitive performance with fine-tuned baselines across multiple\nbenchmarks. DivEye is robust to paraphrasing and adversarial attacks,\ngeneralizes well across domains and models, and improves the performance of\nexisting detectors by up to 18.7% when used as an auxiliary signal. Beyond\ndetection, DivEye provides interpretable insights into why a text is flagged,\npointing to rhythmic unpredictability as a powerful and underexplored signal\nfor LLM detection."
                },
                "authors": [
                    {
                        "name": "Advik Raj Basani"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pin-Yu Chen"
                },
                "author": "Pin-Yu Chen",
                "arxiv_comment": "Project Webpage: https://diveye.vercel.app/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18874v1",
                "updated": "2025-09-23T10:10:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    10,
                    10,
                    37,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T10:10:37Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    10,
                    10,
                    37,
                    1,
                    266,
                    0
                ],
                "title": "When Ads Become Profiles: Large-Scale Audit of Algorithmic Biases and\n  LLM Profiling Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Ads Become Profiles: Large-Scale Audit of Algorithmic Biases and\n  LLM Profiling Risks"
                },
                "summary": "Automated ad targeting on social media is opaque, creating risks of\nexploitation and invisibility to external scrutiny. Users may be steered toward\nharmful content while independent auditing of these processes remains blocked.\nLarge Language Models (LLMs) raise a new concern: the potential to\nreverse-engineer sensitive user attributes from exposure alone. We introduce a\nmulti-stage auditing framework to investigate these risks. First, a large-scale\naudit of over 435,000 ad impressions delivered to 891 Australian Facebook users\nreveals algorithmic biases, including disproportionate Gambling and Politics\nads shown to socioeconomically vulnerable and politically aligned groups.\nSecond, a multimodal LLM can reconstruct users' demographic profiles from ad\nstreams, outperforming census-based baselines and matching or exceeding human\nperformance. Our results provide the first empirical evidence that ad streams\nconstitute rich digital footprints for public AI inference, highlighting urgent\nprivacy risks and the need for content-level auditing and governance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated ad targeting on social media is opaque, creating risks of\nexploitation and invisibility to external scrutiny. Users may be steered toward\nharmful content while independent auditing of these processes remains blocked.\nLarge Language Models (LLMs) raise a new concern: the potential to\nreverse-engineer sensitive user attributes from exposure alone. We introduce a\nmulti-stage auditing framework to investigate these risks. First, a large-scale\naudit of over 435,000 ad impressions delivered to 891 Australian Facebook users\nreveals algorithmic biases, including disproportionate Gambling and Politics\nads shown to socioeconomically vulnerable and politically aligned groups.\nSecond, a multimodal LLM can reconstruct users' demographic profiles from ad\nstreams, outperforming census-based baselines and matching or exceeding human\nperformance. Our results provide the first empirical evidence that ad streams\nconstitute rich digital footprints for public AI inference, highlighting urgent\nprivacy risks and the need for content-level auditing and governance."
                },
                "authors": [
                    {
                        "name": "Baiyu Chen"
                    },
                    {
                        "name": "Benjamin Tag"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Daniel Angus"
                    },
                    {
                        "name": "Flora Salim"
                    }
                ],
                "author_detail": {
                    "name": "Flora Salim"
                },
                "author": "Flora Salim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18869v1",
                "updated": "2025-09-23T10:07:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    10,
                    7,
                    29,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T10:07:29Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    10,
                    7,
                    29,
                    1,
                    266,
                    0
                ],
                "title": "On The Reproducibility Limitations of RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On The Reproducibility Limitations of RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is increasingly employed in generative\nAI-driven scientific workflows to integrate rapidly evolving scientific\nknowledge bases, yet its reliability is frequently compromised by\nnon-determinism in their retrieval components. This paper introduces ReproRAG,\na comprehensive benchmarking framework designed to systematically measure and\nquantify the reproducibility of vector-based retrieval systems. ReproRAG\ninvestigates sources of uncertainty across the entire pipeline, including\ndifferent embedding models, precision, retrieval algorithms, hardware\nconfigurations, and distributed execution environments. Utilizing a suite of\nmetrics, such as Exact Match Rate, Jaccard Similarity, and Kendall's Tau, the\nproposed framework effectively characterizes the trade-offs between\nreproducibility and performance. Our large-scale empirical study reveals\ncritical insights; for instance, we observe that different embedding models\nhave remarkable impact on RAG reproducibility. The open-sourced ReproRAG\nframework provides researchers and engineers productive tools to validate\ndeployments, benchmark reproducibility, and make informed design decisions,\nthereby fostering more trustworthy AI for science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is increasingly employed in generative\nAI-driven scientific workflows to integrate rapidly evolving scientific\nknowledge bases, yet its reliability is frequently compromised by\nnon-determinism in their retrieval components. This paper introduces ReproRAG,\na comprehensive benchmarking framework designed to systematically measure and\nquantify the reproducibility of vector-based retrieval systems. ReproRAG\ninvestigates sources of uncertainty across the entire pipeline, including\ndifferent embedding models, precision, retrieval algorithms, hardware\nconfigurations, and distributed execution environments. Utilizing a suite of\nmetrics, such as Exact Match Rate, Jaccard Similarity, and Kendall's Tau, the\nproposed framework effectively characterizes the trade-offs between\nreproducibility and performance. Our large-scale empirical study reveals\ncritical insights; for instance, we observe that different embedding models\nhave remarkable impact on RAG reproducibility. The open-sourced ReproRAG\nframework provides researchers and engineers productive tools to validate\ndeployments, benchmark reproducibility, and make informed design decisions,\nthereby fostering more trustworthy AI for science."
                },
                "authors": [
                    {
                        "name": "Baiqiang Wang"
                    },
                    {
                        "name": "Dongfang Zhao"
                    },
                    {
                        "name": "Nathan R Tallent"
                    },
                    {
                        "name": "Luanzheng Guo"
                    }
                ],
                "author_detail": {
                    "name": "Luanzheng Guo"
                },
                "author": "Luanzheng Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18868v1",
                "updated": "2025-09-23T10:06:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    10,
                    6,
                    58,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T10:06:58Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    10,
                    6,
                    58,
                    1,
                    266,
                    0
                ],
                "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution"
                },
                "summary": "Under a unified operational definition, we define LLM memory as a persistent\nstate written during pretraining, finetuning, or inference that can later be\naddressed and that stably influences outputs. We propose a four-part taxonomy\n(parametric, contextual, external, procedural/episodic) and a memory quadruple\n(location, persistence, write/access path, controllability). We link mechanism,\nevaluation, and governance via the chain write -> read -> inhibit/update. To\navoid distorted comparisons across heterogeneous setups, we adopt a\nthree-setting protocol (parametric only, offline retrieval, online retrieval)\nthat decouples capability from information availability on the same data and\ntimeline. On this basis we build a layered evaluation: parametric (closed-book\nrecall, edit differential, memorization/privacy), contextual (position curves\nand the mid-sequence drop), external (answer correctness vs snippet\nattribution/faithfulness), and procedural/episodic (cross-session consistency\nand timeline replay, E MARS+). The framework integrates temporal governance and\nleakage auditing (freshness hits, outdated answers, refusal slices) and\nuncertainty reporting via inter-rater agreement plus paired tests with\nmultiple-comparison correction. For updating and forgetting, we present DMM\nGov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),\nand RAG to form an auditable loop covering admission thresholds, rollout,\nmonitoring, rollback, and change audits, with specs for timeliness, conflict\nhandling, and long-horizon consistency. Finally, we give four testable\npropositions: minimum identifiability; a minimal evaluation card; causally\nconstrained editing with verifiable forgetting; and when retrieval with\nsmall-window replay outperforms ultra-long-context reading. This yields a\nreproducible, comparable, and governable coordinate system for research and\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under a unified operational definition, we define LLM memory as a persistent\nstate written during pretraining, finetuning, or inference that can later be\naddressed and that stably influences outputs. We propose a four-part taxonomy\n(parametric, contextual, external, procedural/episodic) and a memory quadruple\n(location, persistence, write/access path, controllability). We link mechanism,\nevaluation, and governance via the chain write -> read -> inhibit/update. To\navoid distorted comparisons across heterogeneous setups, we adopt a\nthree-setting protocol (parametric only, offline retrieval, online retrieval)\nthat decouples capability from information availability on the same data and\ntimeline. On this basis we build a layered evaluation: parametric (closed-book\nrecall, edit differential, memorization/privacy), contextual (position curves\nand the mid-sequence drop), external (answer correctness vs snippet\nattribution/faithfulness), and procedural/episodic (cross-session consistency\nand timeline replay, E MARS+). The framework integrates temporal governance and\nleakage auditing (freshness hits, outdated answers, refusal slices) and\nuncertainty reporting via inter-rater agreement plus paired tests with\nmultiple-comparison correction. For updating and forgetting, we present DMM\nGov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),\nand RAG to form an auditable loop covering admission thresholds, rollout,\nmonitoring, rollback, and change audits, with specs for timeliness, conflict\nhandling, and long-horizon consistency. Finally, we give four testable\npropositions: minimum identifiability; a minimal evaluation card; causally\nconstrained editing with verifiable forgetting; and when retrieval with\nsmall-window replay outperforms ultra-long-context reading. This yields a\nreproducible, comparable, and governable coordinate system for research and\ndeployment."
                },
                "authors": [
                    {
                        "name": "Dianxing Zhang"
                    },
                    {
                        "name": "Wendong Li"
                    },
                    {
                        "name": "Kani Song"
                    },
                    {
                        "name": "Jiaye Lu"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Liuchun Yang"
                    },
                    {
                        "name": "Sheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Li"
                },
                "author": "Sheng Li",
                "arxiv_comment": "50 pages, 1 figure, 8 tables This is a survey/framework paper on LLM\n  memory mechanisms and evaluation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18864v1",
                "updated": "2025-09-23T09:58:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    9,
                    58,
                    37,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T09:58:37Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    9,
                    58,
                    37,
                    1,
                    266,
                    0
                ],
                "title": "Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User\n  Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User\n  Profiling"
                },
                "summary": "User profiling, as a core technique for user understanding, aims to infer\nstructural attributes from user information. Large Language Models (LLMs)\nprovide a promising avenue for user profiling, yet the progress is hindered by\nthe lack of comprehensive benchmarks. To bridge this gap, we propose\nProfileBench, an industrial benchmark derived from a real-world video platform,\nencompassing heterogeneous user data and a well-structured profiling taxonomy.\nHowever, the profiling task remains challenging due to the difficulty of\ncollecting large-scale ground-truth labels, and the heterogeneous and noisy\nuser information can compromise the reliability of LLMs. To approach label-free\nand reliable user profiling, we propose a Confidence-driven Profile reasoning\nframework Conf-Profile, featuring a two-stage paradigm. We first synthesize\nhigh-quality labels by leveraging advanced LLMs with confidence hints, followed\nby confidence-weighted voting for accuracy improvement and confidence\ncalibration for a balanced distribution. The multiple profile results,\nrationales, and confidence scores are aggregated and distilled into a\nlightweight LLM. We further enhance the reasoning ability via confidence-guided\nunsupervised reinforcement learning, which exploits confidence for difficulty\nfiltering, quasi-ground truth voting, and reward weighting. Experimental\nresults demonstrate that Conf-Profile delivers substantial performance through\nthe two-stage training, improving F1 by 13.97 on Qwen3-8B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User profiling, as a core technique for user understanding, aims to infer\nstructural attributes from user information. Large Language Models (LLMs)\nprovide a promising avenue for user profiling, yet the progress is hindered by\nthe lack of comprehensive benchmarks. To bridge this gap, we propose\nProfileBench, an industrial benchmark derived from a real-world video platform,\nencompassing heterogeneous user data and a well-structured profiling taxonomy.\nHowever, the profiling task remains challenging due to the difficulty of\ncollecting large-scale ground-truth labels, and the heterogeneous and noisy\nuser information can compromise the reliability of LLMs. To approach label-free\nand reliable user profiling, we propose a Confidence-driven Profile reasoning\nframework Conf-Profile, featuring a two-stage paradigm. We first synthesize\nhigh-quality labels by leveraging advanced LLMs with confidence hints, followed\nby confidence-weighted voting for accuracy improvement and confidence\ncalibration for a balanced distribution. The multiple profile results,\nrationales, and confidence scores are aggregated and distilled into a\nlightweight LLM. We further enhance the reasoning ability via confidence-guided\nunsupervised reinforcement learning, which exploits confidence for difficulty\nfiltering, quasi-ground truth voting, and reward weighting. Experimental\nresults demonstrate that Conf-Profile delivers substantial performance through\nthe two-stage training, improving F1 by 13.97 on Qwen3-8B."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Jianbo Zhao"
                    },
                    {
                        "name": "Xueyu Ren"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Wangjie You"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Kan Zhou"
                    },
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Jiao Ran"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18862v1",
                "updated": "2025-09-23T09:55:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    9,
                    55,
                    42,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T09:55:42Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    9,
                    55,
                    42,
                    1,
                    266,
                    0
                ],
                "title": "Multi-Hierarchical Feature Detection for Large Language Model Generated\n  Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Hierarchical Feature Detection for Large Language Model Generated\n  Text"
                },
                "summary": "With the rapid advancement of large language model technology, there is\ngrowing interest in whether multi-feature approaches can significantly improve\nAI text detection beyond what single neural models achieve. While intuition\nsuggests that combining semantic, syntactic, and statistical features should\nprovide complementary signals, this assumption has not been rigorously tested\nwith modern LLM-generated text. This paper provides a systematic empirical\ninvestigation of multi-hierarchical feature integration for AI text detection,\nspecifically testing whether the computational overhead of combining multiple\nfeature types is justified by performance gains. We implement MHFD\n(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic\nanalysis, syntactic parsing, and statistical probability features through\nadaptive fusion. Our investigation reveals important negative results: despite\ntheoretical expectations, multi-feature integration provides minimal benefits\n(0.4-0.5% improvement) while incurring substantial computational costs (4.2x\noverhead), suggesting that modern neural language models may already capture\nmost relevant detection signals efficiently. Experimental results on multiple\nbenchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in\nin-domain detection and maintains 84.2% stable performance in cross-domain\ndetection, showing modest improvements of 0.4-2.6% over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of large language model technology, there is\ngrowing interest in whether multi-feature approaches can significantly improve\nAI text detection beyond what single neural models achieve. While intuition\nsuggests that combining semantic, syntactic, and statistical features should\nprovide complementary signals, this assumption has not been rigorously tested\nwith modern LLM-generated text. This paper provides a systematic empirical\ninvestigation of multi-hierarchical feature integration for AI text detection,\nspecifically testing whether the computational overhead of combining multiple\nfeature types is justified by performance gains. We implement MHFD\n(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic\nanalysis, syntactic parsing, and statistical probability features through\nadaptive fusion. Our investigation reveals important negative results: despite\ntheoretical expectations, multi-feature integration provides minimal benefits\n(0.4-0.5% improvement) while incurring substantial computational costs (4.2x\noverhead), suggesting that modern neural language models may already capture\nmost relevant detection signals efficiently. Experimental results on multiple\nbenchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in\nin-domain detection and maintains 84.2% stable performance in cross-domain\ndetection, showing modest improvements of 0.4-2.6% over existing methods."
                },
                "authors": [
                    {
                        "name": "Luyan Zhang"
                    },
                    {
                        "name": "Xinyu Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Xie"
                },
                "author": "Xinyu Xie",
                "arxiv_comment": "9 pages, 6 tables, empirical study on multi-feature AI text detection",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09790v2",
                "updated": "2025-09-23T09:52:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    9,
                    52,
                    43,
                    1,
                    266,
                    0
                ],
                "published": "2025-07-13T21:05:01Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    21,
                    5,
                    1,
                    6,
                    194,
                    0
                ],
                "title": "Prompting for Performance: Exploring LLMs for Configuring Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting for Performance: Exploring LLMs for Configuring Software"
                },
                "summary": "Software systems usually provide numerous configuration options that can\naffect performance metrics such as execution time, memory usage, binary size,\nor bitrate. On the one hand, making informed decisions is challenging and\nrequires domain expertise in options and their combinations. On the other hand,\nmachine learning techniques can search vast configuration spaces, but with a\nhigh computational cost, since concrete executions of numerous configurations\nare required. In this exploratory study, we investigate whether large language\nmodels (LLMs) can assist in performance-oriented software configuration through\nprompts. We evaluate several LLMs on tasks including identifying relevant\noptions, ranking configurations, and recommending performant configurations\nacross various configurable systems, such as compilers, video encoders, and SAT\nsolvers. Our preliminary results reveal both positive abilities and notable\nlimitations: depending on the task and systems, LLMs can well align with expert\nknowledge, whereas hallucinations or superficial reasoning can emerge in other\ncases. These findings represent a first step toward systematic evaluations and\nthe design of LLM-based solutions to assist with software configuration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software systems usually provide numerous configuration options that can\naffect performance metrics such as execution time, memory usage, binary size,\nor bitrate. On the one hand, making informed decisions is challenging and\nrequires domain expertise in options and their combinations. On the other hand,\nmachine learning techniques can search vast configuration spaces, but with a\nhigh computational cost, since concrete executions of numerous configurations\nare required. In this exploratory study, we investigate whether large language\nmodels (LLMs) can assist in performance-oriented software configuration through\nprompts. We evaluate several LLMs on tasks including identifying relevant\noptions, ranking configurations, and recommending performant configurations\nacross various configurable systems, such as compilers, video encoders, and SAT\nsolvers. Our preliminary results reveal both positive abilities and notable\nlimitations: depending on the task and systems, LLMs can well align with expert\nknowledge, whereas hallucinations or superficial reasoning can emerge in other\ncases. These findings represent a first step toward systematic evaluations and\nthe design of LLM-based solutions to assist with software configuration."
                },
                "authors": [
                    {
                        "name": "Helge Spieker"
                    },
                    {
                        "name": "ThÃ©o Matricon"
                    },
                    {
                        "name": "Nassim Belmecheri"
                    },
                    {
                        "name": "JÃ¸rn Eirik Betten"
                    },
                    {
                        "name": "Gauthier Le Bartz Lyan"
                    },
                    {
                        "name": "Heraldo Borges"
                    },
                    {
                        "name": "Quentin Mazouni"
                    },
                    {
                        "name": "Dennis Gross"
                    },
                    {
                        "name": "Arnaud Gotlieb"
                    },
                    {
                        "name": "Mathieu Acher"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Acher"
                },
                "author": "Mathieu Acher",
                "arxiv_comment": "ICTAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18851v1",
                "updated": "2025-09-23T09:38:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    9,
                    38,
                    10,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T09:38:10Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    9,
                    38,
                    10,
                    1,
                    266,
                    0
                ],
                "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NGRPO: Negative-enhanced Group Relative Policy Optimization"
                },
                "summary": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)\nacross various tasks. However, GRPO, a representative RLVR algorithm, suffers\nfrom a critical limitation: when all responses within a group are either\nentirely correct or entirely incorrect, the model fails to learn from these\nhomogeneous responses. This is particularly problematic for homogeneously\nincorrect groups, where GRPO's advantage function yields a value of zero,\nleading to null gradients and the loss of valuable learning signals. To\novercome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy\nOptimization), an algorithm designed to convert homogeneous errors into robust\nlearning signals. First, NGRPO introduces Advantage Calibration. This mechanism\nhypothesizes the existence of a virtual maximum-reward sample during advantage\ncalculation, thereby altering the mean and variance of rewards within a group\nand ensuring that the advantages for homogeneously incorrect samples are no\nlonger zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the\nupdate magnitude for positive samples while imposing stricter constraints on\nthat of negative samples. This serves to stabilize the exploration pressure\nintroduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B\ndemonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,\nDAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and\nAIME2025. These results validate NGRPO's ability to learn from homogeneous\nerrors, leading to stable and substantial improvements in mathematical\nreasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)\nacross various tasks. However, GRPO, a representative RLVR algorithm, suffers\nfrom a critical limitation: when all responses within a group are either\nentirely correct or entirely incorrect, the model fails to learn from these\nhomogeneous responses. This is particularly problematic for homogeneously\nincorrect groups, where GRPO's advantage function yields a value of zero,\nleading to null gradients and the loss of valuable learning signals. To\novercome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy\nOptimization), an algorithm designed to convert homogeneous errors into robust\nlearning signals. First, NGRPO introduces Advantage Calibration. This mechanism\nhypothesizes the existence of a virtual maximum-reward sample during advantage\ncalculation, thereby altering the mean and variance of rewards within a group\nand ensuring that the advantages for homogeneously incorrect samples are no\nlonger zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the\nupdate magnitude for positive samples while imposing stricter constraints on\nthat of negative samples. This serves to stabilize the exploration pressure\nintroduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B\ndemonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,\nDAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and\nAIME2025. These results validate NGRPO's ability to learn from homogeneous\nerrors, leading to stable and substantial improvements in mathematical\nreasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO."
                },
                "authors": [
                    {
                        "name": "Gongrui Nan"
                    },
                    {
                        "name": "Siye Chen"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Mengyu Lu"
                    },
                    {
                        "name": "Dexun Wang"
                    },
                    {
                        "name": "Chunmei Xie"
                    },
                    {
                        "name": "Weiqi Xiong"
                    },
                    {
                        "name": "Xianzhou Zeng"
                    },
                    {
                        "name": "Qixuan Zhou"
                    },
                    {
                        "name": "Yadong Li"
                    },
                    {
                        "name": "Xingzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xingzhong Xu"
                },
                "author": "Xingzhong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]