[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v1",
                "updated": "2024-09-12T15:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01699v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01699v5",
                "updated": "2024-09-12T10:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    35,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-03T04:03:28Z",
                "published_parsed": [
                    2023,
                    3,
                    3,
                    4,
                    3,
                    28,
                    4,
                    62,
                    0
                ],
                "title": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect"
                },
                "summary": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors."
                },
                "authors": [
                    {
                        "name": "Priya Sharma"
                    },
                    {
                        "name": "Alexander V. Balatsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Balatsky"
                },
                "author": "Alexander V. Balatsky",
                "arxiv_doi": "10.1103/PhysRevB.110.094302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.094302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01699v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01699v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. B 110, 094302 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07704v1",
                "updated": "2024-09-12T02:13:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:13:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Super Monotonic Alignment Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super Monotonic Alignment Search"
                },
                "summary": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}."
                },
                "authors": [
                    {
                        "name": "Junhyeok Lee"
                    },
                    {
                        "name": "Hyeongju Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongju Kim"
                },
                "author": "Hyeongju Kim",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v1",
                "updated": "2024-09-11T11:40:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v2",
                "updated": "2024-09-11T08:12:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    12,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.12453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.12453v2",
                "updated": "2024-09-11T02:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    33,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2022-08-26T06:28:08Z",
                "published_parsed": [
                    2022,
                    8,
                    26,
                    6,
                    28,
                    8,
                    4,
                    238,
                    0
                ],
                "title": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems"
                },
                "summary": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shuaifei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Zhang"
                },
                "author": "Jiayi Zhang",
                "arxiv_comment": "The focus of the research has shifted, and the current submission is\n  no longer aligned with our objectives",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.12453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.12453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benoît Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v1",
                "updated": "2024-08-26T03:58:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) aims to train a shared model using data and\ncomputation power on distributed agents coordinated by a central server.\nDecentralized FL (DFL) utilizes local model exchange and aggregation between\nagents to reduce the communication and computation overheads on the central\nserver. However, when agents are mobile, the communication opportunity between\nagents can be sporadic, largely hindering the convergence and accuracy of DFL.\nIn this paper, we study delay-tolerant model spreading and aggregation enabled\nby model caching on mobile agents. Each agent stores not only its own model,\nbut also models of agents encountered in the recent past. When two agents meet,\nthey exchange their own models as well as the cached models. Local model\naggregation works on all models in the cache. We theoretically analyze the\nconvergence of DFL with cached models, explicitly taking into account the model\nstaleness introduced by caching. We design and compare different model caching\nalgorithms for different DFL and mobility scenarios. We conduct detailed case\nstudies in a vehicular network to systematically investigate the interplay\nbetween agent mobility, cache staleness, and model convergence. In our\nexperiments, cached DFL converges quickly, and significantly outperforms DFL\nwithout caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13605v1",
                "updated": "2024-08-24T15:23:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "published": "2024-08-24T15:23:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    15,
                    23,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Edge Computing Networks: Online Low-Latency and Fresh Service\n  Provisioning"
                },
                "summary": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge service caching can significantly mitigate latency and reduce\ncommunication and computing overhead by fetching and initializing services\n(applications) from clouds. The freshness of cached service data is critical\nwhen providing satisfactory services to users, but has been overlooked in\nexisting research efforts. In this paper, we study the online low-latency and\nfresh service provisioning in mobile edge computing (MEC) networks.\nSpecifically, we jointly optimize the service caching, task offloading, and\nresource allocation without knowledge of future system information, which is\nformulated as a joint online long-term optimization problem. This problem is\nNP-hard. To solve the problem, we design a Lyapunov-based online framework that\ndecouples the problem at temporal level into a series of per-time-slot\nsubproblems. For each subproblem, we propose an online integrated\noptimization-deep reinforcement learning (OIODRL) method, which contains an\noptimization stage including a quadratically constrained quadratic program\n(QCQP) transformation and a semidefinite relaxation (SDR) method, and a\nlearning stage including a deep reinforcement learning (DRL) algorithm.\nExtensive simulations show that the proposed OIODRL method achieves a\nnear-optimal solution and outperforms other benchmark methods."
                },
                "authors": [
                    {
                        "name": "Yuhan Yi"
                    },
                    {
                        "name": "Guanglin Zhang"
                    },
                    {
                        "name": "Hai Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Jiang"
                },
                "author": "Hai Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v3",
                "updated": "2024-08-23T17:54:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    54,
                    34,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency without sacrificing performance but the\nconventional wisdom suggests that its efficacy is limited to small batch sizes.\nIn MagicDec, we show that surprisingly SD can achieve speedup even for a high\nthroughput inference regime for moderate to long sequences. More interestingly,\nan intelligent drafting strategy can achieve better speedup with increasing\nbatch size based on our rigorous analysis. MagicDec first identifies the\nbottleneck shifts with increasing batch size and sequence length, and uses\nthese insights to deploy speculative decoding more effectively for high\nthroughput inference. Then, it leverages draft models with sparse KV cache to\naddress the KV bottleneck that scales with both sequence length and batch size.\nThis finding underscores the broad applicability of speculative decoding in\nlong-context serving, as it can enhance throughput and reduce latency without\ncompromising accuracy. For moderate to long sequences, we demonstrate up to 2x\nspeedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving\nbatch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available\nat https://github.com/Infini-AI-Lab/MagicDec/."
                },
                "authors": [
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13165v1",
                "updated": "2024-08-23T15:39:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T15:39:20Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    15,
                    39,
                    20,
                    4,
                    236,
                    0
                ],
                "title": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches"
                },
                "summary": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a variant of the coded caching problem where users connect to two\ntypes of caches, called private caches and access caches. The problem setting\nconsists of a server having a library of files and a set of access caches.\nEvery user, equipped with a private cache, connects to $L$ neighboring access\ncaches in a cyclic wrap-around fashion. The server populates the private and\naccess caches with file contents in either coded or uncoded format. For this\nsetting, we derive a lower bound on the optimal worst-case transmission rate\nusing cut-set arguments. This lower bound applies to both coded and uncoded\nplacements. We then provide an achievable scheme with uncoded placement and\nshow that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for\nthe dedicated cache network in the absence of access caches. Finally, we show\nthat the proposed scheme achieves optimality in large memory regimes and\nprovide numerical plots comparing the rate of the proposed scheme with the\nderived lower bound, demonstrating the optimality of our scheme."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "15 pages, 5 figures and one table. Some overlap of introductory and\n  background materials with our earlier submission arXiv:2407.00677v1 dated 30\n  June 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05332v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05332v5",
                "updated": "2024-08-23T13:25:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    13,
                    25,
                    7,
                    4,
                    236,
                    0
                ],
                "published": "2023-05-09T10:41:36Z",
                "published_parsed": [
                    2023,
                    5,
                    9,
                    10,
                    41,
                    36,
                    1,
                    129,
                    0
                ],
                "title": "Fundamental Limits of Multi-Message Private Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Multi-Message Private Computation"
                },
                "summary": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a typical formulation of the private information retrieval (PIR) problem,\na single user wishes to retrieve one out of $ K$ files from $N$ servers without\nrevealing the demanded file index to any server. This paper formulates an\nextended model of PIR, referred to as multi-message private computation\n(MM-PC), where instead of retrieving a single file, the user wishes to retrieve\n$P>1$ linear combinations of files while preserving the privacy of the demand\ninformation. The MM-PC problem is a generalization of the private computation\n(PC) problem (where the user requests one linear combination of the files), and\nthe multi-message private information retrieval (MM-PIR) problem (where the\nuser requests $P>1$ files). A baseline achievable scheme repeats the optimal PC\nscheme by Sun and Jafar $P$ times, or treats each possible demanded linear\ncombination as an independent file and then uses the near optimal MM-PIR scheme\nby Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that\nsignificantly improves upon the baseline schemes. In doing so, we design the\nqueries inspired by the structure in the cache-aided scalar linear function\nretrieval scheme by Wan {\\it et al.}, which leverages the dependency between\nlinear functions to reduce the amount of communications. To ensure the\ndecodability of our scheme, we propose a new method to benefit from the\nexisting dependency, referred to as the sign assignment step. In the end, we\nuse Maximum Distance Separable matrices to code the queries, which allows the\nreduction of download from the servers, while preserving privacy. By the\nproposed schemes, we characterize the capacity within a multiplicative factor\nof $2$."
                },
                "authors": [
                    {
                        "name": "Ali Gholami"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Tayyebeh Jahani-Nezhad"
                    },
                    {
                        "name": "Hua Sun"
                    },
                    {
                        "name": "Mingyue Ji"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "A version of this paper is submitted to IEEE Transactions on\n  Communications. A short version was accepted and presented at ISIT 2024 in\n  Athens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05332v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05332v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12947v1",
                "updated": "2024-08-23T09:54:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "published": "2024-08-23T09:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    54,
                    22,
                    4,
                    236,
                    0
                ],
                "title": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Part of the Heap is Useful? Improving Heap Liveness Analysis"
                },
                "summary": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing sizes of data structures allocated in heap, understanding\nthe actual use of heap memory is critically important for minimizing cache\nmisses and reclaiming unused memory. A static analysis aimed at this is\ndifficult because the heap locations are unnamed. Using allocation sites to\nname them creates very few distinctions making it difficult to identify\nallocated heap locations that are not used. Heap liveness analysis using access\ngraphs solves this problem by (a) using a storeless model of heap memory by\nnaming the locations with access paths, and (b) representing the unbounded sets\nof access paths (which are regular languages) as finite automata.\n  We improve the scalability and efficiency of heap liveness analysis, and\nreduce the amount of computed heap liveness information by using deterministic\nautomata and by minimizing the inclusion of aliased access paths in the\nlanguage. Practically, our field-, flow-, context-sensitive liveness analysis\non SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5\nkLoC) and improves efficiency even up to 99%. For some of the benchmarks, our\ntechnique shows multifold reduction in the computed liveness information,\nranging from 2 to 100 times (in terms of the number of live access paths),\nwithout compromising on soundness."
                },
                "authors": [
                    {
                        "name": "Vini Kanvar"
                    },
                    {
                        "name": "Uday P. Khedker"
                    }
                ],
                "author_detail": {
                    "name": "Uday P. Khedker"
                },
                "author": "Uday P. Khedker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v1",
                "updated": "2024-08-22T17:56:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "13 pages, 16 figures, Submitted to ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14533v2",
                "updated": "2024-08-22T17:47:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    47,
                    49,
                    3,
                    235,
                    0
                ],
                "published": "2023-09-25T21:17:17Z",
                "published_parsed": [
                    2023,
                    9,
                    25,
                    21,
                    17,
                    17,
                    0,
                    268,
                    0
                ],
                "title": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties"
                },
                "summary": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layered CoO$_2$ is of great interest for its promising properties but is\nmeta-stable in its bulk form. CoO$_2$ was synthesized by converting the\nquasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a\nhydrothermal treatment. The resulting nanostructures were predominantly\nnanoscrolls with very thin walls, which exhibit long-term stability. A detailed\nstructural investigation reveals that the CoO$_2$ is found to crystallize in\nmonoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure.\nIndividual nanoscrolls are characterized electrically and show a p-type\nsemiconducting nature with a high current-carrying capacity of 4$\\cdot$10$^5$ A\ncm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The\nresults demonstrate the possibility to stabilize meta-stable materials in\nlow-dimensional forms and a promising application of the nanoscrolls as\ninterconnect in high-voltage electronic circuitry."
                },
                "authors": [
                    {
                        "name": "Simon Hettler"
                    },
                    {
                        "name": "Kankona Singha Roy"
                    },
                    {
                        "name": "Raul Arenal"
                    },
                    {
                        "name": "Leela S. Panchakarla"
                    }
                ],
                "author_detail": {
                    "name": "Leela S. Panchakarla"
                },
                "author": "Leela S. Panchakarla",
                "arxiv_doi": "10.1002/admi.202400317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/admi.202400317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.14533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Adv. Mater. Interfaces 2024, 2400317",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11506v1",
                "updated": "2024-08-21T10:26:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T10:26:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    10,
                    26,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rheological behavior of molybdenum disulfide (MoS2) inks under electric\n  fields: influence of concentration and voltage"
                },
                "summary": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work provides a complete rheological characterization of molybdenum\ndisulfide (MoS2) inks in the presence of electric fields. Several\nconcentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The\nlubrication effects are present in the ink when the MoS2 concentration is\nhigher than 0.10% w/w. The dielectric properties show the impossibility of a\npositive electrorheological effect for all MoS2-inks studied. The formation of\nvortices and electromigration of MoS2 particles occur under the influence of an\nexternal electric field. These two phenomena affect the rheological behavior of\nMoS2-inks under shear flow condition. Relatively to the extensional rheology\nexperiments, the particle migration and the vortex formation promote anisotropy\non the rheological properties of the inks which affects the relaxation time,\nthe formation of beads-on-a-string and the uniaxial elongational flow condition\nis no longer valid. When the electric field strength is 1.5 kV/mm, the\nformation of Taylor's cone is observed and independent of MoS2 concentration."
                },
                "authors": [
                    {
                        "name": "Pedro C Rijo"
                    },
                    {
                        "name": "Francisco J. Galindo-Rosales"
                    }
                ],
                "author_detail": {
                    "name": "Francisco J. Galindo-Rosales"
                },
                "author": "Francisco J. Galindo-Rosales",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10685v2",
                "updated": "2024-08-21T06:10:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    6,
                    10,
                    2,
                    2,
                    234,
                    0
                ],
                "published": "2024-01-19T13:32:55Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    13,
                    32,
                    55,
                    4,
                    19,
                    0
                ],
                "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards End-to-End GPS Localization with Neural Pseudorange Correction"
                },
                "summary": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pseudorange error is one of the root causes of localization inaccuracy in\nGPS. Previous data-driven methods regress and eliminate pseudorange errors\nusing handcrafted intermediate labels. Unlike them, we propose an end-to-end\nGPS localization framework, E2E-PrNet, to train a neural network for\npseudorange correction (PrNet) directly using the final task loss calculated\nwith the ground truth of GPS receiver states. The gradients of the loss with\nrespect to learnable parameters are backpropagated through a Differentiable\nNonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing\nthe data-driven neural network and the model-based DNLS module is verified with\nGPS data collected by Android phones, showing that E2E-PrNet outperforms the\nbaseline weighted least squares method and the state-of-the-art end-to-end\ndata-driven approach. Finally, we discuss the explainability of E2E-PrNet."
                },
                "authors": [
                    {
                        "name": "Xu Weng"
                    },
                    {
                        "name": "KV Ling"
                    },
                    {
                        "name": "Haochen Liu"
                    },
                    {
                        "name": "Kun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Kun Cao"
                },
                "author": "Kun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11325v1",
                "updated": "2024-08-21T04:16:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "published": "2024-08-21T04:16:49Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    16,
                    49,
                    2,
                    234,
                    0
                ],
                "title": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Telepathic Datacenters: Fast RPCs using Shared CXL Memory"
                },
                "summary": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datacenter applications often rely on remote procedure calls (RPCs) for fast,\nefficient, and secure communication. However, RPCs are slow, inefficient, and\nhard to use as they require expensive serialization and compression to\ncommunicate over a packetized serial network link. Compute Express Link 3.0\n(CXL) offers an alternative solution, allowing applications to share data using\na cache-coherent, shared-memory interface across clusters of machines.\n  RPCool is a new framework that exploits CXL's shared memory capabilities.\nRPCool avoids serialization by passing pointers to data structures in shared\nmemory. While avoiding serialization is useful, directly sharing pointer-rich\ndata eliminates the isolation that copying data over traditional networks\nprovides, leaving the receiver vulnerable to invalid pointers and concurrent\nupdates to shared data by the sender. RPCool restores this safety with careful\nand efficient management of memory permissions. Another significant challenge\nwith CXL shared memory capabilities is that they are unlikely to scale to an\nentire datacenter. RPCool addresses this by falling back to RDMA-based\ncommunication.\n  Overall, RPCool reduces the round-trip latency by 1.93$\\times$ and\n7.2$\\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms,\nrespectively. Moreover, RPCool performs either comparably or better than other\nRPC mechanisms across a range of workloads."
                },
                "authors": [
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ehsan Hajyjasini"
                    },
                    {
                        "name": "Seungjin Lee"
                    },
                    {
                        "name": "Zifeng Zhang"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10970v1",
                "updated": "2024-08-20T16:02:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T16:02:54Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    16,
                    2,
                    54,
                    1,
                    233,
                    0
                ],
                "title": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical\n  Planning and Control"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work has demonstrated that a class of hybrid state-space\nmodel known as recurrent switching linear dynamical systems (rSLDS) discover\nmeaningful behavioural units via the piecewise linear decomposition of complex\ncontinuous dynamics (Linderman et al., 2016). Furthermore, they model how the\nunderlying continuous states drive these discrete mode switches. We propose\nthat the rich representations formed by an rSLDS can provide useful\nabstractions for planning and control. We present a novel hierarchical\nmodel-based algorithm inspired by Active Inference in which a discrete MDP sits\nabove a low-level linear-quadratic controller. The recurrent transition\ndynamics learned by the rSLDS allow us to (1) specify temporally-abstracted\nsub-goals in a method reminiscent of the options framework, (2) lift the\nexploration into discrete space allowing us to exploit information-theoretic\nexploration bonuses and (3) `cache' the approximate solutions to low-level\nproblems in the discrete planner. We successfully apply our model to the sparse\nContinuous Mountain Car task, demonstrating fast system identification via\nenhanced exploration and non-trivial planning through the delineation of\nabstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "4 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10746v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10746v1",
                "updated": "2024-08-20T11:30:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-20T11:30:12Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    11,
                    30,
                    12,
                    1,
                    233,
                    0
                ],
                "title": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI\n  Framework for Personal LLMs Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a plethora of powerful\napplications at the network edge, such as intelligent personal assistants. Data\nprivacy and security concerns have prompted a shift towards edge-based\nfine-tuning of personal LLMs, away from cloud reliance. However, this raises\nissues of computational intensity and resource scarcity, hindering training\nefficiency and feasibility. While current studies investigate\nparameter-efficient fine-tuning (PEFT) techniques to mitigate resource\nconstraints, our analysis indicates that these techniques are not sufficiently\nresource-efficient for edge devices. To tackle these challenges, we propose\nPluto and Charon (PAC), a time and memory efficient collaborative edge AI\nframework for personal LLMs fine-tuning. PAC breaks the resource wall of\npersonal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1)\nAlgorithmically, PAC implements a personal LLMs fine-tuning technique that is\nefficient in terms of parameters, time, and memory. It utilizes Parallel\nAdapters to circumvent the need for a full backward pass through the LLM\nbackbone. Additionally, an activation cache mechanism further streamlining the\nprocess by negating the necessity for repeated forward passes across multiple\nepochs. (2) Systematically, PAC leverages edge devices in close proximity,\npooling them as a collective resource for in-situ personal LLMs fine-tuning,\nutilizing a hybrid data and pipeline parallelism to orchestrate distributed\ntraining. The use of the activation cache eliminates the need for forward pass\nthrough the LLM backbone,enabling exclusive fine-tuning of the Parallel\nAdapters using data parallelism. Extensive evaluation based on prototype\nimplementation demonstrates that PAC remarkably outperforms state-of-the-art\napproaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction\nin memory footprint."
                },
                "authors": [
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Jingyi Li"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by The 53rd International Conference on Parallel Processing\n  (ICPP'24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10746v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10746v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09697v2",
                "updated": "2024-08-20T04:46:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    20,
                    4,
                    46,
                    18,
                    1,
                    233,
                    0
                ],
                "published": "2024-08-19T04:43:56Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    43,
                    56,
                    0,
                    232,
                    0
                ],
                "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heta: Distributed Training of Heterogeneous Graph Neural Networks"
                },
                "summary": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic\nrelationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable\nlearning performance in various applications. However, current distributed GNN\ntraining systems often overlook unique characteristics of HetGs, such as\nvarying feature dimensions and the prevalence of missing features among nodes,\nleading to suboptimal performance or even incompatibility with distributed HGNN\ntraining. We introduce Heta, a framework designed to address the communication\nbottleneck in distributed HGNN training. Heta leverages the inherent structure\nof HGNNs - independent relation-specific aggregations for each relation,\nfollowed by a cross-relation aggregation - and advocates for a novel\nRelation-Aggregation-First computation paradigm. It performs relation-specific\naggregations within graph partitions and then exchanges partial aggregations.\nThis design, coupled with a new graph partitioning method that divides a HetG\nbased on its graph schema and HGNN computation dependency, substantially\nreduces communication overhead. Heta further incorporates an innovative GPU\nfeature caching strategy that accounts for the different cache miss-penalties\nassociated with diverse node types. Comprehensive evaluations of various HGNN\nmodels and large heterogeneous graph datasets demonstrate that Heta outperforms\nstate-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in\nend-to-end epoch time, respectively."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhong"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Minjie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Minjie Wang"
                },
                "author": "Minjie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10104v1",
                "updated": "2024-08-19T15:47:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T15:47:17Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    15,
                    47,
                    17,
                    0,
                    232,
                    0
                ],
                "title": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory"
                },
                "summary": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The strong electric field between the sample and the extractor is the core of\ncathode lenses and a pivotal determinant of high resolution. Nevertheless,\nfields in the range of 3-8 kV/mm can be a source of complications. Local field\nenhancement at sharp edges or microscopic protrusions of cleaved samples may\nresult in field emission or flashovers. Moreover, slow background electrons are\ndrawn into the microscope column, where they contribute to space charge\neffects. A novel front lens configuration, optimized through ray-tracing\nsimulations, significantly reduces the field at the sample and allows even for\nzero field or retarding field, which serves to suppress space charge effects.\nOne or several annular electrodes, situated in a concentric position relative\nto the extractor, serve to form an additional lens within the gap between the\nsample and the extractor. The refractory power of this lens, and consequently\nthe field at the sample surface, can be modified by adjusting the potentials of\nthe annular electrodes. The imaging properties and aberrations of this gap lens\nhave been investigated with regard to momentum imaging and XPEEM. The study\nencompasses the energy range from the few-eV level for laser-ARPES to 6 keV,\nfor hard X-ray ARPES. The additional converging lens situated in close\nproximity to the sample exhibits a reduced field curvature of the k-image in\nthe backfocal plane. This allows for the acquisition of larger fields of view\nin both momentum and real-space imaging."
                },
                "authors": [
                    {
                        "name": "Olena Tkach"
                    },
                    {
                        "name": "Gerd Schoenhense"
                    }
                ],
                "author_detail": {
                    "name": "Gerd Schoenhense"
                },
                "author": "Gerd Schoenhense",
                "arxiv_comment": "17 pages, 4 figures, 44 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09848v1",
                "updated": "2024-08-19T09:50:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T09:50:35Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    9,
                    50,
                    35,
                    0,
                    232,
                    0
                ],
                "title": "Abstract Environment Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstract Environment Trimming"
                },
                "summary": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable sharing is a fundamental property in the static analysis of logic\nprograms, since it is instrumental for ensuring correctness and increasing\nprecision while inferring many useful program properties. Such properties\ninclude modes, determinacy, non-failure, cost, etc. This has motivated\nsignificant work on developing abstract domains to improve the precision and\nperformance of sharing analyses. Much of this work has centered around the\nfamily of set-sharing domains, because of the high precision they offer.\nHowever, this comes at a price: their scalability to a wide set of realistic\nprograms remains challenging and this hinders their wider adoption. In this\nwork, rather than defining new sharing abstract domains, we focus instead on\ndeveloping techniques which can be incorporated in the analyzers to address\naspects that are known to affect the efficiency of these domains, such as the\nnumber of variables, without affecting precision. These techniques are inspired\nin others used in the context of compiler optimizations, such as expression\nreassociation and variable trimming. We present several such techniques and\nprovide an extensive experimental evaluation of over 1100 program modules taken\nfrom both production code and classical benchmarks. This includes the\nSpectector cache analyzer, the s(CASP) system, the libraries of the Ciao\nsystem, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental\nresults are quite encouraging: we have obtained significant speed-ups, and,\nmore importantly, the number of modules that require a timeout was cut in half.\nAs a result, many more programs can be analyzed precisely in reasonable times."
                },
                "authors": [
                    {
                        "name": "Daniel Jurjo-Rivas"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "61 pages, 10 figures, 7 tables, submitted to ICLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10284v1",
                "updated": "2024-08-19T03:27:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "published": "2024-08-19T03:27:15Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    27,
                    15,
                    0,
                    232,
                    0
                ],
                "title": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for\n  Efficient MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models are designed to enhance the efficiency of\nlarge language models (LLMs) without proportionally increasing the\ncomputational demands. However, their deployment on edge devices still faces\nsignificant challenges due to high on-demand loading overheads from managing\nsparsely activated experts. This paper introduces AdapMoE, an algorithm-system\nco-design framework for efficient MoE inference. AdapMoE features adaptive\nexpert gating and management to reduce the on-demand loading overheads. We\nobserve the heterogeneity of experts loading across layers and tokens, based on\nwhich we propose a sensitivity-based strategy to adjust the number of activated\nexperts dynamically. Meanwhile, we also integrate advanced prefetching and\ncache management techniques to further reduce the loading latency. Through\ncomprehensive evaluations on various platforms, we demonstrate AdapMoE\nconsistently outperforms existing techniques, reducing the average number of\nactivated experts by 25% and achieving a 1.35x speedup without accuracy\ndegradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_doi": "10.1145/3676536.3676741",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676741",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.10284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07092v2",
                "updated": "2024-08-18T17:27:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    17,
                    27,
                    17,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-11T18:40:36Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    40,
                    36,
                    6,
                    224,
                    0
                ],
                "title": "Post-Training Sparse Attention with Double Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Training Sparse Attention with Double Sparsity"
                },
                "summary": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process for large language models is slow and memory-intensive,\nwith one of the most critical bottlenecks being excessive Key-Value (KV) cache\naccesses. This paper introduces \"Double Sparsity,\" a novel post-training sparse\nattention technique designed to alleviate this bottleneck by reducing KV cache\naccess. Double Sparsity combines token sparsity, which focuses on utilizing\nonly the important tokens for computing self-attention, with channel sparsity,\nan approach that uses important feature channels for identifying important\ntokens. Our key insight is that the pattern of channel sparsity is relatively\nstatic, allowing us to use offline calibration to make it efficient at runtime,\nthereby enabling accurate and efficient identification of important tokens.\nMoreover, this method can be combined with offloading to achieve significant\nmemory usage reduction. Experimental results demonstrate that Double Sparsity\ncan achieve $\\frac{1}{16}$ token and channel sparsity with minimal impact on\naccuracy across various tasks, including wiki-2 perplexity, key-value\nretrieval, and long context benchmarks with models including Llama-2-7B,\nLlama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\\times$ acceleration in\nattention operations and a 1.9$\\times$ improvement in end-to-end inference on\nGPUs. With offloading, it achieves a decoding speed acceleration of\n16.3$\\times$ compared to state-of-the-art solutions at a sequence length of\n256K. Our code is publicly available at\nhttps://github.com/andy-yang-1/DoubleSparse."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Lianmin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Lianmin Zheng"
                },
                "author": "Lianmin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09483v1",
                "updated": "2024-08-18T13:54:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "published": "2024-08-18T13:54:46Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    13,
                    54,
                    46,
                    6,
                    231,
                    0
                ],
                "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMD: A Cache-assisted GPU Memory Deduplication Architecture"
                },
                "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we\ndivided these accesses into three types: (1) Write, (2) Data-Read, and (3)\nRead-Only. Besides, We find that many writes are duplicate, and the duplication\ncan be inter-dup and intra-dup. While inter-dup means different memory blocks\nare identical, and intra-dup means all the 4B elements in a line are the same.\nIn this work, we propose a cache-assisted GPU memory deduplication architecture\nnamed CMD to reduce the off-chip accesses via utilizing the data duplication in\nGPU applications. CMD includes three key design contributions which aim to\nreduce the three kinds of accesses: (1) A novel GPU memory deduplication\narchitecture that removes the inter-dup and inter-dup lines. As for the\ninter-dup detection, we reduce the extra read requests caused by the\ntraditional read-verify hash process. Besides, we design several techniques to\nmanage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce\nthe reads to duplicate data. When an L2 cache miss wants to read the duplicate\nblock, if the reference block has been fetched to L2 and it is clean, we can\ncopy it to the L2 missed block without accessing off-chip DRAM. As for the\nreads to intra-dup data, CMD uses the on-chip metadata cache to get the data.\n(3) When a cache line is evicted, the clean sectors in the line are invalidated\nwhile the dirty sectors are written back. However, most read-only victims are\nre-referenced from DRAM more than twice. Therefore, we add a full-associate\nFIFO to accommodate the read-only (it is also clean) victims to reduce the\nre-reference counts. Experiments show that CMD can decrease the off-chip\naccesses by 31.01%, reduce the energy by 32.78% and improve performance by\n37.79%. Besides, CMD can improve the performance of memory-intensive workloads\nby 50.18%."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Dan Feng"
                    },
                    {
                        "name": "Wei Tong"
                    },
                    {
                        "name": "Xueliang Wei"
                    },
                    {
                        "name": "Bing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Bing Wu"
                },
                "author": "Bing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v3",
                "updated": "2024-08-16T08:46:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    8,
                    46,
                    33,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nchallenges in memory and time efficiency due to the expanding Key-Value (KV)\ncache required for long-sequence inference. Recent efforts try to reduce KV\ncache size to a given memory budget by evicting vast non-critical cache\nelements during runtime, while preserving generation quality. Our revisiting of\ncurrent eviction methods reveals that they fundamentally minimize an upper\nbound of the $L_1$ eviction loss between the pre- and post-eviction outputs of\nmulti-head self-attention mechanisms. Moreover, our analysis indicates that the\ncommon practices of uniformly assigning budgets across attention heads harm\ntheir post-eviction generation quality. In light of these findings, we propose\na simple yet effective adaptive budget allocation algorithm. This algorithm not\nonly optimizes the theoretical loss upper bound but also reduces the $L_1$\neviction loss in practice by aligning with the varied characteristics across\ndifferent heads. By integrating this algorithm into two state-of-the-art\nmethods, we demonstrate the effectiveness of using adaptive budget allocation\nto optimize KV cache eviction. Extensive evaluations on 16 datasets and the\nNeedle-in-a-Haystack test confirm significant performance improvements across\nvarious tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v1",
                "updated": "2024-08-16T06:11:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained increased popularity due to their\nremarkable success across various tasks, which has led to the active\ndevelopment of a large set of diverse LLMs. However, individual LLMs have\nlimitations when applied to complex tasks because of such factors as training\nbiases, model sizes, and the datasets used. A promising approach is to\nefficiently harness the diverse capabilities of LLMs to overcome these\nindividual limitations. Towards this goal, we introduce a novel LLM selection\nalgorithm called SelectLLM. This algorithm directs input queries to the most\nsuitable subset of LLMs from a large pool, ensuring they collectively provide\nthe correct response efficiently. SelectLLM uses a multi-label classifier,\nutilizing the classifier's predictions and confidence scores to design optimal\npolicies for selecting an optimal, query-aware, and lightweight subset of LLMs.\nOur findings show that the proposed model outperforms individual LLMs and\nachieves competitive performance compared to similarly sized, computationally\nexpensive top-performing LLM subsets. Specifically, with a similarly sized\ntop-performing LLM subset, we achieve a significant reduction in latency on two\nstandard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower\nlatency for MMLU. Additionally, we conduct comprehensive analyses and ablation\nstudies, which validate the robustness of the proposed model."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v2",
                "updated": "2024-08-16T04:12:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    16,
                    4,
                    12,
                    25,
                    4,
                    229,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v3",
                "updated": "2024-08-15T05:24:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    15,
                    5,
                    24,
                    19,
                    3,
                    228,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07853v1",
                "updated": "2024-08-14T23:42:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T23:42:46Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    23,
                    42,
                    46,
                    2,
                    227,
                    0
                ],
                "title": "A Case for Enabling Delegation of 5G Core Decisions to the RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case for Enabling Delegation of 5G Core Decisions to the RAN"
                },
                "summary": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under conventional 5G system design, the authentication and continuous\nmonitoring of user equipment (UE) demands a reliable backhaul connection\nbetween the radio access network (RAN) and the core network functions (AMF,\nAUSF, UDM, etc.). This is not a given, especially in disaster response and\nmilitary operations. We propose that, in these scenarios, decisions made by\ncore functions can be effectively delegated to the RAN by leveraging the RAN's\ncomputing resources and the micro-service programmability of the O-RAN system\narchitecture. This paper presents several concrete designs of core-RAN decision\ndelegation, including caching of core decisions and replicating some of the\ncore decision logic. Each design has revealed interesting performance and\nsecurity trade-offs that warrant further investigation."
                },
                "authors": [
                    {
                        "name": "Lucas Vancina"
                    },
                    {
                        "name": "Geoffrey Xie"
                    }
                ],
                "author_detail": {
                    "name": "Geoffrey Xie"
                },
                "author": "Geoffrey Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v2",
                "updated": "2024-08-14T09:18:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    9,
                    18,
                    2,
                    2,
                    227,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07304v1",
                "updated": "2024-08-14T05:42:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "published": "2024-08-14T05:42:35Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    5,
                    42,
                    35,
                    2,
                    227,
                    0
                ],
                "title": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At Least Factor-of-Two Optimization for RWLE-Based Homomorphic\n  Encryption"
                },
                "summary": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many modern applications that deal with sensitive data, such as healthcare\nand government services, outsource computation to cloud platforms. In such\nuntrusted environments, privacy is of vital importance. One solution to this\nproblem is homomorphic encryption (HE), a family of cryptographic schemes that\nsupport certain algebraic operations on encrypted data without the need for\ndecryption. However, despite major advancements, encryption in modern HE\nschemes still comes with a non-trivial computational overhead that can hamper\ndata-intensive workloads. To resolve this, recent research has shown that\nleveraging caching techniques, such as Rache, can significantly enhance the\nperformance of HE schemes while maintaining security. Rache unfortunately\ndisplays a key limitation in the time complexity of its caching procedure,\nwhich scales with the size of the plaintext space. Smuche is another caching\nscheme that simultaneously improves the scalability of the caching procedure\nand turns the encryption process into a constant-time operation, utilizing only\na single scalar multiplication. Even still, more can be done. In this paper, we\npresent an encryption method we call ``Zinc\" which entirely forgoes the\nmultiple caching process, replacing it with a single scalar addition, and then\ninjecting randomness that takes constant time with respect to the plaintext\nspace. This injection of randomness is similar to Smuche, and a great\nimprovement from Rache, allowing Zinc to achieve efficiency without\ncompromising security. We implement the scheme using Microsoft SEAL and compare\nits performance to vanilla CKKS."
                },
                "authors": [
                    {
                        "name": "Jonathan Ly"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Ly"
                },
                "author": "Jonathan Ly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd Büchner"
                    },
                    {
                        "name": "Leonardo Agudo Jácome"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo Jácome"
                },
                "author": "Leonardo Agudo Jácome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.08264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08264v1",
                "updated": "2024-09-12T17:56:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    56,
                    43,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:56:43Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    56,
                    43,
                    3,
                    256,
                    0
                ],
                "title": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale"
                },
                "summary": "Large language models (LLMs) show remarkable potential to act as computer\nagents, enhancing human productivity and software accessibility in multi-modal\ntasks that require planning and reasoning. However, measuring agent performance\nin realistic environments remains a challenge since: (i) most benchmarks are\nlimited to specific modalities or domains (e.g. text-only, web navigation, Q&A,\ncoding) and (ii) full benchmark evaluations are slow (on order of magnitude of\ndays) given the multi-step sequential nature of tasks. To address these\nchallenges, we introduce the Windows Agent Arena: a reproducible, general\nenvironment focusing exclusively on the Windows operating system (OS) where\nagents can operate freely within a real Windows OS and use the same wide range\nof applications, tools, and web browsers available to human users when solving\ntasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse\nWindows tasks across representative domains that require agent abilities in\nplanning, screen understanding, and tool usage. Our benchmark is scalable and\ncan be seamlessly parallelized in Azure for a full benchmark evaluation in as\nlittle as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we\nalso introduce a new multi-modal agent, Navi. Our agent achieves a success rate\nof 19.5% in the Windows domain, compared to 74.5% performance of an unassisted\nhuman. Navi also demonstrates strong performance on another popular web-based\nbenchmark, Mind2Web. We offer extensive quantitative and qualitative analysis\nof Navi's performance, and provide insights into the opportunities for future\nresearch in agent development and data generation using Windows Agent Arena.\n  Webpage: https://microsoft.github.io/WindowsAgentArena\n  Code: https://github.com/microsoft/WindowsAgentArena",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show remarkable potential to act as computer\nagents, enhancing human productivity and software accessibility in multi-modal\ntasks that require planning and reasoning. However, measuring agent performance\nin realistic environments remains a challenge since: (i) most benchmarks are\nlimited to specific modalities or domains (e.g. text-only, web navigation, Q&A,\ncoding) and (ii) full benchmark evaluations are slow (on order of magnitude of\ndays) given the multi-step sequential nature of tasks. To address these\nchallenges, we introduce the Windows Agent Arena: a reproducible, general\nenvironment focusing exclusively on the Windows operating system (OS) where\nagents can operate freely within a real Windows OS and use the same wide range\nof applications, tools, and web browsers available to human users when solving\ntasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse\nWindows tasks across representative domains that require agent abilities in\nplanning, screen understanding, and tool usage. Our benchmark is scalable and\ncan be seamlessly parallelized in Azure for a full benchmark evaluation in as\nlittle as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we\nalso introduce a new multi-modal agent, Navi. Our agent achieves a success rate\nof 19.5% in the Windows domain, compared to 74.5% performance of an unassisted\nhuman. Navi also demonstrates strong performance on another popular web-based\nbenchmark, Mind2Web. We offer extensive quantitative and qualitative analysis\nof Navi's performance, and provide insights into the opportunities for future\nresearch in agent development and data generation using Windows Agent Arena.\n  Webpage: https://microsoft.github.io/WindowsAgentArena\n  Code: https://github.com/microsoft/WindowsAgentArena"
                },
                "authors": [
                    {
                        "name": "Rogerio Bonatti"
                    },
                    {
                        "name": "Dan Zhao"
                    },
                    {
                        "name": "Francesco Bonacci"
                    },
                    {
                        "name": "Dillon Dupont"
                    },
                    {
                        "name": "Sara Abdali"
                    },
                    {
                        "name": "Yinheng Li"
                    },
                    {
                        "name": "Justin Wagle"
                    },
                    {
                        "name": "Kazuhito Koishida"
                    },
                    {
                        "name": "Arthur Bucker"
                    },
                    {
                        "name": "Lawrence Jang"
                    },
                    {
                        "name": "Zack Hui"
                    }
                ],
                "author_detail": {
                    "name": "Zack Hui"
                },
                "author": "Zack Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08252v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08252v3",
                "updated": "2024-09-12T17:56:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    56,
                    40,
                    3,
                    256,
                    0
                ],
                "published": "2024-08-15T16:47:59Z",
                "published_parsed": [
                    2024,
                    8,
                    15,
                    16,
                    47,
                    59,
                    3,
                    228,
                    0
                ],
                "title": "Derivative-Free Guidance in Continuous and Discrete Diffusion Models\n  with Soft Value-Based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Derivative-Free Guidance in Continuous and Discrete Diffusion Models\n  with Soft Value-Based Decoding"
                },
                "summary": "Diffusion models excel at capturing the natural design spaces of images,\nmolecules, DNA, RNA, and protein sequences. However, rather than merely\ngenerating designs that are natural, we often aim to optimize downstream reward\nfunctions while preserving the naturalness of these design spaces. Existing\nmethods for achieving this goal often require ``differentiable'' proxy models\n(\\textit{e.g.}, classifier guidance or DPS) or involve computationally\nexpensive fine-tuning of diffusion models (\\textit{e.g.}, classifier-free\nguidance, RL-based fine-tuning). In our work, we propose a new method to\naddress these challenges. Our algorithm is an iterative sampling method that\nintegrates soft value functions, which looks ahead to how intermediate noisy\nstates lead to high rewards in the future, into the standard inference\nprocedure of pre-trained diffusion models. Notably, our approach avoids\nfine-tuning generative models and eliminates the need to construct\ndifferentiable models. This enables us to (1) directly utilize\nnon-differentiable features/reward feedback, commonly used in many scientific\ndomains, and (2) apply our method to recent discrete diffusion models in a\nprincipled way. Finally, we demonstrate the effectiveness of our algorithm\nacross several domains, including image generation, molecule generation, and\nDNA/RNA sequence generation. The code is available at\n\\href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models excel at capturing the natural design spaces of images,\nmolecules, DNA, RNA, and protein sequences. However, rather than merely\ngenerating designs that are natural, we often aim to optimize downstream reward\nfunctions while preserving the naturalness of these design spaces. Existing\nmethods for achieving this goal often require ``differentiable'' proxy models\n(\\textit{e.g.}, classifier guidance or DPS) or involve computationally\nexpensive fine-tuning of diffusion models (\\textit{e.g.}, classifier-free\nguidance, RL-based fine-tuning). In our work, we propose a new method to\naddress these challenges. Our algorithm is an iterative sampling method that\nintegrates soft value functions, which looks ahead to how intermediate noisy\nstates lead to high rewards in the future, into the standard inference\nprocedure of pre-trained diffusion models. Notably, our approach avoids\nfine-tuning generative models and eliminates the need to construct\ndifferentiable models. This enables us to (1) directly utilize\nnon-differentiable features/reward feedback, commonly used in many scientific\ndomains, and (2) apply our method to recent discrete diffusion models in a\nprincipled way. Finally, we demonstrate the effectiveness of our algorithm\nacross several domains, including image generation, molecule generation, and\nDNA/RNA sequence generation. The code is available at\n\\href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD}."
                },
                "authors": [
                    {
                        "name": "Xiner Li"
                    },
                    {
                        "name": "Yulai Zhao"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Gabriele Scalia"
                    },
                    {
                        "name": "Gokcen Eraslan"
                    },
                    {
                        "name": "Surag Nair"
                    },
                    {
                        "name": "Tommaso Biancalani"
                    },
                    {
                        "name": "Aviv Regev"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Masatoshi Uehara"
                    }
                ],
                "author_detail": {
                    "name": "Masatoshi Uehara"
                },
                "author": "Masatoshi Uehara",
                "arxiv_comment": "The code is available at https://github.com/masa-ue/SVDD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08252v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08260v1",
                "updated": "2024-09-12T17:55:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    55,
                    37,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:55:37Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    55,
                    37,
                    3,
                    256,
                    0
                ],
                "title": "Improving Text-guided Object Inpainting with Semantic Pre-inpainting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Text-guided Object Inpainting with Semantic Pre-inpainting"
                },
                "summary": "Recent years have witnessed the success of large text-to-image diffusion\nmodels and their remarkable potential to generate high-quality images. The\nfurther pursuit of enhancing the editability of images has sparked significant\ninterest in the downstream task of inpainting a novel object described by a\ntext prompt within a designated region in the image. Nevertheless, the problem\nis not trivial from two aspects: 1) Solely relying on one single U-Net to align\ntext prompt and visual object across all the denoising timesteps is\ninsufficient to generate desired objects; 2) The controllability of object\ngeneration is not guaranteed in the intricate sampling space of diffusion\nmodel. In this paper, we propose to decompose the typical single-stage object\ninpainting into two cascaded processes: 1) semantic pre-inpainting that infers\nthe semantic features of desired objects in a multi-modal feature space; 2)\nhigh-fieldity object generation in diffusion latent space that pivots on such\ninpainted semantic features. To achieve this, we cascade a Transformer-based\nsemantic inpainter and an object inpainting diffusion model, leading to a novel\nCAscaded Transformer-Diffusion (CAT-Diffusion) framework for text-guided object\ninpainting. Technically, the semantic inpainter is trained to predict the\nsemantic features of the target object conditioning on unmasked context and\ntext prompt. The outputs of the semantic inpainter then act as the informative\nvisual prompts to guide high-fieldity object generation through a reference\nadapter layer, leading to controllable object inpainting. Extensive evaluations\non OpenImages-V6 and MSCOCO validate the superiority of CAT-Diffusion against\nthe state-of-the-art methods. Code is available at\n\\url{https://github.com/Nnn-s/CATdiffusion}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the success of large text-to-image diffusion\nmodels and their remarkable potential to generate high-quality images. The\nfurther pursuit of enhancing the editability of images has sparked significant\ninterest in the downstream task of inpainting a novel object described by a\ntext prompt within a designated region in the image. Nevertheless, the problem\nis not trivial from two aspects: 1) Solely relying on one single U-Net to align\ntext prompt and visual object across all the denoising timesteps is\ninsufficient to generate desired objects; 2) The controllability of object\ngeneration is not guaranteed in the intricate sampling space of diffusion\nmodel. In this paper, we propose to decompose the typical single-stage object\ninpainting into two cascaded processes: 1) semantic pre-inpainting that infers\nthe semantic features of desired objects in a multi-modal feature space; 2)\nhigh-fieldity object generation in diffusion latent space that pivots on such\ninpainted semantic features. To achieve this, we cascade a Transformer-based\nsemantic inpainter and an object inpainting diffusion model, leading to a novel\nCAscaded Transformer-Diffusion (CAT-Diffusion) framework for text-guided object\ninpainting. Technically, the semantic inpainter is trained to predict the\nsemantic features of the target object conditioning on unmasked context and\ntext prompt. The outputs of the semantic inpainter then act as the informative\nvisual prompts to guide high-fieldity object generation through a reference\nadapter layer, leading to controllable object inpainting. Extensive evaluations\non OpenImages-V6 and MSCOCO validate the superiority of CAT-Diffusion against\nthe state-of-the-art methods. Code is available at\n\\url{https://github.com/Nnn-s/CATdiffusion}."
                },
                "authors": [
                    {
                        "name": "Yifu Chen"
                    },
                    {
                        "name": "Jingwen Chen"
                    },
                    {
                        "name": "Yingwei Pan"
                    },
                    {
                        "name": "Yehao Li"
                    },
                    {
                        "name": "Ting Yao"
                    },
                    {
                        "name": "Zhineng Chen"
                    },
                    {
                        "name": "Tao Mei"
                    }
                ],
                "author_detail": {
                    "name": "Tao Mei"
                },
                "author": "Tao Mei",
                "arxiv_comment": "ECCV 2024. Source code is available at\n  https://github.com/Nnn-s/CATdiffusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08250v1",
                "updated": "2024-09-12T17:48:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    48,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:48:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    48,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable\n  Personal Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable\n  Personal Question Answering"
                },
                "summary": "People often capture memories through photos, screenshots, and videos. While\nexisting AI-based tools enable querying this data using natural language, they\nmostly only support retrieving individual pieces of information like certain\nobjects in photos and struggle with answering more complex queries that involve\ninterpreting interconnected memories like event sequences. We conducted a\none-month diary study to collect realistic user queries and generated a\ntaxonomy of necessary contextual information for integrating with captured\nmemories. We then introduce OmniQuery, a novel system that is able to answer\ncomplex personal memory-related questions that require extracting and inferring\ncontextual information. OmniQuery augments single captured memories through\nintegrating scattered contextual information from multiple interconnected\nmemories, retrieves relevant memories, and uses a large language model (LLM) to\ncomprehensive answers. In human evaluations, we show the effectiveness of\nOmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG\nsystem, winning or tying in 74.5% of the time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People often capture memories through photos, screenshots, and videos. While\nexisting AI-based tools enable querying this data using natural language, they\nmostly only support retrieving individual pieces of information like certain\nobjects in photos and struggle with answering more complex queries that involve\ninterpreting interconnected memories like event sequences. We conducted a\none-month diary study to collect realistic user queries and generated a\ntaxonomy of necessary contextual information for integrating with captured\nmemories. We then introduce OmniQuery, a novel system that is able to answer\ncomplex personal memory-related questions that require extracting and inferring\ncontextual information. OmniQuery augments single captured memories through\nintegrating scattered contextual information from multiple interconnected\nmemories, retrieves relevant memories, and uses a large language model (LLM) to\ncomprehensive answers. In human evaluations, we show the effectiveness of\nOmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG\nsystem, winning or tying in 74.5% of the time."
                },
                "authors": [
                    {
                        "name": "Jiahao Nick Li"
                    },
                    {
                        "name": "Zhuohao Jerry Zhang"
                    },
                    {
                        "name": "Jiaju Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiaju Ma"
                },
                "author": "Jiaju Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18009v2",
                "updated": "2024-09-12T17:45:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    45,
                    37,
                    3,
                    256,
                    0
                ],
                "published": "2024-06-26T01:38:37Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    1,
                    38,
                    37,
                    2,
                    178,
                    0
                ],
                "title": "E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS"
                },
                "summary": "This paper introduces Embarrassingly Easy Text-to-Speech (E2 TTS), a fully\nnon-autoregressive zero-shot text-to-speech system that offers human-level\nnaturalness and state-of-the-art speaker similarity and intelligibility. In the\nE2 TTS framework, the text input is converted into a character sequence with\nfiller tokens. The flow-matching-based mel spectrogram generator is then\ntrained based on the audio infilling task. Unlike many previous works, it does\nnot require additional components (e.g., duration model, grapheme-to-phoneme)\nor complex techniques (e.g., monotonic alignment search). Despite its\nsimplicity, E2 TTS achieves state-of-the-art zero-shot TTS capabilities that\nare comparable to or surpass previous works, including Voicebox and\nNaturalSpeech 3. The simplicity of E2 TTS also allows for flexibility in the\ninput representation. We propose several variants of E2 TTS to improve\nusability during inference. See https://aka.ms/e2tts/ for demo samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Embarrassingly Easy Text-to-Speech (E2 TTS), a fully\nnon-autoregressive zero-shot text-to-speech system that offers human-level\nnaturalness and state-of-the-art speaker similarity and intelligibility. In the\nE2 TTS framework, the text input is converted into a character sequence with\nfiller tokens. The flow-matching-based mel spectrogram generator is then\ntrained based on the audio infilling task. Unlike many previous works, it does\nnot require additional components (e.g., duration model, grapheme-to-phoneme)\nor complex techniques (e.g., monotonic alignment search). Despite its\nsimplicity, E2 TTS achieves state-of-the-art zero-shot TTS capabilities that\nare comparable to or surpass previous works, including Voicebox and\nNaturalSpeech 3. The simplicity of E2 TTS also allows for flexibility in the\ninput representation. We propose several variants of E2 TTS to improve\nusability during inference. See https://aka.ms/e2tts/ for demo samples."
                },
                "authors": [
                    {
                        "name": "Sefik Emre Eskimez"
                    },
                    {
                        "name": "Xiaofei Wang"
                    },
                    {
                        "name": "Manthan Thakker"
                    },
                    {
                        "name": "Canrun Li"
                    },
                    {
                        "name": "Chung-Hsien Tsai"
                    },
                    {
                        "name": "Zhen Xiao"
                    },
                    {
                        "name": "Hemin Yang"
                    },
                    {
                        "name": "Zirun Zhu"
                    },
                    {
                        "name": "Min Tang"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    },
                    {
                        "name": "Naoyuki Kanda"
                    }
                ],
                "author_detail": {
                    "name": "Naoyuki Kanda"
                },
                "author": "Naoyuki Kanda",
                "arxiv_comment": "Accepted to SLT 2024. Added evaluation data, see\n  https://github.com/microsoft/e2tts-test-suite for more details",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08243v1",
                "updated": "2024-09-12T17:43:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    43,
                    27,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:43:27Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    43,
                    27,
                    3,
                    256,
                    0
                ],
                "title": "Reasoning Around Paradox with Grounded Deduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Around Paradox with Grounded Deduction"
                },
                "summary": "How can we reason around logical paradoxes without falling into them? This\npaper introduces grounded deduction or GD, a Kripke-inspired approach to\nfirst-order logic and arithmetic that is neither classical nor intuitionistic,\nbut nevertheless appears both pragmatically usable and intuitively justifiable.\nGD permits the direct expression of unrestricted recursive definitions -\nincluding paradoxical ones such as 'L := not L' - while adding dynamic typing\npremises to certain inference rules so that such paradoxes do not lead to\ninconsistency. This paper constitutes a preliminary development and\ninvestigation of grounded deduction, to be extended with further elaboration\nand deeper analysis of its intriguing properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we reason around logical paradoxes without falling into them? This\npaper introduces grounded deduction or GD, a Kripke-inspired approach to\nfirst-order logic and arithmetic that is neither classical nor intuitionistic,\nbut nevertheless appears both pragmatically usable and intuitively justifiable.\nGD permits the direct expression of unrestricted recursive definitions -\nincluding paradoxical ones such as 'L := not L' - while adding dynamic typing\npremises to certain inference rules so that such paradoxes do not lead to\ninconsistency. This paper constitutes a preliminary development and\ninvestigation of grounded deduction, to be extended with further elaboration\nand deeper analysis of its intriguing properties."
                },
                "authors": [
                    {
                        "name": "Bryan Ford"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Ford"
                },
                "author": "Bryan Ford",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B60",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08239v1",
                "updated": "2024-09-12T17:39:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    39,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:39:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    39,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real\n  Data Sources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real\n  Data Sources"
                },
                "summary": "Large Language Models still struggle in challenging scenarios that leverage\nstructured data, complex reasoning, or tool usage. In this paper, we propose\nSource2Synth: a new method that can be used for teaching LLMs new skills\nwithout relying on costly human annotations. Source2Synth takes as input a\ncustom data source and produces synthetic data points with intermediate\nreasoning steps grounded in real-world sources. Source2Synth improves the\ndataset quality by discarding low-quality generations based on their\nanswerability. We demonstrate the generality of this approach by applying it to\ntwo challenging domains: we test reasoning abilities in multi-hop question\nanswering (MHQA), and tool usage in tabular question answering (TQA). Our\nmethod improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on\nHotPotQA compared to the fine-tuned baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models still struggle in challenging scenarios that leverage\nstructured data, complex reasoning, or tool usage. In this paper, we propose\nSource2Synth: a new method that can be used for teaching LLMs new skills\nwithout relying on costly human annotations. Source2Synth takes as input a\ncustom data source and produces synthetic data points with intermediate\nreasoning steps grounded in real-world sources. Source2Synth improves the\ndataset quality by discarding low-quality generations based on their\nanswerability. We demonstrate the generality of this approach by applying it to\ntwo challenging domains: we test reasoning abilities in multi-hop question\nanswering (MHQA), and tool usage in tabular question answering (TQA). Our\nmethod improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on\nHotPotQA compared to the fine-tuned baselines."
                },
                "authors": [
                    {
                        "name": "Alisia Lupidi"
                    },
                    {
                        "name": "Carlos Gemmell"
                    },
                    {
                        "name": "Nicola Cancedda"
                    },
                    {
                        "name": "Jane Dwivedi-Yu"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "Maria Lomeli"
                    }
                ],
                "author_detail": {
                    "name": "Maria Lomeli"
                },
                "author": "Maria Lomeli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08238v1",
                "updated": "2024-09-12T17:38:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    38,
                    11,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:38:11Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    38,
                    11,
                    3,
                    256,
                    0
                ],
                "title": "Tracking Network Dynamics using Probabilistic State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracking Network Dynamics using Probabilistic State-Space Models"
                },
                "summary": "This paper introduces a probabilistic approach for tracking the dynamics of\nunweighted and directed graphs using state-space models (SSMs). Unlike\nconventional topology inference methods that assume static graphs and generate\npoint-wise estimates, our method accounts for dynamic changes in the network\nstructure over time. We model the network at each timestep as the state of the\nSSM, and use observations to update beliefs that quantify the probability of\nthe network being in a particular state. Then, by considering the dynamics of\ntransition and observation models through the update and prediction steps,\nrespectively, the proposed method can incorporate the information of real-time\ngraph signals into the beliefs. These beliefs provide a probability\ndistribution of the network at each timestep, being able to provide both an\nestimate for the network and the uncertainty it entails. Our approach is\nevaluated through experiments with synthetic and real-world networks. The\nresults demonstrate that our method effectively estimates network states and\naccounts for the uncertainty in the data, outperforming traditional techniques\nsuch as recursive least squares.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a probabilistic approach for tracking the dynamics of\nunweighted and directed graphs using state-space models (SSMs). Unlike\nconventional topology inference methods that assume static graphs and generate\npoint-wise estimates, our method accounts for dynamic changes in the network\nstructure over time. We model the network at each timestep as the state of the\nSSM, and use observations to update beliefs that quantify the probability of\nthe network being in a particular state. Then, by considering the dynamics of\ntransition and observation models through the update and prediction steps,\nrespectively, the proposed method can incorporate the information of real-time\ngraph signals into the beliefs. These beliefs provide a probability\ndistribution of the network at each timestep, being able to provide both an\nestimate for the network and the uncertainty it entails. Our approach is\nevaluated through experiments with synthetic and real-world networks. The\nresults demonstrate that our method effectively estimates network states and\naccounts for the uncertainty in the data, outperforming traditional techniques\nsuch as recursive least squares."
                },
                "authors": [
                    {
                        "name": "Victor M. Tenorio"
                    },
                    {
                        "name": "Elvin Isufi"
                    },
                    {
                        "name": "Geert Leus"
                    },
                    {
                        "name": "Antonio G. Marques"
                    }
                ],
                "author_detail": {
                    "name": "Antonio G. Marques"
                },
                "author": "Antonio G. Marques",
                "arxiv_comment": "Submitted to the 2025 IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.18258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.18258v2",
                "updated": "2024-09-12T17:38:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    38,
                    3,
                    3,
                    256,
                    0
                ],
                "published": "2023-10-27T16:44:46Z",
                "published_parsed": [
                    2023,
                    10,
                    27,
                    16,
                    44,
                    46,
                    4,
                    300,
                    0
                ],
                "title": "Validating full-spectrum fitting with a synthetic integral-field\n  spectroscopic observation of the Milky Way",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validating full-spectrum fitting with a synthetic integral-field\n  spectroscopic observation of the Milky Way"
                },
                "summary": "Ongoing deep IFS observations of disk galaxies provide opportunities for\ncomparison with the Milky Way (MW) to understand galaxy evolution. However,\nsuch comparisons are marred by many challenges such as selection effects,\ndifferences in observations and methodology, and proper validation of\nfull-spectrum fitting methods. In this study, we present a novel code GalCraft\nto address these challenges by generating mock IFS data cubes of the MW using\nsimple stellar population models and a mock MW stellar catalog derived from\nE-Galaxia. We use the widely adopted full-spectrum fitting code pPXF to\ninvestigate the ability to recover kinematics and stellar populations for an\nedge-on mock MW IFS observation. We confirm that differences in kinematics,\nmean age, [M/H], and [$\\alpha$/Fe] between thin and thick disks can be\ndistinguished. However, the age distribution is overestimated in the ranges\nbetween 2 - 4 and 12 - 14 Gyr compared to the expected values. This is likely\ndue to the age spacing and degeneracy of SSP templates. We find systematic\noffsets in the recovered kinematics due to insufficient spectral resolution and\nthe variation of line-of-sight velocity distribution with age and [M/H]. With\nfuture higher resolution and multi-[$\\alpha$/Fe] SSP templates, GalCraft will\nbe useful to validate key signatures such as [$\\alpha$/Fe]-[M/H] distribution\nat different $R$ and $|z|$ and potentially infer radial migration and kinematic\nheating efficiency to study detailed chemodynamical evolution of MW-like\ngalaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ongoing deep IFS observations of disk galaxies provide opportunities for\ncomparison with the Milky Way (MW) to understand galaxy evolution. However,\nsuch comparisons are marred by many challenges such as selection effects,\ndifferences in observations and methodology, and proper validation of\nfull-spectrum fitting methods. In this study, we present a novel code GalCraft\nto address these challenges by generating mock IFS data cubes of the MW using\nsimple stellar population models and a mock MW stellar catalog derived from\nE-Galaxia. We use the widely adopted full-spectrum fitting code pPXF to\ninvestigate the ability to recover kinematics and stellar populations for an\nedge-on mock MW IFS observation. We confirm that differences in kinematics,\nmean age, [M/H], and [$\\alpha$/Fe] between thin and thick disks can be\ndistinguished. However, the age distribution is overestimated in the ranges\nbetween 2 - 4 and 12 - 14 Gyr compared to the expected values. This is likely\ndue to the age spacing and degeneracy of SSP templates. We find systematic\noffsets in the recovered kinematics due to insufficient spectral resolution and\nthe variation of line-of-sight velocity distribution with age and [M/H]. With\nfuture higher resolution and multi-[$\\alpha$/Fe] SSP templates, GalCraft will\nbe useful to validate key signatures such as [$\\alpha$/Fe]-[M/H] distribution\nat different $R$ and $|z|$ and potentially infer radial migration and kinematic\nheating efficiency to study detailed chemodynamical evolution of MW-like\ngalaxies."
                },
                "authors": [
                    {
                        "name": "Zixian Wang"
                    },
                    {
                        "name": "Michael R. Hayden"
                    },
                    {
                        "name": "Sanjib Sharma"
                    },
                    {
                        "name": "Jesse van de Sande"
                    },
                    {
                        "name": "Joss Bland-Hawthorn"
                    },
                    {
                        "name": "Sam Vaughan"
                    },
                    {
                        "name": "Marie Martig"
                    },
                    {
                        "name": "Francesca Pinna"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Pinna"
                },
                "arxiv_affiliation": "Purmortal",
                "author": "Francesca Pinna",
                "arxiv_comment": "30 pages, 30 figures, accepted by MNRAS. GalCraft available via\n  https://github.com/purmortal/galcraft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.18258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.18258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08234v1",
                "updated": "2024-09-12T17:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    33,
                    6,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:33:06Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    33,
                    6,
                    3,
                    256,
                    0
                ],
                "title": "LLM Honeypot: Leveraging Large Language Models as Advanced Interactive\n  Honeypot Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Honeypot: Leveraging Large Language Models as Advanced Interactive\n  Honeypot Systems"
                },
                "summary": "The rapid evolution of cyber threats necessitates innovative solutions for\ndetecting and analyzing malicious activity. Honeypots, which are decoy systems\ndesigned to lure and interact with attackers, have emerged as a critical\ncomponent in cybersecurity. In this paper, we present a novel approach to\ncreating realistic and interactive honeypot systems using Large Language Models\n(LLMs). By fine-tuning a pre-trained open-source language model on a diverse\ndataset of attacker-generated commands and responses, we developed a honeypot\ncapable of sophisticated engagement with attackers. Our methodology involved\nseveral key steps: data collection and processing, prompt engineering, model\nselection, and supervised fine-tuning to optimize the model's performance.\nEvaluation through similarity metrics and live deployment demonstrated that our\napproach effectively generates accurate and informative responses. The results\nhighlight the potential of LLMs to revolutionize honeypot technology, providing\ncybersecurity professionals with a powerful tool to detect and analyze\nmalicious activity, thereby enhancing overall security infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of cyber threats necessitates innovative solutions for\ndetecting and analyzing malicious activity. Honeypots, which are decoy systems\ndesigned to lure and interact with attackers, have emerged as a critical\ncomponent in cybersecurity. In this paper, we present a novel approach to\ncreating realistic and interactive honeypot systems using Large Language Models\n(LLMs). By fine-tuning a pre-trained open-source language model on a diverse\ndataset of attacker-generated commands and responses, we developed a honeypot\ncapable of sophisticated engagement with attackers. Our methodology involved\nseveral key steps: data collection and processing, prompt engineering, model\nselection, and supervised fine-tuning to optimize the model's performance.\nEvaluation through similarity metrics and live deployment demonstrated that our\napproach effectively generates accurate and informative responses. The results\nhighlight the potential of LLMs to revolutionize honeypot technology, providing\ncybersecurity professionals with a powerful tool to detect and analyze\nmalicious activity, thereby enhancing overall security infrastructure."
                },
                "authors": [
                    {
                        "name": "Hakan T. Otal"
                    },
                    {
                        "name": "M. Abdullah Canbaz"
                    }
                ],
                "author_detail": {
                    "name": "M. Abdullah Canbaz"
                },
                "author": "M. Abdullah Canbaz",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68M10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; D.4.6; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2112.14356v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2112.14356v4",
                "updated": "2024-09-12T16:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    56,
                    19,
                    3,
                    256,
                    0
                ],
                "published": "2021-12-29T01:30:39Z",
                "published_parsed": [
                    2021,
                    12,
                    29,
                    1,
                    30,
                    39,
                    2,
                    363,
                    0
                ],
                "title": "Private Private Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private Private Information"
                },
                "summary": "Private signals model noisy information about an unknown state. Although\nthese signals are called \"private,\" they may still carry information about each\nother. Our paper introduces the concept of private private signals, which\ncontain information about the state but not about other signals. To achieve\nprivacy, signal quality may need to be sacrificed. We study the informativeness\nof private private signals and characterize those that are optimal in the sense\nthat they cannot be made more informative without violating privacy. We discuss\nimplications for privacy in recommendation systems, information design, causal\ninference, and mechanism design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private signals model noisy information about an unknown state. Although\nthese signals are called \"private,\" they may still carry information about each\nother. Our paper introduces the concept of private private signals, which\ncontain information about the state but not about other signals. To achieve\nprivacy, signal quality may need to be sacrificed. We study the informativeness\nof private private signals and characterize those that are optimal in the sense\nthat they cannot be made more informative without violating privacy. We discuss\nimplications for privacy in recommendation systems, information design, causal\ninference, and mechanism design."
                },
                "authors": [
                    {
                        "name": "Kevin He"
                    },
                    {
                        "name": "Fedor Sandomirskiy"
                    },
                    {
                        "name": "Omer Tamuz"
                    }
                ],
                "author_detail": {
                    "name": "Omer Tamuz"
                },
                "author": "Omer Tamuz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2112.14356v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2112.14356v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08212v1",
                "updated": "2024-09-12T16:51:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    51,
                    58,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T16:51:58Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    51,
                    58,
                    3,
                    256,
                    0
                ],
                "title": "Adaptive Language-Guided Abstraction from Contrastive Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Language-Guided Abstraction from Contrastive Explanations"
                },
                "summary": "Many approaches to robot learning begin by inferring a reward function from a\nset of human demonstrations. To learn a good reward, it is necessary to\ndetermine which features of the environment are relevant before determining how\nthese features should be used to compute reward. End-to-end methods for joint\nfeature and reward learning (e.g., using deep networks or program synthesis\ntechniques) often yield brittle reward functions that are sensitive to spurious\nstate features. By contrast, humans can often generalizably learn from a small\nnumber of demonstrations by incorporating strong priors about what features of\na demonstration are likely meaningful for a task of interest. How do we build\nrobots that leverage this kind of background knowledge when learning from new\ndemonstrations? This paper describes a method named ALGAE (Adaptive\nLanguage-Guided Abstraction from [Contrastive] Explanations) which alternates\nbetween using language models to iteratively identify human-meaningful features\nneeded to explain demonstrated behavior, then standard inverse reinforcement\nlearning techniques to assign weights to these features. Experiments across a\nvariety of both simulated and real-world robot environments show that ALGAE\nlearns generalizable reward functions defined on interpretable features using\nonly small numbers of demonstrations. Importantly, ALGAE can recognize when\nfeatures are missing, then extract and define those features without any human\ninput -- making it possible to quickly and efficiently acquire rich\nrepresentations of user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many approaches to robot learning begin by inferring a reward function from a\nset of human demonstrations. To learn a good reward, it is necessary to\ndetermine which features of the environment are relevant before determining how\nthese features should be used to compute reward. End-to-end methods for joint\nfeature and reward learning (e.g., using deep networks or program synthesis\ntechniques) often yield brittle reward functions that are sensitive to spurious\nstate features. By contrast, humans can often generalizably learn from a small\nnumber of demonstrations by incorporating strong priors about what features of\na demonstration are likely meaningful for a task of interest. How do we build\nrobots that leverage this kind of background knowledge when learning from new\ndemonstrations? This paper describes a method named ALGAE (Adaptive\nLanguage-Guided Abstraction from [Contrastive] Explanations) which alternates\nbetween using language models to iteratively identify human-meaningful features\nneeded to explain demonstrated behavior, then standard inverse reinforcement\nlearning techniques to assign weights to these features. Experiments across a\nvariety of both simulated and real-world robot environments show that ALGAE\nlearns generalizable reward functions defined on interpretable features using\nonly small numbers of demonstrations. Importantly, ALGAE can recognize when\nfeatures are missing, then extract and define those features without any human\ninput -- making it possible to quickly and efficiently acquire rich\nrepresentations of user behavior."
                },
                "authors": [
                    {
                        "name": "Andi Peng"
                    },
                    {
                        "name": "Belinda Z. Li"
                    },
                    {
                        "name": "Ilia Sucholutsky"
                    },
                    {
                        "name": "Nishanth Kumar"
                    },
                    {
                        "name": "Julie A. Shah"
                    },
                    {
                        "name": "Jacob Andreas"
                    },
                    {
                        "name": "Andreea Bobu"
                    }
                ],
                "author_detail": {
                    "name": "Andreea Bobu"
                },
                "author": "Andreea Bobu",
                "arxiv_comment": "CoRL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.09090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.09090v2",
                "updated": "2024-09-12T16:40:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    40,
                    18,
                    3,
                    256,
                    0
                ],
                "published": "2024-01-17T09:51:32Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    9,
                    51,
                    32,
                    2,
                    17,
                    0
                ],
                "title": "Understanding the concerns and choices of public when using large\n  language models for healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the concerns and choices of public when using large\n  language models for healthcare"
                },
                "summary": "Large language models (LLMs) have shown their potential in biomedical fields.\nHowever, how the public uses them for healthcare purposes such as medical Q\\&A,\nself-diagnosis, and daily healthcare information seeking is under-investigated.\nThis paper adopts a mixed-methods approach, including surveys (N=214) and\ninterviews (N=17) to investigate how and why the public uses LLMs for\nhealthcare. We found that participants generally believed LLMs as a healthcare\ntool have gained popularity, and are often used in combination with other\ninformation channels such as search engines and online health communities to\noptimize information quality. Based on the findings, we reflect on the ethical\nand effective use of LLMs for healthcare and propose future research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown their potential in biomedical fields.\nHowever, how the public uses them for healthcare purposes such as medical Q\\&A,\nself-diagnosis, and daily healthcare information seeking is under-investigated.\nThis paper adopts a mixed-methods approach, including surveys (N=214) and\ninterviews (N=17) to investigate how and why the public uses LLMs for\nhealthcare. We found that participants generally believed LLMs as a healthcare\ntool have gained popularity, and are often used in combination with other\ninformation channels such as search engines and online health communities to\noptimize information quality. Based on the findings, we reflect on the ethical\nand effective use of LLMs for healthcare and propose future research\ndirections."
                },
                "authors": [
                    {
                        "name": "Yunpeng Xiao"
                    },
                    {
                        "name": "Kyrie Zhixuan Zhou"
                    },
                    {
                        "name": "Yueqing Liang"
                    },
                    {
                        "name": "Kai Shu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shu"
                },
                "author": "Kai Shu",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.09090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.09090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08197v1",
                "updated": "2024-09-12T16:35:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    35,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T16:35:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    35,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Quantifying the breakdown scale of pionless effective field theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the breakdown scale of pionless effective field theory"
                },
                "summary": "We use Bayesian statistics to infer the breakdown scale of pionless effective\nfield theory in its standard power counting and with renormalization of\nobservables carried out using the power-divergence subtraction scheme and\ncutoff regularization. We condition our inference on predictions of the total\nneutron-proton scattering cross section up next-to-next-to leading order. We\nquantify a median breakdown scale of approximately 1.4$m_\\pi$. The 68% degree\nof belief interval is $[0.96,1.69]m_\\pi$. This result confirms the canonical\nexpectation that the pion mass is a relevant scale in low-energy nuclear\nphysics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We use Bayesian statistics to infer the breakdown scale of pionless effective\nfield theory in its standard power counting and with renormalization of\nobservables carried out using the power-divergence subtraction scheme and\ncutoff regularization. We condition our inference on predictions of the total\nneutron-proton scattering cross section up next-to-next-to leading order. We\nquantify a median breakdown scale of approximately 1.4$m_\\pi$. The 68% degree\nof belief interval is $[0.96,1.69]m_\\pi$. This result confirms the canonical\nexpectation that the pion mass is a relevant scale in low-energy nuclear\nphysics."
                },
                "authors": [
                    {
                        "name": "Andreas Ekström"
                    },
                    {
                        "name": "Lucas Platter"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Platter"
                },
                "author": "Lucas Platter",
                "arxiv_comment": "6 pages,5 figures and 2 pages of supplemental material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14573v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14573v5",
                "updated": "2024-09-12T16:22:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    22,
                    52,
                    3,
                    256,
                    0
                ],
                "published": "2024-07-21T06:27:45Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    6,
                    27,
                    45,
                    6,
                    203,
                    0
                ],
                "title": "Trading Devil Final: Backdoor attack via Stock market and Bayesian\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trading Devil Final: Backdoor attack via Stock market and Bayesian\n  Optimization"
                },
                "summary": "Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs."
                },
                "authors": [
                    {
                        "name": "Orson Mengara"
                    }
                ],
                "author_detail": {
                    "name": "Orson Mengara"
                },
                "author": "Orson Mengara",
                "arxiv_comment": "END (will never be modified again!!) :Jumps-Diffusion and stock\n  market: Better quantify uncertainty in financial simulations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14573v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14573v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08185v1",
                "updated": "2024-09-12T16:20:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    20,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T16:20:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    20,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Fine-tuning Large Language Models for Entity Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models for Entity Matching"
                },
                "summary": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and their ability to generalize to unseen entities. Existing\nresearch on using LLMs for entity matching has focused on prompt engineering\nand in-context learning. This paper explores the potential of fine-tuning LLMs\nfor entity matching. We analyze fine-tuning along two dimensions: 1) The\nrepresentation of training examples, where we experiment with adding different\ntypes of LLM-generated explanations to the training set, and 2) the selection\nand generation of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodel's ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o Mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and their ability to generalize to unseen entities. Existing\nresearch on using LLMs for entity matching has focused on prompt engineering\nand in-context learning. This paper explores the potential of fine-tuning LLMs\nfor entity matching. We analyze fine-tuning along two dimensions: 1) The\nrepresentation of training examples, where we experiment with adding different\ntypes of LLM-generated explanations to the training set, and 2) the selection\nand generation of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodel's ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o Mini."
                },
                "authors": [
                    {
                        "name": "Aaron Steiner"
                    },
                    {
                        "name": "Ralph Peeters"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "arxiv_comment": "8 pages, 4 figures. For related code and data, see this\n  https://github.com/wbsg-uni-mannheim/TailorMatch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08148v1",
                "updated": "2024-09-12T15:43:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    43,
                    10,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:43:10Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    43,
                    10,
                    3,
                    256,
                    0
                ],
                "title": "Faster Speech-LLaMA Inference with Multi-token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Speech-LLaMA Inference with Multi-token Prediction"
                },
                "summary": "Large language models (LLMs) have become proficient at solving a wide variety\nof tasks, including those involving multi-modal inputs. In particular,\ninstantiating an LLM (such as LLaMA) with a speech encoder and training it on\npaired data imparts speech recognition (ASR) abilities to the decoder-only\nmodel, hence called Speech-LLaMA. Nevertheless, due to the sequential nature of\nauto-regressive inference and the relatively large decoder, Speech-LLaMA models\nrequire relatively high inference time. In this work, we propose to speed up\nSpeech-LLaMA inference by predicting multiple tokens in the same decoding step.\nWe explore several model architectures that enable this, and investigate their\nperformance using threshold-based and verification-based inference strategies.\nWe also propose a prefix-based beam search decoding method that allows\nefficient minimum word error rate (MWER) training for such models. We evaluate\nour models on a variety of public benchmarks, where they reduce the number of\ndecoder calls by ~3.2x while maintaining or improving WER performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become proficient at solving a wide variety\nof tasks, including those involving multi-modal inputs. In particular,\ninstantiating an LLM (such as LLaMA) with a speech encoder and training it on\npaired data imparts speech recognition (ASR) abilities to the decoder-only\nmodel, hence called Speech-LLaMA. Nevertheless, due to the sequential nature of\nauto-regressive inference and the relatively large decoder, Speech-LLaMA models\nrequire relatively high inference time. In this work, we propose to speed up\nSpeech-LLaMA inference by predicting multiple tokens in the same decoding step.\nWe explore several model architectures that enable this, and investigate their\nperformance using threshold-based and verification-based inference strategies.\nWe also propose a prefix-based beam search decoding method that allows\nefficient minimum word error rate (MWER) training for such models. We evaluate\nour models on a variety of public benchmarks, where they reduce the number of\ndecoder calls by ~3.2x while maintaining or improving WER performance."
                },
                "authors": [
                    {
                        "name": "Desh Raj"
                    },
                    {
                        "name": "Gil Keren"
                    },
                    {
                        "name": "Junteng Jia"
                    },
                    {
                        "name": "Jay Mahadeokar"
                    },
                    {
                        "name": "Ozlem Kalinli"
                    }
                ],
                "author_detail": {
                    "name": "Ozlem Kalinli"
                },
                "author": "Ozlem Kalinli",
                "arxiv_comment": "Submitted to IEEE ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08147v1",
                "updated": "2024-09-12T15:40:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    40,
                    45,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:40:45Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    40,
                    45,
                    3,
                    256,
                    0
                ],
                "title": "LLM-POTUS Score: A Framework of Analyzing Presidential Debates with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-POTUS Score: A Framework of Analyzing Presidential Debates with\n  Large Language Models"
                },
                "summary": "Large language models have demonstrated remarkable capabilities in natural\nlanguage processing, yet their application to political discourse analysis\nremains underexplored. This paper introduces a novel approach to evaluating\npresidential debate performances using LLMs, addressing the longstanding\nchallenge of objectively assessing debate outcomes. We propose a framework that\nanalyzes candidates' \"Policies, Persona, and Perspective\" (3P) and how they\nresonate with the \"Interests, Ideologies, and Identity\" (3I) of four key\naudience groups: voters, businesses, donors, and politicians. Our method\nemploys large language models to generate the LLM-POTUS Score, a quantitative\nmeasure of debate performance based on the alignment between 3P and 3I. We\napply this framework to analyze transcripts from recent U.S. presidential\ndebates, demonstrating its ability to provide nuanced, multi-dimensional\nassessments of candidate performances. Our results reveal insights into the\neffectiveness of different debating strategies and their impact on various\naudience segments. This study not only offers a new tool for political analysis\nbut also explores the potential and limitations of using LLMs as impartial\njudges in complex social contexts. In addition, this framework provides\nindividual citizens with an independent tool to evaluate presidential debate\nperformances, which enhances democratic engagement and reduces reliance on\npotentially biased media interpretations and institutional influence, thereby\nstrengthening the foundation of informed civic participation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable capabilities in natural\nlanguage processing, yet their application to political discourse analysis\nremains underexplored. This paper introduces a novel approach to evaluating\npresidential debate performances using LLMs, addressing the longstanding\nchallenge of objectively assessing debate outcomes. We propose a framework that\nanalyzes candidates' \"Policies, Persona, and Perspective\" (3P) and how they\nresonate with the \"Interests, Ideologies, and Identity\" (3I) of four key\naudience groups: voters, businesses, donors, and politicians. Our method\nemploys large language models to generate the LLM-POTUS Score, a quantitative\nmeasure of debate performance based on the alignment between 3P and 3I. We\napply this framework to analyze transcripts from recent U.S. presidential\ndebates, demonstrating its ability to provide nuanced, multi-dimensional\nassessments of candidate performances. Our results reveal insights into the\neffectiveness of different debating strategies and their impact on various\naudience segments. This study not only offers a new tool for political analysis\nbut also explores the potential and limitations of using LLMs as impartial\njudges in complex social contexts. In addition, this framework provides\nindividual citizens with an independent tool to evaluate presidential debate\nperformances, which enhances democratic engagement and reduces reliance on\npotentially biased media interpretations and institutional influence, thereby\nstrengthening the foundation of informed civic participation."
                },
                "authors": [
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Oleksandra Zolotarevych"
                    },
                    {
                        "name": "Rongwei Yang"
                    },
                    {
                        "name": "Tianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Liu"
                },
                "author": "Tianming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15280v2",
                "updated": "2024-09-12T15:13:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    13,
                    55,
                    3,
                    256,
                    0
                ],
                "published": "2024-06-21T16:18:23Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    16,
                    18,
                    23,
                    4,
                    173,
                    0
                ],
                "title": "Gravity.jl: fast and accurate gravitational lens modeling in Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravity.jl: fast and accurate gravitational lens modeling in Julia"
                },
                "summary": "We present Gravity.jl, a new software for the modeling of gravitational lens\nsystems. Gravity.jl is written in the Julia programming language, and is\ndesigned to be fast, accurate, and flexible. It can be used to model\ngravitational lens systems composed of multiple lensing planes, and to perform\nBayesian inference on the lens model parameters. In this paper we present the\ntheoretical and statistical ideas behind the code, and we describe its main\nfeatures. In this first paper of the series, we focus on the modeling of\npoint-like and small extended sources, for which we can linearize the lens\nequation. We show a practical use of Gravity.jl on a galaxy-scale lens, and we\ncompare the results with those obtained with other codes. We also show how\nGravity.jl can be used to perform Bayesian inference on cosmological\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Gravity.jl, a new software for the modeling of gravitational lens\nsystems. Gravity.jl is written in the Julia programming language, and is\ndesigned to be fast, accurate, and flexible. It can be used to model\ngravitational lens systems composed of multiple lensing planes, and to perform\nBayesian inference on the lens model parameters. In this paper we present the\ntheoretical and statistical ideas behind the code, and we describe its main\nfeatures. In this first paper of the series, we focus on the modeling of\npoint-like and small extended sources, for which we can linearize the lens\nequation. We show a practical use of Gravity.jl on a galaxy-scale lens, and we\ncompare the results with those obtained with other codes. We also show how\nGravity.jl can be used to perform Bayesian inference on cosmological\nparameters."
                },
                "authors": [
                    {
                        "name": "Marco Lombardi"
                    }
                ],
                "author_detail": {
                    "name": "Marco Lombardi"
                },
                "author": "Marco Lombardi",
                "arxiv_comment": "17 pages, A&A in press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08122v1",
                "updated": "2024-09-12T15:11:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    11,
                    35,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:11:35Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    11,
                    35,
                    3,
                    256,
                    0
                ],
                "title": "GAZEploit: Remote Keystroke Inference Attack by Gaze Estimation from\n  Avatar Views in VR/MR Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAZEploit: Remote Keystroke Inference Attack by Gaze Estimation from\n  Avatar Views in VR/MR Devices"
                },
                "summary": "The advent and growing popularity of Virtual Reality (VR) and Mixed Reality\n(MR) solutions have revolutionized the way we interact with digital platforms.\nThe cutting-edge gaze-controlled typing methods, now prevalent in high-end\nmodels of these devices, e.g., Apple Vision Pro, have not only improved user\nexperience but also mitigated traditional keystroke inference attacks that\nrelied on hand gestures, head movements and acoustic side-channels. However,\nthis advancement has paradoxically given birth to a new, potentially more\ninsidious cyber threat, GAZEploit.\n  In this paper, we unveil GAZEploit, a novel eye-tracking based attack\nspecifically designed to exploit these eye-tracking information by leveraging\nthe common use of virtual appearances in VR applications. This widespread usage\nsignificantly enhances the practicality and feasibility of our attack compared\nto existing methods. GAZEploit takes advantage of this vulnerability to\nremotely extract gaze estimations and steal sensitive keystroke information\nacross various typing scenarios-including messages, passwords, URLs, emails,\nand passcodes. Our research, involving 30 participants, achieved over 80%\naccuracy in keystroke inference. Alarmingly, our study also identified over 15\ntop-rated apps in the Apple Store as vulnerable to the GAZEploit attack,\nemphasizing the urgent need for bolstered security measures for this\nstate-of-the-art VR/MR text entry method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent and growing popularity of Virtual Reality (VR) and Mixed Reality\n(MR) solutions have revolutionized the way we interact with digital platforms.\nThe cutting-edge gaze-controlled typing methods, now prevalent in high-end\nmodels of these devices, e.g., Apple Vision Pro, have not only improved user\nexperience but also mitigated traditional keystroke inference attacks that\nrelied on hand gestures, head movements and acoustic side-channels. However,\nthis advancement has paradoxically given birth to a new, potentially more\ninsidious cyber threat, GAZEploit.\n  In this paper, we unveil GAZEploit, a novel eye-tracking based attack\nspecifically designed to exploit these eye-tracking information by leveraging\nthe common use of virtual appearances in VR applications. This widespread usage\nsignificantly enhances the practicality and feasibility of our attack compared\nto existing methods. GAZEploit takes advantage of this vulnerability to\nremotely extract gaze estimations and steal sensitive keystroke information\nacross various typing scenarios-including messages, passwords, URLs, emails,\nand passcodes. Our research, involving 30 participants, achieved over 80%\naccuracy in keystroke inference. Alarmingly, our study also identified over 15\ntop-rated apps in the Apple Store as vulnerable to the GAZEploit attack,\nemphasizing the urgent need for bolstered security measures for this\nstate-of-the-art VR/MR text entry method."
                },
                "authors": [
                    {
                        "name": "Hanqiu Wang"
                    },
                    {
                        "name": "Zihao Zhan"
                    },
                    {
                        "name": "Haoqi Shan"
                    },
                    {
                        "name": "Siqi Dai"
                    },
                    {
                        "name": "Max Panoff"
                    },
                    {
                        "name": "Shuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Wang"
                },
                "author": "Shuo Wang",
                "arxiv_comment": "15 pages, 20 figures, Accepted at ACM CCS'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08117v1",
                "updated": "2024-09-12T15:08:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    8,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:08:15Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    8,
                    15,
                    3,
                    256,
                    0
                ],
                "title": "JWST ice band profiles reveal mixed ice compositions in the HH 48 NE\n  disk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST ice band profiles reveal mixed ice compositions in the HH 48 NE\n  disk"
                },
                "summary": "Planet formation is strongly influenced by the composition and distribution\nof volatiles within protoplanetary disks. With JWST, it is now possible to\nobtain direct observational constraints on disk ices, as recently demonstrated\nby the detection of ice absorption features towards the edge-on HH 48 NE disk\nas part of the Ice Age Early Release Science program. Here, we introduce a new\nradiative transfer modeling framework designed to retrieve the composition and\nmixing status of disk ices using their band profiles, and apply it to interpret\nthe H2O, CO2, and CO ice bands observed towards the HH 48 NE disk. We show that\nthe ices are largely present as mixtures, with strong evidence for CO trapping\nin both H2O and CO2 ice. The HH 48 NE disk ice composition (pure vs. polar vs.\napolar fractions) is markedly different from earlier protostellar stages,\nimplying thermal and/or chemical reprocessing during the formation or evolution\nof the disk. We infer low ice-phase C/O ratios around 0.1 throughout the disk,\nand also demonstrate that the mixing and entrapment of disk ices can\ndramatically affect the radial dependence of the C/O ratio. It is therefore\nimperative that realistic disk ice compositions are considered when comparing\nplanetary compositions with potential formation scenarios, which will\nfortunately be possible for an increasing number of disks with JWST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planet formation is strongly influenced by the composition and distribution\nof volatiles within protoplanetary disks. With JWST, it is now possible to\nobtain direct observational constraints on disk ices, as recently demonstrated\nby the detection of ice absorption features towards the edge-on HH 48 NE disk\nas part of the Ice Age Early Release Science program. Here, we introduce a new\nradiative transfer modeling framework designed to retrieve the composition and\nmixing status of disk ices using their band profiles, and apply it to interpret\nthe H2O, CO2, and CO ice bands observed towards the HH 48 NE disk. We show that\nthe ices are largely present as mixtures, with strong evidence for CO trapping\nin both H2O and CO2 ice. The HH 48 NE disk ice composition (pure vs. polar vs.\napolar fractions) is markedly different from earlier protostellar stages,\nimplying thermal and/or chemical reprocessing during the formation or evolution\nof the disk. We infer low ice-phase C/O ratios around 0.1 throughout the disk,\nand also demonstrate that the mixing and entrapment of disk ices can\ndramatically affect the radial dependence of the C/O ratio. It is therefore\nimperative that realistic disk ice compositions are considered when comparing\nplanetary compositions with potential formation scenarios, which will\nfortunately be possible for an increasing number of disks with JWST."
                },
                "authors": [
                    {
                        "name": "Jennifer B. Bergner"
                    },
                    {
                        "name": "J. A. Sturm"
                    },
                    {
                        "name": "Elettra L. Piacentino"
                    },
                    {
                        "name": "M. K. McClure"
                    },
                    {
                        "name": "Karin I. Oberg"
                    },
                    {
                        "name": "A. C. A. Boogert"
                    },
                    {
                        "name": "E. Dartois"
                    },
                    {
                        "name": "M. N. Drozdovskaya"
                    },
                    {
                        "name": "H. J. Fraser"
                    },
                    {
                        "name": "Daniel Harsono"
                    },
                    {
                        "name": "Sergio Ioppolo"
                    },
                    {
                        "name": "Charles J. Law"
                    },
                    {
                        "name": "Dariusz C. Lis"
                    },
                    {
                        "name": "Brett A. McGuire"
                    },
                    {
                        "name": "Gary J. Melnick"
                    },
                    {
                        "name": "Jennifer A. Noble"
                    },
                    {
                        "name": "M. E. Palumbo"
                    },
                    {
                        "name": "Yvonne J. Pendleton"
                    },
                    {
                        "name": "Giulia Perotti"
                    },
                    {
                        "name": "Danna Qasim"
                    },
                    {
                        "name": "W. R. M. Rocha"
                    },
                    {
                        "name": "E. F. van Dishoeck"
                    }
                ],
                "author_detail": {
                    "name": "E. F. van Dishoeck"
                },
                "author": "E. F. van Dishoeck",
                "arxiv_comment": "Accepted to ApJ. 24 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06857v2",
                "updated": "2024-09-12T15:04:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    4,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-10T20:45:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    45,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "What is the Role of Small Models in the LLM Era: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Small Models in the LLM Era: A Survey"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models"
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gaël Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gaël Varoquaux"
                },
                "author": "Gaël Varoquaux",
                "arxiv_comment": "a survey paper of small models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08107v1",
                "updated": "2024-09-12T15:00:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    0,
                    56,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:00:56Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    0,
                    56,
                    3,
                    256,
                    0
                ],
                "title": "WhisperNER: Unified Open Named Entity and Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WhisperNER: Unified Open Named Entity and Speech Recognition"
                },
                "summary": "Integrating named entity recognition (NER) with automatic speech recognition\n(ASR) can significantly enhance transcription accuracy and informativeness. In\nthis paper, we introduce WhisperNER, a novel model that allows joint speech\ntranscription and entity recognition. WhisperNER supports open-type NER,\nenabling recognition of diverse and evolving entities at inference. Building on\nrecent advancements in open NER research, we augment a large synthetic dataset\nwith synthetic speech samples. This allows us to train WhisperNER on a large\nnumber of examples with diverse NER tags. During training, the model is\nprompted with NER labels and optimized to output the transcribed utterance\nalong with the corresponding tagged entities. To evaluate WhisperNER, we\ngenerate synthetic speech for commonly used NER benchmarks and annotate\nexisting ASR datasets with open NER tags. Our experiments demonstrate that\nWhisperNER outperforms natural baselines on both out-of-domain open type NER\nand supervised finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating named entity recognition (NER) with automatic speech recognition\n(ASR) can significantly enhance transcription accuracy and informativeness. In\nthis paper, we introduce WhisperNER, a novel model that allows joint speech\ntranscription and entity recognition. WhisperNER supports open-type NER,\nenabling recognition of diverse and evolving entities at inference. Building on\nrecent advancements in open NER research, we augment a large synthetic dataset\nwith synthetic speech samples. This allows us to train WhisperNER on a large\nnumber of examples with diverse NER tags. During training, the model is\nprompted with NER labels and optimized to output the transcribed utterance\nalong with the corresponding tagged entities. To evaluate WhisperNER, we\ngenerate synthetic speech for commonly used NER benchmarks and annotate\nexisting ASR datasets with open NER tags. Our experiments demonstrate that\nWhisperNER outperforms natural baselines on both out-of-domain open type NER\nand supervised finetuning."
                },
                "authors": [
                    {
                        "name": "Gil Ayache"
                    },
                    {
                        "name": "Menachem Pirchi"
                    },
                    {
                        "name": "Aviv Navon"
                    },
                    {
                        "name": "Aviv Shamsian"
                    },
                    {
                        "name": "Gill Hetz"
                    },
                    {
                        "name": "Joseph Keshet"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Keshet"
                },
                "author": "Joseph Keshet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02387v3",
                "updated": "2024-09-12T14:56:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    56,
                    35,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-04T02:30:12Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    30,
                    12,
                    2,
                    248,
                    0
                ],
                "title": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges"
                },
                "summary": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence."
                },
                "authors": [
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08102v1",
                "updated": "2024-09-12T14:54:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    54,
                    31,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:54:31Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    54,
                    31,
                    3,
                    256,
                    0
                ],
                "title": "Bayesian Self-Training for Semi-Supervised 3D Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Self-Training for Semi-Supervised 3D Segmentation"
                },
                "summary": "3D segmentation is a core problem in computer vision and, similarly to many\nother dense prediction tasks, it requires large amounts of annotated data for\nadequate training. However, densely labeling 3D point clouds to employ\nfully-supervised training remains too labor intensive and expensive.\nSemi-supervised training provides a more practical alternative, where only a\nsmall set of labeled data is given, accompanied by a larger unlabeled set. This\narea thus studies the effective use of unlabeled data to reduce the performance\ngap that arises due to the lack of annotations. In this work, inspired by\nBayesian deep learning, we first propose a Bayesian self-training framework for\nsemi-supervised 3D semantic segmentation. Employing stochastic inference, we\ngenerate an initial set of pseudo-labels and then filter these based on\nestimated point-wise uncertainty. By constructing a heuristic $n$-partite\nmatching algorithm, we extend the method to semi-supervised 3D instance\nsegmentation, and finally, with the same building blocks, to dense 3D visual\ngrounding. We demonstrate state-of-the-art results for our semi-supervised\nmethod on SemanticKITTI and ScribbleKITTI for 3D semantic segmentation and on\nScanNet and S3DIS for 3D instance segmentation. We further achieve substantial\nimprovements in dense 3D visual grounding over supervised-only baselines on\nScanRefer. Our project page is available at ouenal.github.io/bst/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D segmentation is a core problem in computer vision and, similarly to many\nother dense prediction tasks, it requires large amounts of annotated data for\nadequate training. However, densely labeling 3D point clouds to employ\nfully-supervised training remains too labor intensive and expensive.\nSemi-supervised training provides a more practical alternative, where only a\nsmall set of labeled data is given, accompanied by a larger unlabeled set. This\narea thus studies the effective use of unlabeled data to reduce the performance\ngap that arises due to the lack of annotations. In this work, inspired by\nBayesian deep learning, we first propose a Bayesian self-training framework for\nsemi-supervised 3D semantic segmentation. Employing stochastic inference, we\ngenerate an initial set of pseudo-labels and then filter these based on\nestimated point-wise uncertainty. By constructing a heuristic $n$-partite\nmatching algorithm, we extend the method to semi-supervised 3D instance\nsegmentation, and finally, with the same building blocks, to dense 3D visual\ngrounding. We demonstrate state-of-the-art results for our semi-supervised\nmethod on SemanticKITTI and ScribbleKITTI for 3D semantic segmentation and on\nScanNet and S3DIS for 3D instance segmentation. We further achieve substantial\nimprovements in dense 3D visual grounding over supervised-only baselines on\nScanRefer. Our project page is available at ouenal.github.io/bst/."
                },
                "authors": [
                    {
                        "name": "Ozan Unal"
                    },
                    {
                        "name": "Christos Sakaridis"
                    },
                    {
                        "name": "Luc Van Gool"
                    }
                ],
                "author_detail": {
                    "name": "Luc Van Gool"
                },
                "author": "Luc Van Gool",
                "arxiv_comment": "Accepted at ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08098v1",
                "updated": "2024-09-12T14:51:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    51,
                    43,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:51:43Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    51,
                    43,
                    3,
                    256,
                    0
                ],
                "title": "The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK\n  Employment Tribunal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK\n  Employment Tribunal"
                },
                "summary": "This paper explores the intersection of technological innovation and access\nto justice by developing a benchmark for predicting case outcomes in the UK\nEmployment Tribunal (UKET). To address the challenge of extensive manual\nannotation, the study employs a large language model (LLM) for automatic\nannotation, resulting in the creation of the CLC-UKET dataset. The dataset\nconsists of approximately 19,000 UKET cases and their metadata. Comprehensive\nlegal annotations cover facts, claims, precedent references, statutory\nreferences, case outcomes, reasons and jurisdiction codes. Facilitated by the\nCLC-UKET data, we examine a multi-class case outcome prediction task in the\nUKET. Human predictions are collected to establish a performance reference for\nmodel comparison. Empirical results from baseline models indicate that\nfinetuned transformer models outperform zero-shot and few-shot LLMs on the UKET\nprediction task. The performance of zero-shot LLMs can be enhanced by\nintegrating task-related information into few-shot examples. We hope that the\nCLC-UKET dataset, along with human annotations and empirical findings, can\nserve as a valuable benchmark for employment-related dispute resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the intersection of technological innovation and access\nto justice by developing a benchmark for predicting case outcomes in the UK\nEmployment Tribunal (UKET). To address the challenge of extensive manual\nannotation, the study employs a large language model (LLM) for automatic\nannotation, resulting in the creation of the CLC-UKET dataset. The dataset\nconsists of approximately 19,000 UKET cases and their metadata. Comprehensive\nlegal annotations cover facts, claims, precedent references, statutory\nreferences, case outcomes, reasons and jurisdiction codes. Facilitated by the\nCLC-UKET data, we examine a multi-class case outcome prediction task in the\nUKET. Human predictions are collected to establish a performance reference for\nmodel comparison. Empirical results from baseline models indicate that\nfinetuned transformer models outperform zero-shot and few-shot LLMs on the UKET\nprediction task. The performance of zero-shot LLMs can be enhanced by\nintegrating task-related information into few-shot examples. We hope that the\nCLC-UKET dataset, along with human annotations and empirical findings, can\nserve as a valuable benchmark for employment-related dispute resolution."
                },
                "authors": [
                    {
                        "name": "Huiyuan Xie"
                    },
                    {
                        "name": "Felix Steffek"
                    },
                    {
                        "name": "Joana Ribeiro de Faria"
                    },
                    {
                        "name": "Christine Carter"
                    },
                    {
                        "name": "Jonathan Rutherford"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Rutherford"
                },
                "author": "Jonathan Rutherford",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08087v1",
                "updated": "2024-09-12T14:42:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    42,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:42:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    42,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "Securing Large Language Models: Addressing Bias, Misinformation, and\n  Prompt Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Large Language Models: Addressing Bias, Misinformation, and\n  Prompt Attacks"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious fields, yet their increasing use raises critical security concerns.\nThis article reviews recent literature addressing key issues in LLM security,\nwith a focus on accuracy, bias, content detection, and vulnerability to\nattacks. Issues related to inaccurate or misleading outputs from LLMs is\ndiscussed, with emphasis on the implementation from fact-checking methodologies\nto enhance response reliability. Inherent biases within LLMs are critically\nexamined through diverse evaluation techniques, including controlled input\nstudies and red teaming exercises. A comprehensive analysis of bias mitigation\nstrategies is presented, including approaches from pre-processing interventions\nto in-training adjustments and post-processing refinements. The article also\nprobes the complexity of distinguishing LLM-generated content from\nhuman-produced text, introducing detection mechanisms like DetectGPT and\nwatermarking techniques while noting the limitations of machine learning\nenabled classifiers under intricate circumstances. Moreover, LLM\nvulnerabilities, including jailbreak attacks and prompt injection exploits, are\nanalyzed by looking into different case studies and large-scale competitions\nlike HackAPrompt. This review is concluded by retrospecting defense mechanisms\nto safeguard LLMs, accentuating the need for more extensive research into the\nLLM security field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious fields, yet their increasing use raises critical security concerns.\nThis article reviews recent literature addressing key issues in LLM security,\nwith a focus on accuracy, bias, content detection, and vulnerability to\nattacks. Issues related to inaccurate or misleading outputs from LLMs is\ndiscussed, with emphasis on the implementation from fact-checking methodologies\nto enhance response reliability. Inherent biases within LLMs are critically\nexamined through diverse evaluation techniques, including controlled input\nstudies and red teaming exercises. A comprehensive analysis of bias mitigation\nstrategies is presented, including approaches from pre-processing interventions\nto in-training adjustments and post-processing refinements. The article also\nprobes the complexity of distinguishing LLM-generated content from\nhuman-produced text, introducing detection mechanisms like DetectGPT and\nwatermarking techniques while noting the limitations of machine learning\nenabled classifiers under intricate circumstances. Moreover, LLM\nvulnerabilities, including jailbreak attacks and prompt injection exploits, are\nanalyzed by looking into different case studies and large-scale competitions\nlike HackAPrompt. This review is concluded by retrospecting defense mechanisms\nto safeguard LLMs, accentuating the need for more extensive research into the\nLLM security field."
                },
                "authors": [
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Qian Niu"
                    }
                ],
                "author_detail": {
                    "name": "Qian Niu"
                },
                "author": "Qian Niu",
                "arxiv_comment": "17 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08069v1",
                "updated": "2024-09-12T14:24:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    24,
                    45,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:24:45Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    24,
                    45,
                    3,
                    256,
                    0
                ],
                "title": "TravelAgent: An AI Assistant for Personalized Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TravelAgent: An AI Assistant for Personalized Travel Planning"
                },
                "summary": "As global tourism expands and artificial intelligence technology advances,\nintelligent travel planning services have emerged as a significant research\nfocus. Within dynamic real-world travel scenarios with multi-dimensional\nconstraints, services that support users in automatically creating practical\nand customized travel itineraries must address three key objectives:\nRationality, Comprehensiveness, and Personalization. However, existing systems\nwith rule-based combinations or LLM-based planning methods struggle to fully\nsatisfy these criteria. To overcome the challenges, we introduce TravelAgent, a\ntravel planning system powered by large language models (LLMs) designed to\nprovide reasonable, comprehensive, and personalized travel itineraries grounded\nin dynamic scenarios. TravelAgent comprises four modules: Tool-usage,\nRecommendation, Planning, and Memory Module. We evaluate TravelAgent's\nperformance with human and simulated users, demonstrating its overall\neffectiveness in three criteria and confirming the accuracy of personalized\nrecommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As global tourism expands and artificial intelligence technology advances,\nintelligent travel planning services have emerged as a significant research\nfocus. Within dynamic real-world travel scenarios with multi-dimensional\nconstraints, services that support users in automatically creating practical\nand customized travel itineraries must address three key objectives:\nRationality, Comprehensiveness, and Personalization. However, existing systems\nwith rule-based combinations or LLM-based planning methods struggle to fully\nsatisfy these criteria. To overcome the challenges, we introduce TravelAgent, a\ntravel planning system powered by large language models (LLMs) designed to\nprovide reasonable, comprehensive, and personalized travel itineraries grounded\nin dynamic scenarios. TravelAgent comprises four modules: Tool-usage,\nRecommendation, Planning, and Memory Module. We evaluate TravelAgent's\nperformance with human and simulated users, demonstrating its overall\neffectiveness in three criteria and confirming the accuracy of personalized\nrecommendations."
                },
                "authors": [
                    {
                        "name": "Aili Chen"
                    },
                    {
                        "name": "Xuyang Ge"
                    },
                    {
                        "name": "Ziquan Fu"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Jiangjie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiangjie Chen"
                },
                "author": "Jiangjie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11698v2",
                "updated": "2024-09-12T14:18:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    18,
                    22,
                    3,
                    256,
                    0
                ],
                "published": "2024-07-16T13:16:49Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    13,
                    16,
                    49,
                    1,
                    198,
                    0
                ],
                "title": "NITRO-D: Native Integer-only Training of Deep Convolutional Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NITRO-D: Native Integer-only Training of Deep Convolutional Neural\n  Networks"
                },
                "summary": "Quantization has become increasingly pivotal in addressing the steadily\nincreasing computational and memory requirements of Deep Neural Networks\n(DNNs). By reducing the number of bits used to represent weights and\nactivations (typically from 32-bit floating-point to 16-bit or 8-bit integers),\nquantization reduces the memory footprint, energy consumption, and execution\ntime of DNN models. However, traditional quantization methods typically focus\non the inference of DNNs, while the training process still relies on\nfloating-point operations. To date, only one work in the literature has\naddressed integer-only training for Multi-Layer Perceptron (MLP) architectures.\nThis work introduces NITRO-D, a new framework for training arbitrarily deep\ninteger-only Convolutional Neural Networks (CNNs) that operate entirely in the\ninteger-only domain for both training and inference. NITRO-D is the first\nframework in the literature enabling the training of integer-only CNNs without\nthe need to introduce a quantization scheme. Specifically, NITRO-D introduces a\nnovel architecture integrating multiple integer local-loss blocks, which\ninclude the proposed NITRO Scaling Layer and the NITRO-ReLU activation\nfunction. Additionally, it introduces a novel integer-only learning algorithm\nderived from Local Error Signals (LES), utilizing IntegerSGD, an optimizer\nspecifically designed to operate in an integer-only context. NITRO-D is\nimplemented in an open-source Python library. Extensive experimental\nevaluations demonstrate its effectiveness across several state-of-the-art image\nrecognition datasets. Results show significant performance improvements from\n2.47% to 5.96% for integer-only MLP architectures over the state-of-the-art\nsolution, and the capability of training integer-only CNN architectures with\nminimal accuracy degradation from -0.15% to -4.22% compared to floating-point\nLES.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has become increasingly pivotal in addressing the steadily\nincreasing computational and memory requirements of Deep Neural Networks\n(DNNs). By reducing the number of bits used to represent weights and\nactivations (typically from 32-bit floating-point to 16-bit or 8-bit integers),\nquantization reduces the memory footprint, energy consumption, and execution\ntime of DNN models. However, traditional quantization methods typically focus\non the inference of DNNs, while the training process still relies on\nfloating-point operations. To date, only one work in the literature has\naddressed integer-only training for Multi-Layer Perceptron (MLP) architectures.\nThis work introduces NITRO-D, a new framework for training arbitrarily deep\ninteger-only Convolutional Neural Networks (CNNs) that operate entirely in the\ninteger-only domain for both training and inference. NITRO-D is the first\nframework in the literature enabling the training of integer-only CNNs without\nthe need to introduce a quantization scheme. Specifically, NITRO-D introduces a\nnovel architecture integrating multiple integer local-loss blocks, which\ninclude the proposed NITRO Scaling Layer and the NITRO-ReLU activation\nfunction. Additionally, it introduces a novel integer-only learning algorithm\nderived from Local Error Signals (LES), utilizing IntegerSGD, an optimizer\nspecifically designed to operate in an integer-only context. NITRO-D is\nimplemented in an open-source Python library. Extensive experimental\nevaluations demonstrate its effectiveness across several state-of-the-art image\nrecognition datasets. Results show significant performance improvements from\n2.47% to 5.96% for integer-only MLP architectures over the state-of-the-art\nsolution, and the capability of training integer-only CNN architectures with\nminimal accuracy degradation from -0.15% to -4.22% compared to floating-point\nLES."
                },
                "authors": [
                    {
                        "name": "Alberto Pirillo"
                    },
                    {
                        "name": "Luca Colombo"
                    },
                    {
                        "name": "Manuel Roveri"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Roveri"
                },
                "author": "Manuel Roveri",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08067v1",
                "updated": "2024-09-12T14:17:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    17,
                    40,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:17:40Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    17,
                    40,
                    3,
                    256,
                    0
                ],
                "title": "HD 222237 b: a long period super-Jupiter around a nearby star revealed\n  by radial-velocity and Hipparcos-Gaia astrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HD 222237 b: a long period super-Jupiter around a nearby star revealed\n  by radial-velocity and Hipparcos-Gaia astrometry"
                },
                "summary": "Giant planets on long period orbits around the nearest stars are among the\neasiest to directly image. Unfortunately these planets are difficult to fully\nconstrain by indirect methods, e.g., transit and radial velocity (RV). In this\nstudy, we present the discovery of a super-Jupiter, HD 222237 b, orbiting a\nstar located $11.445\\pm0.002$ pc away. By combining RV data, Hipparcos and\nmulti-epoch Gaia astrometry, we estimate the planetary mass to be\n${5.19}_{-0.58}^{+0.58}\\,M_{\\rm Jup}$, with an eccentricity of\n${0.56}_{-0.03}^{+0.03}$ and a period of ${40.8}_{-4.5}^{+5.8}$ yr, making HD\n222237 b a promising target for imaging using the Mid-Infrared Instrument\n(MIRI) of JWST. A comparative analysis suggests that our method can break the\ninclination degeneracy and thus differentiate between prograde and retrograde\norbits of a companion. We further find that the inferred contrast ratio between\nthe planet and the host star in the F1550C filter ($15.50\\,\\mu \\rm m$) is\napproximately $1.9\\times10^{-4}$, which is comparable with the measured limit\nof the MIRI coronagraphs. The relatively low metallicity of the host star\n($\\rm-0.32\\,dex$) combined with the unique orbital architecture of this system\npresents an excellent opportunity to probe the planet-metallicity correlation\nand the formation scenarios of giant planets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Giant planets on long period orbits around the nearest stars are among the\neasiest to directly image. Unfortunately these planets are difficult to fully\nconstrain by indirect methods, e.g., transit and radial velocity (RV). In this\nstudy, we present the discovery of a super-Jupiter, HD 222237 b, orbiting a\nstar located $11.445\\pm0.002$ pc away. By combining RV data, Hipparcos and\nmulti-epoch Gaia astrometry, we estimate the planetary mass to be\n${5.19}_{-0.58}^{+0.58}\\,M_{\\rm Jup}$, with an eccentricity of\n${0.56}_{-0.03}^{+0.03}$ and a period of ${40.8}_{-4.5}^{+5.8}$ yr, making HD\n222237 b a promising target for imaging using the Mid-Infrared Instrument\n(MIRI) of JWST. A comparative analysis suggests that our method can break the\ninclination degeneracy and thus differentiate between prograde and retrograde\norbits of a companion. We further find that the inferred contrast ratio between\nthe planet and the host star in the F1550C filter ($15.50\\,\\mu \\rm m$) is\napproximately $1.9\\times10^{-4}$, which is comparable with the measured limit\nof the MIRI coronagraphs. The relatively low metallicity of the host star\n($\\rm-0.32\\,dex$) combined with the unique orbital architecture of this system\npresents an excellent opportunity to probe the planet-metallicity correlation\nand the formation scenarios of giant planets."
                },
                "authors": [
                    {
                        "name": "Guang-Yao Xiao"
                    },
                    {
                        "name": "Fabo Feng"
                    },
                    {
                        "name": "Stephen A. Shectman"
                    },
                    {
                        "name": "C. G. Tinney"
                    },
                    {
                        "name": "Johanna K. Teske"
                    },
                    {
                        "name": "B. D. Carter"
                    },
                    {
                        "name": "H. R. A. Jones"
                    },
                    {
                        "name": "Robert A. Wittenmyer"
                    },
                    {
                        "name": "Matías R. Díaz"
                    },
                    {
                        "name": "Jeffrey D. Crane"
                    },
                    {
                        "name": "Sharon X. Wang"
                    },
                    {
                        "name": "J. Bailey"
                    },
                    {
                        "name": "S. J. O'Toole"
                    },
                    {
                        "name": "Adina D. Feinstein"
                    },
                    {
                        "name": "Malena Rice"
                    },
                    {
                        "name": "Zahra Essack"
                    },
                    {
                        "name": "Benjamin T. Montet"
                    },
                    {
                        "name": "Avi Shporer"
                    },
                    {
                        "name": "R. Paul Butler"
                    }
                ],
                "author_detail": {
                    "name": "R. Paul Butler"
                },
                "author": "R. Paul Butler",
                "arxiv_comment": "18 pages, 10 figures, 3 tables, accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08059v1",
                "updated": "2024-09-12T14:08:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    8,
                    10,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:08:10Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    8,
                    10,
                    3,
                    256,
                    0
                ],
                "title": "Causal inference and racial bias in policing: New estimands and the\n  importance of mobility data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference and racial bias in policing: New estimands and the\n  importance of mobility data"
                },
                "summary": "Studying racial bias in policing is a critically important problem, but one\nthat comes with a number of inherent difficulties due to the nature of the\navailable data. In this manuscript we tackle multiple key issues in the causal\nanalysis of racial bias in policing. First, we formalize race and place\npolicing, the idea that individuals of one race are policed differently when\nthey are in neighborhoods primarily made up of individuals of other races. We\ndevelop an estimand to study this question rigorously, show the assumptions\nnecessary for causal identification, and develop sensitivity analyses to assess\nrobustness to violations of key assumptions. Additionally, we investigate\ndifficulties with existing estimands targeting racial bias in policing. We show\nfor these estimands, and the estimands developed in this manuscript, that\nestimation can benefit from incorporating mobility data into analyses. We apply\nthese ideas to a study in New York City, where we find a large amount of racial\nbias, as well as race and place policing, and that these findings are robust to\nlarge violations of untestable assumptions. We additionally show that mobility\ndata can make substantial impacts on the resulting estimates, suggesting it\nshould be used whenever possible in subsequent studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studying racial bias in policing is a critically important problem, but one\nthat comes with a number of inherent difficulties due to the nature of the\navailable data. In this manuscript we tackle multiple key issues in the causal\nanalysis of racial bias in policing. First, we formalize race and place\npolicing, the idea that individuals of one race are policed differently when\nthey are in neighborhoods primarily made up of individuals of other races. We\ndevelop an estimand to study this question rigorously, show the assumptions\nnecessary for causal identification, and develop sensitivity analyses to assess\nrobustness to violations of key assumptions. Additionally, we investigate\ndifficulties with existing estimands targeting racial bias in policing. We show\nfor these estimands, and the estimands developed in this manuscript, that\nestimation can benefit from incorporating mobility data into analyses. We apply\nthese ideas to a study in New York City, where we find a large amount of racial\nbias, as well as race and place policing, and that these findings are robust to\nlarge violations of untestable assumptions. We additionally show that mobility\ndata can make substantial impacts on the resulting estimates, suggesting it\nshould be used whenever possible in subsequent studies."
                },
                "authors": [
                    {
                        "name": "Zhuochao Huang"
                    },
                    {
                        "name": "Brenden Beck"
                    },
                    {
                        "name": "Joseph Antonelli"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Antonelli"
                },
                "author": "Joseph Antonelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09536v2",
                "updated": "2024-09-12T13:53:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    53,
                    31,
                    3,
                    256,
                    0
                ],
                "published": "2024-04-15T07:59:11Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    59,
                    11,
                    0,
                    106,
                    0
                ],
                "title": "Noiseless Privacy-Preserving Decentralized Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noiseless Privacy-Preserving Decentralized Learning"
                },
                "summary": "Decentralized learning (DL) enables collaborative learning without a server\nand without training data leaving the users' devices. However, the models\nshared in DL can still be used to infer training data. Conventional defenses\nsuch as differential privacy and secure aggregation fall short in effectively\nsafeguarding user privacy in DL, either sacrificing model utility or\nefficiency. We introduce Shatter, a novel DL approach in which nodes create\nvirtual nodes (VNs) to disseminate chunks of their full model on their behalf.\nThis enhances privacy by (i) preventing attackers from collecting full models\nfrom other nodes, and (ii) hiding the identity of the original node that\nproduced a given model chunk. We theoretically prove the convergence of Shatter\nand provide a formal analysis demonstrating how Shatter reduces the efficacy of\nattacks compared to when exchanging full models between nodes. We evaluate the\nconvergence and attack resilience of Shatter with existing DL algorithms, with\nheterogeneous datasets, and against three standard privacy attacks. Our\nevaluation shows that Shatter not only renders these privacy attacks infeasible\nwhen each node operates 16 VNs but also exhibits a positive impact on model\nutility compared to standard DL. In summary, Shatter enhances the privacy of DL\nwhile maintaining the utility and efficiency of the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized learning (DL) enables collaborative learning without a server\nand without training data leaving the users' devices. However, the models\nshared in DL can still be used to infer training data. Conventional defenses\nsuch as differential privacy and secure aggregation fall short in effectively\nsafeguarding user privacy in DL, either sacrificing model utility or\nefficiency. We introduce Shatter, a novel DL approach in which nodes create\nvirtual nodes (VNs) to disseminate chunks of their full model on their behalf.\nThis enhances privacy by (i) preventing attackers from collecting full models\nfrom other nodes, and (ii) hiding the identity of the original node that\nproduced a given model chunk. We theoretically prove the convergence of Shatter\nand provide a formal analysis demonstrating how Shatter reduces the efficacy of\nattacks compared to when exchanging full models between nodes. We evaluate the\nconvergence and attack resilience of Shatter with existing DL algorithms, with\nheterogeneous datasets, and against three standard privacy attacks. Our\nevaluation shows that Shatter not only renders these privacy attacks infeasible\nwhen each node operates 16 VNs but also exhibits a positive impact on model\nutility compared to standard DL. In summary, Shatter enhances the privacy of DL\nwhile maintaining the utility and efficiency of the model."
                },
                "authors": [
                    {
                        "name": "Sayan Biswas"
                    },
                    {
                        "name": "Mathieu Even"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Laurent Massoulie"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Rishi Sharma"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_comment": "Accepted at PETS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08045v1",
                "updated": "2024-09-12T13:50:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    50,
                    22,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T13:50:22Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    50,
                    22,
                    3,
                    256,
                    0
                ],
                "title": "Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks\n  against RAG-based Inference in Scale and Severity Using Jailbreaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks\n  against RAG-based Inference in Scale and Severity Using Jailbreaking"
                },
                "summary": "In this paper, we show that with the ability to jailbreak a GenAI model,\nattackers can escalate the outcome of attacks against RAG-based GenAI-powered\napplications in severity and scale. In the first part of the paper, we show\nthat attackers can escalate RAG membership inference attacks and RAG entity\nextraction attacks to RAG documents extraction attacks, forcing a more severe\noutcome compared to existing attacks. We evaluate the results obtained from\nthree extraction methods, the influence of the type and the size of five\nembeddings algorithms employed, the size of the provided context, and the GenAI\nengine. We show that attackers can extract 80%-99.8% of the data stored in the\ndatabase used by the RAG of a Q&A chatbot. In the second part of the paper, we\nshow that attackers can escalate the scale of RAG data poisoning attacks from\ncompromising a single GenAI-powered application to compromising the entire\nGenAI ecosystem, forcing a greater scale of damage. This is done by crafting an\nadversarial self-replicating prompt that triggers a chain reaction of a\ncomputer worm within the ecosystem and forces each affected application to\nperform a malicious activity and compromise the RAG of additional applications.\nWe evaluate the performance of the worm in creating a chain of confidential\ndata extraction about users within a GenAI ecosystem of GenAI-powered email\nassistants and analyze how the performance of the worm is affected by the size\nof the context, the adversarial self-replicating prompt used, the type and size\nof the embeddings algorithm employed, and the number of hops in the\npropagation. Finally, we review and analyze guardrails to protect RAG-based\ninference and discuss the tradeoffs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we show that with the ability to jailbreak a GenAI model,\nattackers can escalate the outcome of attacks against RAG-based GenAI-powered\napplications in severity and scale. In the first part of the paper, we show\nthat attackers can escalate RAG membership inference attacks and RAG entity\nextraction attacks to RAG documents extraction attacks, forcing a more severe\noutcome compared to existing attacks. We evaluate the results obtained from\nthree extraction methods, the influence of the type and the size of five\nembeddings algorithms employed, the size of the provided context, and the GenAI\nengine. We show that attackers can extract 80%-99.8% of the data stored in the\ndatabase used by the RAG of a Q&A chatbot. In the second part of the paper, we\nshow that attackers can escalate the scale of RAG data poisoning attacks from\ncompromising a single GenAI-powered application to compromising the entire\nGenAI ecosystem, forcing a greater scale of damage. This is done by crafting an\nadversarial self-replicating prompt that triggers a chain reaction of a\ncomputer worm within the ecosystem and forces each affected application to\nperform a malicious activity and compromise the RAG of additional applications.\nWe evaluate the performance of the worm in creating a chain of confidential\ndata extraction about users within a GenAI ecosystem of GenAI-powered email\nassistants and analyze how the performance of the worm is affected by the size\nof the context, the adversarial self-replicating prompt used, the type and size\nof the embeddings algorithm employed, and the number of hops in the\npropagation. Finally, we review and analyze guardrails to protect RAG-based\ninference and discuss the tradeoffs."
                },
                "authors": [
                    {
                        "name": "Stav Cohen"
                    },
                    {
                        "name": "Ron Bitton"
                    },
                    {
                        "name": "Ben Nassi"
                    }
                ],
                "author_detail": {
                    "name": "Ben Nassi"
                },
                "author": "Ben Nassi",
                "arxiv_comment": "for Github, see\n  https://github.com/StavC/UnleashingWorms-ExtractingData",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00134v2",
                "updated": "2024-09-12T13:49:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    49,
                    0,
                    3,
                    256,
                    0
                ],
                "published": "2024-08-29T12:55:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    55,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale"
                },
                "summary": "Multi-agent pathfinding (MAPF) is a challenging computational problem that\ntypically requires to find collision-free paths for multiple agents in a shared\nenvironment. Solving MAPF optimally is NP-hard, yet efficient solutions are\ncritical for numerous applications, including automated warehouses and\ntransportation systems. Recently, learning-based approaches to MAPF have gained\nattention, particularly those leveraging deep reinforcement learning. Following\ncurrent trends in machine learning, we have created a foundation model for the\nMAPF problems called MAPF-GPT. Using imitation learning, we have trained a\npolicy on a set of pre-collected sub-optimal expert trajectories that can\ngenerate actions in conditions of partial observability without additional\nheuristics, reward functions, or communication with other agents. The resulting\nMAPF-GPT model demonstrates zero-shot learning abilities when solving the MAPF\nproblem instances that were not present in the training dataset. We show that\nMAPF-GPT notably outperforms the current best-performing learnable-MAPF solvers\non a diverse range of problem instances and is efficient in terms of\ncomputation (in the inference mode).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent pathfinding (MAPF) is a challenging computational problem that\ntypically requires to find collision-free paths for multiple agents in a shared\nenvironment. Solving MAPF optimally is NP-hard, yet efficient solutions are\ncritical for numerous applications, including automated warehouses and\ntransportation systems. Recently, learning-based approaches to MAPF have gained\nattention, particularly those leveraging deep reinforcement learning. Following\ncurrent trends in machine learning, we have created a foundation model for the\nMAPF problems called MAPF-GPT. Using imitation learning, we have trained a\npolicy on a set of pre-collected sub-optimal expert trajectories that can\ngenerate actions in conditions of partial observability without additional\nheuristics, reward functions, or communication with other agents. The resulting\nMAPF-GPT model demonstrates zero-shot learning abilities when solving the MAPF\nproblem instances that were not present in the training dataset. We show that\nMAPF-GPT notably outperforms the current best-performing learnable-MAPF solvers\non a diverse range of problem instances and is efficient in terms of\ncomputation (in the inference mode)."
                },
                "authors": [
                    {
                        "name": "Anton Andreychuk"
                    },
                    {
                        "name": "Konstantin Yakovlev"
                    },
                    {
                        "name": "Aleksandr Panov"
                    },
                    {
                        "name": "Alexey Skrynnik"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Skrynnik"
                },
                "author": "Alexey Skrynnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08027v1",
                "updated": "2024-09-12T13:18:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    18,
                    41,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T13:18:41Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    18,
                    41,
                    3,
                    256,
                    0
                ],
                "title": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework\n  for Student Performance Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework\n  for Student Performance Feedback"
                },
                "summary": "Recent advances in eXplainable AI (XAI) for education have highlighted a\ncritical challenge: ensuring that explanations for state-of-the-art AI models\nare understandable for non-technical users such as educators and students. In\nresponse, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI\npipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is\ndesigned to deliver theory-driven, actionable feedback to students in online\ncourses. iLLuMinaTE navigates three main stages - causal connection,\nexplanation selection, and explanation presentation - with variations drawing\nfrom eight social science theories (e.g. Abnormal Conditions, Pearl's Model of\nExplanation, Necessity and Robustness Selection, Contrastive Explanation). We\nextensively evaluate 21,915 natural language explanations of iLLuMinaTE\nextracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different\nunderlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from\nthree diverse online courses. Our evaluation involves analyses of explanation\nalignment to the social science theory, understandability of the explanation,\nand a real-world user preference study with 114 university students containing\na novel actionability simulation. We find that students prefer iLLuMinaTE\nexplanations over traditional explainers 89.52% of the time. Our work provides\na robust, ready-to-use framework for effectively communicating hybrid\nXAI-driven insights in education, with significant generalization potential for\nother human-centric fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in eXplainable AI (XAI) for education have highlighted a\ncritical challenge: ensuring that explanations for state-of-the-art AI models\nare understandable for non-technical users such as educators and students. In\nresponse, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI\npipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is\ndesigned to deliver theory-driven, actionable feedback to students in online\ncourses. iLLuMinaTE navigates three main stages - causal connection,\nexplanation selection, and explanation presentation - with variations drawing\nfrom eight social science theories (e.g. Abnormal Conditions, Pearl's Model of\nExplanation, Necessity and Robustness Selection, Contrastive Explanation). We\nextensively evaluate 21,915 natural language explanations of iLLuMinaTE\nextracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different\nunderlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from\nthree diverse online courses. Our evaluation involves analyses of explanation\nalignment to the social science theory, understandability of the explanation,\nand a real-world user preference study with 114 university students containing\na novel actionability simulation. We find that students prefer iLLuMinaTE\nexplanations over traditional explainers 89.52% of the time. Our work provides\na robust, ready-to-use framework for effectively communicating hybrid\nXAI-driven insights in education, with significant generalization potential for\nother human-centric fields."
                },
                "authors": [
                    {
                        "name": "Vinitra Swamy"
                    },
                    {
                        "name": "Davide Romano"
                    },
                    {
                        "name": "Bhargav Srinivasa Desikan"
                    },
                    {
                        "name": "Oana-Maria Camburu"
                    },
                    {
                        "name": "Tanja Käser"
                    }
                ],
                "author_detail": {
                    "name": "Tanja Käser"
                },
                "author": "Tanja Käser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08023v1",
                "updated": "2024-09-12T13:05:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    5,
                    28,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T13:05:28Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    5,
                    28,
                    3,
                    256,
                    0
                ],
                "title": "Edge-Wise Graph-Instructed Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-Wise Graph-Instructed Neural Networks"
                },
                "summary": "The problem of multi-task regression over graph nodes has been recently\napproached through Graph-Instructed Neural Network (GINN), which is a promising\narchitecture belonging to the subset of message-passing graph neural networks.\nIn this work, we discuss the limitations of the Graph-Instructed (GI) layer,\nand we formalize a novel edge-wise GI (EWGI) layer. We discuss the advantages\nof the EWGI layer and we provide numerical evidence that EWGINNs perform better\nthan GINNs over graph-structured input data with chaotic connectivity, like the\nones inferred from the Erdos-R\\'enyi graph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of multi-task regression over graph nodes has been recently\napproached through Graph-Instructed Neural Network (GINN), which is a promising\narchitecture belonging to the subset of message-passing graph neural networks.\nIn this work, we discuss the limitations of the Graph-Instructed (GI) layer,\nand we formalize a novel edge-wise GI (EWGI) layer. We discuss the advantages\nof the EWGI layer and we provide numerical evidence that EWGINNs perform better\nthan GINNs over graph-structured input data with chaotic connectivity, like the\nones inferred from the Erdos-R\\'enyi graph."
                },
                "authors": [
                    {
                        "name": "Francesco Della Santa"
                    },
                    {
                        "name": "Antonio Mastropietro"
                    },
                    {
                        "name": "Sandra Pieraccini"
                    },
                    {
                        "name": "Francesco Vaccarino"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Vaccarino"
                },
                "author": "Francesco Vaccarino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "05C21, 65D15, 68T07, 90C35",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10566v3",
                "updated": "2024-09-12T12:57:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    57,
                    25,
                    3,
                    256,
                    0
                ],
                "published": "2024-08-20T06:05:52Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    6,
                    5,
                    52,
                    1,
                    233,
                    0
                ],
                "title": "SparseGrow: Addressing Growth-Induced Forgetting in Task-Agnostic\n  Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseGrow: Addressing Growth-Induced Forgetting in Task-Agnostic\n  Continual Learning"
                },
                "summary": "In continual learning (CL), model growth enhances adaptability over new data,\nimproving knowledge retention for more tasks. However, improper model growth\ncan lead to severe degradation of previously learned knowledge, an issue we\nname as growth-induced forgetting (GIFt), especially in task-agnostic CL using\nentire grown model for inference. Existing works, despite adopting model growth\nand random initialization for better adaptability, often fail to recognize the\npresence of GIFt caused by improper model growth. This oversight limits\ncomprehensive control of forgetting and hinders full utilization of model\ngrowth. We are the first in CL to identify this issue and conduct an in-depth\nstudy on root cause of GIFt, where layer expansion stands out among model\ngrowth strategies, widening layers without affecting model functionality. Yet,\ndirect adoption of layer expansion presents challenges. It lacks data-driven\ncontrol and initialization of expanded parameters to balance adaptability and\nknowledge retention. This paper presents a novel SparseGrow approach to\novercome the issue of GIFt while enhancing adaptability over new data.\nSparseGrow employs data-driven sparse layer expansion to control efficient\nparameter usage during growth, reducing GIFt from excessive growth and\nfunctionality changes. It also combines sparse growth with on-data\ninitialization at training late-stage to create partially 0-valued expansions\nthat fit learned distribution, enhancing retention and adaptability. To further\nminimize forgetting, freezing is applied by calculating the sparse mask,\nallowing data-driven preservation of important parameters. Through experiments\nacross datasets with various settings, cases and task numbers, we demonstrate\nthe necessity of layer expansion and showcase the effectiveness of SparseGrow\nin overcoming GIFt, highlighting its adaptability and knowledge retention for\nincremental tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In continual learning (CL), model growth enhances adaptability over new data,\nimproving knowledge retention for more tasks. However, improper model growth\ncan lead to severe degradation of previously learned knowledge, an issue we\nname as growth-induced forgetting (GIFt), especially in task-agnostic CL using\nentire grown model for inference. Existing works, despite adopting model growth\nand random initialization for better adaptability, often fail to recognize the\npresence of GIFt caused by improper model growth. This oversight limits\ncomprehensive control of forgetting and hinders full utilization of model\ngrowth. We are the first in CL to identify this issue and conduct an in-depth\nstudy on root cause of GIFt, where layer expansion stands out among model\ngrowth strategies, widening layers without affecting model functionality. Yet,\ndirect adoption of layer expansion presents challenges. It lacks data-driven\ncontrol and initialization of expanded parameters to balance adaptability and\nknowledge retention. This paper presents a novel SparseGrow approach to\novercome the issue of GIFt while enhancing adaptability over new data.\nSparseGrow employs data-driven sparse layer expansion to control efficient\nparameter usage during growth, reducing GIFt from excessive growth and\nfunctionality changes. It also combines sparse growth with on-data\ninitialization at training late-stage to create partially 0-valued expansions\nthat fit learned distribution, enhancing retention and adaptability. To further\nminimize forgetting, freezing is applied by calculating the sparse mask,\nallowing data-driven preservation of important parameters. Through experiments\nacross datasets with various settings, cases and task numbers, we demonstrate\nthe necessity of layer expansion and showcase the effectiveness of SparseGrow\nin overcoming GIFt, highlighting its adaptability and knowledge retention for\nincremental tasks."
                },
                "authors": [
                    {
                        "name": "Yuqing Zhao"
                    },
                    {
                        "name": "Divya Saxena"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Xiaoyun Liu"
                    },
                    {
                        "name": "Changlin Song"
                    }
                ],
                "author_detail": {
                    "name": "Changlin Song"
                },
                "author": "Changlin Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08014v1",
                "updated": "2024-09-12T12:57:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    57,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T12:57:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    57,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "An Evaluation Framework for Attributed Information Retrieval using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Evaluation Framework for Attributed Information Retrieval using Large\n  Language Models"
                },
                "summary": "With the growing success of Large Language models (LLMs) in\ninformation-seeking scenarios, search engines are now adopting generative\napproaches to provide answers along with in-line citations as attribution.\nWhile existing work focuses mainly on attributed question answering, in this\npaper, we target information-seeking scenarios which are often more challenging\ndue to the open-ended nature of the queries and the size of the label space in\nterms of the diversity of candidate-attributed answers per query. We propose a\nreproducible framework to evaluate and benchmark attributed information\nseeking, using any backbone LLM, and different architectural designs: (1)\nGenerate (2) Retrieve then Generate, and (3) Generate then Retrieve.\nExperiments using HAGRID, an attributed information-seeking dataset, show the\nimpact of different scenarios on both the correctness and attributability of\nanswers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing success of Large Language models (LLMs) in\ninformation-seeking scenarios, search engines are now adopting generative\napproaches to provide answers along with in-line citations as attribution.\nWhile existing work focuses mainly on attributed question answering, in this\npaper, we target information-seeking scenarios which are often more challenging\ndue to the open-ended nature of the queries and the size of the label space in\nterms of the diversity of candidate-attributed answers per query. We propose a\nreproducible framework to evaluate and benchmark attributed information\nseeking, using any backbone LLM, and different architectural designs: (1)\nGenerate (2) Retrieve then Generate, and (3) Generate then Retrieve.\nExperiments using HAGRID, an attributed information-seeking dataset, show the\nimpact of different scenarios on both the correctness and attributability of\nanswers."
                },
                "authors": [
                    {
                        "name": "Hanane Djeddal"
                    },
                    {
                        "name": "Pierre Erbacher"
                    },
                    {
                        "name": "Raouf Toukal"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Karen Pinel-Sauvagnat"
                    },
                    {
                        "name": "Sophia Katrenko"
                    },
                    {
                        "name": "Lynda Tamine"
                    }
                ],
                "author_detail": {
                    "name": "Lynda Tamine"
                },
                "author": "Lynda Tamine",
                "arxiv_doi": "10.1145/3627673.3679172",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3679172",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.08014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07984v1",
                "updated": "2024-09-12T12:30:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    30,
                    4,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T12:30:04Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    30,
                    4,
                    3,
                    256,
                    0
                ],
                "title": "SPARK: Self-supervised Personalized Real-time Monocular Face Capture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPARK: Self-supervised Personalized Real-time Monocular Face Capture"
                },
                "summary": "Feedforward monocular face capture methods seek to reconstruct posed faces\nfrom a single image of a person. Current state of the art approaches have the\nability to regress parametric 3D face models in real-time across a wide range\nof identities, lighting conditions and poses by leveraging large image datasets\nof human faces. These methods however suffer from clear limitations in that the\nunderlying parametric face model only provides a coarse estimation of the face\nshape, thereby limiting their practical applicability in tasks that require\nprecise 3D reconstruction (aging, face swapping, digital make-up, ...). In this\npaper, we propose a method for high-precision 3D face capture taking advantage\nof a collection of unconstrained videos of a subject as prior information. Our\nproposal builds on a two stage approach. We start with the reconstruction of a\ndetailed 3D face avatar of the person, capturing both precise geometry and\nappearance from a collection of videos. We then use the encoder from a\npre-trained monocular face reconstruction method, substituting its decoder with\nour personalized model, and proceed with transfer learning on the video\ncollection. Using our pre-estimated image formation model, we obtain a more\nprecise self-supervision objective, enabling improved expression and pose\nalignment. This results in a trained encoder capable of efficiently regressing\npose and expression parameters in real-time from previously unseen images,\nwhich combined with our personalized geometry model yields more accurate and\nhigh fidelity mesh inference. Through extensive qualitative and quantitative\nevaluation, we showcase the superiority of our final model as compared to\nstate-of-the-art baselines, and demonstrate its generalization ability to\nunseen pose, expression and lighting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedforward monocular face capture methods seek to reconstruct posed faces\nfrom a single image of a person. Current state of the art approaches have the\nability to regress parametric 3D face models in real-time across a wide range\nof identities, lighting conditions and poses by leveraging large image datasets\nof human faces. These methods however suffer from clear limitations in that the\nunderlying parametric face model only provides a coarse estimation of the face\nshape, thereby limiting their practical applicability in tasks that require\nprecise 3D reconstruction (aging, face swapping, digital make-up, ...). In this\npaper, we propose a method for high-precision 3D face capture taking advantage\nof a collection of unconstrained videos of a subject as prior information. Our\nproposal builds on a two stage approach. We start with the reconstruction of a\ndetailed 3D face avatar of the person, capturing both precise geometry and\nappearance from a collection of videos. We then use the encoder from a\npre-trained monocular face reconstruction method, substituting its decoder with\nour personalized model, and proceed with transfer learning on the video\ncollection. Using our pre-estimated image formation model, we obtain a more\nprecise self-supervision objective, enabling improved expression and pose\nalignment. This results in a trained encoder capable of efficiently regressing\npose and expression parameters in real-time from previously unseen images,\nwhich combined with our personalized geometry model yields more accurate and\nhigh fidelity mesh inference. Through extensive qualitative and quantitative\nevaluation, we showcase the superiority of our final model as compared to\nstate-of-the-art baselines, and demonstrate its generalization ability to\nunseen pose, expression and lighting."
                },
                "authors": [
                    {
                        "name": "Kelian Baert"
                    },
                    {
                        "name": "Shrisha Bharadwaj"
                    },
                    {
                        "name": "Fabien Castan"
                    },
                    {
                        "name": "Benoit Maujean"
                    },
                    {
                        "name": "Marc Christie"
                    },
                    {
                        "name": "Victoria Abrevaya"
                    },
                    {
                        "name": "Adnane Boukhayma"
                    }
                ],
                "author_detail": {
                    "name": "Adnane Boukhayma"
                },
                "author": "Adnane Boukhayma",
                "arxiv_doi": "10.1145/3680528.3687704",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680528.3687704",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.07984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SIGGRAPH Asia 2024 Conference Paper. Project page:\n  https://kelianb.github.io/SPARK/",
                "arxiv_journal_ref": "SIGGRAPH Asia 2024 Conference Papers (SA Conference Papers '24),\n  December 3-6, 2024, Tokyo, Japan",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07976v1",
                "updated": "2024-09-12T12:16:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    16,
                    0,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T12:16:00Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    16,
                    0,
                    3,
                    256,
                    0
                ],
                "title": "Constraints on the Primordial Black Hole Abundance through\n  Scalar-Induced Gravitational Waves from Advanced LIGO and Virgo's First Three\n  Observing Runs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraints on the Primordial Black Hole Abundance through\n  Scalar-Induced Gravitational Waves from Advanced LIGO and Virgo's First Three\n  Observing Runs"
                },
                "summary": "As a promising dark matter candidate, primordial black holes (PBHs) lighter\nthan $\\sim10^{-18}M_{\\odot}$ are supposed to have evaporated by today through\nHawking radiation. This scenario is challenged by the memory burden effect,\nwhich suggests that the evaporation of black holes may slow down significantly\nafter they have emitted about half of their initial mass. We explore the\nastrophysical implications of the memory burden effect on the PBH abundance by\ntoday and the possibility for PBHs lighter than $\\sim10^{-18}M_{\\odot}$ to\npersist as dark matter. Our analysis utilizes current LIGO-Virgo-KAGRA data to\nconstrain the primordial power spectrum and infers the PBH abundance. We find a\nnull detection of scalar-induced gravitational waves that accompanied the\nformation of the PBHs. Then we place an upper limit on the primordial power\nspectrum and the PBH abundance to be $f_{\\mathrm{pbh}}\\simeq0.3$ for PBHs with\nmasses $\\sim10^{-24}M_{\\odot}$. Furthermore, we expect that next-generation\ngravitational wave detectors, such as the Einstein Telescope and the Cosmic\nExplorer, will provide even more stringent constraints. Our results indicate\nthat future detectors can reach sensitivities that could rule out PBH as dark\nmatter within $\\sim[10^{-29}M_{\\odot},10^{-19}M_{\\odot}]$ in the null detection\nof scalar-induced gravitational waves.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising dark matter candidate, primordial black holes (PBHs) lighter\nthan $\\sim10^{-18}M_{\\odot}$ are supposed to have evaporated by today through\nHawking radiation. This scenario is challenged by the memory burden effect,\nwhich suggests that the evaporation of black holes may slow down significantly\nafter they have emitted about half of their initial mass. We explore the\nastrophysical implications of the memory burden effect on the PBH abundance by\ntoday and the possibility for PBHs lighter than $\\sim10^{-18}M_{\\odot}$ to\npersist as dark matter. Our analysis utilizes current LIGO-Virgo-KAGRA data to\nconstrain the primordial power spectrum and infers the PBH abundance. We find a\nnull detection of scalar-induced gravitational waves that accompanied the\nformation of the PBHs. Then we place an upper limit on the primordial power\nspectrum and the PBH abundance to be $f_{\\mathrm{pbh}}\\simeq0.3$ for PBHs with\nmasses $\\sim10^{-24}M_{\\odot}$. Furthermore, we expect that next-generation\ngravitational wave detectors, such as the Einstein Telescope and the Cosmic\nExplorer, will provide even more stringent constraints. Our results indicate\nthat future detectors can reach sensitivities that could rule out PBH as dark\nmatter within $\\sim[10^{-29}M_{\\odot},10^{-19}M_{\\odot}]$ in the null detection\nof scalar-induced gravitational waves."
                },
                "authors": [
                    {
                        "name": "Yang Jiang"
                    },
                    {
                        "name": "Chen Yuan"
                    },
                    {
                        "name": "Chong-Zhi Li"
                    },
                    {
                        "name": "Qing-Guo Huang"
                    }
                ],
                "author_detail": {
                    "name": "Qing-Guo Huang"
                },
                "author": "Qing-Guo Huang",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11386v2",
                "updated": "2024-09-12T12:08:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    8,
                    4,
                    3,
                    256,
                    0
                ],
                "published": "2024-08-21T07:30:11Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    30,
                    11,
                    2,
                    234,
                    0
                ],
                "title": "Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for\n  Business Process Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for\n  Business Process Management"
                },
                "summary": "To promote sustainable business practices, and to achieve climate neutrality\nby 2050, the EU has developed the taxonomy of sustainable activities, which\ndescribes when exactly business practices can be considered sustainable. While\nthe taxonomy has only been recently established, progressively more companies\nwill have to report how much of their revenue was created via sustainably\nexecuted business processes. To help companies prepare to assess whether their\nbusiness processes comply with the constraints outlined in the taxonomy, we\ninvestigate in how far these criteria can be used for conformance checking,\nthat is, assessing in a data-driven manner, whether business process executions\nadhere to regulatory constraints. For this, we develop a few-shot learning\npipeline to characterize the constraints of the taxonomy with the help of an\nLLM as to the process dimensions they relate to. We find that many constraints\nof the taxonomy are useable for conformance checking, particularly in the\nsectors of energy, manufacturing, and transport. This will aid companies in\npreparing to monitor regulatory compliance with the taxonomy automatically, by\ncharacterizing what kind of information they need to extract, and by providing\na better understanding of sectors where such an assessment is feasible and\nwhere it is not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To promote sustainable business practices, and to achieve climate neutrality\nby 2050, the EU has developed the taxonomy of sustainable activities, which\ndescribes when exactly business practices can be considered sustainable. While\nthe taxonomy has only been recently established, progressively more companies\nwill have to report how much of their revenue was created via sustainably\nexecuted business processes. To help companies prepare to assess whether their\nbusiness processes comply with the constraints outlined in the taxonomy, we\ninvestigate in how far these criteria can be used for conformance checking,\nthat is, assessing in a data-driven manner, whether business process executions\nadhere to regulatory constraints. For this, we develop a few-shot learning\npipeline to characterize the constraints of the taxonomy with the help of an\nLLM as to the process dimensions they relate to. We find that many constraints\nof the taxonomy are useable for conformance checking, particularly in the\nsectors of energy, manufacturing, and transport. This will aid companies in\npreparing to monitor regulatory compliance with the taxonomy automatically, by\ncharacterizing what kind of information they need to extract, and by providing\na better understanding of sectors where such an assessment is feasible and\nwhere it is not."
                },
                "authors": [
                    {
                        "name": "Finn Klessascheck"
                    },
                    {
                        "name": "Stephan A. Fahrenkrog-Petersen"
                    },
                    {
                        "name": "Jan Mendling"
                    },
                    {
                        "name": "Luise Pufahl"
                    }
                ],
                "author_detail": {
                    "name": "Luise Pufahl"
                },
                "author": "Luise Pufahl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07968v1",
                "updated": "2024-09-12T12:02:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    2,
                    51,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T12:02:51Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    2,
                    51,
                    3,
                    256,
                    0
                ],
                "title": "Localized Schrödinger Bridge Sampler",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Schrödinger Bridge Sampler"
                },
                "summary": "We consider the generative problem of sampling from an unknown distribution\nfor which only a sufficiently large number of training samples are available.\nIn this paper, we build on previous work combining Schr\\\"odinger bridges and\nLangevin dynamics. A key bottleneck of this approach is the exponential\ndependence of the required training samples on the dimension, $d$, of the\nambient state space. We propose a localization strategy which exploits\nconditional independence of conditional expectation values. Localization thus\nreplaces a single high-dimensional Schr\\\"odinger bridge problem by $d$\nlow-dimensional Schr\\\"odinger bridge problems over the available training\nsamples. As for the original approach, the localized sampler is stable and\ngeometric ergodic. The sampler also naturally extends to conditional sampling\nand to Bayesian inference. We demonstrate the performance of our proposed\nscheme through experiments on a Gaussian problem with increasing dimensions and\non a stochastic subgrid-scale parametrization conditional sampling problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the generative problem of sampling from an unknown distribution\nfor which only a sufficiently large number of training samples are available.\nIn this paper, we build on previous work combining Schr\\\"odinger bridges and\nLangevin dynamics. A key bottleneck of this approach is the exponential\ndependence of the required training samples on the dimension, $d$, of the\nambient state space. We propose a localization strategy which exploits\nconditional independence of conditional expectation values. Localization thus\nreplaces a single high-dimensional Schr\\\"odinger bridge problem by $d$\nlow-dimensional Schr\\\"odinger bridge problems over the available training\nsamples. As for the original approach, the localized sampler is stable and\ngeometric ergodic. The sampler also naturally extends to conditional sampling\nand to Bayesian inference. We demonstrate the performance of our proposed\nscheme through experiments on a Gaussian problem with increasing dimensions and\non a stochastic subgrid-scale parametrization conditional sampling problem."
                },
                "authors": [
                    {
                        "name": "Georg A. Gottwald"
                    },
                    {
                        "name": "Sebastian Reich"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Reich"
                },
                "author": "Sebastian Reich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60H10, 62F15, 62F30, 65C05, 65C40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05639v2",
                "updated": "2024-09-12T12:00:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    0,
                    26,
                    3,
                    256,
                    0
                ],
                "published": "2024-06-09T04:42:19Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    4,
                    42,
                    19,
                    6,
                    161,
                    0
                ],
                "title": "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on\n  Automated Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on\n  Automated Program Repair"
                },
                "summary": "Automated Program Repair (APR) aims to fix bugs by generating patches. And\nexisting work has demonstrated that \"pre-training and fine-tuning\" paradigm\nenables Large Language Models (LLMs) improve fixing capabilities on APR.\nHowever, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR\nand limited research has been conducted on the execution-based evaluation of\nParameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can\nreduce computing resource consumption without compromising performance and has\nbeen widely adopted to other software engineering tasks.\n  To fill this gap, we enhance the existing APR dataset by employing prompt\nengineering to create an instruction dataset, APR-INSTRUCTION, at first.\nSecondly, we fine-tune four pre-trained LLMs using four different PEFT methods\nwith APR-INSTRUCTION. The best fine-tuned model fixes 58% more bugs than the\nstate-of-the-art LLM-based APR techniques. The results also show that $(IA)^3$\nimproves the creativity of LLMs more effectively through fine-tuning and\nachieves the highest fixing capability compared to the other three PEFT\nmethods. Thirdly, we explore the optimal configuration of PEFT hyperparameters,\nand assess the impact of instruction dataset size, showing that a larger number\nof parameters and a larger training dataset do not necessarily result in better\nperformance for PEFT. Lastly, we analyze peak memory usage and trainable\nparameters to show the efficiency of PEFT.\n  This work provides a comprehensive exploration of PEFT on APR and suggests\npotentially promising directions for extension to other software engineering\ndownstream tasks. APR-INSTRUCTION, PEFT weights, and the fine-tuning code are\npublicly available as open-source resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) aims to fix bugs by generating patches. And\nexisting work has demonstrated that \"pre-training and fine-tuning\" paradigm\nenables Large Language Models (LLMs) improve fixing capabilities on APR.\nHowever, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR\nand limited research has been conducted on the execution-based evaluation of\nParameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can\nreduce computing resource consumption without compromising performance and has\nbeen widely adopted to other software engineering tasks.\n  To fill this gap, we enhance the existing APR dataset by employing prompt\nengineering to create an instruction dataset, APR-INSTRUCTION, at first.\nSecondly, we fine-tune four pre-trained LLMs using four different PEFT methods\nwith APR-INSTRUCTION. The best fine-tuned model fixes 58% more bugs than the\nstate-of-the-art LLM-based APR techniques. The results also show that $(IA)^3$\nimproves the creativity of LLMs more effectively through fine-tuning and\nachieves the highest fixing capability compared to the other three PEFT\nmethods. Thirdly, we explore the optimal configuration of PEFT hyperparameters,\nand assess the impact of instruction dataset size, showing that a larger number\nof parameters and a larger training dataset do not necessarily result in better\nperformance for PEFT. Lastly, we analyze peak memory usage and trainable\nparameters to show the efficiency of PEFT.\n  This work provides a comprehensive exploration of PEFT on APR and suggests\npotentially promising directions for extension to other software engineering\ndownstream tasks. APR-INSTRUCTION, PEFT weights, and the fine-tuning code are\npublicly available as open-source resources."
                },
                "authors": [
                    {
                        "name": "Guochang Li"
                    },
                    {
                        "name": "Chen Zhi"
                    },
                    {
                        "name": "Jialiang Chen"
                    },
                    {
                        "name": "Junxiao Han"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "ASE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03302v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03302v4",
                "updated": "2024-09-12T11:51:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    11,
                    51,
                    51,
                    3,
                    256,
                    0
                ],
                "published": "2024-04-04T08:52:30Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    8,
                    52,
                    30,
                    3,
                    95,
                    0
                ],
                "title": "How Easily do Irrelevant Inputs Skew the Responses of Large Language\n  Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Easily do Irrelevant Inputs Skew the Responses of Large Language\n  Models?"
                },
                "summary": "By leveraging the retrieval of information from external knowledge databases,\nLarge Language Models (LLMs) exhibit enhanced capabilities for accomplishing\nmany knowledge-intensive tasks. However, due to the inherent flaws of current\nretrieval systems, there might exist irrelevant information within those\nretrieving top-ranked passages. In this work, we present a comprehensive\ninvestigation into the robustness of LLMs to different types of irrelevant\ninformation under various conditions. We initially introduce a framework to\nconstruct high-quality irrelevant information that ranges from semantically\nunrelated, partially related, and related to questions. Furthermore, our\nanalysis demonstrates that the constructed irrelevant information not only\nscores highly on similarity metrics, being highly retrieved by existing\nsystems, but also bears semantic connections to the context. Our investigation\nreveals that current LLMs still face challenges in discriminating highly\nsemantically related information and can be easily distracted by these\nirrelevant yet misleading content. Besides, we also find that current solutions\nfor handling irrelevant information have limitations in improving the\nrobustness of LLMs to such distractions. All the resources are available on\nGitHub at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By leveraging the retrieval of information from external knowledge databases,\nLarge Language Models (LLMs) exhibit enhanced capabilities for accomplishing\nmany knowledge-intensive tasks. However, due to the inherent flaws of current\nretrieval systems, there might exist irrelevant information within those\nretrieving top-ranked passages. In this work, we present a comprehensive\ninvestigation into the robustness of LLMs to different types of irrelevant\ninformation under various conditions. We initially introduce a framework to\nconstruct high-quality irrelevant information that ranges from semantically\nunrelated, partially related, and related to questions. Furthermore, our\nanalysis demonstrates that the constructed irrelevant information not only\nscores highly on similarity metrics, being highly retrieved by existing\nsystems, but also bears semantic connections to the context. Our investigation\nreveals that current LLMs still face challenges in discriminating highly\nsemantically related information and can be easily distracted by these\nirrelevant yet misleading content. Besides, we also find that current solutions\nfor handling irrelevant information have limitations in improving the\nrobustness of LLMs to such distractions. All the resources are available on\nGitHub at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information."
                },
                "authors": [
                    {
                        "name": "Siye Wu"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Tinghui Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03302v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03302v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07964v1",
                "updated": "2024-09-12T11:48:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    11,
                    48,
                    1,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T11:48:01Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    11,
                    48,
                    1,
                    3,
                    256,
                    0
                ],
                "title": "WirelessAgent: Large Language Model Agents for Intelligent Wireless\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WirelessAgent: Large Language Model Agents for Intelligent Wireless\n  Networks"
                },
                "summary": "Wireless networks are increasingly facing challenges due to their expanding\nscale and complexity. These challenges underscore the need for advanced\nAI-driven strategies, particularly in the upcoming 6G networks. In this\narticle, we introduce WirelessAgent, a novel approach leveraging large language\nmodels (LLMs) to develop AI agents capable of managing complex tasks in\nwireless networks. It can effectively improve network performance through\nadvanced reasoning, multimodal data processing, and autonomous decision making.\nThereafter, we demonstrate the practical applicability and benefits of\nWirelessAgent for network slicing management. The experimental results show\nthat WirelessAgent is capable of accurately understanding user intent,\neffectively allocating slice resources, and consistently maintaining optimal\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless networks are increasingly facing challenges due to their expanding\nscale and complexity. These challenges underscore the need for advanced\nAI-driven strategies, particularly in the upcoming 6G networks. In this\narticle, we introduce WirelessAgent, a novel approach leveraging large language\nmodels (LLMs) to develop AI agents capable of managing complex tasks in\nwireless networks. It can effectively improve network performance through\nadvanced reasoning, multimodal data processing, and autonomous decision making.\nThereafter, we demonstrate the practical applicability and benefits of\nWirelessAgent for network slicing management. The experimental results show\nthat WirelessAgent is capable of accurately understanding user intent,\neffectively allocating slice resources, and consistently maintaining optimal\nperformance."
                },
                "authors": [
                    {
                        "name": "Jingwen Tong"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Zijian Li"
                    },
                    {
                        "name": "Zehong Lin"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03156v2",
                "updated": "2024-09-12T11:28:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    11,
                    28,
                    7,
                    3,
                    256,
                    0
                ],
                "published": "2024-08-06T12:55:17Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    12,
                    55,
                    17,
                    1,
                    219,
                    0
                ],
                "title": "Iterative CT Reconstruction via Latent Variable Optimization of Shallow\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative CT Reconstruction via Latent Variable Optimization of Shallow\n  Diffusion Models"
                },
                "summary": "Image-generative artificial intelligence (AI) has garnered significant\nattention in recent years. In particular, the diffusion model, a core component\nof generative AI, produces high-quality images with rich diversity. In this\nstudy, we proposed a novel computed tomography (CT) reconstruction method by\ncombining the denoising diffusion probabilistic model with iterative CT\nreconstruction. In sharp contrast to previous studies, we optimized the\nfidelity loss of CT reconstruction with respect to the latent variable of the\ndiffusion model, instead of the image and model parameters. To suppress the\nchanges in anatomical structures produced by the diffusion model, we shallowed\nthe diffusion and reverse processes and fixed a set of added noises in the\nreverse process to make it deterministic during the inference. We demonstrated\nthe effectiveness of the proposed method through the sparse-projection CT\nreconstruction of 1/10 projection data. Despite the simplicity of the\nimplementation, the proposed method has the potential to reconstruct\nhigh-quality images while preserving the patient's anatomical structures and\nwas found to outperform existing methods, including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as the structural similarity index and peak\nsignal-to-noise ratio. We also explored further sparse-projection CT\nreconstruction using 1/20 projection data with the same trained diffusion\nmodel. As the number of iterations increased, the image quality improved\ncomparable to that of 1/10 sparse-projection CT reconstruction. In principle,\nthis method can be widely applied not only to CT but also to other imaging\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image-generative artificial intelligence (AI) has garnered significant\nattention in recent years. In particular, the diffusion model, a core component\nof generative AI, produces high-quality images with rich diversity. In this\nstudy, we proposed a novel computed tomography (CT) reconstruction method by\ncombining the denoising diffusion probabilistic model with iterative CT\nreconstruction. In sharp contrast to previous studies, we optimized the\nfidelity loss of CT reconstruction with respect to the latent variable of the\ndiffusion model, instead of the image and model parameters. To suppress the\nchanges in anatomical structures produced by the diffusion model, we shallowed\nthe diffusion and reverse processes and fixed a set of added noises in the\nreverse process to make it deterministic during the inference. We demonstrated\nthe effectiveness of the proposed method through the sparse-projection CT\nreconstruction of 1/10 projection data. Despite the simplicity of the\nimplementation, the proposed method has the potential to reconstruct\nhigh-quality images while preserving the patient's anatomical structures and\nwas found to outperform existing methods, including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as the structural similarity index and peak\nsignal-to-noise ratio. We also explored further sparse-projection CT\nreconstruction using 1/20 projection data with the same trained diffusion\nmodel. As the number of iterations increased, the image quality improved\ncomparable to that of 1/10 sparse-projection CT reconstruction. In principle,\nthis method can be widely applied not only to CT but also to other imaging\nmodalities."
                },
                "authors": [
                    {
                        "name": "Sho Ozaki"
                    },
                    {
                        "name": "Shizuo Kaji"
                    },
                    {
                        "name": "Toshikazu Imae"
                    },
                    {
                        "name": "Kanabu Nawa"
                    },
                    {
                        "name": "Hideomi Yamashita"
                    },
                    {
                        "name": "Keiichi Nakagawa"
                    }
                ],
                "author_detail": {
                    "name": "Keiichi Nakagawa"
                },
                "author": "Keiichi Nakagawa",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07947v1",
                "updated": "2024-09-12T11:18:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    11,
                    18,
                    0,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T11:18:00Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    11,
                    18,
                    0,
                    3,
                    256,
                    0
                ],
                "title": "Data-efficient multi-fidelity training for high-fidelity machine\n  learning interatomic potentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-efficient multi-fidelity training for high-fidelity machine\n  learning interatomic potentials"
                },
                "summary": "Machine learning interatomic potentials (MLIPs) are used to estimate\npotential energy surfaces (PES) from ab initio calculations, providing near\nquantum-level accuracy with reduced computational costs. However, the high cost\nof assembling high-fidelity databases hampers the application of MLIPs to\nsystems that require high chemical accuracy. Utilizing an equivariant graph\nneural network, we present an MLIP framework that trains on multi-fidelity\ndatabases simultaneously. This approach enables the accurate learning of\nhigh-fidelity PES with minimal high-fidelity data. We test this framework on\nthe Li$_6$PS$_5$Cl and In$_x$Ga$_{1-x}$N systems. The computational results\nindicate that geometric and compositional spaces not covered by the\nhigh-fidelity meta-gradient generalized approximation (meta-GGA) database can\nbe effectively inferred from low-fidelity GGA data, thus enhancing accuracy and\nmolecular dynamics stability. We also develop a general-purpose MLIP that\nutilizes both GGA and meta-GGA data from the Materials Project, significantly\nenhancing MLIP performance for high-accuracy tasks such as predicting energies\nabove hull for crystals in general. Furthermore, we demonstrate that the\npresent multi-fidelity learning is more effective than transfer learning or\n$\\Delta$-learning an d that it can also be applied to learn higher-fidelity up\nto the coupled-cluster level. We believe this methodology holds promise for\ncreating highly accurate bespoke or universal MLIPs by effectively expanding\nthe high-fidelity dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning interatomic potentials (MLIPs) are used to estimate\npotential energy surfaces (PES) from ab initio calculations, providing near\nquantum-level accuracy with reduced computational costs. However, the high cost\nof assembling high-fidelity databases hampers the application of MLIPs to\nsystems that require high chemical accuracy. Utilizing an equivariant graph\nneural network, we present an MLIP framework that trains on multi-fidelity\ndatabases simultaneously. This approach enables the accurate learning of\nhigh-fidelity PES with minimal high-fidelity data. We test this framework on\nthe Li$_6$PS$_5$Cl and In$_x$Ga$_{1-x}$N systems. The computational results\nindicate that geometric and compositional spaces not covered by the\nhigh-fidelity meta-gradient generalized approximation (meta-GGA) database can\nbe effectively inferred from low-fidelity GGA data, thus enhancing accuracy and\nmolecular dynamics stability. We also develop a general-purpose MLIP that\nutilizes both GGA and meta-GGA data from the Materials Project, significantly\nenhancing MLIP performance for high-accuracy tasks such as predicting energies\nabove hull for crystals in general. Furthermore, we demonstrate that the\npresent multi-fidelity learning is more effective than transfer learning or\n$\\Delta$-learning an d that it can also be applied to learn higher-fidelity up\nto the coupled-cluster level. We believe this methodology holds promise for\ncreating highly accurate bespoke or universal MLIPs by effectively expanding\nthe high-fidelity dataset."
                },
                "authors": [
                    {
                        "name": "Jaesun Kim"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaehoon Kim"
                    },
                    {
                        "name": "Jiho Lee"
                    },
                    {
                        "name": "Yutack Park"
                    },
                    {
                        "name": "Youngho Kang"
                    },
                    {
                        "name": "Seungwu Han"
                    }
                ],
                "author_detail": {
                    "name": "Seungwu Han"
                },
                "author": "Seungwu Han",
                "arxiv_comment": "17 pages, 4 figures, 1 tables, Supplementary information included as\n  ancillary file (+16 pages)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07946v1",
                "updated": "2024-09-12T11:14:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    11,
                    14,
                    25,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T11:14:25Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    11,
                    14,
                    25,
                    3,
                    256,
                    0
                ],
                "title": "Collaborative Automatic Modulation Classification via Deep Edge\n  Inference for Hierarchical Cognitive Radio Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Automatic Modulation Classification via Deep Edge\n  Inference for Hierarchical Cognitive Radio Networks"
                },
                "summary": "In hierarchical cognitive radio networks, edge or cloud servers utilize the\ndata collected by edge devices for modulation classification, which, however,\nis faced with problems of the transmission overhead, data privacy, and\ncomputation load. In this article, an edge learning (EL) based framework\njointly mobilizing the edge device and the edge server for intelligent\nco-inference is proposed to realize the collaborative automatic modulation\nclassification (C-AMC) between them. A spectrum semantic compression neural\nnetwork (SSCNet) with the lightweight structure is designed for the edge device\nto compress the collected raw data into a compact semantic message that is then\nsent to the edge server via the wireless channel. On the edge server side, a\nmodulation classification neural network (MCNet) combining bidirectional long\nshort-term memory (Bi?LSTM) and multi-head attention layers is elaborated to\ndeter?mine the modulation type from the noisy semantic message. By leveraging\nthe computation resources of both the edge device and the edge server, high\ntransmission overhead and risks of data privacy leakage are avoided. The\nsimulation results verify the effectiveness of the proposed C-AMC framework,\nsignificantly reducing the model size and computational complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In hierarchical cognitive radio networks, edge or cloud servers utilize the\ndata collected by edge devices for modulation classification, which, however,\nis faced with problems of the transmission overhead, data privacy, and\ncomputation load. In this article, an edge learning (EL) based framework\njointly mobilizing the edge device and the edge server for intelligent\nco-inference is proposed to realize the collaborative automatic modulation\nclassification (C-AMC) between them. A spectrum semantic compression neural\nnetwork (SSCNet) with the lightweight structure is designed for the edge device\nto compress the collected raw data into a compact semantic message that is then\nsent to the edge server via the wireless channel. On the edge server side, a\nmodulation classification neural network (MCNet) combining bidirectional long\nshort-term memory (Bi?LSTM) and multi-head attention layers is elaborated to\ndeter?mine the modulation type from the noisy semantic message. By leveraging\nthe computation resources of both the edge device and the edge server, high\ntransmission overhead and risks of data privacy leakage are avoided. The\nsimulation results verify the effectiveness of the proposed C-AMC framework,\nsignificantly reducing the model size and computational complexity."
                },
                "authors": [
                    {
                        "name": "Chaowei He"
                    },
                    {
                        "name": "Peihao Dong"
                    },
                    {
                        "name": "Fuhui Zhou"
                    },
                    {
                        "name": "Qihui Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qihui Wu"
                },
                "author": "Qihui Wu",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2407.20772",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11548v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11548v3",
                "updated": "2024-09-12T10:48:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    48,
                    30,
                    3,
                    256,
                    0
                ],
                "published": "2024-06-17T13:44:53Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    13,
                    44,
                    53,
                    0,
                    169,
                    0
                ],
                "title": "AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic\n  Manipulation"
                },
                "summary": "The ability to reflect on and correct failures is crucial for robotic systems\nto interact stably with real-life objects.Observing the generalization and\nreasoning capabilities of Multimodal Large Language Models (MLLMs), previous\napproaches have aimed to utilize these models to enhance robotic systems\naccordingly.However, these methods typically focus on high-level planning\ncorrections using an additional MLLM, with limited utilization of failed\nsamples to correct low-level contact poses. To address this gap, we propose an\nAutonomous Interactive Correction (AIC) MLLM, which makes use of previous\nlow-level interaction experiences to correct SE(3) pose predictions.\nSpecifically, AIC MLLM is initially fine-tuned to acquire both pose prediction\nand feedback prompt comprehension abilities.We carefully design two types of\nprompt instructions through interactions with objects: 1) visual masks to\nhighlight unmovable parts for position correction, and 2)textual descriptions\nto indicate potential directions for rotation correction.During inference, a\nFeedback Information Extraction module is introduced to recognize the failure\ncause, allowing AIC MLLM to adaptively correct the pose prediction using the\ncorresponding prompts. To further enhance manipulation stability, we devise a\nTest Time Adaptation strategy that enables AIC MLLM to better adapt to the\ncurrent scene configuration.Finally, extensive experiments are conducted in\nboth simulated and real-world environments to evaluate the proposed method. The\nresults demonstrate that our AIC MLLM can efficiently correct failure samples\nby leveraging interaction experience prompts.Real-world demonstration can be\nfound at https://sites.google.com/view/aic-mllm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to reflect on and correct failures is crucial for robotic systems\nto interact stably with real-life objects.Observing the generalization and\nreasoning capabilities of Multimodal Large Language Models (MLLMs), previous\napproaches have aimed to utilize these models to enhance robotic systems\naccordingly.However, these methods typically focus on high-level planning\ncorrections using an additional MLLM, with limited utilization of failed\nsamples to correct low-level contact poses. To address this gap, we propose an\nAutonomous Interactive Correction (AIC) MLLM, which makes use of previous\nlow-level interaction experiences to correct SE(3) pose predictions.\nSpecifically, AIC MLLM is initially fine-tuned to acquire both pose prediction\nand feedback prompt comprehension abilities.We carefully design two types of\nprompt instructions through interactions with objects: 1) visual masks to\nhighlight unmovable parts for position correction, and 2)textual descriptions\nto indicate potential directions for rotation correction.During inference, a\nFeedback Information Extraction module is introduced to recognize the failure\ncause, allowing AIC MLLM to adaptively correct the pose prediction using the\ncorresponding prompts. To further enhance manipulation stability, we devise a\nTest Time Adaptation strategy that enables AIC MLLM to better adapt to the\ncurrent scene configuration.Finally, extensive experiments are conducted in\nboth simulated and real-world environments to evaluate the proposed method. The\nresults demonstrate that our AIC MLLM can efficiently correct failure samples\nby leveraging interaction experience prompts.Real-world demonstration can be\nfound at https://sites.google.com/view/aic-mllm"
                },
                "authors": [
                    {
                        "name": "Chuyan Xiong"
                    },
                    {
                        "name": "Chengyu Shen"
                    },
                    {
                        "name": "Xiaoqi Li"
                    },
                    {
                        "name": "Kaichen Zhou"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Ruiping Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11548v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11548v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07908v1",
                "updated": "2024-09-12T10:24:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    24,
                    1,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T10:24:01Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    24,
                    1,
                    3,
                    256,
                    0
                ],
                "title": "Parameter constraints for accreting millisecond pulsars with synthetic\n  NICER data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter constraints for accreting millisecond pulsars with synthetic\n  NICER data"
                },
                "summary": "Pulse profile modelling (PPM) is a technique for inferring mass, radius and\nhotspot properties of millisecond pulsars. PPM is now regularly used for\nanalysis of rotation-powered millisecond pulsars (RMPs) with data from the\nNeutron Star Interior Composition ExploreR (NICER). Extending PPM to accreting\nmillisecond pulsars (AMPs) is attractive, because they are a different source\nclass featuring bright X-ray radiation from hotspots powered by accretion. In\nthis paper, we present a modification of one of the PPM codes, X-PSI, so that\nit can be used for AMPs. In particular, we implement a model of an accretion\ndisc and atmosphere model appropriate for the hotspots of AMPs, and improve the\noverall computational efficiency. We then test parameter recovery with\nsynthetic NICER data in two scenarios with reasonable parameters for AMPs. We\nfind in the first scenario, where the hotspot is large, that we are able to\ntightly and accurately constrain all parameters including mass and radius. In\nthe second scenario, which is a high inclination system with a smaller hotspot,\nwe find degeneracy between a subset of model parameters and a slight bias in\nthe inferred mass and radius. This analysis of synthetic data lays the ground\nwork for future analysis of AMPs with NICER data. Such an analysis could be\ncomplemented by future (joint) analysis of polarization data from the Imaging\nX-ray Polarimetry Explorer (IXPE).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pulse profile modelling (PPM) is a technique for inferring mass, radius and\nhotspot properties of millisecond pulsars. PPM is now regularly used for\nanalysis of rotation-powered millisecond pulsars (RMPs) with data from the\nNeutron Star Interior Composition ExploreR (NICER). Extending PPM to accreting\nmillisecond pulsars (AMPs) is attractive, because they are a different source\nclass featuring bright X-ray radiation from hotspots powered by accretion. In\nthis paper, we present a modification of one of the PPM codes, X-PSI, so that\nit can be used for AMPs. In particular, we implement a model of an accretion\ndisc and atmosphere model appropriate for the hotspots of AMPs, and improve the\noverall computational efficiency. We then test parameter recovery with\nsynthetic NICER data in two scenarios with reasonable parameters for AMPs. We\nfind in the first scenario, where the hotspot is large, that we are able to\ntightly and accurately constrain all parameters including mass and radius. In\nthe second scenario, which is a high inclination system with a smaller hotspot,\nwe find degeneracy between a subset of model parameters and a slight bias in\nthe inferred mass and radius. This analysis of synthetic data lays the ground\nwork for future analysis of AMPs with NICER data. Such an analysis could be\ncomplemented by future (joint) analysis of polarization data from the Imaging\nX-ray Polarimetry Explorer (IXPE)."
                },
                "authors": [
                    {
                        "name": "Bas Dorsman"
                    },
                    {
                        "name": "Tuomo Salmi"
                    },
                    {
                        "name": "Anna L. Watts"
                    },
                    {
                        "name": "Mason Ng"
                    },
                    {
                        "name": "Satish Kamath"
                    },
                    {
                        "name": "Anna Bobrikova"
                    },
                    {
                        "name": "Juri Poutanen"
                    },
                    {
                        "name": "Vladislav Loktev"
                    },
                    {
                        "name": "Yves Kini"
                    },
                    {
                        "name": "Devarshi Choudhury"
                    },
                    {
                        "name": "Serena Vinciguerra"
                    },
                    {
                        "name": "Slavko Bogdanov"
                    },
                    {
                        "name": "Deepto Chakrabarty"
                    }
                ],
                "author_detail": {
                    "name": "Deepto Chakrabarty"
                },
                "author": "Deepto Chakrabarty",
                "arxiv_comment": "14 pages, 5 figures, 3 tables. Paper submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.10614v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.10614v3",
                "updated": "2024-09-12T10:20:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    20,
                    43,
                    3,
                    256,
                    0
                ],
                "published": "2023-06-18T18:38:10Z",
                "published_parsed": [
                    2023,
                    6,
                    18,
                    18,
                    38,
                    10,
                    6,
                    169,
                    0
                ],
                "title": "Identifiable causal inference with noisy treatment and no side\n  information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifiable causal inference with noisy treatment and no side\n  information"
                },
                "summary": "In some causal inference scenarios, the treatment variable is measured\ninaccurately, for instance in epidemiology or econometrics. Failure to correct\nfor the effect of this measurement error can lead to biased causal effect\nestimates. Previous research has not studied methods that address this issue\nfrom a causal viewpoint while allowing for complex nonlinear dependencies and\nwithout assuming access to side information. For such a scenario, this study\nproposes a model that assumes a continuous treatment variable that is\ninaccurately measured. Building on existing results for measurement error\nmodels, we prove that our model's causal effect estimates are identifiable,\neven without side information and knowledge of the measurement error variance.\nOur method relies on a deep latent variable model in which Gaussian\nconditionals are parameterized by neural networks, and we develop an amortized\nimportance-weighted variational objective for training the model. Empirical\nresults demonstrate the method's good performance with unknown measurement\nerror. More broadly, our work extends the range of applications in which\nreliable causal inference can be conducted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In some causal inference scenarios, the treatment variable is measured\ninaccurately, for instance in epidemiology or econometrics. Failure to correct\nfor the effect of this measurement error can lead to biased causal effect\nestimates. Previous research has not studied methods that address this issue\nfrom a causal viewpoint while allowing for complex nonlinear dependencies and\nwithout assuming access to side information. For such a scenario, this study\nproposes a model that assumes a continuous treatment variable that is\ninaccurately measured. Building on existing results for measurement error\nmodels, we prove that our model's causal effect estimates are identifiable,\neven without side information and knowledge of the measurement error variance.\nOur method relies on a deep latent variable model in which Gaussian\nconditionals are parameterized by neural networks, and we develop an amortized\nimportance-weighted variational objective for training the model. Empirical\nresults demonstrate the method's good performance with unknown measurement\nerror. More broadly, our work extends the range of applications in which\nreliable causal inference can be conducted."
                },
                "authors": [
                    {
                        "name": "Antti Pöllänen"
                    },
                    {
                        "name": "Pekka Marttinen"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Marttinen"
                },
                "author": "Pekka Marttinen",
                "arxiv_comment": "20 pages, 10 figures. Changes consist of polishing the previous\n  version. The experiments and results remain the same",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.10614v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.10614v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T37",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07902v1",
                "updated": "2024-09-12T10:12:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    12,
                    43,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T10:12:43Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    12,
                    43,
                    3,
                    256,
                    0
                ],
                "title": "Conformal Distributed Remote Inference in Sensor Networks Under\n  Reliability and Communication Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Distributed Remote Inference in Sensor Networks Under\n  Reliability and Communication Constraints"
                },
                "summary": "This paper presents communication-constrained distributed conformal risk\ncontrol (CD-CRC) framework, a novel decision-making framework for sensor\nnetworks under communication constraints. Targeting multi-label classification\nproblems, such as segmentation, CD-CRC dynamically adjusts local and global\nthresholds used to identify significant labels with the goal of ensuring a\ntarget false negative rate (FNR), while adhering to communication capacity\nlimits. CD-CRC builds on online exponentiated gradient descent to estimate the\nrelative quality of the observations of different sensors, and on online\nconformal risk control (CRC) as a mechanism to control local and global\nthresholds. CD-CRC is proved to offer deterministic worst-case performance\nguarantees in terms of FNR and communication overhead, while the regret\nperformance in terms of false positive rate (FPR) is characterized as a\nfunction of the key hyperparameters. Simulation results highlight the\neffectiveness of CD-CRC, particularly in communication resource-constrained\nenvironments, making it a valuable tool for enhancing the performance and\nreliability of distributed sensor networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents communication-constrained distributed conformal risk\ncontrol (CD-CRC) framework, a novel decision-making framework for sensor\nnetworks under communication constraints. Targeting multi-label classification\nproblems, such as segmentation, CD-CRC dynamically adjusts local and global\nthresholds used to identify significant labels with the goal of ensuring a\ntarget false negative rate (FNR), while adhering to communication capacity\nlimits. CD-CRC builds on online exponentiated gradient descent to estimate the\nrelative quality of the observations of different sensors, and on online\nconformal risk control (CRC) as a mechanism to control local and global\nthresholds. CD-CRC is proved to offer deterministic worst-case performance\nguarantees in terms of FNR and communication overhead, while the regret\nperformance in terms of false positive rate (FPR) is characterized as a\nfunction of the key hyperparameters. Simulation results highlight the\neffectiveness of CD-CRC, particularly in communication resource-constrained\nenvironments, making it a valuable tool for enhancing the performance and\nreliability of distributed sensor networks."
                },
                "authors": [
                    {
                        "name": "Meiyi Zhu"
                    },
                    {
                        "name": "Matteo Zecchin"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Caili Guo"
                    },
                    {
                        "name": "Chunyan Feng"
                    },
                    {
                        "name": "Petar Popovski"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19297v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19297v2",
                "updated": "2024-09-12T10:04:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    4,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-05-29T17:25:59Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    17,
                    25,
                    59,
                    2,
                    150,
                    0
                ],
                "title": "Genuine Retrieval of the AGN Host Stellar Population (GRAHSP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genuine Retrieval of the AGN Host Stellar Population (GRAHSP)"
                },
                "summary": "The assembly and co-evolution of supermassive black holes (SMBH) and their\nhost galaxy stellar population is a key open questions in galaxy evolution.\nStellar mass ($M_\\star$) and star formation rate (SFR), are inferred by\nmodeling the spectral energy distribution (SED). For galaxies triggering SMBH\nactivity, the active galactic nucleus (AGN) contaminates the light at all\nwavelengths, hampering the inference of galaxy parameters. Incomplete AGN\ntemplates can lead to systematic overestimates of the stellar mass, biasing our\nunderstanding of AGN-galaxy co-evolution. This challenge has gained further\nimpetus with the advent of sensitive wide-area surveys with millions of\nluminous AGN, including by eROSITA, Euclid and LSST. We aim to estimate the\naccuracy and bias of AGN host galaxy parameters and improve upon existing\ntechniques. This work makes two contributions: 1) a new SED fitting code,\nGRAHSP, with a flexible, empirically motivated AGN model including a power law\ncontinuum emission lines, a FeII forest and a flexible infrared torus. We\nverify that our model reproduces published X-ray to infrared SEDs of AGN to\nbetter than 20\\% accuracy. A fully Bayesian fit with nested sampling includes\nuncertainties in the model and the data, making the inference highly robust. 2)\nwe created a benchmark photometric dataset where pure quasars are merged with\nnon-AGN pure galaxies into a hybrid (Chimera) object but with known galaxy and\nAGN properties. Comparing the true and retrieved $M_\\star$, SFR and AGN\nluminosities shows that previous codes systematically over-estimate $M_\\star$\nand SFR by 0.5 dex with a wide scatter of 0.7 dex, at AGN luminosities above\n10^44 erg/s. In contrast, GRAHSP shows no bias on $M_\\star$ and SFR. GRAHSP\nalso estimates more realistic uncertainties. GRAHSP enables characterization of\nthe environmental conditions conducive to black hole growth. (abridged)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The assembly and co-evolution of supermassive black holes (SMBH) and their\nhost galaxy stellar population is a key open questions in galaxy evolution.\nStellar mass ($M_\\star$) and star formation rate (SFR), are inferred by\nmodeling the spectral energy distribution (SED). For galaxies triggering SMBH\nactivity, the active galactic nucleus (AGN) contaminates the light at all\nwavelengths, hampering the inference of galaxy parameters. Incomplete AGN\ntemplates can lead to systematic overestimates of the stellar mass, biasing our\nunderstanding of AGN-galaxy co-evolution. This challenge has gained further\nimpetus with the advent of sensitive wide-area surveys with millions of\nluminous AGN, including by eROSITA, Euclid and LSST. We aim to estimate the\naccuracy and bias of AGN host galaxy parameters and improve upon existing\ntechniques. This work makes two contributions: 1) a new SED fitting code,\nGRAHSP, with a flexible, empirically motivated AGN model including a power law\ncontinuum emission lines, a FeII forest and a flexible infrared torus. We\nverify that our model reproduces published X-ray to infrared SEDs of AGN to\nbetter than 20\\% accuracy. A fully Bayesian fit with nested sampling includes\nuncertainties in the model and the data, making the inference highly robust. 2)\nwe created a benchmark photometric dataset where pure quasars are merged with\nnon-AGN pure galaxies into a hybrid (Chimera) object but with known galaxy and\nAGN properties. Comparing the true and retrieved $M_\\star$, SFR and AGN\nluminosities shows that previous codes systematically over-estimate $M_\\star$\nand SFR by 0.5 dex with a wide scatter of 0.7 dex, at AGN luminosities above\n10^44 erg/s. In contrast, GRAHSP shows no bias on $M_\\star$ and SFR. GRAHSP\nalso estimates more realistic uncertainties. GRAHSP enables characterization of\nthe environmental conditions conducive to black hole growth. (abridged)"
                },
                "authors": [
                    {
                        "name": "Johannes Buchner"
                    },
                    {
                        "name": "Hattie Starck"
                    },
                    {
                        "name": "Mara Salvato"
                    },
                    {
                        "name": "Hagai Netzer"
                    },
                    {
                        "name": "Zsofi Igo"
                    },
                    {
                        "name": "Brivael Laloux"
                    },
                    {
                        "name": "Antonis Georgakakis"
                    },
                    {
                        "name": "Isabelle Gauger"
                    },
                    {
                        "name": "Anna Olechowska"
                    },
                    {
                        "name": "Nicolas Lopez"
                    },
                    {
                        "name": "Suraj D Shankar"
                    },
                    {
                        "name": "Junyao Li"
                    },
                    {
                        "name": "Kirpal Nandra"
                    },
                    {
                        "name": "Andrea Merloni"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Merloni"
                },
                "author": "Andrea Merloni",
                "arxiv_comment": "accepted in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19297v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19297v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07871v1",
                "updated": "2024-09-12T09:28:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    9,
                    28,
                    34,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T09:28:34Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    9,
                    28,
                    34,
                    3,
                    256,
                    0
                ],
                "title": "Objection Overruled! Lay People can Distinguish Large Language Models\n  from Lawyers, but still Favour Advice from an LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objection Overruled! Lay People can Distinguish Large Language Models\n  from Lawyers, but still Favour Advice from an LLM"
                },
                "summary": "Large Language Models (LLMs) are seemingly infiltrating every domain, and the\nlegal context is no exception. In this paper, we present the results of three\nexperiments (total N=288) that investigated lay people's willingness to act\nupon, and their ability to discriminate between, LLM- and lawyer-generated\nlegal advice. In Experiment 1, participants judged their willingness to act on\nlegal advice when the source of the advice was either known or unknown. When\nthe advice source was unknown, participants indicated that they were\nsignificantly more willing to act on the LLM-generated advice. This result was\nreplicated in Experiment 2. Intriguingly, despite participants indicating\nhigher willingness to act on LLM-generated advice in Experiments 1 and 2,\nparticipants discriminated between the LLM- and lawyer-generated texts\nsignificantly above chance-level in Experiment 3. Lastly, we discuss potential\nexplanations and risks of our findings, limitations and future work, and the\nimportance of language complexity and real-world comparability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are seemingly infiltrating every domain, and the\nlegal context is no exception. In this paper, we present the results of three\nexperiments (total N=288) that investigated lay people's willingness to act\nupon, and their ability to discriminate between, LLM- and lawyer-generated\nlegal advice. In Experiment 1, participants judged their willingness to act on\nlegal advice when the source of the advice was either known or unknown. When\nthe advice source was unknown, participants indicated that they were\nsignificantly more willing to act on the LLM-generated advice. This result was\nreplicated in Experiment 2. Intriguingly, despite participants indicating\nhigher willingness to act on LLM-generated advice in Experiments 1 and 2,\nparticipants discriminated between the LLM- and lawyer-generated texts\nsignificantly above chance-level in Experiment 3. Lastly, we discuss potential\nexplanations and risks of our findings, limitations and future work, and the\nimportance of language complexity and real-world comparability."
                },
                "authors": [
                    {
                        "name": "Eike Schneiders"
                    },
                    {
                        "name": "Tina Seabrooke"
                    },
                    {
                        "name": "Joshua Krook"
                    },
                    {
                        "name": "Richard Hyde"
                    },
                    {
                        "name": "Natalie Leesakul"
                    },
                    {
                        "name": "Jeremie Clos"
                    },
                    {
                        "name": "Joel Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Joel Fischer"
                },
                "author": "Joel Fischer",
                "arxiv_comment": "13 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05181v2",
                "updated": "2024-09-12T09:08:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    9,
                    8,
                    56,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-08T18:37:08Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    18,
                    37,
                    8,
                    6,
                    252,
                    0
                ],
                "title": "Sliding-Window Thompson Sampling for Non-Stationary Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sliding-Window Thompson Sampling for Non-Stationary Settings"
                },
                "summary": "$\\textit{Restless Bandits}$ describe sequential decision-making problems in\nwhich the rewards evolve with time independently from the actions taken by the\npolicy-maker. It has been shown that classical Bandit algorithms fail when the\nunderlying environment is changing, making clear that in order to tackle more\nchallenging scenarios specifically crafted algorithms are needed. In this\npaper, extending and correcting the work by \\cite{trovo2020sliding}, we analyze\ntwo Thompson-Sampling inspired algorithms, namely $\\texttt{BETA-SWTS}$ and\n$\\texttt{$\\gamma$-SWGTS}$, introduced to face the additional complexity given\nby the non-stationary nature of the settings; in particular we derive a general\nformulation for the regret in $\\textit{any}$ arbitrary restless environment for\nboth Bernoulli and Subgaussian rewards, and, through the introduction of new\nquantities, we delve in what contribution lays the deeper foundations of the\nerror made by the algorithms. Finally, we infer from the general formulation\nthe regret for two of the most common non-stationary settings: the\n$\\textit{Abruptly Changing}$ and the $\\textit{Smoothly Changing}$ environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\textit{Restless Bandits}$ describe sequential decision-making problems in\nwhich the rewards evolve with time independently from the actions taken by the\npolicy-maker. It has been shown that classical Bandit algorithms fail when the\nunderlying environment is changing, making clear that in order to tackle more\nchallenging scenarios specifically crafted algorithms are needed. In this\npaper, extending and correcting the work by \\cite{trovo2020sliding}, we analyze\ntwo Thompson-Sampling inspired algorithms, namely $\\texttt{BETA-SWTS}$ and\n$\\texttt{$\\gamma$-SWGTS}$, introduced to face the additional complexity given\nby the non-stationary nature of the settings; in particular we derive a general\nformulation for the regret in $\\textit{any}$ arbitrary restless environment for\nboth Bernoulli and Subgaussian rewards, and, through the introduction of new\nquantities, we delve in what contribution lays the deeper foundations of the\nerror made by the algorithms. Finally, we infer from the general formulation\nthe regret for two of the most common non-stationary settings: the\n$\\textit{Abruptly Changing}$ and the $\\textit{Smoothly Changing}$ environments."
                },
                "authors": [
                    {
                        "name": "Marco Fiandri"
                    },
                    {
                        "name": "Alberto Maria Metelli"
                    },
                    {
                        "name": "Francesco Trovò"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Trovò"
                },
                "author": "Francesco Trovò",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07859v1",
                "updated": "2024-09-12T09:05:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    9,
                    5,
                    40,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T09:05:40Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    9,
                    5,
                    40,
                    3,
                    256,
                    0
                ],
                "title": "Bootstrap Adaptive Lasso Solution Path Unit Root Tests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrap Adaptive Lasso Solution Path Unit Root Tests"
                },
                "summary": "We propose sieve wild bootstrap analogues to the adaptive Lasso solution path\nunit root tests of Arnold and Reinschl\\\"ussel (2024) arXiv:2404.06205 to\nimprove finite sample properties and extend their applicability to a\ngeneralised framework, allowing for non-stationary volatility. Numerical\nevidence shows the bootstrap to improve the tests' precision for error\nprocesses that promote spurious rejections of the unit root null, depending on\nthe detrending procedure. The bootstrap mitigates finite-sample size\ndistortions and restores asymptotically valid inference when the data features\ntime-varying unconditional variance. We apply the bootstrap tests to real\nresidential property prices of the top six Eurozone economies and find evidence\nof stationarity to be period-specific, supporting the conjecture that\nexuberance in the housing market characterises the development of Euro-era\nresidential property prices in the recent past.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose sieve wild bootstrap analogues to the adaptive Lasso solution path\nunit root tests of Arnold and Reinschl\\\"ussel (2024) arXiv:2404.06205 to\nimprove finite sample properties and extend their applicability to a\ngeneralised framework, allowing for non-stationary volatility. Numerical\nevidence shows the bootstrap to improve the tests' precision for error\nprocesses that promote spurious rejections of the unit root null, depending on\nthe detrending procedure. The bootstrap mitigates finite-sample size\ndistortions and restores asymptotically valid inference when the data features\ntime-varying unconditional variance. We apply the bootstrap tests to real\nresidential property prices of the top six Eurozone economies and find evidence\nof stationarity to be period-specific, supporting the conjecture that\nexuberance in the housing market characterises the development of Euro-era\nresidential property prices in the recent past."
                },
                "authors": [
                    {
                        "name": "Martin C. Arnold"
                    },
                    {
                        "name": "Thilo Reinschlüssel"
                    }
                ],
                "author_detail": {
                    "name": "Thilo Reinschlüssel"
                },
                "author": "Thilo Reinschlüssel",
                "arxiv_comment": "30 pages, 1 figure (colour)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05344v2",
                "updated": "2024-09-12T08:49:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    8,
                    49,
                    20,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-09T06:02:17Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    2,
                    17,
                    0,
                    253,
                    0
                ],
                "title": "GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep\n  Reinforcement Learning"
                },
                "summary": "Robotic object packing has broad practical applications in the logistics and\nautomation industry, often formulated by researchers as the online 3D Bin\nPacking Problem (3D-BPP). However, existing DRL-based methods primarily focus\non enhancing performance in limited packing environments while neglecting the\nability to generalize across multiple environments characterized by different\nbin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin\nPacking approach via Transformer-based deep reinforcement learning (DRL).\nFirst, we design a Placement Generator module to yield finite subspaces as\nplacement candidates and the representation of the bin. Second, we propose a\nPacking Transformer, which fuses the features of the items and bin, to identify\nthe spatial correlation between the item to be packed and available sub-spaces\nwithin the bin. Coupling these two components enables GOPT's ability to perform\ninference on bins of varying dimensions. We conduct extensive experiments and\ndemonstrate that GOPT not only achieves superior performance against the\nbaselines, but also exhibits excellent generalization capabilities.\nFurthermore, the deployment with a robot showcases the practical applicability\nof our method in the real world. The source code will be publicly available at\nhttps://github.com/Xiong5Heng/GOPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic object packing has broad practical applications in the logistics and\nautomation industry, often formulated by researchers as the online 3D Bin\nPacking Problem (3D-BPP). However, existing DRL-based methods primarily focus\non enhancing performance in limited packing environments while neglecting the\nability to generalize across multiple environments characterized by different\nbin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin\nPacking approach via Transformer-based deep reinforcement learning (DRL).\nFirst, we design a Placement Generator module to yield finite subspaces as\nplacement candidates and the representation of the bin. Second, we propose a\nPacking Transformer, which fuses the features of the items and bin, to identify\nthe spatial correlation between the item to be packed and available sub-spaces\nwithin the bin. Coupling these two components enables GOPT's ability to perform\ninference on bins of varying dimensions. We conduct extensive experiments and\ndemonstrate that GOPT not only achieves superior performance against the\nbaselines, but also exhibits excellent generalization capabilities.\nFurthermore, the deployment with a robot showcases the practical applicability\nof our method in the real world. The source code will be publicly available at\nhttps://github.com/Xiong5Heng/GOPT."
                },
                "authors": [
                    {
                        "name": "Heng Xiong"
                    },
                    {
                        "name": "Changrong Guo"
                    },
                    {
                        "name": "Jian Peng"
                    },
                    {
                        "name": "Kai Ding"
                    },
                    {
                        "name": "Wenjie Chen"
                    },
                    {
                        "name": "Xuchong Qiu"
                    },
                    {
                        "name": "Long Bai"
                    },
                    {
                        "name": "Jianfeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Xu"
                },
                "author": "Jianfeng Xu",
                "arxiv_comment": "8 pages, 6 figures. This paper has been accepted by IEEE Robotics and\n  Automation Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07843v1",
                "updated": "2024-09-12T08:44:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    8,
                    44,
                    35,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T08:44:35Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    8,
                    44,
                    35,
                    3,
                    256,
                    0
                ],
                "title": "Real-time Multi-view Omnidirectional Depth Estimation System for Robots\n  and Autonomous Driving on Real Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Multi-view Omnidirectional Depth Estimation System for Robots\n  and Autonomous Driving on Real Scenes"
                },
                "summary": "Omnidirectional Depth Estimation has broad application prospects in fields\nsuch as robotic navigation and autonomous driving. In this paper, we propose a\nrobotic prototype system and corresponding algorithm designed to validate\nomnidirectional depth estimation for navigation and obstacle avoidance in\nreal-world scenarios for both robots and vehicles. The proposed HexaMODE system\ncaptures 360$^\\circ$ depth maps using six surrounding arranged fisheye cameras.\nWe introduce a combined spherical sweeping method and optimize the model\narchitecture for proposed RtHexa-OmniMVS algorithm to achieve real-time\nomnidirectional depth estimation. To ensure high accuracy, robustness, and\ngeneralization in real-world environments, we employ a teacher-student\nself-training strategy, utilizing large-scale unlabeled real-world data for\nmodel training. The proposed algorithm demonstrates high accuracy in various\ncomplex real-world scenarios, both indoors and outdoors, achieving an inference\nspeed of 15 fps on edge computing platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omnidirectional Depth Estimation has broad application prospects in fields\nsuch as robotic navigation and autonomous driving. In this paper, we propose a\nrobotic prototype system and corresponding algorithm designed to validate\nomnidirectional depth estimation for navigation and obstacle avoidance in\nreal-world scenarios for both robots and vehicles. The proposed HexaMODE system\ncaptures 360$^\\circ$ depth maps using six surrounding arranged fisheye cameras.\nWe introduce a combined spherical sweeping method and optimize the model\narchitecture for proposed RtHexa-OmniMVS algorithm to achieve real-time\nomnidirectional depth estimation. To ensure high accuracy, robustness, and\ngeneralization in real-world environments, we employ a teacher-student\nself-training strategy, utilizing large-scale unlabeled real-world data for\nmodel training. The proposed algorithm demonstrates high accuracy in various\ncomplex real-world scenarios, both indoors and outdoors, achieving an inference\nspeed of 15 fps on edge computing platforms."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Xiong Yang"
                    },
                    {
                        "name": "Chaofan Wu"
                    },
                    {
                        "name": "Jiaheng Li"
                    },
                    {
                        "name": "Pinzhi Wang"
                    },
                    {
                        "name": "Xuejiao Hu"
                    },
                    {
                        "name": "Sidan Du"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07808v3",
                "updated": "2024-09-12T08:38:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    8,
                    38,
                    39,
                    3,
                    256,
                    0
                ],
                "published": "2023-09-14T15:54:56Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    15,
                    54,
                    56,
                    3,
                    257,
                    0
                ],
                "title": "What Matters to Enhance Traffic Rule Compliance of Imitation Learning\n  for End-to-End Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Matters to Enhance Traffic Rule Compliance of Imitation Learning\n  for End-to-End Autonomous Driving"
                },
                "summary": "End-to-end autonomous driving, where the entire driving pipeline is replaced\nwith a single neural network, has recently gained research attention because of\nits simpler structure and faster inference time. Despite this appealing\napproach largely reducing the complexity in the driving pipeline, it also leads\nto safety issues because the trained policy is not always compliant with the\ntraffic rules. In this paper, we proposed P-CSG, a penalty-based imitation\nlearning approach with contrastive-based cross semantics generation sensor\nfusion technologies to increase the overall performance of end-to-end\nautonomous driving. In this method, we introduce three penalties - red light,\nstop sign, and curvature speed penalty to make the agent more sensitive to\ntraffic rules. The proposed cross semantics generation helps to align the\nshared information of different input modalities. We assessed our model's\nperformance using the CARLA Leaderboard - Town 05 Long Benchmark and Longest6\nBenchmark, achieving 8.5% and 2.0% driving score improvement compared to the\nbaselines. Furthermore, we conducted robustness evaluations against adversarial\nattacks like FGSM and Dot attacks, revealing a substantial increase in\nrobustness compared to other baseline models. More detailed information can be\nfound at https://hk-zh.github.io/p-csg-plus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end autonomous driving, where the entire driving pipeline is replaced\nwith a single neural network, has recently gained research attention because of\nits simpler structure and faster inference time. Despite this appealing\napproach largely reducing the complexity in the driving pipeline, it also leads\nto safety issues because the trained policy is not always compliant with the\ntraffic rules. In this paper, we proposed P-CSG, a penalty-based imitation\nlearning approach with contrastive-based cross semantics generation sensor\nfusion technologies to increase the overall performance of end-to-end\nautonomous driving. In this method, we introduce three penalties - red light,\nstop sign, and curvature speed penalty to make the agent more sensitive to\ntraffic rules. The proposed cross semantics generation helps to align the\nshared information of different input modalities. We assessed our model's\nperformance using the CARLA Leaderboard - Town 05 Long Benchmark and Longest6\nBenchmark, achieving 8.5% and 2.0% driving score improvement compared to the\nbaselines. Furthermore, we conducted robustness evaluations against adversarial\nattacks like FGSM and Dot attacks, revealing a substantial increase in\nrobustness compared to other baseline models. More detailed information can be\nfound at https://hk-zh.github.io/p-csg-plus."
                },
                "authors": [
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Wei Cao"
                    },
                    {
                        "name": "Aifen Sui"
                    },
                    {
                        "name": "Zhenshan Bing"
                    }
                ],
                "author_detail": {
                    "name": "Zhenshan Bing"
                },
                "author": "Zhenshan Bing",
                "arxiv_comment": "14 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13696v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13696v2",
                "updated": "2024-09-12T08:36:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    8,
                    36,
                    47,
                    3,
                    256,
                    0
                ],
                "published": "2024-07-18T17:00:23Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    17,
                    0,
                    23,
                    3,
                    200,
                    0
                ],
                "title": "Do These LLM Benchmarks Agree? Fixing Benchmark Evaluation with\n  BenchBench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do These LLM Benchmarks Agree? Fixing Benchmark Evaluation with\n  BenchBench"
                },
                "summary": "Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: github.com/IBM/BenchBench\n  Leaderboard: hf.co/spaces/IBM/BenchBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: github.com/IBM/BenchBench\n  Leaderboard: hf.co/spaces/IBM/BenchBench"
                },
                "authors": [
                    {
                        "name": "Yotam Perlitz"
                    },
                    {
                        "name": "Ariel Gera"
                    },
                    {
                        "name": "Ofir Arviv"
                    },
                    {
                        "name": "Asaf Yehudai"
                    },
                    {
                        "name": "Elron Bandel"
                    },
                    {
                        "name": "Eyal Shnarch"
                    },
                    {
                        "name": "Michal Shmueli-Scheuer"
                    },
                    {
                        "name": "Leshem Choshen"
                    }
                ],
                "author_detail": {
                    "name": "Leshem Choshen"
                },
                "author": "Leshem Choshen",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13696v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13696v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07829v1",
                "updated": "2024-09-12T08:25:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    8,
                    25,
                    33,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T08:25:33Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    8,
                    25,
                    33,
                    3,
                    256,
                    0
                ],
                "title": "Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs:\n  A Case Study in WeChat",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs:\n  A Case Study in WeChat"
                },
                "summary": "UI automation tests play a crucial role in ensuring the quality of mobile\napplications. Despite the growing popularity of machine learning techniques to\ngenerate these tests, they still face several challenges, such as the mismatch\nof UI elements. The recent advances in Large Language Models (LLMs) have\naddressed these issues by leveraging their semantic understanding capabilities.\nHowever, a significant gap remains in applying these models to industrial-level\napp testing, particularly in terms of cost optimization and knowledge\nlimitation. To address this, we introduce CAT to create cost-effective UI\nautomation tests for industry apps by combining machine learning and LLMs with\nbest practices. Given the task description, CAT employs Retrieval Augmented\nGeneration (RAG) to source examples of industrial app usage as the few-shot\nlearning context, assisting LLMs in generating the specific sequence of\nactions. CAT then employs machine learning techniques, with LLMs serving as a\ncomplementary optimizer, to map the target element on the UI screen. Our\nevaluations on the WeChat testing dataset demonstrate the CAT's performance and\ncost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming\nthe state-of-the-art. We have also integrated our approach into the real-world\nWeChat testing platform, demonstrating its usefulness in detecting 141 bugs and\nenhancing the developers' testing process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UI automation tests play a crucial role in ensuring the quality of mobile\napplications. Despite the growing popularity of machine learning techniques to\ngenerate these tests, they still face several challenges, such as the mismatch\nof UI elements. The recent advances in Large Language Models (LLMs) have\naddressed these issues by leveraging their semantic understanding capabilities.\nHowever, a significant gap remains in applying these models to industrial-level\napp testing, particularly in terms of cost optimization and knowledge\nlimitation. To address this, we introduce CAT to create cost-effective UI\nautomation tests for industry apps by combining machine learning and LLMs with\nbest practices. Given the task description, CAT employs Retrieval Augmented\nGeneration (RAG) to source examples of industrial app usage as the few-shot\nlearning context, assisting LLMs in generating the specific sequence of\nactions. CAT then employs machine learning techniques, with LLMs serving as a\ncomplementary optimizer, to map the target element on the UI screen. Our\nevaluations on the WeChat testing dataset demonstrate the CAT's performance and\ncost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming\nthe state-of-the-art. We have also integrated our approach into the real-world\nWeChat testing platform, demonstrating its usefulness in detecting 141 bugs and\nenhancing the developers' testing process."
                },
                "authors": [
                    {
                        "name": "Sidong Feng"
                    },
                    {
                        "name": "Haochuan Lu"
                    },
                    {
                        "name": "Jianqin Jiang"
                    },
                    {
                        "name": "Ting Xiong"
                    },
                    {
                        "name": "Likun Huang"
                    },
                    {
                        "name": "Yinglin Liang"
                    },
                    {
                        "name": "Xiaoqin Li"
                    },
                    {
                        "name": "Yuetang Deng"
                    },
                    {
                        "name": "Aldeida Aleti"
                    }
                ],
                "author_detail": {
                    "name": "Aldeida Aleti"
                },
                "author": "Aldeida Aleti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.11741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.11741v2",
                "updated": "2024-09-12T07:53:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    7,
                    53,
                    17,
                    3,
                    256,
                    0
                ],
                "published": "2023-10-18T06:54:20Z",
                "published_parsed": [
                    2023,
                    10,
                    18,
                    6,
                    54,
                    20,
                    2,
                    291,
                    0
                ],
                "title": "Graph of Graphs: From Nodes to Supernodes in Graphical Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph of Graphs: From Nodes to Supernodes in Graphical Models"
                },
                "summary": "High-dimensional data analysis typically focuses on low-dimensional\nstructure, often to aid interpretation and computational efficiency. Graphical\nmodels provide a powerful methodology for learning the conditional independence\nstructure in multivariate data by representing variables as nodes and\ndependencies as edges. Inference is often focused on individual edges in the\nlatent graph. Nonetheless, there is increasing interest in determining more\ncomplex structures, such as communities of nodes, for multiple reasons,\nincluding more effective information retrieval and better interpretability. In\nthis work, we propose a hierarchical graphical model where we first cluster\nnodes and then, at the higher level, investigate the relationships among groups\nof nodes. Specifically, nodes are partitioned into supernodes with a\ndata-coherent size-biased tessellation prior which combines ideas from Bayesian\nnonparametrics and Voronoi tessellations. This construct also allows accounting\nfor the dependence of nodes within supernodes. At the higher level, dependence\nstructure among supernodes is modeled through a Gaussian graphical model, where\nthe focus of inference is on superedges. We provide theoretical justification\nfor our modeling choices. We design tailored Markov chain Monte Carlo schemes,\nwhich also enable parallel computations. We demonstrate the effectiveness of\nour approach for large-scale structure learning in simulations and a\ntranscriptomics application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional data analysis typically focuses on low-dimensional\nstructure, often to aid interpretation and computational efficiency. Graphical\nmodels provide a powerful methodology for learning the conditional independence\nstructure in multivariate data by representing variables as nodes and\ndependencies as edges. Inference is often focused on individual edges in the\nlatent graph. Nonetheless, there is increasing interest in determining more\ncomplex structures, such as communities of nodes, for multiple reasons,\nincluding more effective information retrieval and better interpretability. In\nthis work, we propose a hierarchical graphical model where we first cluster\nnodes and then, at the higher level, investigate the relationships among groups\nof nodes. Specifically, nodes are partitioned into supernodes with a\ndata-coherent size-biased tessellation prior which combines ideas from Bayesian\nnonparametrics and Voronoi tessellations. This construct also allows accounting\nfor the dependence of nodes within supernodes. At the higher level, dependence\nstructure among supernodes is modeled through a Gaussian graphical model, where\nthe focus of inference is on superedges. We provide theoretical justification\nfor our modeling choices. We design tailored Markov chain Monte Carlo schemes,\nwhich also enable parallel computations. We demonstrate the effectiveness of\nour approach for large-scale structure learning in simulations and a\ntranscriptomics application."
                },
                "authors": [
                    {
                        "name": "Maria De Iorio"
                    },
                    {
                        "name": "Willem van den Boom"
                    },
                    {
                        "name": "Alexandros Beskos"
                    },
                    {
                        "name": "Ajay Jasra"
                    },
                    {
                        "name": "Andrea Cremaschi"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Cremaschi"
                },
                "author": "Andrea Cremaschi",
                "arxiv_comment": "76 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.11741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.11741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07813v1",
                "updated": "2024-09-12T07:46:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    7,
                    46,
                    58,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T07:46:58Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    7,
                    46,
                    58,
                    3,
                    256,
                    0
                ],
                "title": "What is YOLOv9: An In-Depth Exploration of the Internal Features of the\n  Next-Generation Object Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is YOLOv9: An In-Depth Exploration of the Internal Features of the\n  Next-Generation Object Detector"
                },
                "summary": "This study provides a comprehensive analysis of the YOLOv9 object detection\nmodel, focusing on its architectural innovations, training methodologies, and\nperformance improvements over its predecessors. Key advancements, such as the\nGeneralized Efficient Layer Aggregation Network GELAN and Programmable Gradient\nInformation PGI, significantly enhance feature extraction and gradient flow,\nleading to improved accuracy and efficiency. By incorporating Depthwise\nConvolutions and the lightweight C3Ghost architecture, YOLOv9 reduces\ncomputational complexity while maintaining high precision. Benchmark tests on\nMicrosoft COCO demonstrate its superior mean Average Precision mAP and faster\ninference times, outperforming YOLOv8 across multiple metrics. The model\nversatility is highlighted by its seamless deployment across various hardware\nplatforms, from edge devices to high performance GPUs, with built in support\nfor PyTorch and TensorRT integration. This paper provides the first in depth\nexploration of YOLOv9s internal features and their real world applicability,\nestablishing it as a state of the art solution for real time object detection\nacross industries, from IoT devices to large scale industrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study provides a comprehensive analysis of the YOLOv9 object detection\nmodel, focusing on its architectural innovations, training methodologies, and\nperformance improvements over its predecessors. Key advancements, such as the\nGeneralized Efficient Layer Aggregation Network GELAN and Programmable Gradient\nInformation PGI, significantly enhance feature extraction and gradient flow,\nleading to improved accuracy and efficiency. By incorporating Depthwise\nConvolutions and the lightweight C3Ghost architecture, YOLOv9 reduces\ncomputational complexity while maintaining high precision. Benchmark tests on\nMicrosoft COCO demonstrate its superior mean Average Precision mAP and faster\ninference times, outperforming YOLOv8 across multiple metrics. The model\nversatility is highlighted by its seamless deployment across various hardware\nplatforms, from edge devices to high performance GPUs, with built in support\nfor PyTorch and TensorRT integration. This paper provides the first in depth\nexploration of YOLOv9s internal features and their real world applicability,\nestablishing it as a state of the art solution for real time object detection\nacross industries, from IoT devices to large scale industrial applications."
                },
                "authors": [
                    {
                        "name": "Muhammad Yaseen"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Yaseen"
                },
                "author": "Muhammad Yaseen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12935v2",
                "updated": "2024-09-12T07:45:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    7,
                    45,
                    50,
                    3,
                    256,
                    0
                ],
                "published": "2024-08-23T09:33:48Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    33,
                    48,
                    4,
                    236,
                    0
                ],
                "title": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural\n  Framework for AI Safety with Challenges and Mitigations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural\n  Framework for AI Safety with Challenges and Mitigations"
                },
                "summary": "AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Ziyao Liu"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Si Qi Goh"
                    },
                    {
                        "name": "Kwok-Yan Lam"
                    }
                ],
                "author_detail": {
                    "name": "Kwok-Yan Lam"
                },
                "author": "Kwok-Yan Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07805v1",
                "updated": "2024-09-12T07:32:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    7,
                    32,
                    24,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T07:32:24Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    7,
                    32,
                    24,
                    3,
                    256,
                    0
                ],
                "title": "Extremely Dense Gas around Little Red Dots and High-redshift AGNs: A\n  Non-stellar Origin of the Balmer Break and Absorption Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extremely Dense Gas around Little Red Dots and High-redshift AGNs: A\n  Non-stellar Origin of the Balmer Break and Absorption Features"
                },
                "summary": "The James Webb Space Telescope (JWST) has uncovered low-luminosity active\ngalactic nuclei (AGNs) at high redshifts of $z\\gtrsim 4-7$, powered by\naccreting black holes (BHs) with masses of $\\sim 10^{6-8}~M_\\odot$. These AGN\npopulations are considered crucial for understanding early BH assembly and\ncoevolution with their host galaxies. One remarkable distinction of these\nJWST-identified AGNs, compared to their low-redshift counterparts, is that at\nleast $\\sim 20\\%$ of them present H$\\alpha$ and/or H$\\beta$ absorption, which\nmust be associated with extremely dense ($\\gtrsim 10^9$ cm$^{-3}$) gas along\nthe line of sight. These Balmer absorption features unavoidably imply the\npresence of a Balmer break caused by the same dense gas. In this Letter, we\nquantitatively demonstrate that a Balmer-break feature can form in AGN spectra\nwithout stellar components, when the accretion disk is heavily embedded in\ndense neutral gas clumps with densities of $\\sim 10^{9-11}$ cm$^{-3}$, where\nhydrogen atoms are collisionally excited to the $n=2$ states and effectively\nabsorb the AGN continuum at the bluer side of the Balmer limit. The non-stellar\norigin of a Balmer break offers a potential solution to the large stellar\nmasses and densities inferred for little red dots (LRDs) when assuming that\ntheir continuum is primarily due to stellar light. Our calculations of\nhydrogen-level populations indicate that the observed Balmer absorption\nblueshifted by a few hundreds km s$^{-1}$ suggests the presence of dense\noutflows at parsec scales in the nucleus. The outflow rate likely exceeds the\nEddington accretion rate, driven by powerful radiation from a super-Eddington\naccretion disk. Other spectral features such as higher equivalent widths of\nbroad H$\\alpha$ emission and presence of OI lines observed in high-redshift\nAGNs including LRDs align with the predicted signatures of a dense\nsuper-Eddington accretion disk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The James Webb Space Telescope (JWST) has uncovered low-luminosity active\ngalactic nuclei (AGNs) at high redshifts of $z\\gtrsim 4-7$, powered by\naccreting black holes (BHs) with masses of $\\sim 10^{6-8}~M_\\odot$. These AGN\npopulations are considered crucial for understanding early BH assembly and\ncoevolution with their host galaxies. One remarkable distinction of these\nJWST-identified AGNs, compared to their low-redshift counterparts, is that at\nleast $\\sim 20\\%$ of them present H$\\alpha$ and/or H$\\beta$ absorption, which\nmust be associated with extremely dense ($\\gtrsim 10^9$ cm$^{-3}$) gas along\nthe line of sight. These Balmer absorption features unavoidably imply the\npresence of a Balmer break caused by the same dense gas. In this Letter, we\nquantitatively demonstrate that a Balmer-break feature can form in AGN spectra\nwithout stellar components, when the accretion disk is heavily embedded in\ndense neutral gas clumps with densities of $\\sim 10^{9-11}$ cm$^{-3}$, where\nhydrogen atoms are collisionally excited to the $n=2$ states and effectively\nabsorb the AGN continuum at the bluer side of the Balmer limit. The non-stellar\norigin of a Balmer break offers a potential solution to the large stellar\nmasses and densities inferred for little red dots (LRDs) when assuming that\ntheir continuum is primarily due to stellar light. Our calculations of\nhydrogen-level populations indicate that the observed Balmer absorption\nblueshifted by a few hundreds km s$^{-1}$ suggests the presence of dense\noutflows at parsec scales in the nucleus. The outflow rate likely exceeds the\nEddington accretion rate, driven by powerful radiation from a super-Eddington\naccretion disk. Other spectral features such as higher equivalent widths of\nbroad H$\\alpha$ emission and presence of OI lines observed in high-redshift\nAGNs including LRDs align with the predicted signatures of a dense\nsuper-Eddington accretion disk."
                },
                "authors": [
                    {
                        "name": "Kohei Inayoshi"
                    },
                    {
                        "name": "Roberto Maiolino"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Maiolino"
                },
                "author": "Roberto Maiolino",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.15288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.15288v2",
                "updated": "2024-09-12T07:26:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    7,
                    26,
                    21,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-27T15:10:19Z",
                "published_parsed": [
                    2023,
                    3,
                    27,
                    15,
                    10,
                    19,
                    0,
                    86,
                    0
                ],
                "title": "Memory-Efficient 3D Denoising Diffusion Models for Medical Image\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient 3D Denoising Diffusion Models for Medical Image\n  Processing"
                },
                "summary": "Denoising diffusion models have recently achieved state-of-the-art\nperformance in many image-generation tasks. They do, however, require a large\namount of computational resources. This limits their application to medical\ntasks, where we often deal with large 3D volumes, like high-resolution\nthree-dimensional data. In this work, we present a number of different ways to\nreduce the resource consumption for 3D diffusion models and apply them to a\ndataset of 3D images. The main contribution of this paper is the\nmemory-efficient patch-based diffusion model \\textit{PatchDDM}, which can be\napplied to the total volume during inference while the training is performed\nonly on patches. While the proposed diffusion model can be applied to any image\ngeneration tasks, we evaluate the method on the tumor segmentation task of the\nBraTS2020 dataset and demonstrate that we can generate meaningful\nthree-dimensional segmentations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising diffusion models have recently achieved state-of-the-art\nperformance in many image-generation tasks. They do, however, require a large\namount of computational resources. This limits their application to medical\ntasks, where we often deal with large 3D volumes, like high-resolution\nthree-dimensional data. In this work, we present a number of different ways to\nreduce the resource consumption for 3D diffusion models and apply them to a\ndataset of 3D images. The main contribution of this paper is the\nmemory-efficient patch-based diffusion model \\textit{PatchDDM}, which can be\napplied to the total volume during inference while the training is performed\nonly on patches. While the proposed diffusion model can be applied to any image\ngeneration tasks, we evaluate the method on the tumor segmentation task of the\nBraTS2020 dataset and demonstrate that we can generate meaningful\nthree-dimensional segmentations."
                },
                "authors": [
                    {
                        "name": "Florentin Bieder"
                    },
                    {
                        "name": "Julia Wolleb"
                    },
                    {
                        "name": "Alicia Durrer"
                    },
                    {
                        "name": "Robin Sandkühler"
                    },
                    {
                        "name": "Philippe C. Cattin"
                    }
                ],
                "author_detail": {
                    "name": "Philippe C. Cattin"
                },
                "author": "Philippe C. Cattin",
                "arxiv_comment": "Accepted at MIDL 2023",
                "arxiv_journal_ref": "Medical Imaging with Deep Learning, PMLR 227:552-567, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.15288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.15288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12093v2",
                "updated": "2024-09-12T07:18:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    7,
                    18,
                    0,
                    3,
                    256,
                    0
                ],
                "published": "2024-08-22T03:03:04Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    3,
                    4,
                    3,
                    235,
                    0
                ],
                "title": "LLM-enhanced Scene Graph Learning for Household Rearrangement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-enhanced Scene Graph Learning for Household Rearrangement"
                },
                "summary": "The household rearrangement task involves spotting misplaced objects in a\nscene and accommodate them with proper places. It depends both on common-sense\nknowledge on the objective side and human user preference on the subjective\nside. In achieving such task, we propose to mine object functionality with user\npreference alignment directly from the scene itself, without relying on human\nintervention. To do so, we work with scene graph representation and propose\nLLM-enhanced scene graph learning which transforms the input scene graph into\nan affordance-enhanced graph (AEG) with information-enhanced nodes and newly\ndiscovered edges (relations). In AEG, the nodes corresponding to the receptacle\nobjects are augmented with context-induced affordance which encodes what kind\nof carriable objects can be placed on it. New edges are discovered with newly\ndiscovered non-local relations. With AEG, we perform task planning for scene\nrearrangement by detecting misplaced carriables and determining a proper\nplacement for each of them. We test our method by implementing a tiding robot\nin simulator and perform evaluation on a new benchmark we build. Extensive\nevaluations demonstrate that our method achieves state-of-the-art performance\non misplacement detection and the following rearrangement planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The household rearrangement task involves spotting misplaced objects in a\nscene and accommodate them with proper places. It depends both on common-sense\nknowledge on the objective side and human user preference on the subjective\nside. In achieving such task, we propose to mine object functionality with user\npreference alignment directly from the scene itself, without relying on human\nintervention. To do so, we work with scene graph representation and propose\nLLM-enhanced scene graph learning which transforms the input scene graph into\nan affordance-enhanced graph (AEG) with information-enhanced nodes and newly\ndiscovered edges (relations). In AEG, the nodes corresponding to the receptacle\nobjects are augmented with context-induced affordance which encodes what kind\nof carriable objects can be placed on it. New edges are discovered with newly\ndiscovered non-local relations. With AEG, we perform task planning for scene\nrearrangement by detecting misplaced carriables and determining a proper\nplacement for each of them. We test our method by implementing a tiding robot\nin simulator and perform evaluation on a new benchmark we build. Extensive\nevaluations demonstrate that our method achieves state-of-the-art performance\non misplacement detection and the following rearrangement planning."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Zhiyuan Yu"
                    },
                    {
                        "name": "Qijin She"
                    },
                    {
                        "name": "Zhinan Yu"
                    },
                    {
                        "name": "Yuqing Lan"
                    },
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Ruizhen Hu"
                    },
                    {
                        "name": "Kai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Xu"
                },
                "author": "Kai Xu",
                "arxiv_comment": "SIGGRAPH ASIA 2024 conference accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07796v1",
                "updated": "2024-09-12T06:56:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    56,
                    52,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T06:56:52Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    56,
                    52,
                    3,
                    256,
                    0
                ],
                "title": "In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for\n  Efficient Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for\n  Efficient Adaptation"
                },
                "summary": "Wildlife monitoring via camera traps has become an essential tool in ecology,\nbut the deployment of machine learning models for on-device animal\nclassification faces significant challenges due to domain shifts and resource\nconstraints. This paper introduces WildFit, a novel approach that reconciles\nthe conflicting goals of achieving high domain generalization performance and\nensuring efficient inference for camera trap applications. WildFit leverages\ncontinuous background-aware model fine-tuning to deploy ML models tailored to\nthe current location and time window, allowing it to maintain robust\nclassification accuracy in the new environment without requiring significant\ncomputational resources. This is achieved by background-aware data synthesis,\nwhich generates training images representing the new domain by blending\nbackground images with animal images from the source domain. We further enhance\nfine-tuning effectiveness through background drift detection and class\ndistribution drift detection, which optimize the quality of synthesized data\nand improve generalization performance. Our extensive evaluation across\nmultiple camera trap datasets demonstrates that WildFit achieves significant\nimprovements in classification accuracy and computational efficiency compared\nto traditional approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wildlife monitoring via camera traps has become an essential tool in ecology,\nbut the deployment of machine learning models for on-device animal\nclassification faces significant challenges due to domain shifts and resource\nconstraints. This paper introduces WildFit, a novel approach that reconciles\nthe conflicting goals of achieving high domain generalization performance and\nensuring efficient inference for camera trap applications. WildFit leverages\ncontinuous background-aware model fine-tuning to deploy ML models tailored to\nthe current location and time window, allowing it to maintain robust\nclassification accuracy in the new environment without requiring significant\ncomputational resources. This is achieved by background-aware data synthesis,\nwhich generates training images representing the new domain by blending\nbackground images with animal images from the source domain. We further enhance\nfine-tuning effectiveness through background drift detection and class\ndistribution drift detection, which optimize the quality of synthesized data\nand improve generalization performance. Our extensive evaluation across\nmultiple camera trap datasets demonstrates that WildFit achieves significant\nimprovements in classification accuracy and computational efficiency compared\nto traditional approaches."
                },
                "authors": [
                    {
                        "name": "Mohammad Mehdi Rastikerdar"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Hui Guan"
                    },
                    {
                        "name": "Deepak Ganesan"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Ganesan"
                },
                "author": "Deepak Ganesan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07790v1",
                "updated": "2024-09-12T06:50:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    50,
                    45,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T06:50:45Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    50,
                    45,
                    3,
                    256,
                    0
                ],
                "title": "Full-text Error Correction for Chinese Speech Recognition with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-text Error Correction for Chinese Speech Recognition with Large\n  Language Model"
                },
                "summary": "Large Language Models (LLMs) have demonstrated substantial potential for\nerror correction in Automatic Speech Recognition (ASR). However, most research\nfocuses on utterances from short-duration speech recordings, which are the\npredominant form of speech data for supervised ASR training. This paper\ninvestigates the effectiveness of LLMs for error correction in full-text\ngenerated by ASR systems from longer speech recordings, such as transcripts\nfrom podcasts, news broadcasts, and meetings. First, we develop a Chinese\ndataset for full-text error correction, named ChFT, utilizing a pipeline that\ninvolves text-to-speech synthesis, ASR, and error-correction pair extractor.\nThis dataset enables us to correct errors across contexts, including both\nfull-text and segment, and to address a broader range of error types, such as\npunctuation restoration and inverse text normalization, thus making the\ncorrection process comprehensive. Second, we fine-tune a pre-trained LLM on the\nconstructed dataset using a diverse set of prompts and target formats, and\nevaluate its performance on full-text error correction. Specifically, we design\nprompts based on full-text and segment, considering various output formats,\nsuch as directly corrected text and JSON-based error-correction pairs. Through\nvarious test settings, including homogeneous, up-to-date, and hard test sets,\nwe find that the fine-tuned LLMs perform well in the full-text setting with\ndifferent prompts, each presenting its own strengths and weaknesses. This\nestablishes a promising baseline for further research. The dataset is available\non the website.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated substantial potential for\nerror correction in Automatic Speech Recognition (ASR). However, most research\nfocuses on utterances from short-duration speech recordings, which are the\npredominant form of speech data for supervised ASR training. This paper\ninvestigates the effectiveness of LLMs for error correction in full-text\ngenerated by ASR systems from longer speech recordings, such as transcripts\nfrom podcasts, news broadcasts, and meetings. First, we develop a Chinese\ndataset for full-text error correction, named ChFT, utilizing a pipeline that\ninvolves text-to-speech synthesis, ASR, and error-correction pair extractor.\nThis dataset enables us to correct errors across contexts, including both\nfull-text and segment, and to address a broader range of error types, such as\npunctuation restoration and inverse text normalization, thus making the\ncorrection process comprehensive. Second, we fine-tune a pre-trained LLM on the\nconstructed dataset using a diverse set of prompts and target formats, and\nevaluate its performance on full-text error correction. Specifically, we design\nprompts based on full-text and segment, considering various output formats,\nsuch as directly corrected text and JSON-based error-correction pairs. Through\nvarious test settings, including homogeneous, up-to-date, and hard test sets,\nwe find that the fine-tuned LLMs perform well in the full-text setting with\ndifferent prompts, each presenting its own strengths and weaknesses. This\nestablishes a promising baseline for further research. The dataset is available\non the website."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Tang"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Shen Huang"
                    },
                    {
                        "name": "Shidong Shang"
                    }
                ],
                "author_detail": {
                    "name": "Shidong Shang"
                },
                "author": "Shidong Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07772v1",
                "updated": "2024-09-12T06:10:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    10,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T06:10:15Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    10,
                    15,
                    3,
                    256,
                    0
                ],
                "title": "Alignment with Preference Optimization Is All You Need for LLM Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment with Preference Optimization Is All You Need for LLM Safety"
                },
                "summary": "We demonstrate that preference optimization methods can effectively enhance\nLLM safety. Applying various alignment techniques to the Falcon 11B model using\nsafety datasets, we achieve a significant boost in global safety score (from\n$57.64\\%$ to $99.90\\%$) as measured by LlamaGuard 3 8B, competing with\nstate-of-the-art models. On toxicity benchmarks, average scores in adversarial\nsettings dropped from over $0.6$ to less than $0.07$. However, this safety\nimprovement comes at the cost of reduced general capabilities, particularly in\nmath, suggesting a trade-off. We identify noise contrastive alignment\n(Safe-NCA) as an optimal method for balancing safety and performance. Our study\nultimately shows that alignment techniques can be sufficient for building safe\nand robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that preference optimization methods can effectively enhance\nLLM safety. Applying various alignment techniques to the Falcon 11B model using\nsafety datasets, we achieve a significant boost in global safety score (from\n$57.64\\%$ to $99.90\\%$) as measured by LlamaGuard 3 8B, competing with\nstate-of-the-art models. On toxicity benchmarks, average scores in adversarial\nsettings dropped from over $0.6$ to less than $0.07$. However, this safety\nimprovement comes at the cost of reduced general capabilities, particularly in\nmath, suggesting a trade-off. We identify noise contrastive alignment\n(Safe-NCA) as an optimal method for balancing safety and performance. Our study\nultimately shows that alignment techniques can be sufficient for building safe\nand robust models."
                },
                "authors": [
                    {
                        "name": "Reda Alami"
                    },
                    {
                        "name": "Ali Khalifa Almansoori"
                    },
                    {
                        "name": "Ahmed Alzubaidi"
                    },
                    {
                        "name": "Mohamed El Amine Seddik"
                    },
                    {
                        "name": "Mugariya Farooq"
                    },
                    {
                        "name": "Hakim Hacid"
                    }
                ],
                "author_detail": {
                    "name": "Hakim Hacid"
                },
                "author": "Hakim Hacid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07756v1",
                "updated": "2024-09-12T05:18:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    5,
                    18,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T05:18:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    5,
                    18,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "DiTAS: Quantizing Diffusion Transformers via Enhanced Activation\n  Smoothing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTAS: Quantizing Diffusion Transformers via Enhanced Activation\n  Smoothing"
                },
                "summary": "Diffusion Transformers (DiTs) have recently attracted significant interest\nfrom both industry and academia due to their enhanced capabilities in visual\ngeneration, surpassing the performance of traditional diffusion models that\nemploy U-Net. However, the improved performance of DiTs comes at the expense of\nhigher parameter counts and implementation costs, which significantly limits\ntheir deployment on resource-constrained devices like mobile phones. We propose\nDiTAS, a data-free post-training quantization (PTQ) method for efficient DiT\ninference. DiTAS relies on the proposed temporal-aggregated smoothing\ntechniques to mitigate the impact of the channel-wise outliers within the input\nactivations, leading to much lower quantization error under extremely low\nbitwidth. To further enhance the performance of the quantized DiT, we adopt the\nlayer-wise grid search strategy to optimize the smoothing factor. Experimental\nresults demonstrate that our approach enables 4-bit weight, 8-bit activation\n(W4A8) quantization for DiTs while maintaining comparable performance as the\nfull-precision model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have recently attracted significant interest\nfrom both industry and academia due to their enhanced capabilities in visual\ngeneration, surpassing the performance of traditional diffusion models that\nemploy U-Net. However, the improved performance of DiTs comes at the expense of\nhigher parameter counts and implementation costs, which significantly limits\ntheir deployment on resource-constrained devices like mobile phones. We propose\nDiTAS, a data-free post-training quantization (PTQ) method for efficient DiT\ninference. DiTAS relies on the proposed temporal-aggregated smoothing\ntechniques to mitigate the impact of the channel-wise outliers within the input\nactivations, leading to much lower quantization error under extremely low\nbitwidth. To further enhance the performance of the quantized DiT, we adopt the\nlayer-wise grid search strategy to optimize the smoothing factor. Experimental\nresults demonstrate that our approach enables 4-bit weight, 8-bit activation\n(W4A8) quantization for DiTs while maintaining comparable performance as the\nfull-precision model."
                },
                "authors": [
                    {
                        "name": "Zhenyuan Dong"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07751v1",
                "updated": "2024-09-12T04:51:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    51,
                    27,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T04:51:27Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    51,
                    27,
                    3,
                    256,
                    0
                ],
                "title": "Efficient Privacy-Preserving KAN Inference Using Homomorphic Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Privacy-Preserving KAN Inference Using Homomorphic Encryption"
                },
                "summary": "The recently proposed Kolmogorov-Arnold Networks (KANs) offer enhanced\ninterpretability and greater model expressiveness. However, KANs also present\nchallenges related to privacy leakage during inference. Homomorphic encryption\n(HE) facilitates privacy-preserving inference for deep learning models,\nenabling resource-limited users to benefit from deep learning services while\nensuring data security. Yet, the complex structure of KANs, incorporating\nnonlinear elements like the SiLU activation function and B-spline functions,\nrenders existing privacy-preserving inference techniques inadequate. To address\nthis issue, we propose an accurate and efficient privacy-preserving inference\nscheme tailored for KANs. Our approach introduces a task-specific polynomial\napproximation for the SiLU activation function, dynamically adjusting the\napproximation range to ensure high accuracy on real-world datasets.\nAdditionally, we develop an efficient method for computing B-spline functions\nwithin the HE domain, leveraging techniques such as repeat packing, lazy\ncombination, and comparison functions. We evaluate the effectiveness of our\nprivacy-preserving KAN inference scheme on both symbolic formula evaluation and\nimage classification. The experimental results show that our model achieves\naccuracy comparable to plaintext KANs across various datasets and outperforms\nplaintext MLPs. Additionally, on the CIFAR-10 dataset, our inference latency\nachieves over 7 times speedup compared to the naive method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently proposed Kolmogorov-Arnold Networks (KANs) offer enhanced\ninterpretability and greater model expressiveness. However, KANs also present\nchallenges related to privacy leakage during inference. Homomorphic encryption\n(HE) facilitates privacy-preserving inference for deep learning models,\nenabling resource-limited users to benefit from deep learning services while\nensuring data security. Yet, the complex structure of KANs, incorporating\nnonlinear elements like the SiLU activation function and B-spline functions,\nrenders existing privacy-preserving inference techniques inadequate. To address\nthis issue, we propose an accurate and efficient privacy-preserving inference\nscheme tailored for KANs. Our approach introduces a task-specific polynomial\napproximation for the SiLU activation function, dynamically adjusting the\napproximation range to ensure high accuracy on real-world datasets.\nAdditionally, we develop an efficient method for computing B-spline functions\nwithin the HE domain, leveraging techniques such as repeat packing, lazy\ncombination, and comparison functions. We evaluate the effectiveness of our\nprivacy-preserving KAN inference scheme on both symbolic formula evaluation and\nimage classification. The experimental results show that our model achieves\naccuracy comparable to plaintext KANs across various datasets and outperforms\nplaintext MLPs. Additionally, on the CIFAR-10 dataset, our inference latency\nachieves over 7 times speedup compared to the naive method."
                },
                "authors": [
                    {
                        "name": "Zhizheng Lai"
                    },
                    {
                        "name": "Yufei Zhou"
                    },
                    {
                        "name": "Peijia Zheng"
                    },
                    {
                        "name": "Lin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lin Chen"
                },
                "author": "Lin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03426v2",
                "updated": "2024-09-12T04:47:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    47,
                    33,
                    3,
                    256,
                    0
                ],
                "published": "2024-01-07T09:06:58Z",
                "published_parsed": [
                    2024,
                    1,
                    7,
                    9,
                    6,
                    58,
                    6,
                    7,
                    0
                ],
                "title": "On Leveraging Large Language Models for Enhancing Entity Resolution: A\n  Cost-efficient Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Leveraging Large Language Models for Enhancing Entity Resolution: A\n  Cost-efficient Approach"
                },
                "summary": "Entity resolution, the task of identifying and merging records that refer to\nthe same real-world entity, is crucial in sectors like e-commerce, healthcare,\nand law enforcement. Large Language Models (LLMs) introduce an innovative\napproach to this task, capitalizing on their advanced linguistic capabilities\nand a ``pay-as-you-go'' model that provides significant advantages to those\nwithout extensive data science expertise. However, current LLMs are costly due\nto per-API request billing. Existing methods often either lack quality or\nbecome prohibitively expensive at scale. To address these problems, we propose\nan uncertainty reduction framework using LLMs to improve entity resolution\nresults. We first initialize possible partitions of the entity cluster, refer\nto the same entity, and define the uncertainty of the result. Then, we reduce\nthe uncertainty by selecting a few valuable matching questions for LLM\nverification. Upon receiving the answers, we update the probability\ndistribution of the possible partitions. To further reduce costs, we design an\nefficient algorithm to judiciously select the most valuable matching pairs to\nquery. Additionally, we create error-tolerant techniques to handle LLM mistakes\nand a dynamic adjustment method to reach truly correct partitions. Experimental\nresults show that our method is efficient and effective, offering promising\napplications in real-world tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity resolution, the task of identifying and merging records that refer to\nthe same real-world entity, is crucial in sectors like e-commerce, healthcare,\nand law enforcement. Large Language Models (LLMs) introduce an innovative\napproach to this task, capitalizing on their advanced linguistic capabilities\nand a ``pay-as-you-go'' model that provides significant advantages to those\nwithout extensive data science expertise. However, current LLMs are costly due\nto per-API request billing. Existing methods often either lack quality or\nbecome prohibitively expensive at scale. To address these problems, we propose\nan uncertainty reduction framework using LLMs to improve entity resolution\nresults. We first initialize possible partitions of the entity cluster, refer\nto the same entity, and define the uncertainty of the result. Then, we reduce\nthe uncertainty by selecting a few valuable matching questions for LLM\nverification. Upon receiving the answers, we update the probability\ndistribution of the possible partitions. To further reduce costs, we design an\nefficient algorithm to judiciously select the most valuable matching pairs to\nquery. Additionally, we create error-tolerant techniques to handle LLM mistakes\nand a dynamic adjustment method to reach truly correct partitions. Experimental\nresults show that our method is efficient and effective, offering promising\napplications in real-world tasks."
                },
                "authors": [
                    {
                        "name": "Huahang Li"
                    },
                    {
                        "name": "Longyu Feng"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Fei Hao"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Yuanfeng Song"
                    }
                ],
                "author_detail": {
                    "name": "Yuanfeng Song"
                },
                "author": "Yuanfeng Song",
                "arxiv_comment": "9 pages, preprint under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07743v1",
                "updated": "2024-09-12T04:28:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    28,
                    22,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T04:28:22Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    28,
                    22,
                    3,
                    256,
                    0
                ],
                "title": "LOCKEY: A Novel Approach to Model Authentication and Deepfake Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOCKEY: A Novel Approach to Model Authentication and Deepfake Tracking"
                },
                "summary": "This paper presents a novel approach to deter unauthorized deepfakes and\nenable user tracking in generative models, even when the user has full access\nto the model parameters, by integrating key-based model authentication with\nwatermarking techniques. Our method involves providing users with model\nparameters accompanied by a unique, user-specific key. During inference, the\nmodel is conditioned upon the key along with the standard input. A valid key\nresults in the expected output, while an invalid key triggers a degraded\noutput, thereby enforcing key-based model authentication. For user tracking,\nthe model embeds the user's unique key as a watermark within the generated\ncontent, facilitating the identification of the user's ID. We demonstrate the\neffectiveness of our approach on two types of models, audio codecs and\nvocoders, utilizing the SilentCipher watermarking method. Additionally, we\nassess the robustness of the embedded watermarks against various distortions,\nvalidating their reliability in various scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to deter unauthorized deepfakes and\nenable user tracking in generative models, even when the user has full access\nto the model parameters, by integrating key-based model authentication with\nwatermarking techniques. Our method involves providing users with model\nparameters accompanied by a unique, user-specific key. During inference, the\nmodel is conditioned upon the key along with the standard input. A valid key\nresults in the expected output, while an invalid key triggers a degraded\noutput, thereby enforcing key-based model authentication. For user tracking,\nthe model embeds the user's unique key as a watermark within the generated\ncontent, facilitating the identification of the user's ID. We demonstrate the\neffectiveness of our approach on two types of models, audio codecs and\nvocoders, utilizing the SilentCipher watermarking method. Additionally, we\nassess the robustness of the embedded watermarks against various distortions,\nvalidating their reliability in various scenarios."
                },
                "authors": [
                    {
                        "name": "Mayank Kumar Singh"
                    },
                    {
                        "name": "Naoya Takahashi"
                    },
                    {
                        "name": "Wei-Hsiang Liao"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "Authenticating deep generative models, 5 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07741v1",
                "updated": "2024-09-12T04:16:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    16,
                    22,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T04:16:22Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    16,
                    22,
                    3,
                    256,
                    0
                ],
                "title": "Handling expression evaluation under interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling expression evaluation under interference"
                },
                "summary": "Hoare-style inference rules for program constructs permit the copying of\nexpressions and tests from program text into logical contexts. It is known that\nthis requires care even for sequential programs but further issues arise for\nconcurrent programs because of potential interference to the values of\nvariables. The \"rely-guarantee\" approach does tackle the issue of recording\nacceptable interference and offers a way to provide safe inference rules. This\npaper shows how the algebraic presentation of rely-guarantee ideas can clarify\nand formalise the conditions for safely re-using expressions and tests from\nprogram text in logical contexts for reasoning about programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hoare-style inference rules for program constructs permit the copying of\nexpressions and tests from program text into logical contexts. It is known that\nthis requires care even for sequential programs but further issues arise for\nconcurrent programs because of potential interference to the values of\nvariables. The \"rely-guarantee\" approach does tackle the issue of recording\nacceptable interference and offers a way to provide safe inference rules. This\npaper shows how the algebraic presentation of rely-guarantee ideas can clarify\nand formalise the conditions for safely re-using expressions and tests from\nprogram text in logical contexts for reasoning about programs."
                },
                "authors": [
                    {
                        "name": "Ian J. Hayes"
                    },
                    {
                        "name": "Cliff B. Jones"
                    },
                    {
                        "name": "Larissa A. Meinicke"
                    }
                ],
                "author_detail": {
                    "name": "Larissa A. Meinicke"
                },
                "author": "Larissa A. Meinicke",
                "arxiv_comment": "17 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.3.1; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07738v1",
                "updated": "2024-09-12T04:09:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    9,
                    9,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T04:09:09Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    9,
                    9,
                    3,
                    256,
                    0
                ],
                "title": "A model-based approach for clustering binned data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A model-based approach for clustering binned data"
                },
                "summary": "Binned data often appears in different fields of research, and it is\ngenerated after summarizing the original data in a sequence of pairs of bins\n(or their midpoints) and frequencies. There may exist different reasons to only\nprovide this summary, but more importantly, it is necessary being able to\nperform statistical analyses based only on it. We present a Bayesian\nnonparametric model for clustering applicable for binned data. Clusters are\nmodeled via random partitions, and within them a model-based approach is\nassumed. Inferences are performed by a Markov chain Monte Carlo method and the\ncomplete proposal is tested using simulated and real data. Having particular\ninterest in studying marine populations, we analyze samples of Lobatus\n(Strobus) gigas' lengths and found the presence of up to three cohorts along\nthe year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binned data often appears in different fields of research, and it is\ngenerated after summarizing the original data in a sequence of pairs of bins\n(or their midpoints) and frequencies. There may exist different reasons to only\nprovide this summary, but more importantly, it is necessary being able to\nperform statistical analyses based only on it. We present a Bayesian\nnonparametric model for clustering applicable for binned data. Clusters are\nmodeled via random partitions, and within them a model-based approach is\nassumed. Inferences are performed by a Markov chain Monte Carlo method and the\ncomplete proposal is tested using simulated and real data. Having particular\ninterest in studying marine populations, we analyze samples of Lobatus\n(Strobus) gigas' lengths and found the presence of up to three cohorts along\nthe year."
                },
                "authors": [
                    {
                        "name": "Asael Fabian Martínez"
                    },
                    {
                        "name": "Carlos Díaz-Avalos"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Díaz-Avalos"
                },
                "author": "Carlos Díaz-Avalos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07737v1",
                "updated": "2024-09-12T04:06:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    6,
                    31,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T04:06:31Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    6,
                    31,
                    3,
                    256,
                    0
                ],
                "title": "Ruri: Japanese General Text Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ruri: Japanese General Text Embeddings"
                },
                "summary": "We report the development of Ruri, a series of Japanese general text\nembedding models. While the development of general-purpose text embedding\nmodels in English and multilingual contexts has been active in recent years,\nmodel development in Japanese remains insufficient. The primary reasons for\nthis are the lack of datasets and the absence of necessary expertise. In this\nreport, we provide a detailed account of the development process of Ruri.\nSpecifically, we discuss the training of embedding models using synthesized\ndatasets generated by LLMs, the construction of the reranker for dataset\nfiltering and knowledge distillation, and the performance evaluation of the\nresulting general-purpose text embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the development of Ruri, a series of Japanese general text\nembedding models. While the development of general-purpose text embedding\nmodels in English and multilingual contexts has been active in recent years,\nmodel development in Japanese remains insufficient. The primary reasons for\nthis are the lack of datasets and the absence of necessary expertise. In this\nreport, we provide a detailed account of the development process of Ruri.\nSpecifically, we discuss the training of embedding models using synthesized\ndatasets generated by LLMs, the construction of the reranker for dataset\nfiltering and knowledge distillation, and the performance evaluation of the\nresulting general-purpose text embedding models."
                },
                "authors": [
                    {
                        "name": "Hayato Tsukagoshi"
                    },
                    {
                        "name": "Ryohei Sasano"
                    }
                ],
                "author_detail": {
                    "name": "Ryohei Sasano"
                },
                "author": "Ryohei Sasano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07732v1",
                "updated": "2024-09-12T03:41:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    3,
                    41,
                    39,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T03:41:39Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    3,
                    41,
                    39,
                    3,
                    256,
                    0
                ],
                "title": "Large Language Models are Pattern Matchers: Editing Semi-Structured and\n  Structured Documents with ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Pattern Matchers: Editing Semi-Structured and\n  Structured Documents with ChatGPT"
                },
                "summary": "Large Language Models (LLMs) offer numerous applications, the full extent of\nwhich is not yet understood. This paper investigates if LLMs can be applied for\nediting structured and semi-structured documents with minimal effort. Using a\nqualitative research approach, we conduct two case studies with ChatGPT and\nthoroughly analyze the results. Our experiments indicate that LLMs can\neffectively edit structured and semi-structured documents when provided with\nbasic, straightforward prompts. ChatGPT demonstrates a strong ability to\nrecognize and process the structure of annotated documents. This suggests that\nexplicitly structuring tasks and data in prompts might enhance an LLM's ability\nto understand and solve tasks. Furthermore, the experiments also reveal\nimpressive pattern matching skills in ChatGPT. This observation deserves\nfurther investigation, as it may contribute to understanding the processes\nleading to hallucinations in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) offer numerous applications, the full extent of\nwhich is not yet understood. This paper investigates if LLMs can be applied for\nediting structured and semi-structured documents with minimal effort. Using a\nqualitative research approach, we conduct two case studies with ChatGPT and\nthoroughly analyze the results. Our experiments indicate that LLMs can\neffectively edit structured and semi-structured documents when provided with\nbasic, straightforward prompts. ChatGPT demonstrates a strong ability to\nrecognize and process the structure of annotated documents. This suggests that\nexplicitly structuring tasks and data in prompts might enhance an LLM's ability\nto understand and solve tasks. Furthermore, the experiments also reveal\nimpressive pattern matching skills in ChatGPT. This observation deserves\nfurther investigation, as it may contribute to understanding the processes\nleading to hallucinations in LLMs."
                },
                "authors": [
                    {
                        "name": "Irene Weber"
                    }
                ],
                "author_detail": {
                    "name": "Irene Weber"
                },
                "author": "Irene Weber",
                "arxiv_doi": "10.18420/AKWI2024-001",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18420/AKWI2024-001",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.07732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "AKWI Jahrestagung 2024, Lecture Notes in Informatics (LNI) Bd. 357\n  (2024)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10025v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10025v4",
                "updated": "2024-09-12T03:17:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    3,
                    17,
                    18,
                    3,
                    256,
                    0
                ],
                "published": "2023-04-20T00:39:20Z",
                "published_parsed": [
                    2023,
                    4,
                    20,
                    0,
                    39,
                    20,
                    3,
                    110,
                    0
                ],
                "title": "Identification and multiply robust estimation in causal mediation\n  analysis across principal strata",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification and multiply robust estimation in causal mediation\n  analysis across principal strata"
                },
                "summary": "We consider assessing causal mediation in the presence of a post-treatment\nevent (examples include noncompliance, a clinical event, or death). We identify\nnatural mediation effects for the entire study population and for each\nprincipal stratum characterized by the joint potential values of the\npost-treatment event. We derive the efficient influence function for each\nmediation estimand, which motivates a set of multiply robust estimators for\ninference. The multiply robust estimators are consistent under four types of\nmisspecifications and are efficient when all nuisance models are correctly\nspecified. We also develop a nonparametric efficient estimator that leverages\ndata-adaptive machine learners to achieve efficient inference and discuss\nsensitivity methods to address key identification assumptions. We illustrate\nour methods via simulations and two real data examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider assessing causal mediation in the presence of a post-treatment\nevent (examples include noncompliance, a clinical event, or death). We identify\nnatural mediation effects for the entire study population and for each\nprincipal stratum characterized by the joint potential values of the\npost-treatment event. We derive the efficient influence function for each\nmediation estimand, which motivates a set of multiply robust estimators for\ninference. The multiply robust estimators are consistent under four types of\nmisspecifications and are efficient when all nuisance models are correctly\nspecified. We also develop a nonparametric efficient estimator that leverages\ndata-adaptive machine learners to achieve efficient inference and discuss\nsensitivity methods to address key identification assumptions. We illustrate\nour methods via simulations and two real data examples."
                },
                "authors": [
                    {
                        "name": "Chao Cheng"
                    },
                    {
                        "name": "Fan Li"
                    }
                ],
                "author_detail": {
                    "name": "Fan Li"
                },
                "author": "Fan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10025v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10025v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14206v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14206v3",
                "updated": "2024-09-12T02:57:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    57,
                    7,
                    3,
                    256,
                    0
                ],
                "published": "2023-12-21T08:15:02Z",
                "published_parsed": [
                    2023,
                    12,
                    21,
                    8,
                    15,
                    2,
                    3,
                    355,
                    0
                ],
                "title": "LLM4VG: Large Language Models Evaluation for Video Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4VG: Large Language Models Evaluation for Video Grounding"
                },
                "summary": "Recently, researchers have attempted to investigate the capability of LLMs in\nhandling videos and proposed several video LLM models. However, the ability of\nLLMs to handle video grounding (VG), which is an important time-related video\ntask requiring the model to precisely locate the start and end timestamps of\ntemporal moments in videos that match the given textual queries, still remains\nunclear and unexplored in literature. To fill the gap, in this paper, we\npropose the LLM4VG benchmark, which systematically evaluates the performance of\ndifferent LLMs on video grounding tasks. Based on our proposed LLM4VG, we\ndesign extensive experiments to examine two groups of video LLM models on video\ngrounding: (i) the video LLMs trained on the text-video pairs (denoted as\nVidLLM), and (ii) the LLMs combined with pretrained visual description models\nsuch as the video/image captioning model. We propose prompt methods to\nintegrate the instruction of VG and description from different kinds of\ngenerators, including caption-based generators for direct visual description\nand VQA-based generators for information enhancement. We also provide\ncomprehensive comparisons of various VidLLMs and explore the influence of\ndifferent choices of visual models, LLMs, prompt designs, etc, as well. Our\nexperimental evaluations lead to two conclusions: (i) the existing VidLLMs are\nstill far away from achieving satisfactory video grounding performance, and\nmore time-related video tasks should be included to further fine-tune these\nmodels, and (ii) the combination of LLMs and visual models shows preliminary\nabilities for video grounding with considerable potential for improvement by\nresorting to more reliable models and further guidance of prompt instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, researchers have attempted to investigate the capability of LLMs in\nhandling videos and proposed several video LLM models. However, the ability of\nLLMs to handle video grounding (VG), which is an important time-related video\ntask requiring the model to precisely locate the start and end timestamps of\ntemporal moments in videos that match the given textual queries, still remains\nunclear and unexplored in literature. To fill the gap, in this paper, we\npropose the LLM4VG benchmark, which systematically evaluates the performance of\ndifferent LLMs on video grounding tasks. Based on our proposed LLM4VG, we\ndesign extensive experiments to examine two groups of video LLM models on video\ngrounding: (i) the video LLMs trained on the text-video pairs (denoted as\nVidLLM), and (ii) the LLMs combined with pretrained visual description models\nsuch as the video/image captioning model. We propose prompt methods to\nintegrate the instruction of VG and description from different kinds of\ngenerators, including caption-based generators for direct visual description\nand VQA-based generators for information enhancement. We also provide\ncomprehensive comparisons of various VidLLMs and explore the influence of\ndifferent choices of visual models, LLMs, prompt designs, etc, as well. Our\nexperimental evaluations lead to two conclusions: (i) the existing VidLLMs are\nstill far away from achieving satisfactory video grounding performance, and\nmore time-related video tasks should be included to further fine-tune these\nmodels, and (ii) the combination of LLMs and visual models shows preliminary\nabilities for video grounding with considerable potential for improvement by\nresorting to more reliable models and further guidance of prompt instructions."
                },
                "authors": [
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Zeyang Zhang"
                    },
                    {
                        "name": "Houlun Chen"
                    },
                    {
                        "name": "Zihan Song"
                    },
                    {
                        "name": "Yuwei Zhou"
                    },
                    {
                        "name": "Yuekui Yang"
                    },
                    {
                        "name": "Haiyang Wu"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14206v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14206v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07713v1",
                "updated": "2024-09-12T02:40:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    40,
                    28,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:40:28Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    40,
                    28,
                    3,
                    256,
                    0
                ],
                "title": "Experimenting with Legal AI Solutions: The Case of Question-Answering\n  for Access to Justice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimenting with Legal AI Solutions: The Case of Question-Answering\n  for Access to Justice"
                },
                "summary": "Generative AI models, such as the GPT and Llama series, have significant\npotential to assist laypeople in answering legal questions. However, little\nprior work focuses on the data sourcing, inference, and evaluation of these\nmodels in the context of laypersons. To this end, we propose a human-centric\nlegal NLP pipeline, covering data sourcing, inference, and evaluation. We\nintroduce and release a dataset, LegalQA, with real and specific legal\nquestions spanning from employment law to criminal law, corresponding answers\nwritten by legal experts, and citations for each answer. We develop an\nautomatic evaluation protocol for this dataset, then show that\nretrieval-augmented generation from only 850 citations in the train set can\nmatch or outperform internet-wide retrieval, despite containing 9 orders of\nmagnitude less data. Finally, we propose future directions for open-sourced\nefforts, which fall behind closed-sourced models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI models, such as the GPT and Llama series, have significant\npotential to assist laypeople in answering legal questions. However, little\nprior work focuses on the data sourcing, inference, and evaluation of these\nmodels in the context of laypersons. To this end, we propose a human-centric\nlegal NLP pipeline, covering data sourcing, inference, and evaluation. We\nintroduce and release a dataset, LegalQA, with real and specific legal\nquestions spanning from employment law to criminal law, corresponding answers\nwritten by legal experts, and citations for each answer. We develop an\nautomatic evaluation protocol for this dataset, then show that\nretrieval-augmented generation from only 850 citations in the train set can\nmatch or outperform internet-wide retrieval, despite containing 9 orders of\nmagnitude less data. Finally, we propose future directions for open-sourced\nefforts, which fall behind closed-sourced models."
                },
                "authors": [
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Rohan Bhambhoria"
                    },
                    {
                        "name": "Samuel Dahan"
                    },
                    {
                        "name": "Xiaodan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodan Zhu"
                },
                "author": "Xiaodan Zhu",
                "arxiv_comment": "Accepted into GenLaw '24 (ICML 2024 workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07706v1",
                "updated": "2024-09-12T02:19:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    19,
                    16,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:19:16Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    19,
                    16,
                    3,
                    256,
                    0
                ],
                "title": "Attack End-to-End Autonomous Driving through Module-Wise Noise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attack End-to-End Autonomous Driving through Module-Wise Noise"
                },
                "summary": "With recent breakthroughs in deep neural networks, numerous tasks within\nautonomous driving have exhibited remarkable performance. However, deep\nlearning models are susceptible to adversarial attacks, presenting significant\nsecurity risks to autonomous driving systems. Presently, end-to-end\narchitectures have emerged as the predominant solution for autonomous driving,\nowing to their collaborative nature across different tasks. Yet, the\nimplications of adversarial attacks on such models remain relatively\nunexplored. In this paper, we conduct comprehensive adversarial security\nresearch on the modular end-to-end autonomous driving model for the first time.\nWe thoroughly consider the potential vulnerabilities in the model inference\nprocess and design a universal attack scheme through module-wise noise\ninjection. We conduct large-scale experiments on the full-stack autonomous\ndriving model and demonstrate that our attack method outperforms previous\nattack methods. We trust that our research will offer fresh insights into\nensuring the safety and reliability of autonomous driving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With recent breakthroughs in deep neural networks, numerous tasks within\nautonomous driving have exhibited remarkable performance. However, deep\nlearning models are susceptible to adversarial attacks, presenting significant\nsecurity risks to autonomous driving systems. Presently, end-to-end\narchitectures have emerged as the predominant solution for autonomous driving,\nowing to their collaborative nature across different tasks. Yet, the\nimplications of adversarial attacks on such models remain relatively\nunexplored. In this paper, we conduct comprehensive adversarial security\nresearch on the modular end-to-end autonomous driving model for the first time.\nWe thoroughly consider the potential vulnerabilities in the model inference\nprocess and design a universal attack scheme through module-wise noise\ninjection. We conduct large-scale experiments on the full-stack autonomous\ndriving model and demonstrate that our attack method outperforms previous\nattack methods. We trust that our research will offer fresh insights into\nensuring the safety and reliability of autonomous driving systems."
                },
                "authors": [
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Yikai Han"
                    },
                    {
                        "name": "Muyang Fang"
                    },
                    {
                        "name": "Ting Jin"
                    },
                    {
                        "name": "Jiaqi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Kang"
                },
                "author": "Jiaqi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07703v1",
                "updated": "2024-09-12T02:08:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    8,
                    0,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:08:00Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    8,
                    0,
                    3,
                    256,
                    0
                ],
                "title": "DSBench: How Far Are Data Science Agents to Becoming Data Science\n  Experts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSBench: How Far Are Data Science Agents to Becoming Data Science\n  Experts?"
                },
                "summary": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have\ndemonstrated impressive language/vision reasoning abilities, igniting the\nrecent trend of building agents for targeted applications such as shopping\nassistants or AI software engineers. Recently, many data science benchmarks\nhave been proposed to investigate their performance in the data science domain.\nHowever, existing data science benchmarks still fall short when compared to\nreal-world data science applications due to their simplified settings. To\nbridge this gap, we introduce DSBench, a comprehensive benchmark designed to\nevaluate data science agents with realistic tasks. This benchmark includes 466\ndata analysis tasks and 74 data modeling tasks, sourced from Eloquence and\nKaggle competitions. DSBench offers a realistic setting by encompassing long\ncontexts, multimodal task backgrounds, reasoning with large data files and\nmulti-table structures, and performing end-to-end data modeling tasks. Our\nevaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle\nwith most tasks, with the best agent solving only 34.12% of data analysis tasks\nand achieving a 34.74% Relative Performance Gap (RPG). These findings\nunderscore the need for further advancements in developing more practical,\nintelligent, and autonomous data science agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have\ndemonstrated impressive language/vision reasoning abilities, igniting the\nrecent trend of building agents for targeted applications such as shopping\nassistants or AI software engineers. Recently, many data science benchmarks\nhave been proposed to investigate their performance in the data science domain.\nHowever, existing data science benchmarks still fall short when compared to\nreal-world data science applications due to their simplified settings. To\nbridge this gap, we introduce DSBench, a comprehensive benchmark designed to\nevaluate data science agents with realistic tasks. This benchmark includes 466\ndata analysis tasks and 74 data modeling tasks, sourced from Eloquence and\nKaggle competitions. DSBench offers a realistic setting by encompassing long\ncontexts, multimodal task backgrounds, reasoning with large data files and\nmulti-table structures, and performing end-to-end data modeling tasks. Our\nevaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle\nwith most tasks, with the best agent solving only 34.12% of data analysis tasks\nand achieving a 34.74% Relative Performance Gap (RPG). These findings\nunderscore the need for further advancements in developing more practical,\nintelligent, and autonomous data science agents."
                },
                "authors": [
                    {
                        "name": "Liqiang Jing"
                    },
                    {
                        "name": "Zhehui Huang"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Wenlin Yao"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Kaixin Ma"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Xinya Du"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07693v1",
                "updated": "2024-09-12T01:55:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    1,
                    55,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T01:55:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    1,
                    55,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "Cooperative Inference with Interleaved Operator Partitioning for CNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Inference with Interleaved Operator Partitioning for CNNs"
                },
                "summary": "Deploying deep learning models on Internet of Things (IoT) devices often\nfaces challenges due to limited memory resources and computing capabilities.\nCooperative inference is an important method for addressing this issue,\nrequiring the partitioning and distributive deployment of an intelligent model.\nTo perform horizontal partitions, existing cooperative inference methods take\neither the output channel of operators or the height and width of feature maps\nas the partition dimensions. In this manner, since the activation of operators\nis distributed, they have to be concatenated together before being fed to the\nnext operator, which incurs the delay for cooperative inference. In this paper,\nwe propose the Interleaved Operator Partitioning (IOP) strategy for CNN models.\nBy partitioning an operator based on the output channel dimension and its\nsuccessive operator based on the input channel dimension, activation\nconcatenation becomes unnecessary, thereby reducing the number of communication\nconnections, which consequently reduces cooperative inference de-lay. Based on\nIOP, we further present a model segmentation algorithm for minimizing\ncooperative inference time, which greedily selects operators for IOP pairing\nbased on the inference delay benefit harvested. Experimental results\ndemonstrate that compared with the state-of-the-art partition approaches used\nin CoEdge, the IOP strategy achieves 6.39% ~ 16.83% faster acceleration and\nreduces peak memory footprint by 21.22% ~ 49.98% for three classical image\nclassification models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying deep learning models on Internet of Things (IoT) devices often\nfaces challenges due to limited memory resources and computing capabilities.\nCooperative inference is an important method for addressing this issue,\nrequiring the partitioning and distributive deployment of an intelligent model.\nTo perform horizontal partitions, existing cooperative inference methods take\neither the output channel of operators or the height and width of feature maps\nas the partition dimensions. In this manner, since the activation of operators\nis distributed, they have to be concatenated together before being fed to the\nnext operator, which incurs the delay for cooperative inference. In this paper,\nwe propose the Interleaved Operator Partitioning (IOP) strategy for CNN models.\nBy partitioning an operator based on the output channel dimension and its\nsuccessive operator based on the input channel dimension, activation\nconcatenation becomes unnecessary, thereby reducing the number of communication\nconnections, which consequently reduces cooperative inference de-lay. Based on\nIOP, we further present a model segmentation algorithm for minimizing\ncooperative inference time, which greedily selects operators for IOP pairing\nbased on the inference delay benefit harvested. Experimental results\ndemonstrate that compared with the state-of-the-art partition approaches used\nin CoEdge, the IOP strategy achieves 6.39% ~ 16.83% faster acceleration and\nreduces peak memory footprint by 21.22% ~ 49.98% for three classical image\nclassification models."
                },
                "authors": [
                    {
                        "name": "Zhibang Liu"
                    },
                    {
                        "name": "Chaonong Xu"
                    },
                    {
                        "name": "Zhizhuo Liu"
                    },
                    {
                        "name": "Lekai Huang"
                    },
                    {
                        "name": "Jiachen Wei"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05448v2",
                "updated": "2024-09-12T01:32:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    1,
                    32,
                    25,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-09T09:04:56Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    9,
                    4,
                    56,
                    0,
                    253,
                    0
                ],
                "title": "Representational Analysis of Binding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representational Analysis of Binding in Large Language Models"
                },
                "summary": "Entity tracking is essential for complex reasoning. To perform in-context\nentity tracking, language models (LMs) must bind an entity to its attribute\n(e.g., bind a container to its content) to recall attribute for a given entity.\nFor example, given a context mentioning ``The coffee is in Box Z, the stone is\nin Box M, the map is in Box H'', to infer ``Box Z contains the coffee'' later,\nLMs must bind ``Box Z'' to ``coffee''. To explain the binding behaviour of LMs,\nFeng and Steinhardt (2023) introduce a Binding ID mechanism and state that LMs\nuse a abstract concept called Binding ID (BI) to internally mark\nentity-attribute pairs. However, they have not directly captured the BI\ndeterminant information from entity activations. In this work, we provide a\nnovel view of the Binding ID mechanism by localizing the prototype of BI\ninformation. Specifically, we discover that there exists a low-rank subspace in\nthe hidden state (or activation) of LMs, that primarily encodes the order of\nentity and attribute and which is used as the prototype of BI to causally\ndetermine the binding. To identify this subspace, we choose principle component\nanalysis as our first attempt and it is empirically proven to be effective.\nMoreover, we also discover that when editing representations along directions\nin the subspace, LMs tend to bind a given entity to other attributes\naccordingly. For example, by patching activations along the BI encoding\ndirection we can make the LM to infer ``Box Z contains the stone'' and ``Box Z\ncontains the map''.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity tracking is essential for complex reasoning. To perform in-context\nentity tracking, language models (LMs) must bind an entity to its attribute\n(e.g., bind a container to its content) to recall attribute for a given entity.\nFor example, given a context mentioning ``The coffee is in Box Z, the stone is\nin Box M, the map is in Box H'', to infer ``Box Z contains the coffee'' later,\nLMs must bind ``Box Z'' to ``coffee''. To explain the binding behaviour of LMs,\nFeng and Steinhardt (2023) introduce a Binding ID mechanism and state that LMs\nuse a abstract concept called Binding ID (BI) to internally mark\nentity-attribute pairs. However, they have not directly captured the BI\ndeterminant information from entity activations. In this work, we provide a\nnovel view of the Binding ID mechanism by localizing the prototype of BI\ninformation. Specifically, we discover that there exists a low-rank subspace in\nthe hidden state (or activation) of LMs, that primarily encodes the order of\nentity and attribute and which is used as the prototype of BI to causally\ndetermine the binding. To identify this subspace, we choose principle component\nanalysis as our first attempt and it is empirically proven to be effective.\nMoreover, we also discover that when editing representations along directions\nin the subspace, LMs tend to bind a given entity to other attributes\naccordingly. For example, by patching activations along the BI encoding\ndirection we can make the LM to infer ``Box Z contains the stone'' and ``Box Z\ncontains the map''."
                },
                "authors": [
                    {
                        "name": "Qin Dai"
                    },
                    {
                        "name": "Benjamin Heinzerling"
                    },
                    {
                        "name": "Kentaro Inui"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Inui"
                },
                "author": "Kentaro Inui",
                "arxiv_comment": "The key phrase \"BI Subspace\" might be misleading, because it sounds\n  like the subspace that directly encodes BI, and which is different with its\n  intended meaning that the subspace that is the base (or prototype) of BI.\n  Therefore, the naming of the subspace and its corresponding wording needs\n  further discussion and review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15680v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15680v4",
                "updated": "2024-09-12T00:47:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    0,
                    47,
                    6,
                    3,
                    256,
                    0
                ],
                "published": "2024-01-28T15:25:04Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    15,
                    25,
                    4,
                    6,
                    28,
                    0
                ],
                "title": "How to achieve model-robust inference in stepped wedge trials with\n  model-based methods?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to achieve model-robust inference in stepped wedge trials with\n  model-based methods?"
                },
                "summary": "A stepped wedge design is a unidirectional crossover design where clusters\nare randomized to distinct treatment sequences. While model-based analysis of\nstepped wedge designs is standard practice to evaluate treatment effects\naccounting for clustering and adjusting for covariates, their properties under\nmisspecification have not been systematically explored. In this article, we\nfocus on model-based methods, including linear mixed models and generalized\nestimating equations with an independence, simple exchangeable, or nested\nexchangeable working correlation structure. We study when a potentially\nmisspecified working model can offer consistent estimation of the marginal\ntreatment effect estimands, which are defined nonparametrically with potential\noutcomes and may be functions of calendar time and/or exposure time. We prove a\ncentral result that consistency for nonparametric estimands usually requires a\ncorrectly specified treatment effect structure, but generally not the remaining\naspects of the working model (functional form of covariates, random effects,\nand residual distribution), and valid inference is obtained via the sandwich\nvariance estimator. Furthermore, an additional g-computation step is required\nto achieve model-robust inference under non-identity link functions or for\nratio estimands. The theoretical results are illustrated via several simulation\nexperiments and re-analysis of a completed stepped wedge cluster randomized\ntrial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A stepped wedge design is a unidirectional crossover design where clusters\nare randomized to distinct treatment sequences. While model-based analysis of\nstepped wedge designs is standard practice to evaluate treatment effects\naccounting for clustering and adjusting for covariates, their properties under\nmisspecification have not been systematically explored. In this article, we\nfocus on model-based methods, including linear mixed models and generalized\nestimating equations with an independence, simple exchangeable, or nested\nexchangeable working correlation structure. We study when a potentially\nmisspecified working model can offer consistent estimation of the marginal\ntreatment effect estimands, which are defined nonparametrically with potential\noutcomes and may be functions of calendar time and/or exposure time. We prove a\ncentral result that consistency for nonparametric estimands usually requires a\ncorrectly specified treatment effect structure, but generally not the remaining\naspects of the working model (functional form of covariates, random effects,\nand residual distribution), and valid inference is obtained via the sandwich\nvariance estimator. Furthermore, an additional g-computation step is required\nto achieve model-robust inference under non-identity link functions or for\nratio estimands. The theoretical results are illustrated via several simulation\nexperiments and re-analysis of a completed stepped wedge cluster randomized\ntrial."
                },
                "authors": [
                    {
                        "name": "Bingkai Wang"
                    },
                    {
                        "name": "Xueqi Wang"
                    },
                    {
                        "name": "Fan Li"
                    }
                ],
                "author_detail": {
                    "name": "Fan Li"
                },
                "author": "Fan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15680v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15680v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06852v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06852v4",
                "updated": "2024-09-12T00:27:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    0,
                    27,
                    6,
                    3,
                    256,
                    0
                ],
                "published": "2024-06-10T23:54:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    23,
                    54,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "A Survey of Backdoor Attacks and Defenses on Large Language Models:\n  Implications for Security Measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Backdoor Attacks and Defenses on Large Language Models:\n  Implications for Security Measures"
                },
                "summary": "Large Language Models (LLMs), which bridge the gap between human language\nunderstanding and complex problem-solving, achieve state-of-the-art performance\non several NLP tasks, particularly in few-shot and zero-shot settings. Despite\nthe demonstrable efficacy of LLMs, due to constraints on computational\nresources, users have to engage with open-source language models or outsource\nthe entire training process to third-party platforms. However, research has\ndemonstrated that language models are susceptible to potential security\nvulnerabilities, particularly in backdoor attacks. Backdoor attacks are\ndesigned to introduce targeted vulnerabilities into language models by\npoisoning training samples or model weights, allowing attackers to manipulate\nmodel responses through malicious triggers. While existing surveys on backdoor\nattacks provide a comprehensive overview, they lack an in-depth examination of\nbackdoor attacks specifically targeting LLMs. To bridge this gap and grasp the\nlatest trends in the field, this paper presents a novel perspective on backdoor\nattacks for LLMs by focusing on fine-tuning methods. Specifically, we\nsystematically classify backdoor attacks into three categories: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on\ninsights from a substantial review, we also discuss crucial issues for future\nresearch on backdoor attacks, such as further exploring attack algorithms that\ndo not require fine-tuning, or developing more covert attack algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), which bridge the gap between human language\nunderstanding and complex problem-solving, achieve state-of-the-art performance\non several NLP tasks, particularly in few-shot and zero-shot settings. Despite\nthe demonstrable efficacy of LLMs, due to constraints on computational\nresources, users have to engage with open-source language models or outsource\nthe entire training process to third-party platforms. However, research has\ndemonstrated that language models are susceptible to potential security\nvulnerabilities, particularly in backdoor attacks. Backdoor attacks are\ndesigned to introduce targeted vulnerabilities into language models by\npoisoning training samples or model weights, allowing attackers to manipulate\nmodel responses through malicious triggers. While existing surveys on backdoor\nattacks provide a comprehensive overview, they lack an in-depth examination of\nbackdoor attacks specifically targeting LLMs. To bridge this gap and grasp the\nlatest trends in the field, this paper presents a novel perspective on backdoor\nattacks for LLMs by focusing on fine-tuning methods. Specifically, we\nsystematically classify backdoor attacks into three categories: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on\ninsights from a substantial review, we also discuss crucial issues for future\nresearch on backdoor attacks, such as further exploring attack algorithms that\ndo not require fine-tuning, or developing more covert attack algorithms."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Meihuizi Jia"
                    },
                    {
                        "name": "Zhongliang Guo"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Xiaoyu Xu"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Yichao Feng"
                    },
                    {
                        "name": "Fengjun Pan"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    }
                ],
                "author_detail": {
                    "name": "Luu Anh Tuan"
                },
                "author": "Luu Anh Tuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06852v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06852v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08365v2",
                "updated": "2024-09-12T00:19:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    0,
                    19,
                    19,
                    3,
                    256,
                    0
                ],
                "published": "2024-04-12T10:09:06Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    10,
                    9,
                    6,
                    4,
                    103,
                    0
                ],
                "title": "Estimation and Inference for Three-Dimensional Panel Data Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation and Inference for Three-Dimensional Panel Data Models"
                },
                "summary": "Hierarchical panel data models have recently garnered significant attention.\nThis study contributes to the relevant literature by introducing a novel\nthree-dimensional (3D) hierarchical panel data model, which integrates panel\nregression with three sets of latent factor structures: one set of global\nfactors and two sets of local factors. Instead of aggregating latent factors\nfrom various nodes, as seen in the literature of distributed principal\ncomponent analysis (PCA), we propose an estimation approach capable of\nrecovering the parameters of interest and disentangling latent factors at\ndifferent levels and across different dimensions. We establish an asymptotic\ntheory and provide a bootstrap procedure to obtain inference for the parameters\nof interest while accommodating various types of cross-sectional dependence and\ntime series autocorrelation. Finally, we demonstrate the applicability of our\nframework by examining productivity convergence in manufacturing industries\nworldwide.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical panel data models have recently garnered significant attention.\nThis study contributes to the relevant literature by introducing a novel\nthree-dimensional (3D) hierarchical panel data model, which integrates panel\nregression with three sets of latent factor structures: one set of global\nfactors and two sets of local factors. Instead of aggregating latent factors\nfrom various nodes, as seen in the literature of distributed principal\ncomponent analysis (PCA), we propose an estimation approach capable of\nrecovering the parameters of interest and disentangling latent factors at\ndifferent levels and across different dimensions. We establish an asymptotic\ntheory and provide a bootstrap procedure to obtain inference for the parameters\nof interest while accommodating various types of cross-sectional dependence and\ntime series autocorrelation. Finally, we demonstrate the applicability of our\nframework by examining productivity convergence in manufacturing industries\nworldwide."
                },
                "authors": [
                    {
                        "name": "Guohua Feng"
                    },
                    {
                        "name": "Jiti Gao"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Bin Peng"
                    }
                ],
                "author_detail": {
                    "name": "Bin Peng"
                },
                "author": "Bin Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07667v1",
                "updated": "2024-09-11T23:59:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    23,
                    59,
                    59,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T23:59:59Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    23,
                    59,
                    59,
                    2,
                    255,
                    0
                ],
                "title": "Unsupervised anomaly detection in spatio-temporal stream network sensor\n  data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised anomaly detection in spatio-temporal stream network sensor\n  data"
                },
                "summary": "The use of in-situ digital sensors for water quality monitoring is becoming\nincreasingly common worldwide. While these sensors provide near real-time data\nfor science, the data are prone to technical anomalies that can undermine the\ntrustworthiness of the data and the accuracy of statistical inferences,\nparticularly in spatial and temporal analyses. Here we propose a framework for\ndetecting anomalies in sensor data recorded in stream networks, which takes\nadvantage of spatial and temporal autocorrelation to improve detection rates.\nThe proposed framework involves the implementation of effective data imputation\nto handle missing data, alignment of time-series to address temporal\ndisparities, and the identification of water quality events. We explore the\neffectiveness of a suite of state-of-the-art statistical methods including\nposterior predictive distributions, finite mixtures, and Hidden Markov Models\n(HMM). We showcase the practical implementation of automated anomaly detection\nin near-real time by employing a Bayesian recursive approach. This\ndemonstration is conducted through a comprehensive simulation study and a\npractical application to a substantive case study situated in the Herbert\nRiver, located in Queensland, Australia, which flows into the Great Barrier\nReef. We found that methods such as posterior predictive distributions and HMM\nproduce the best performance in detecting multiple types of anomalies.\nUtilizing data from multiple sensors deployed relatively near one another\nenhances the ability to distinguish between water quality events and technical\nanomalies, thereby significantly improving the accuracy of anomaly detection.\nThus, uncertainty and biases in water quality reporting, interpretation, and\nmodelling are reduced, and the effectiveness of subsequent management actions\nimproved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of in-situ digital sensors for water quality monitoring is becoming\nincreasingly common worldwide. While these sensors provide near real-time data\nfor science, the data are prone to technical anomalies that can undermine the\ntrustworthiness of the data and the accuracy of statistical inferences,\nparticularly in spatial and temporal analyses. Here we propose a framework for\ndetecting anomalies in sensor data recorded in stream networks, which takes\nadvantage of spatial and temporal autocorrelation to improve detection rates.\nThe proposed framework involves the implementation of effective data imputation\nto handle missing data, alignment of time-series to address temporal\ndisparities, and the identification of water quality events. We explore the\neffectiveness of a suite of state-of-the-art statistical methods including\nposterior predictive distributions, finite mixtures, and Hidden Markov Models\n(HMM). We showcase the practical implementation of automated anomaly detection\nin near-real time by employing a Bayesian recursive approach. This\ndemonstration is conducted through a comprehensive simulation study and a\npractical application to a substantive case study situated in the Herbert\nRiver, located in Queensland, Australia, which flows into the Great Barrier\nReef. We found that methods such as posterior predictive distributions and HMM\nproduce the best performance in detecting multiple types of anomalies.\nUtilizing data from multiple sensors deployed relatively near one another\nenhances the ability to distinguish between water quality events and technical\nanomalies, thereby significantly improving the accuracy of anomaly detection.\nThus, uncertainty and biases in water quality reporting, interpretation, and\nmodelling are reduced, and the effectiveness of subsequent management actions\nimproved."
                },
                "authors": [
                    {
                        "name": "Edgar Santos-Fernandez"
                    },
                    {
                        "name": "Jay M. Ver Hoef"
                    },
                    {
                        "name": "Erin E. Peterson"
                    },
                    {
                        "name": "James McGree"
                    },
                    {
                        "name": "Cesar A. Villa"
                    },
                    {
                        "name": "Catherine Leigh"
                    },
                    {
                        "name": "Ryan Turner"
                    },
                    {
                        "name": "Cameron Roberts"
                    },
                    {
                        "name": "Kerrie Mengersen"
                    }
                ],
                "author_detail": {
                    "name": "Kerrie Mengersen"
                },
                "author": "Kerrie Mengersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07109v2",
                "updated": "2024-09-11T23:15:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    23,
                    15,
                    44,
                    2,
                    255,
                    0
                ],
                "published": "2023-10-11T01:11:30Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    1,
                    11,
                    30,
                    2,
                    284,
                    0
                ],
                "title": "SparseCoder: Advancing Source Code Analysis with Sparse Attention and\n  Learned Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseCoder: Advancing Source Code Analysis with Sparse Attention and\n  Learned Token Pruning"
                },
                "summary": "As software projects rapidly evolve, software artifacts become more complex\nand defects behind get harder to identify. The emerging Transformer-based\napproaches, though achieving remarkable performance, struggle with long code\nsequences due to their self-attention mechanism, which scales quadratically\nwith the sequence length. This paper introduces SparseCoder, an innovative\napproach incorporating sparse attention and learned token pruning (LTP) method\n(adapted from natural language processing) to address this limitation. Compared\nto previous state-of-the-art models CodeBERT, RoBERTa, and CodeT5, our\nexperiments demonstrate that SparseCoder can handle significantly longer input\nsequences--at least twice as long, within the limits of our hardware resources\nand data statistics. Additionally, SparseCoder is four times faster than other\nmethods measured in runtime, achieving a 50% reduction in floating point\noperations per second (FLOPs) with a negligible performance drop of less than\n1% compared to Transformers using sparse attention (Sparse Atten). Plotting\nFLOPs of model inference against token lengths reveals that SparseCoder scales\nlinearly, whereas other methods, including the current state-of-the-art model\nCodeT5, scale quadratically. Moreover, SparseCoder enhances interpretability by\nvisualizing non-trivial tokens layer-wise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As software projects rapidly evolve, software artifacts become more complex\nand defects behind get harder to identify. The emerging Transformer-based\napproaches, though achieving remarkable performance, struggle with long code\nsequences due to their self-attention mechanism, which scales quadratically\nwith the sequence length. This paper introduces SparseCoder, an innovative\napproach incorporating sparse attention and learned token pruning (LTP) method\n(adapted from natural language processing) to address this limitation. Compared\nto previous state-of-the-art models CodeBERT, RoBERTa, and CodeT5, our\nexperiments demonstrate that SparseCoder can handle significantly longer input\nsequences--at least twice as long, within the limits of our hardware resources\nand data statistics. Additionally, SparseCoder is four times faster than other\nmethods measured in runtime, achieving a 50% reduction in floating point\noperations per second (FLOPs) with a negligible performance drop of less than\n1% compared to Transformers using sparse attention (Sparse Atten). Plotting\nFLOPs of model inference against token lengths reveals that SparseCoder scales\nlinearly, whereas other methods, including the current state-of-the-art model\nCodeT5, scale quadratically. Moreover, SparseCoder enhances interpretability by\nvisualizing non-trivial tokens layer-wise."
                },
                "authors": [
                    {
                        "name": "Xueqi Yang"
                    },
                    {
                        "name": "Mariusz Jakubowski"
                    },
                    {
                        "name": "Li Kang"
                    },
                    {
                        "name": "Haojie Yu"
                    },
                    {
                        "name": "Tim Menzies"
                    }
                ],
                "author_detail": {
                    "name": "Tim Menzies"
                },
                "author": "Tim Menzies",
                "arxiv_comment": "34 pages, 9 figures, pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04732v2",
                "updated": "2024-09-11T23:12:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    23,
                    12,
                    53,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-07T06:33:12Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    6,
                    33,
                    12,
                    5,
                    251,
                    0
                ],
                "title": "VidLPRO: A $\\underline{Vid}$eo-$\\underline{L}$anguage\n  $\\underline{P}$re-training Framework for $\\underline{Ro}$botic and\n  Laparoscopic Surgery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidLPRO: A $\\underline{Vid}$eo-$\\underline{L}$anguage\n  $\\underline{P}$re-training Framework for $\\underline{Ro}$botic and\n  Laparoscopic Surgery"
                },
                "summary": "We introduce VidLPRO, a novel video-language (VL) pre-training framework\ndesigned specifically for robotic and laparoscopic surgery. While existing\nsurgical VL models primarily rely on contrastive learning, we propose a more\ncomprehensive approach to capture the intricate temporal dynamics and align\nvideo with language. VidLPRO integrates video-text contrastive learning,\nvideo-text matching, and masked language modeling objectives to learn rich VL\nrepresentations. To support this framework, we present GenSurg+, a carefully\ncurated dataset derived from GenSurgery, comprising 17k surgical video clips\npaired with captions generated by GPT-4 using transcripts extracted by the\nWhisper model. This dataset addresses the need for large-scale, high-quality VL\ndata in the surgical domain. Extensive experiments on benchmark datasets,\nincluding Cholec80 and AutoLaparo, demonstrate the efficacy of our approach.\nVidLPRO achieves state-of-the-art performance in zero-shot surgical phase\nrecognition, significantly outperforming existing surgical VL models such as\nSurgVLP and HecVL. Our model demonstrates improvements of up to 21.5\\% in\naccuracy and 15.7% in F1 score, setting a new benchmark in the field. Notably,\nVidLPRO exhibits robust performance even with single-frame inference, while\neffectively scaling with increased temporal context. Ablation studies reveal\nthe impact of frame sampling strategies on model performance and computational\nefficiency. These results underscore VidLPRO's potential as a foundation model\nfor surgical video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce VidLPRO, a novel video-language (VL) pre-training framework\ndesigned specifically for robotic and laparoscopic surgery. While existing\nsurgical VL models primarily rely on contrastive learning, we propose a more\ncomprehensive approach to capture the intricate temporal dynamics and align\nvideo with language. VidLPRO integrates video-text contrastive learning,\nvideo-text matching, and masked language modeling objectives to learn rich VL\nrepresentations. To support this framework, we present GenSurg+, a carefully\ncurated dataset derived from GenSurgery, comprising 17k surgical video clips\npaired with captions generated by GPT-4 using transcripts extracted by the\nWhisper model. This dataset addresses the need for large-scale, high-quality VL\ndata in the surgical domain. Extensive experiments on benchmark datasets,\nincluding Cholec80 and AutoLaparo, demonstrate the efficacy of our approach.\nVidLPRO achieves state-of-the-art performance in zero-shot surgical phase\nrecognition, significantly outperforming existing surgical VL models such as\nSurgVLP and HecVL. Our model demonstrates improvements of up to 21.5\\% in\naccuracy and 15.7% in F1 score, setting a new benchmark in the field. Notably,\nVidLPRO exhibits robust performance even with single-frame inference, while\neffectively scaling with increased temporal context. Ablation studies reveal\nthe impact of frame sampling strategies on model performance and computational\nefficiency. These results underscore VidLPRO's potential as a foundation model\nfor surgical video understanding."
                },
                "authors": [
                    {
                        "name": "Mohammadmahdi Honarmand"
                    },
                    {
                        "name": "Muhammad Abdullah Jamal"
                    },
                    {
                        "name": "Omid Mohareri"
                    }
                ],
                "author_detail": {
                    "name": "Omid Mohareri"
                },
                "author": "Omid Mohareri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07641v1",
                "updated": "2024-09-11T21:53:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    53,
                    20,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T21:53:20Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    53,
                    20,
                    2,
                    255,
                    0
                ],
                "title": "SimulBench: Evaluating Language Models with Creative Simulation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimulBench: Evaluating Language Models with Creative Simulation Tasks"
                },
                "summary": "We introduce SimulBench, a benchmark designed to evaluate large language\nmodels (LLMs) across a diverse collection of creative simulation scenarios,\nsuch as acting as a Linux terminal or playing text games with users. While\nthese simulation tasks serve as effective measures of an LLM's general\nintelligence, they are seldom incorporated into existing benchmarks. A major\nchallenge is to develop an evaluation framework for testing different LLMs\nfairly while preserving the multi-round interactive nature of simulation tasks\nbetween users and AI. To tackle this issue, we suggest using a fixed LLM as a\nuser agent to engage with an LLM to collect dialogues first under different\ntasks. Then, challenging dialogue scripts are extracted for evaluating\ndifferent target LLMs. To facilitate automatic assessment on \\DataName{}, GPT-4\nis employed as the evaluator, tasked with reviewing the quality of the final\nresponse generated by the target LLMs given multi-turn dialogue scripts. Our\ncomprehensive experiments indicate that these simulation tasks continue to pose\na significant challenge with their unique natures and show the gap between\nproprietary models and the most advanced open LLMs. For example, GPT-4-turbo\noutperforms LLaMA-3-70b-Chat on 18.55\\% more cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SimulBench, a benchmark designed to evaluate large language\nmodels (LLMs) across a diverse collection of creative simulation scenarios,\nsuch as acting as a Linux terminal or playing text games with users. While\nthese simulation tasks serve as effective measures of an LLM's general\nintelligence, they are seldom incorporated into existing benchmarks. A major\nchallenge is to develop an evaluation framework for testing different LLMs\nfairly while preserving the multi-round interactive nature of simulation tasks\nbetween users and AI. To tackle this issue, we suggest using a fixed LLM as a\nuser agent to engage with an LLM to collect dialogues first under different\ntasks. Then, challenging dialogue scripts are extracted for evaluating\ndifferent target LLMs. To facilitate automatic assessment on \\DataName{}, GPT-4\nis employed as the evaluator, tasked with reviewing the quality of the final\nresponse generated by the target LLMs given multi-turn dialogue scripts. Our\ncomprehensive experiments indicate that these simulation tasks continue to pose\na significant challenge with their unique natures and show the gap between\nproprietary models and the most advanced open LLMs. For example, GPT-4-turbo\noutperforms LLaMA-3-70b-Chat on 18.55\\% more cases."
                },
                "authors": [
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Bill Yuchen Lin"
                },
                "author": "Bill Yuchen Lin",
                "arxiv_comment": "Website: https://simulbench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03166v2",
                "updated": "2024-09-11T21:52:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    52,
                    22,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-05T01:51:54Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    51,
                    54,
                    3,
                    249,
                    0
                ],
                "title": "Continual Skill and Task Learning via Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Skill and Task Learning via Dialogue"
                },
                "summary": "Continual and interactive robot learning is a challenging problem as the\nrobot is present with human users who expect the robot to learn novel skills to\nsolve novel tasks perpetually with sample efficiency. In this work we present a\nframework for robots to query and learn visuo-motor robot skills and task\nrelevant information via natural language dialog interactions with human users.\nPrevious approaches either focus on improving the performance of instruction\nfollowing agents, or passively learn novel skills or concepts. Instead, we used\ndialog combined with a language-skill grounding embedding to query or confirm\nskills and/or tasks requested by a user. To achieve this goal, we developed and\nintegrated three different components for our agent. Firstly, we propose a\nnovel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA),\nwhich enables the existing SoTA ACT model to perform few-shot continual\nlearning. Secondly, we develop an alignment model that projects demonstrations\nacross skill embodiments into a shared embedding allowing us to know when to\nask questions and/or demonstrations from users. Finally, we integrated an\nexisting LLM to interact with a human user to perform grounded interactive\ncontinual skill learning to solve a task. Our ACT-LoRA model learns novel\nfine-tuned skills with a 100% accuracy when trained with only five\ndemonstrations for a novel skill while still maintaining a 74.75% accuracy on\npre-trained skills in the RLBench dataset where other models fall significantly\nshort. We also performed a human-subjects study with 8 subjects to demonstrate\nthe continual learning capabilities of our combined framework. We achieve a\nsuccess rate of 75% in the task of sandwich making with the real robot learning\nfrom participant data demonstrating that robots can learn novel skills or task\nknowledge from dialogue with non-expert users using our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual and interactive robot learning is a challenging problem as the\nrobot is present with human users who expect the robot to learn novel skills to\nsolve novel tasks perpetually with sample efficiency. In this work we present a\nframework for robots to query and learn visuo-motor robot skills and task\nrelevant information via natural language dialog interactions with human users.\nPrevious approaches either focus on improving the performance of instruction\nfollowing agents, or passively learn novel skills or concepts. Instead, we used\ndialog combined with a language-skill grounding embedding to query or confirm\nskills and/or tasks requested by a user. To achieve this goal, we developed and\nintegrated three different components for our agent. Firstly, we propose a\nnovel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA),\nwhich enables the existing SoTA ACT model to perform few-shot continual\nlearning. Secondly, we develop an alignment model that projects demonstrations\nacross skill embodiments into a shared embedding allowing us to know when to\nask questions and/or demonstrations from users. Finally, we integrated an\nexisting LLM to interact with a human user to perform grounded interactive\ncontinual skill learning to solve a task. Our ACT-LoRA model learns novel\nfine-tuned skills with a 100% accuracy when trained with only five\ndemonstrations for a novel skill while still maintaining a 74.75% accuracy on\npre-trained skills in the RLBench dataset where other models fall significantly\nshort. We also performed a human-subjects study with 8 subjects to demonstrate\nthe continual learning capabilities of our combined framework. We achieve a\nsuccess rate of 75% in the task of sandwich making with the real robot learning\nfrom participant data demonstrating that robots can learn novel skills or task\nknowledge from dialogue with non-expert users using our approach."
                },
                "authors": [
                    {
                        "name": "Weiwei Gu"
                    },
                    {
                        "name": "Suresh Kondepudi"
                    },
                    {
                        "name": "Lixiao Huang"
                    },
                    {
                        "name": "Nakul Gopalan"
                    }
                ],
                "author_detail": {
                    "name": "Nakul Gopalan"
                },
                "author": "Nakul Gopalan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07638v1",
                "updated": "2024-09-11T21:48:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    48,
                    33,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T21:48:33Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    48,
                    33,
                    2,
                    255,
                    0
                ],
                "title": "Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4\n  Capabilities"
                },
                "summary": "In this paper we explore evaluation of LLM capabilities. We present\nmeasurements of GPT-4 performance on several deterministic tasks; each task\ninvolves a basic calculation and takes as input parameter some element drawn\nfrom a large well-defined population (e.g., count elements in a list, multiply\ntwo k-digit numbers, etc). We examine several conditions per-task and perform\nenough trials so that statistically significant differences can be detected.\nThis allows us to investigate the sensitivity of task-accuracy both to query\nphrasing and input parameter population. We find that seemingly trivial\nmodifications in the task-prompt or input population can yield differences far\nlarger than can be explained by sampling effects. For example, performance on a\nsimple list-counting task varies with query-phrasing and list-length, but also\nwith list composition (i.e., the thing-to-be-counted) and object frequency\n(e.g., success when an element accounts for $\\approx$ 50\\% of a list is\ndifferent from when it accounts for $\\approx$ 70\\% etc).\n  We conclude that efforts to quantify LLM capabilities easily succumb to the\nlanguage-as-fixed-effect fallacy, where experimental observations are\nimproperly generalized beyond what the data supports. A consequence appears to\nbe that intuitions that have been formed based on interactions with humans form\na very unreliable guide as to which input modifications should ``make no\ndifference'' to LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we explore evaluation of LLM capabilities. We present\nmeasurements of GPT-4 performance on several deterministic tasks; each task\ninvolves a basic calculation and takes as input parameter some element drawn\nfrom a large well-defined population (e.g., count elements in a list, multiply\ntwo k-digit numbers, etc). We examine several conditions per-task and perform\nenough trials so that statistically significant differences can be detected.\nThis allows us to investigate the sensitivity of task-accuracy both to query\nphrasing and input parameter population. We find that seemingly trivial\nmodifications in the task-prompt or input population can yield differences far\nlarger than can be explained by sampling effects. For example, performance on a\nsimple list-counting task varies with query-phrasing and list-length, but also\nwith list composition (i.e., the thing-to-be-counted) and object frequency\n(e.g., success when an element accounts for $\\approx$ 50\\% of a list is\ndifferent from when it accounts for $\\approx$ 70\\% etc).\n  We conclude that efforts to quantify LLM capabilities easily succumb to the\nlanguage-as-fixed-effect fallacy, where experimental observations are\nimproperly generalized beyond what the data supports. A consequence appears to\nbe that intuitions that have been formed based on interactions with humans form\na very unreliable guide as to which input modifications should ``make no\ndifference'' to LLM performance."
                },
                "authors": [
                    {
                        "name": "Thomas Ball"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Cormac Herley"
                    }
                ],
                "author_detail": {
                    "name": "Cormac Herley"
                },
                "author": "Cormac Herley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.07619v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.07619v5",
                "updated": "2024-09-11T21:23:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    23,
                    4,
                    2,
                    255,
                    0
                ],
                "published": "2023-04-15T19:22:37Z",
                "published_parsed": [
                    2023,
                    4,
                    15,
                    19,
                    22,
                    37,
                    5,
                    105,
                    0
                ],
                "title": "Can ChatGPT Forecast Stock Price Movements? Return Predictability and\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can ChatGPT Forecast Stock Price Movements? Return Predictability and\n  Large Language Models"
                },
                "summary": "We document the capability of large language models (LLMs) like ChatGPT to\npredict stock price movements using news headlines, even without direct\nfinancial training. ChatGPT scores significantly predict out-of-sample daily\nstock returns, subsuming traditional methods, and predictability is stronger\namong smaller stocks and following negative news. To explain these findings, we\ndevelop a theoretical model incorporating information capacity constraints,\nunderreaction, limits-to-arbitrage, and LLMs. The model generates several key\npredictions, which we empirically test: (i) it establishes a critical threshold\nin AI capabilities necessary for profitable predictions, (ii) it demonstrates\nthat only advanced LLMs can effectively interpret complex information, and\n(iii) it predicts that widespread LLM adoption can enhance market efficiency.\nOur results suggest that sophisticated return forecasting is an emerging\ncapability of AI systems and that these technologies can alter information\ndiffusion and decision-making processes in financial markets. Finally, we\nintroduce an interpretability framework to evaluate LLMs' reasoning,\ncontributing to AI transparency and economic decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We document the capability of large language models (LLMs) like ChatGPT to\npredict stock price movements using news headlines, even without direct\nfinancial training. ChatGPT scores significantly predict out-of-sample daily\nstock returns, subsuming traditional methods, and predictability is stronger\namong smaller stocks and following negative news. To explain these findings, we\ndevelop a theoretical model incorporating information capacity constraints,\nunderreaction, limits-to-arbitrage, and LLMs. The model generates several key\npredictions, which we empirically test: (i) it establishes a critical threshold\nin AI capabilities necessary for profitable predictions, (ii) it demonstrates\nthat only advanced LLMs can effectively interpret complex information, and\n(iii) it predicts that widespread LLM adoption can enhance market efficiency.\nOur results suggest that sophisticated return forecasting is an emerging\ncapability of AI systems and that these technologies can alter information\ndiffusion and decision-making processes in financial markets. Finally, we\nintroduce an interpretability framework to evaluate LLMs' reasoning,\ncontributing to AI transparency and economic decision-making."
                },
                "authors": [
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Yuehua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yuehua Tang"
                },
                "author": "Yuehua Tang",
                "arxiv_comment": "Previously posted in SSRN\n  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4412788",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.07619v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.07619v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13751v2",
                "updated": "2024-09-11T20:55:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    55,
                    32,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-24T19:12:37Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    19,
                    12,
                    37,
                    2,
                    24,
                    0
                ],
                "title": "A Training Rate and Survival Heuristic for Inference and Robustness\n  Evaluation (TRASHFIRE)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training Rate and Survival Heuristic for Inference and Robustness\n  Evaluation (TRASHFIRE)"
                },
                "summary": "Machine learning models -- deep neural networks in particular -- have\nperformed remarkably well on benchmark datasets across a wide variety of\ndomains. However, the ease of finding adversarial counter-examples remains a\npersistent problem when training times are measured in hours or days and the\ntime needed to find a successful adversarial counter-example is measured in\nseconds. Much work has gone into generating and defending against these\nadversarial counter-examples, however the relative costs of attacks and\ndefences are rarely discussed. Additionally, machine learning research is\nalmost entirely guided by test/train metrics, but these would require billions\nof samples to meet industry standards. The present work addresses the problem\nof understanding and predicting how particular model hyper-parameters influence\nthe performance of a model in the presence of an adversary. The proposed\napproach uses survival models, worst-case examples, and a cost-aware analysis\nto precisely and accurately reject a particular model change during routine\nmodel training procedures rather than relying on real-world deployment,\nexpensive formal verification methods, or accurate simulations of very\ncomplicated systems (\\textit{e.g.}, digitally recreating every part of a car or\na plane). Through an evaluation of many pre-processing techniques, adversarial\ncounter-examples, and neural network configurations, the conclusion is that\ndeeper models do offer marginal gains in survival times compared to more\nshallow counterparts. However, we show that those gains are driven more by the\nmodel inference time than inherent robustness properties. Using the proposed\nmethodology, we show that ResNet is hopelessly insecure against even the\nsimplest of white box attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models -- deep neural networks in particular -- have\nperformed remarkably well on benchmark datasets across a wide variety of\ndomains. However, the ease of finding adversarial counter-examples remains a\npersistent problem when training times are measured in hours or days and the\ntime needed to find a successful adversarial counter-example is measured in\nseconds. Much work has gone into generating and defending against these\nadversarial counter-examples, however the relative costs of attacks and\ndefences are rarely discussed. Additionally, machine learning research is\nalmost entirely guided by test/train metrics, but these would require billions\nof samples to meet industry standards. The present work addresses the problem\nof understanding and predicting how particular model hyper-parameters influence\nthe performance of a model in the presence of an adversary. The proposed\napproach uses survival models, worst-case examples, and a cost-aware analysis\nto precisely and accurately reject a particular model change during routine\nmodel training procedures rather than relying on real-world deployment,\nexpensive formal verification methods, or accurate simulations of very\ncomplicated systems (\\textit{e.g.}, digitally recreating every part of a car or\na plane). Through an evaluation of many pre-processing techniques, adversarial\ncounter-examples, and neural network configurations, the conclusion is that\ndeeper models do offer marginal gains in survival times compared to more\nshallow counterparts. However, we show that those gains are driven more by the\nmodel inference time than inherent robustness properties. Using the proposed\nmethodology, we show that ResNet is hopelessly insecure against even the\nsimplest of white box attacks."
                },
                "authors": [
                    {
                        "name": "Charles Meyers"
                    },
                    {
                        "name": "Mohammad Reza Saleh Sedghpour"
                    },
                    {
                        "name": "Tommy Löfstedt"
                    },
                    {
                        "name": "Erik Elmroth"
                    }
                ],
                "author_detail": {
                    "name": "Erik Elmroth"
                },
                "author": "Erik Elmroth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.13751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07615v1",
                "updated": "2024-09-11T20:55:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    55,
                    12,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T20:55:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    55,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Zero-Shot Machine-Generated Text Detection Using Mixture of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Machine-Generated Text Detection Using Mixture of Large\n  Language Models"
                },
                "summary": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities has vastly increased the\nthreats posed by generative AI technologies by reducing the cost of producing\nharmful, toxic, faked or forged content. In response, various proposals have\nbeen made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a classification problem.\nMost approaches evaluate an input document by a well-chosen detector LLM,\nassuming that low-perplexity scores reliably signal machine-made content. As\nusing one single detector can induce brittleness of performance, we instead\nconsider several and derive a new, theoretically grounded approach to combine\ntheir respective strengths. Our experiments, using a variety of generator LLMs,\nsuggest that our method effectively increases the robustness of detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities has vastly increased the\nthreats posed by generative AI technologies by reducing the cost of producing\nharmful, toxic, faked or forged content. In response, various proposals have\nbeen made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a classification problem.\nMost approaches evaluate an input document by a well-chosen detector LLM,\nassuming that low-perplexity scores reliably signal machine-made content. As\nusing one single detector can induce brittleness of performance, we instead\nconsider several and derive a new, theoretically grounded approach to combine\ntheir respective strengths. Our experiments, using a variety of generator LLMs,\nsuggest that our method effectively increases the robustness of detection."
                },
                "authors": [
                    {
                        "name": "Matthieu Dubois"
                    },
                    {
                        "name": "François Yvon"
                    },
                    {
                        "name": "Pablo Piantanida"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Piantanida"
                },
                "author": "Pablo Piantanida",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07614v1",
                "updated": "2024-09-11T20:54:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    54,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T20:54:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    54,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "FlowSep: Language-Queried Sound Separation with Rectified Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowSep: Language-Queried Sound Separation with Rectified Flow Matching"
                },
                "summary": "Language-queried audio source separation (LASS) focuses on separating sounds\nusing textual descriptions of the desired sources. Current methods mainly use\ndiscriminative approaches, such as time-frequency masking, to separate target\nsounds and minimize interference from other sources. However, these models face\nchallenges when separating overlapping soundtracks, which may lead to artifacts\nsuch as spectral holes or incomplete separation. Rectified flow matching (RFM),\na generative model that establishes linear relations between the distribution\nof data and noise, offers superior theoretical properties and simplicity, but\nhas not yet been explored in sound separation. In this work, we introduce\nFlowSep, a new generative model based on RFM for LASS tasks. FlowSep learns\nlinear flow trajectories from noise to target source features within the\nvariational autoencoder (VAE) latent space. During inference, the RFM-generated\nlatent features are reconstructed into a mel-spectrogram via the pre-trained\nVAE decoder, followed by a pre-trained vocoder to synthesize the waveform.\nTrained on 1,680 hours of audio data, FlowSep outperforms the state-of-the-art\nmodels across multiple benchmarks, as evaluated with subjective and objective\nmetrics. Additionally, our results show that FlowSep surpasses a\ndiffusion-based LASS model in both separation quality and inference efficiency,\nhighlighting its strong potential for audio source separation tasks. Code,\npre-trained models and demos can be found at:\nhttps://audio-agi.github.io/FlowSep_demo/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried audio source separation (LASS) focuses on separating sounds\nusing textual descriptions of the desired sources. Current methods mainly use\ndiscriminative approaches, such as time-frequency masking, to separate target\nsounds and minimize interference from other sources. However, these models face\nchallenges when separating overlapping soundtracks, which may lead to artifacts\nsuch as spectral holes or incomplete separation. Rectified flow matching (RFM),\na generative model that establishes linear relations between the distribution\nof data and noise, offers superior theoretical properties and simplicity, but\nhas not yet been explored in sound separation. In this work, we introduce\nFlowSep, a new generative model based on RFM for LASS tasks. FlowSep learns\nlinear flow trajectories from noise to target source features within the\nvariational autoencoder (VAE) latent space. During inference, the RFM-generated\nlatent features are reconstructed into a mel-spectrogram via the pre-trained\nVAE decoder, followed by a pre-trained vocoder to synthesize the waveform.\nTrained on 1,680 hours of audio data, FlowSep outperforms the state-of-the-art\nmodels across multiple benchmarks, as evaluated with subjective and objective\nmetrics. Additionally, our results show that FlowSep surpasses a\ndiffusion-based LASS model in both separation quality and inference efficiency,\nhighlighting its strong potential for audio source separation tasks. Code,\npre-trained models and demos can be found at:\nhttps://audio-agi.github.io/FlowSep_demo/."
                },
                "authors": [
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Xubo Liu"
                    },
                    {
                        "name": "Haohe Liu"
                    },
                    {
                        "name": "Mark D. Plumbley"
                    },
                    {
                        "name": "Wenwu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Wang"
                },
                "author": "Wenwu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07613v1",
                "updated": "2024-09-11T20:50:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    50,
                    41,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T20:50:41Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    50,
                    41,
                    2,
                    255,
                    0
                ],
                "title": "Token Turing Machines are Efficient Vision Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Turing Machines are Efficient Vision Models"
                },
                "summary": "We propose Vision Token Turing Machines (ViTTM), an efficient, low-latency,\nmemory-augmented Vision Transformer (ViT). Our approach builds on Neural Turing\nMachines and Token Turing Machines, which were applied to NLP and sequential\nvisual understanding tasks. ViTTMs are designed for non-sequential computer\nvision tasks such as image classification and segmentation. Our model creates\ntwo sets of tokens: process tokens and memory tokens; process tokens pass\nthrough encoder blocks and read-write from memory tokens at each encoder block\nin the network, allowing them to store and retrieve information from memory. By\nensuring that there are fewer process tokens than memory tokens, we are able to\nreduce the inference time of the network while maintaining its accuracy. On\nImageNet-1K, the state-of-the-art ViT-B has median latency of 529.5ms and 81.0%\naccuracy, while our ViTTM-B is 56% faster (234.1ms), with 2.4 times fewer\nFLOPs, with an accuracy of 82.9%. On ADE20K semantic segmentation, ViT-B\nachieves 45.65mIoU at 13.8 frame-per-second (FPS) whereas our ViTTM-B model\nacheives a 45.17 mIoU with 26.8 FPS (+94%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Vision Token Turing Machines (ViTTM), an efficient, low-latency,\nmemory-augmented Vision Transformer (ViT). Our approach builds on Neural Turing\nMachines and Token Turing Machines, which were applied to NLP and sequential\nvisual understanding tasks. ViTTMs are designed for non-sequential computer\nvision tasks such as image classification and segmentation. Our model creates\ntwo sets of tokens: process tokens and memory tokens; process tokens pass\nthrough encoder blocks and read-write from memory tokens at each encoder block\nin the network, allowing them to store and retrieve information from memory. By\nensuring that there are fewer process tokens than memory tokens, we are able to\nreduce the inference time of the network while maintaining its accuracy. On\nImageNet-1K, the state-of-the-art ViT-B has median latency of 529.5ms and 81.0%\naccuracy, while our ViTTM-B is 56% faster (234.1ms), with 2.4 times fewer\nFLOPs, with an accuracy of 82.9%. On ADE20K semantic segmentation, ViT-B\nachieves 45.65mIoU at 13.8 frame-per-second (FPS) whereas our ViTTM-B model\nacheives a 45.17 mIoU with 26.8 FPS (+94%)."
                },
                "authors": [
                    {
                        "name": "Purvish Jajal"
                    },
                    {
                        "name": "Nick John Eliopoulos"
                    },
                    {
                        "name": "Benjamin Shiue-Hal Chou"
                    },
                    {
                        "name": "George K. Thiravathukal"
                    },
                    {
                        "name": "James C. Davis"
                    },
                    {
                        "name": "Yung-Hsiang Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yung-Hsiang Lu"
                },
                "author": "Yung-Hsiang Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07605v1",
                "updated": "2024-09-11T20:33:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    33,
                    5,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T20:33:05Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    33,
                    5,
                    2,
                    255,
                    0
                ],
                "title": "Dual scale Residual-Network for turbulent flow sub grid scale resolving:\n  A prior analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual scale Residual-Network for turbulent flow sub grid scale resolving:\n  A prior analysis"
                },
                "summary": "This paper introduces generative Residual Networks (ResNet) as a surrogate\nMachine Learning (ML) tool for Large Eddy Simulation (LES) Sub Grid Scale (SGS)\nresolving. The study investigates the impact of incorporating Dual Scale\nResidual Blocks (DS-RB) within the ResNet architecture. Two LES SGS resolving\nmodels are proposed and tested for prior analysis test cases: a\nsuper-resolution model (SR-ResNet) and a SGS stress tensor inference model\n(SGS-ResNet). The SR-ResNet model task is to upscale LES solutions from coarse\nto finer grids by inferring unresolved SGS velocity fluctuations, exhibiting\nsuccess in preserving high-frequency velocity fluctuation information, and\naligning with higher-resolution LES solutions' energy spectrum. Furthermore,\nemploying DS-RB enhances prediction accuracy and precision of high-frequency\nvelocity fields compared to Single Scale Residual Blocks (SS-RB), evident in\nboth spatial and spectral domains. The SR-ResNet model is tested and trained on\nfiltered/downsampled 2-D LES planar jet injection problems at two Reynolds\nnumbers, two jet configurations, and two upscale ratios. In the case of SGS\nstress tensor inference, both SS-RB and DS-RB exhibit higher prediction\naccuracy over the Smagorinsky model with reference to the true DNS SGS stress\ntensor, with DS-RB-based SGS-ResNet showing stronger statistical alignment with\nDNS data. The SGS-ResNet model is tested on a filtered/downsampled 2-D DNS\nisotropic homogenous decay turbulence problem. The adoption of DS-RB incurs\nnotable increases in network size, training time, and forward inference time,\nwith the network size expanding by over tenfold, and training and forward\ninference times increasing by approximately 0.5 and 3 times, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces generative Residual Networks (ResNet) as a surrogate\nMachine Learning (ML) tool for Large Eddy Simulation (LES) Sub Grid Scale (SGS)\nresolving. The study investigates the impact of incorporating Dual Scale\nResidual Blocks (DS-RB) within the ResNet architecture. Two LES SGS resolving\nmodels are proposed and tested for prior analysis test cases: a\nsuper-resolution model (SR-ResNet) and a SGS stress tensor inference model\n(SGS-ResNet). The SR-ResNet model task is to upscale LES solutions from coarse\nto finer grids by inferring unresolved SGS velocity fluctuations, exhibiting\nsuccess in preserving high-frequency velocity fluctuation information, and\naligning with higher-resolution LES solutions' energy spectrum. Furthermore,\nemploying DS-RB enhances prediction accuracy and precision of high-frequency\nvelocity fields compared to Single Scale Residual Blocks (SS-RB), evident in\nboth spatial and spectral domains. The SR-ResNet model is tested and trained on\nfiltered/downsampled 2-D LES planar jet injection problems at two Reynolds\nnumbers, two jet configurations, and two upscale ratios. In the case of SGS\nstress tensor inference, both SS-RB and DS-RB exhibit higher prediction\naccuracy over the Smagorinsky model with reference to the true DNS SGS stress\ntensor, with DS-RB-based SGS-ResNet showing stronger statistical alignment with\nDNS data. The SGS-ResNet model is tested on a filtered/downsampled 2-D DNS\nisotropic homogenous decay turbulence problem. The adoption of DS-RB incurs\nnotable increases in network size, training time, and forward inference time,\nwith the network size expanding by over tenfold, and training and forward\ninference times increasing by approximately 0.5 and 3 times, respectively."
                },
                "authors": [
                    {
                        "name": "Omar Sallam"
                    },
                    {
                        "name": "Mirjam Fürth"
                    }
                ],
                "author_detail": {
                    "name": "Mirjam Fürth"
                },
                "author": "Mirjam Fürth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.08264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08264v1",
                "updated": "2024-09-12T17:56:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    56,
                    43,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:56:43Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    56,
                    43,
                    3,
                    256,
                    0
                ],
                "title": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale"
                },
                "summary": "Large language models (LLMs) show remarkable potential to act as computer\nagents, enhancing human productivity and software accessibility in multi-modal\ntasks that require planning and reasoning. However, measuring agent performance\nin realistic environments remains a challenge since: (i) most benchmarks are\nlimited to specific modalities or domains (e.g. text-only, web navigation, Q&A,\ncoding) and (ii) full benchmark evaluations are slow (on order of magnitude of\ndays) given the multi-step sequential nature of tasks. To address these\nchallenges, we introduce the Windows Agent Arena: a reproducible, general\nenvironment focusing exclusively on the Windows operating system (OS) where\nagents can operate freely within a real Windows OS and use the same wide range\nof applications, tools, and web browsers available to human users when solving\ntasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse\nWindows tasks across representative domains that require agent abilities in\nplanning, screen understanding, and tool usage. Our benchmark is scalable and\ncan be seamlessly parallelized in Azure for a full benchmark evaluation in as\nlittle as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we\nalso introduce a new multi-modal agent, Navi. Our agent achieves a success rate\nof 19.5% in the Windows domain, compared to 74.5% performance of an unassisted\nhuman. Navi also demonstrates strong performance on another popular web-based\nbenchmark, Mind2Web. We offer extensive quantitative and qualitative analysis\nof Navi's performance, and provide insights into the opportunities for future\nresearch in agent development and data generation using Windows Agent Arena.\n  Webpage: https://microsoft.github.io/WindowsAgentArena\n  Code: https://github.com/microsoft/WindowsAgentArena",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show remarkable potential to act as computer\nagents, enhancing human productivity and software accessibility in multi-modal\ntasks that require planning and reasoning. However, measuring agent performance\nin realistic environments remains a challenge since: (i) most benchmarks are\nlimited to specific modalities or domains (e.g. text-only, web navigation, Q&A,\ncoding) and (ii) full benchmark evaluations are slow (on order of magnitude of\ndays) given the multi-step sequential nature of tasks. To address these\nchallenges, we introduce the Windows Agent Arena: a reproducible, general\nenvironment focusing exclusively on the Windows operating system (OS) where\nagents can operate freely within a real Windows OS and use the same wide range\nof applications, tools, and web browsers available to human users when solving\ntasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse\nWindows tasks across representative domains that require agent abilities in\nplanning, screen understanding, and tool usage. Our benchmark is scalable and\ncan be seamlessly parallelized in Azure for a full benchmark evaluation in as\nlittle as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we\nalso introduce a new multi-modal agent, Navi. Our agent achieves a success rate\nof 19.5% in the Windows domain, compared to 74.5% performance of an unassisted\nhuman. Navi also demonstrates strong performance on another popular web-based\nbenchmark, Mind2Web. We offer extensive quantitative and qualitative analysis\nof Navi's performance, and provide insights into the opportunities for future\nresearch in agent development and data generation using Windows Agent Arena.\n  Webpage: https://microsoft.github.io/WindowsAgentArena\n  Code: https://github.com/microsoft/WindowsAgentArena"
                },
                "authors": [
                    {
                        "name": "Rogerio Bonatti"
                    },
                    {
                        "name": "Dan Zhao"
                    },
                    {
                        "name": "Francesco Bonacci"
                    },
                    {
                        "name": "Dillon Dupont"
                    },
                    {
                        "name": "Sara Abdali"
                    },
                    {
                        "name": "Yinheng Li"
                    },
                    {
                        "name": "Justin Wagle"
                    },
                    {
                        "name": "Kazuhito Koishida"
                    },
                    {
                        "name": "Arthur Bucker"
                    },
                    {
                        "name": "Lawrence Jang"
                    },
                    {
                        "name": "Zack Hui"
                    }
                ],
                "author_detail": {
                    "name": "Zack Hui"
                },
                "author": "Zack Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08250v1",
                "updated": "2024-09-12T17:48:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    48,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:48:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    48,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable\n  Personal Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable\n  Personal Question Answering"
                },
                "summary": "People often capture memories through photos, screenshots, and videos. While\nexisting AI-based tools enable querying this data using natural language, they\nmostly only support retrieving individual pieces of information like certain\nobjects in photos and struggle with answering more complex queries that involve\ninterpreting interconnected memories like event sequences. We conducted a\none-month diary study to collect realistic user queries and generated a\ntaxonomy of necessary contextual information for integrating with captured\nmemories. We then introduce OmniQuery, a novel system that is able to answer\ncomplex personal memory-related questions that require extracting and inferring\ncontextual information. OmniQuery augments single captured memories through\nintegrating scattered contextual information from multiple interconnected\nmemories, retrieves relevant memories, and uses a large language model (LLM) to\ncomprehensive answers. In human evaluations, we show the effectiveness of\nOmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG\nsystem, winning or tying in 74.5% of the time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People often capture memories through photos, screenshots, and videos. While\nexisting AI-based tools enable querying this data using natural language, they\nmostly only support retrieving individual pieces of information like certain\nobjects in photos and struggle with answering more complex queries that involve\ninterpreting interconnected memories like event sequences. We conducted a\none-month diary study to collect realistic user queries and generated a\ntaxonomy of necessary contextual information for integrating with captured\nmemories. We then introduce OmniQuery, a novel system that is able to answer\ncomplex personal memory-related questions that require extracting and inferring\ncontextual information. OmniQuery augments single captured memories through\nintegrating scattered contextual information from multiple interconnected\nmemories, retrieves relevant memories, and uses a large language model (LLM) to\ncomprehensive answers. In human evaluations, we show the effectiveness of\nOmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG\nsystem, winning or tying in 74.5% of the time."
                },
                "authors": [
                    {
                        "name": "Jiahao Nick Li"
                    },
                    {
                        "name": "Zhuohao Jerry Zhang"
                    },
                    {
                        "name": "Jiaju Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jiaju Ma"
                },
                "author": "Jiaju Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08239v1",
                "updated": "2024-09-12T17:39:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    39,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:39:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    39,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real\n  Data Sources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real\n  Data Sources"
                },
                "summary": "Large Language Models still struggle in challenging scenarios that leverage\nstructured data, complex reasoning, or tool usage. In this paper, we propose\nSource2Synth: a new method that can be used for teaching LLMs new skills\nwithout relying on costly human annotations. Source2Synth takes as input a\ncustom data source and produces synthetic data points with intermediate\nreasoning steps grounded in real-world sources. Source2Synth improves the\ndataset quality by discarding low-quality generations based on their\nanswerability. We demonstrate the generality of this approach by applying it to\ntwo challenging domains: we test reasoning abilities in multi-hop question\nanswering (MHQA), and tool usage in tabular question answering (TQA). Our\nmethod improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on\nHotPotQA compared to the fine-tuned baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models still struggle in challenging scenarios that leverage\nstructured data, complex reasoning, or tool usage. In this paper, we propose\nSource2Synth: a new method that can be used for teaching LLMs new skills\nwithout relying on costly human annotations. Source2Synth takes as input a\ncustom data source and produces synthetic data points with intermediate\nreasoning steps grounded in real-world sources. Source2Synth improves the\ndataset quality by discarding low-quality generations based on their\nanswerability. We demonstrate the generality of this approach by applying it to\ntwo challenging domains: we test reasoning abilities in multi-hop question\nanswering (MHQA), and tool usage in tabular question answering (TQA). Our\nmethod improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on\nHotPotQA compared to the fine-tuned baselines."
                },
                "authors": [
                    {
                        "name": "Alisia Lupidi"
                    },
                    {
                        "name": "Carlos Gemmell"
                    },
                    {
                        "name": "Nicola Cancedda"
                    },
                    {
                        "name": "Jane Dwivedi-Yu"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "Maria Lomeli"
                    }
                ],
                "author_detail": {
                    "name": "Maria Lomeli"
                },
                "author": "Maria Lomeli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08234v1",
                "updated": "2024-09-12T17:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    33,
                    6,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T17:33:06Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    17,
                    33,
                    6,
                    3,
                    256,
                    0
                ],
                "title": "LLM Honeypot: Leveraging Large Language Models as Advanced Interactive\n  Honeypot Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Honeypot: Leveraging Large Language Models as Advanced Interactive\n  Honeypot Systems"
                },
                "summary": "The rapid evolution of cyber threats necessitates innovative solutions for\ndetecting and analyzing malicious activity. Honeypots, which are decoy systems\ndesigned to lure and interact with attackers, have emerged as a critical\ncomponent in cybersecurity. In this paper, we present a novel approach to\ncreating realistic and interactive honeypot systems using Large Language Models\n(LLMs). By fine-tuning a pre-trained open-source language model on a diverse\ndataset of attacker-generated commands and responses, we developed a honeypot\ncapable of sophisticated engagement with attackers. Our methodology involved\nseveral key steps: data collection and processing, prompt engineering, model\nselection, and supervised fine-tuning to optimize the model's performance.\nEvaluation through similarity metrics and live deployment demonstrated that our\napproach effectively generates accurate and informative responses. The results\nhighlight the potential of LLMs to revolutionize honeypot technology, providing\ncybersecurity professionals with a powerful tool to detect and analyze\nmalicious activity, thereby enhancing overall security infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of cyber threats necessitates innovative solutions for\ndetecting and analyzing malicious activity. Honeypots, which are decoy systems\ndesigned to lure and interact with attackers, have emerged as a critical\ncomponent in cybersecurity. In this paper, we present a novel approach to\ncreating realistic and interactive honeypot systems using Large Language Models\n(LLMs). By fine-tuning a pre-trained open-source language model on a diverse\ndataset of attacker-generated commands and responses, we developed a honeypot\ncapable of sophisticated engagement with attackers. Our methodology involved\nseveral key steps: data collection and processing, prompt engineering, model\nselection, and supervised fine-tuning to optimize the model's performance.\nEvaluation through similarity metrics and live deployment demonstrated that our\napproach effectively generates accurate and informative responses. The results\nhighlight the potential of LLMs to revolutionize honeypot technology, providing\ncybersecurity professionals with a powerful tool to detect and analyze\nmalicious activity, thereby enhancing overall security infrastructure."
                },
                "authors": [
                    {
                        "name": "Hakan T. Otal"
                    },
                    {
                        "name": "M. Abdullah Canbaz"
                    }
                ],
                "author_detail": {
                    "name": "M. Abdullah Canbaz"
                },
                "author": "M. Abdullah Canbaz",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68M10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; D.4.6; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.09090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.09090v2",
                "updated": "2024-09-12T16:40:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    40,
                    18,
                    3,
                    256,
                    0
                ],
                "published": "2024-01-17T09:51:32Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    9,
                    51,
                    32,
                    2,
                    17,
                    0
                ],
                "title": "Understanding the concerns and choices of public when using large\n  language models for healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the concerns and choices of public when using large\n  language models for healthcare"
                },
                "summary": "Large language models (LLMs) have shown their potential in biomedical fields.\nHowever, how the public uses them for healthcare purposes such as medical Q\\&A,\nself-diagnosis, and daily healthcare information seeking is under-investigated.\nThis paper adopts a mixed-methods approach, including surveys (N=214) and\ninterviews (N=17) to investigate how and why the public uses LLMs for\nhealthcare. We found that participants generally believed LLMs as a healthcare\ntool have gained popularity, and are often used in combination with other\ninformation channels such as search engines and online health communities to\noptimize information quality. Based on the findings, we reflect on the ethical\nand effective use of LLMs for healthcare and propose future research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown their potential in biomedical fields.\nHowever, how the public uses them for healthcare purposes such as medical Q\\&A,\nself-diagnosis, and daily healthcare information seeking is under-investigated.\nThis paper adopts a mixed-methods approach, including surveys (N=214) and\ninterviews (N=17) to investigate how and why the public uses LLMs for\nhealthcare. We found that participants generally believed LLMs as a healthcare\ntool have gained popularity, and are often used in combination with other\ninformation channels such as search engines and online health communities to\noptimize information quality. Based on the findings, we reflect on the ethical\nand effective use of LLMs for healthcare and propose future research\ndirections."
                },
                "authors": [
                    {
                        "name": "Yunpeng Xiao"
                    },
                    {
                        "name": "Kyrie Zhixuan Zhou"
                    },
                    {
                        "name": "Yueqing Liang"
                    },
                    {
                        "name": "Kai Shu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shu"
                },
                "author": "Kai Shu",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.09090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.09090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; K.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14573v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14573v5",
                "updated": "2024-09-12T16:22:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    22,
                    52,
                    3,
                    256,
                    0
                ],
                "published": "2024-07-21T06:27:45Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    6,
                    27,
                    45,
                    6,
                    203,
                    0
                ],
                "title": "Trading Devil Final: Backdoor attack via Stock market and Bayesian\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trading Devil Final: Backdoor attack via Stock market and Bayesian\n  Optimization"
                },
                "summary": "Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs."
                },
                "authors": [
                    {
                        "name": "Orson Mengara"
                    }
                ],
                "author_detail": {
                    "name": "Orson Mengara"
                },
                "author": "Orson Mengara",
                "arxiv_comment": "END (will never be modified again!!) :Jumps-Diffusion and stock\n  market: Better quantify uncertainty in financial simulations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14573v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14573v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08185v1",
                "updated": "2024-09-12T16:20:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    20,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T16:20:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    16,
                    20,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Fine-tuning Large Language Models for Entity Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models for Entity Matching"
                },
                "summary": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and their ability to generalize to unseen entities. Existing\nresearch on using LLMs for entity matching has focused on prompt engineering\nand in-context learning. This paper explores the potential of fine-tuning LLMs\nfor entity matching. We analyze fine-tuning along two dimensions: 1) The\nrepresentation of training examples, where we experiment with adding different\ntypes of LLM-generated explanations to the training set, and 2) the selection\nand generation of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodel's ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o Mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) are a promising alternative to\npre-trained language models for entity matching due to their high zero-shot\nperformance and their ability to generalize to unseen entities. Existing\nresearch on using LLMs for entity matching has focused on prompt engineering\nand in-context learning. This paper explores the potential of fine-tuning LLMs\nfor entity matching. We analyze fine-tuning along two dimensions: 1) The\nrepresentation of training examples, where we experiment with adding different\ntypes of LLM-generated explanations to the training set, and 2) the selection\nand generation of training examples using LLMs. In addition to the matching\nperformance on the source dataset, we investigate how fine-tuning affects the\nmodel's ability to generalize to other in-domain datasets as well as across\ntopical domains. Our experiments show that fine-tuning significantly improves\nthe performance of the smaller models while the results for the larger models\nare mixed. Fine-tuning also improves the generalization to in-domain datasets\nwhile hurting cross-domain transfer. We show that adding structured\nexplanations to the training set has a positive impact on the performance of\nthree out of four LLMs, while the proposed example selection and generation\nmethods only improve the performance of Llama 3.1 8B while decreasing the\nperformance of GPT-4o Mini."
                },
                "authors": [
                    {
                        "name": "Aaron Steiner"
                    },
                    {
                        "name": "Ralph Peeters"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "arxiv_comment": "8 pages, 4 figures. For related code and data, see this\n  https://github.com/wbsg-uni-mannheim/TailorMatch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10551v3",
                "updated": "2024-09-12T15:43:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    43,
                    59,
                    3,
                    256,
                    0
                ],
                "published": "2023-11-17T14:34:45Z",
                "published_parsed": [
                    2023,
                    11,
                    17,
                    14,
                    34,
                    45,
                    4,
                    321,
                    0
                ],
                "title": "A Tutorial on 5G Positioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tutorial on 5G Positioning"
                },
                "summary": "The widespread adoption of the fifth generation (5G) of cellular networks has\nbrought new opportunities for the development of localization-based services.\nHigh-accuracy positioning use cases and functionalities defined by the\nstandards are drawing the interest of vertical industries. In the transition\ntowards the deployment, this paper aims to provide an in-depth tutorial on 5G\npositioning, summarizing the evolutionary path that led to the standardization\nof cellular-based positioning, describing the localization elements in current\nand forthcoming releases of the Third Generation Partnership Project (3GPP)\nstandard, and the major research trends. By providing fundamental notions on\nwireless localization, comprehensive definitions of measurements and\narchitectures, examples of algorithms, and details on simulation approaches,\nthis paper is intended to represent an exhaustive guide for researchers and\npractitioners. Our approach aims to merge practical aspects of enabled use\ncases and related requirements with theoretical methodologies and fundamental\nbounds, allowing to understand the trade-off between system complexity and\nachievable, i.e., tangible, benefits of 5G positioning services. We analyze the\nperformance of 3GPP Rel-16 positioning by standard-compliant simulations in\nrealistic outdoor and indoor propagation environments, investigating the impact\nof the system configuration and the limitations to be resolved for delivering\naccurate positioning solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of the fifth generation (5G) of cellular networks has\nbrought new opportunities for the development of localization-based services.\nHigh-accuracy positioning use cases and functionalities defined by the\nstandards are drawing the interest of vertical industries. In the transition\ntowards the deployment, this paper aims to provide an in-depth tutorial on 5G\npositioning, summarizing the evolutionary path that led to the standardization\nof cellular-based positioning, describing the localization elements in current\nand forthcoming releases of the Third Generation Partnership Project (3GPP)\nstandard, and the major research trends. By providing fundamental notions on\nwireless localization, comprehensive definitions of measurements and\narchitectures, examples of algorithms, and details on simulation approaches,\nthis paper is intended to represent an exhaustive guide for researchers and\npractitioners. Our approach aims to merge practical aspects of enabled use\ncases and related requirements with theoretical methodologies and fundamental\nbounds, allowing to understand the trade-off between system complexity and\nachievable, i.e., tangible, benefits of 5G positioning services. We analyze the\nperformance of 3GPP Rel-16 positioning by standard-compliant simulations in\nrealistic outdoor and indoor propagation environments, investigating the impact\nof the system configuration and the limitations to be resolved for delivering\naccurate positioning solutions."
                },
                "authors": [
                    {
                        "name": "Lorenzo Italiano"
                    },
                    {
                        "name": "Bernardo Camajori Tedeschini"
                    },
                    {
                        "name": "Mattia Brambilla"
                    },
                    {
                        "name": "Huiping Huang"
                    },
                    {
                        "name": "Monica Nicoli"
                    },
                    {
                        "name": "Henk Wymeersch"
                    }
                ],
                "author_detail": {
                    "name": "Henk Wymeersch"
                },
                "author": "Henk Wymeersch",
                "arxiv_doi": "10.1109/COMST.2024.3449031",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/COMST.2024.3449031",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.10551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work has been accepted in IEEE Communications Surveys &\n  Tutorials. Copyright may be transferred without notice, after which this\n  version may no longer be accessible",
                "arxiv_journal_ref": "IEEE Communications Surveys & Tutorials 2024",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08148v1",
                "updated": "2024-09-12T15:43:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    43,
                    10,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:43:10Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    43,
                    10,
                    3,
                    256,
                    0
                ],
                "title": "Faster Speech-LLaMA Inference with Multi-token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Speech-LLaMA Inference with Multi-token Prediction"
                },
                "summary": "Large language models (LLMs) have become proficient at solving a wide variety\nof tasks, including those involving multi-modal inputs. In particular,\ninstantiating an LLM (such as LLaMA) with a speech encoder and training it on\npaired data imparts speech recognition (ASR) abilities to the decoder-only\nmodel, hence called Speech-LLaMA. Nevertheless, due to the sequential nature of\nauto-regressive inference and the relatively large decoder, Speech-LLaMA models\nrequire relatively high inference time. In this work, we propose to speed up\nSpeech-LLaMA inference by predicting multiple tokens in the same decoding step.\nWe explore several model architectures that enable this, and investigate their\nperformance using threshold-based and verification-based inference strategies.\nWe also propose a prefix-based beam search decoding method that allows\nefficient minimum word error rate (MWER) training for such models. We evaluate\nour models on a variety of public benchmarks, where they reduce the number of\ndecoder calls by ~3.2x while maintaining or improving WER performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become proficient at solving a wide variety\nof tasks, including those involving multi-modal inputs. In particular,\ninstantiating an LLM (such as LLaMA) with a speech encoder and training it on\npaired data imparts speech recognition (ASR) abilities to the decoder-only\nmodel, hence called Speech-LLaMA. Nevertheless, due to the sequential nature of\nauto-regressive inference and the relatively large decoder, Speech-LLaMA models\nrequire relatively high inference time. In this work, we propose to speed up\nSpeech-LLaMA inference by predicting multiple tokens in the same decoding step.\nWe explore several model architectures that enable this, and investigate their\nperformance using threshold-based and verification-based inference strategies.\nWe also propose a prefix-based beam search decoding method that allows\nefficient minimum word error rate (MWER) training for such models. We evaluate\nour models on a variety of public benchmarks, where they reduce the number of\ndecoder calls by ~3.2x while maintaining or improving WER performance."
                },
                "authors": [
                    {
                        "name": "Desh Raj"
                    },
                    {
                        "name": "Gil Keren"
                    },
                    {
                        "name": "Junteng Jia"
                    },
                    {
                        "name": "Jay Mahadeokar"
                    },
                    {
                        "name": "Ozlem Kalinli"
                    }
                ],
                "author_detail": {
                    "name": "Ozlem Kalinli"
                },
                "author": "Ozlem Kalinli",
                "arxiv_comment": "Submitted to IEEE ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08147v1",
                "updated": "2024-09-12T15:40:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    40,
                    45,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:40:45Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    40,
                    45,
                    3,
                    256,
                    0
                ],
                "title": "LLM-POTUS Score: A Framework of Analyzing Presidential Debates with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-POTUS Score: A Framework of Analyzing Presidential Debates with\n  Large Language Models"
                },
                "summary": "Large language models have demonstrated remarkable capabilities in natural\nlanguage processing, yet their application to political discourse analysis\nremains underexplored. This paper introduces a novel approach to evaluating\npresidential debate performances using LLMs, addressing the longstanding\nchallenge of objectively assessing debate outcomes. We propose a framework that\nanalyzes candidates' \"Policies, Persona, and Perspective\" (3P) and how they\nresonate with the \"Interests, Ideologies, and Identity\" (3I) of four key\naudience groups: voters, businesses, donors, and politicians. Our method\nemploys large language models to generate the LLM-POTUS Score, a quantitative\nmeasure of debate performance based on the alignment between 3P and 3I. We\napply this framework to analyze transcripts from recent U.S. presidential\ndebates, demonstrating its ability to provide nuanced, multi-dimensional\nassessments of candidate performances. Our results reveal insights into the\neffectiveness of different debating strategies and their impact on various\naudience segments. This study not only offers a new tool for political analysis\nbut also explores the potential and limitations of using LLMs as impartial\njudges in complex social contexts. In addition, this framework provides\nindividual citizens with an independent tool to evaluate presidential debate\nperformances, which enhances democratic engagement and reduces reliance on\npotentially biased media interpretations and institutional influence, thereby\nstrengthening the foundation of informed civic participation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable capabilities in natural\nlanguage processing, yet their application to political discourse analysis\nremains underexplored. This paper introduces a novel approach to evaluating\npresidential debate performances using LLMs, addressing the longstanding\nchallenge of objectively assessing debate outcomes. We propose a framework that\nanalyzes candidates' \"Policies, Persona, and Perspective\" (3P) and how they\nresonate with the \"Interests, Ideologies, and Identity\" (3I) of four key\naudience groups: voters, businesses, donors, and politicians. Our method\nemploys large language models to generate the LLM-POTUS Score, a quantitative\nmeasure of debate performance based on the alignment between 3P and 3I. We\napply this framework to analyze transcripts from recent U.S. presidential\ndebates, demonstrating its ability to provide nuanced, multi-dimensional\nassessments of candidate performances. Our results reveal insights into the\neffectiveness of different debating strategies and their impact on various\naudience segments. This study not only offers a new tool for political analysis\nbut also explores the potential and limitations of using LLMs as impartial\njudges in complex social contexts. In addition, this framework provides\nindividual citizens with an independent tool to evaluate presidential debate\nperformances, which enhances democratic engagement and reduces reliance on\npotentially biased media interpretations and institutional influence, thereby\nstrengthening the foundation of informed civic participation."
                },
                "authors": [
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Oleksandra Zolotarevych"
                    },
                    {
                        "name": "Rongwei Yang"
                    },
                    {
                        "name": "Tianming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Liu"
                },
                "author": "Tianming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07409v2",
                "updated": "2024-09-12T15:35:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    35,
                    49,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-11T16:50:29Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    50,
                    29,
                    2,
                    255,
                    0
                ],
                "title": "Robust Robot Walker: Learning Agile Locomotion over Tiny Traps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Robot Walker: Learning Agile Locomotion over Tiny Traps"
                },
                "summary": "Quadruped robots must exhibit robust walking capabilities in practical\napplications. In this work, we propose a novel approach that enables quadruped\nrobots to pass various small obstacles, or \"tiny traps\". Existing methods often\nrely on exteroceptive sensors, which can be unreliable for detecting such tiny\ntraps. To overcome this limitation, our approach focuses solely on\nproprioceptive inputs. We introduce a two-stage training framework\nincorporating a contact encoder and a classification head to learn implicit\nrepresentations of different traps. Additionally, we design a set of tailored\nreward functions to improve both the stability of training and the ease of\ndeployment for goal-tracking tasks. To benefit further research, we design a\nnew benchmark for tiny trap task. Extensive experiments in both simulation and\nreal-world settings demonstrate the effectiveness and robustness of our method.\nProject Page: https://robust-robot-walker.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quadruped robots must exhibit robust walking capabilities in practical\napplications. In this work, we propose a novel approach that enables quadruped\nrobots to pass various small obstacles, or \"tiny traps\". Existing methods often\nrely on exteroceptive sensors, which can be unreliable for detecting such tiny\ntraps. To overcome this limitation, our approach focuses solely on\nproprioceptive inputs. We introduce a two-stage training framework\nincorporating a contact encoder and a classification head to learn implicit\nrepresentations of different traps. Additionally, we design a set of tailored\nreward functions to improve both the stability of training and the ease of\ndeployment for goal-tracking tasks. To benefit further research, we design a\nnew benchmark for tiny trap task. Extensive experiments in both simulation and\nreal-world settings demonstrate the effectiveness and robustness of our method.\nProject Page: https://robust-robot-walker.github.io/"
                },
                "authors": [
                    {
                        "name": "Shaoting Zhu"
                    },
                    {
                        "name": "Runhan Huang"
                    },
                    {
                        "name": "Linzhan Mou"
                    },
                    {
                        "name": "Hang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhao"
                },
                "author": "Hang Zhao",
                "arxiv_comment": "10 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06857v2",
                "updated": "2024-09-12T15:04:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    4,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-10T20:45:43Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    20,
                    45,
                    43,
                    1,
                    254,
                    0
                ],
                "title": "What is the Role of Small Models in the LLM Era: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is the Role of Small Models in the LLM Era: A Survey"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in advancing\nartificial general intelligence (AGI), leading to the development of\nincreasingly large models such as GPT-4 and LLaMA-405B. However, scaling up\nmodel sizes results in exponentially higher computational costs and energy\nconsumption, making these models impractical for academic researchers and\nbusinesses with limited resources. At the same time, Small Models (SMs) are\nfrequently used in practical settings, although their significance is currently\nunderestimated. This raises important questions about the role of small models\nin the era of LLMs, a topic that has received limited attention in prior\nresearch. In this work, we systematically examine the relationship between LLMs\nand SMs from two key perspectives: Collaboration and Competition. We hope this\nsurvey provides valuable insights for practitioners, fostering a deeper\nunderstanding of the contribution of small models and promoting more efficient\nuse of computational resources. The code is available at\nhttps://github.com/tigerchen52/role_of_small_models"
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Gaël Varoquaux"
                    }
                ],
                "author_detail": {
                    "name": "Gaël Varoquaux"
                },
                "author": "Gaël Varoquaux",
                "arxiv_comment": "a survey paper of small models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02387v3",
                "updated": "2024-09-12T14:56:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    56,
                    35,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-04T02:30:12Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    30,
                    12,
                    2,
                    248,
                    0
                ],
                "title": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges"
                },
                "summary": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence."
                },
                "authors": [
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08098v1",
                "updated": "2024-09-12T14:51:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    51,
                    43,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:51:43Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    51,
                    43,
                    3,
                    256,
                    0
                ],
                "title": "The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK\n  Employment Tribunal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK\n  Employment Tribunal"
                },
                "summary": "This paper explores the intersection of technological innovation and access\nto justice by developing a benchmark for predicting case outcomes in the UK\nEmployment Tribunal (UKET). To address the challenge of extensive manual\nannotation, the study employs a large language model (LLM) for automatic\nannotation, resulting in the creation of the CLC-UKET dataset. The dataset\nconsists of approximately 19,000 UKET cases and their metadata. Comprehensive\nlegal annotations cover facts, claims, precedent references, statutory\nreferences, case outcomes, reasons and jurisdiction codes. Facilitated by the\nCLC-UKET data, we examine a multi-class case outcome prediction task in the\nUKET. Human predictions are collected to establish a performance reference for\nmodel comparison. Empirical results from baseline models indicate that\nfinetuned transformer models outperform zero-shot and few-shot LLMs on the UKET\nprediction task. The performance of zero-shot LLMs can be enhanced by\nintegrating task-related information into few-shot examples. We hope that the\nCLC-UKET dataset, along with human annotations and empirical findings, can\nserve as a valuable benchmark for employment-related dispute resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the intersection of technological innovation and access\nto justice by developing a benchmark for predicting case outcomes in the UK\nEmployment Tribunal (UKET). To address the challenge of extensive manual\nannotation, the study employs a large language model (LLM) for automatic\nannotation, resulting in the creation of the CLC-UKET dataset. The dataset\nconsists of approximately 19,000 UKET cases and their metadata. Comprehensive\nlegal annotations cover facts, claims, precedent references, statutory\nreferences, case outcomes, reasons and jurisdiction codes. Facilitated by the\nCLC-UKET data, we examine a multi-class case outcome prediction task in the\nUKET. Human predictions are collected to establish a performance reference for\nmodel comparison. Empirical results from baseline models indicate that\nfinetuned transformer models outperform zero-shot and few-shot LLMs on the UKET\nprediction task. The performance of zero-shot LLMs can be enhanced by\nintegrating task-related information into few-shot examples. We hope that the\nCLC-UKET dataset, along with human annotations and empirical findings, can\nserve as a valuable benchmark for employment-related dispute resolution."
                },
                "authors": [
                    {
                        "name": "Huiyuan Xie"
                    },
                    {
                        "name": "Felix Steffek"
                    },
                    {
                        "name": "Joana Ribeiro de Faria"
                    },
                    {
                        "name": "Christine Carter"
                    },
                    {
                        "name": "Jonathan Rutherford"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Rutherford"
                },
                "author": "Jonathan Rutherford",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08087v1",
                "updated": "2024-09-12T14:42:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    42,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:42:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    42,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "Securing Large Language Models: Addressing Bias, Misinformation, and\n  Prompt Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Large Language Models: Addressing Bias, Misinformation, and\n  Prompt Attacks"
                },
                "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious fields, yet their increasing use raises critical security concerns.\nThis article reviews recent literature addressing key issues in LLM security,\nwith a focus on accuracy, bias, content detection, and vulnerability to\nattacks. Issues related to inaccurate or misleading outputs from LLMs is\ndiscussed, with emphasis on the implementation from fact-checking methodologies\nto enhance response reliability. Inherent biases within LLMs are critically\nexamined through diverse evaluation techniques, including controlled input\nstudies and red teaming exercises. A comprehensive analysis of bias mitigation\nstrategies is presented, including approaches from pre-processing interventions\nto in-training adjustments and post-processing refinements. The article also\nprobes the complexity of distinguishing LLM-generated content from\nhuman-produced text, introducing detection mechanisms like DetectGPT and\nwatermarking techniques while noting the limitations of machine learning\nenabled classifiers under intricate circumstances. Moreover, LLM\nvulnerabilities, including jailbreak attacks and prompt injection exploits, are\nanalyzed by looking into different case studies and large-scale competitions\nlike HackAPrompt. This review is concluded by retrospecting defense mechanisms\nto safeguard LLMs, accentuating the need for more extensive research into the\nLLM security field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate impressive capabilities across\nvarious fields, yet their increasing use raises critical security concerns.\nThis article reviews recent literature addressing key issues in LLM security,\nwith a focus on accuracy, bias, content detection, and vulnerability to\nattacks. Issues related to inaccurate or misleading outputs from LLMs is\ndiscussed, with emphasis on the implementation from fact-checking methodologies\nto enhance response reliability. Inherent biases within LLMs are critically\nexamined through diverse evaluation techniques, including controlled input\nstudies and red teaming exercises. A comprehensive analysis of bias mitigation\nstrategies is presented, including approaches from pre-processing interventions\nto in-training adjustments and post-processing refinements. The article also\nprobes the complexity of distinguishing LLM-generated content from\nhuman-produced text, introducing detection mechanisms like DetectGPT and\nwatermarking techniques while noting the limitations of machine learning\nenabled classifiers under intricate circumstances. Moreover, LLM\nvulnerabilities, including jailbreak attacks and prompt injection exploits, are\nanalyzed by looking into different case studies and large-scale competitions\nlike HackAPrompt. This review is concluded by retrospecting defense mechanisms\nto safeguard LLMs, accentuating the need for more extensive research into the\nLLM security field."
                },
                "authors": [
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Qian Niu"
                    }
                ],
                "author_detail": {
                    "name": "Qian Niu"
                },
                "author": "Qian Niu",
                "arxiv_comment": "17 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08078v1",
                "updated": "2024-09-12T14:31:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    31,
                    2,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:31:02Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    31,
                    2,
                    3,
                    256,
                    0
                ],
                "title": "MosquitoMiner: A Light Weight Rover for Detecting and Eliminating\n  Mosquito Breeding Sites",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MosquitoMiner: A Light Weight Rover for Detecting and Eliminating\n  Mosquito Breeding Sites"
                },
                "summary": "In this paper, we present a novel approach to the development and deployment\nof an autonomous mosquito breeding place detector rover with the object and\nobstacle detection capabilities to control mosquitoes. Mosquito-borne diseases\ncontinue to pose significant health threats globally, with conventional control\nmethods proving slow and inefficient. Amidst rising concerns over the rapid\nspread of these diseases, there is an urgent need for innovative and efficient\nstrategies to manage mosquito populations and prevent disease transmission. To\nmitigate the limitations of manual labor and traditional methods, our rover\nemploys autonomous control strategies. Leveraging our own custom dataset, the\nrover can autonomously navigate along a pre-defined path, identifying and\nmitigating potential breeding grounds with precision. It then proceeds to\neliminate these breeding grounds by spraying a chemical agent, effectively\neradicating mosquito habitats. Our project demonstrates the effectiveness that\nis absent in traditional ways of controlling and safeguarding public health.\nThe code for this project is available on GitHub at -\nhttps://github.com/faiyazabdullah/MosquitoMiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a novel approach to the development and deployment\nof an autonomous mosquito breeding place detector rover with the object and\nobstacle detection capabilities to control mosquitoes. Mosquito-borne diseases\ncontinue to pose significant health threats globally, with conventional control\nmethods proving slow and inefficient. Amidst rising concerns over the rapid\nspread of these diseases, there is an urgent need for innovative and efficient\nstrategies to manage mosquito populations and prevent disease transmission. To\nmitigate the limitations of manual labor and traditional methods, our rover\nemploys autonomous control strategies. Leveraging our own custom dataset, the\nrover can autonomously navigate along a pre-defined path, identifying and\nmitigating potential breeding grounds with precision. It then proceeds to\neliminate these breeding grounds by spraying a chemical agent, effectively\neradicating mosquito habitats. Our project demonstrates the effectiveness that\nis absent in traditional ways of controlling and safeguarding public health.\nThe code for this project is available on GitHub at -\nhttps://github.com/faiyazabdullah/MosquitoMiner"
                },
                "authors": [
                    {
                        "name": "Md. Adnanul Islam"
                    },
                    {
                        "name": "Md. Faiyaz Abdullah Sayeedi"
                    },
                    {
                        "name": "Jannatul Ferdous Deepti"
                    },
                    {
                        "name": "Shahanur Rahman Bappy"
                    },
                    {
                        "name": "Safrin Sanzida Islam"
                    },
                    {
                        "name": "Fahim Hafiz"
                    }
                ],
                "author_detail": {
                    "name": "Fahim Hafiz"
                },
                "author": "Fahim Hafiz",
                "arxiv_comment": "Accepted - 2024 IEEE Region 10 Symposium (TENSYMP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08069v1",
                "updated": "2024-09-12T14:24:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    24,
                    45,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T14:24:45Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    24,
                    45,
                    3,
                    256,
                    0
                ],
                "title": "TravelAgent: An AI Assistant for Personalized Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TravelAgent: An AI Assistant for Personalized Travel Planning"
                },
                "summary": "As global tourism expands and artificial intelligence technology advances,\nintelligent travel planning services have emerged as a significant research\nfocus. Within dynamic real-world travel scenarios with multi-dimensional\nconstraints, services that support users in automatically creating practical\nand customized travel itineraries must address three key objectives:\nRationality, Comprehensiveness, and Personalization. However, existing systems\nwith rule-based combinations or LLM-based planning methods struggle to fully\nsatisfy these criteria. To overcome the challenges, we introduce TravelAgent, a\ntravel planning system powered by large language models (LLMs) designed to\nprovide reasonable, comprehensive, and personalized travel itineraries grounded\nin dynamic scenarios. TravelAgent comprises four modules: Tool-usage,\nRecommendation, Planning, and Memory Module. We evaluate TravelAgent's\nperformance with human and simulated users, demonstrating its overall\neffectiveness in three criteria and confirming the accuracy of personalized\nrecommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As global tourism expands and artificial intelligence technology advances,\nintelligent travel planning services have emerged as a significant research\nfocus. Within dynamic real-world travel scenarios with multi-dimensional\nconstraints, services that support users in automatically creating practical\nand customized travel itineraries must address three key objectives:\nRationality, Comprehensiveness, and Personalization. However, existing systems\nwith rule-based combinations or LLM-based planning methods struggle to fully\nsatisfy these criteria. To overcome the challenges, we introduce TravelAgent, a\ntravel planning system powered by large language models (LLMs) designed to\nprovide reasonable, comprehensive, and personalized travel itineraries grounded\nin dynamic scenarios. TravelAgent comprises four modules: Tool-usage,\nRecommendation, Planning, and Memory Module. We evaluate TravelAgent's\nperformance with human and simulated users, demonstrating its overall\neffectiveness in three criteria and confirming the accuracy of personalized\nrecommendations."
                },
                "authors": [
                    {
                        "name": "Aili Chen"
                    },
                    {
                        "name": "Xuyang Ge"
                    },
                    {
                        "name": "Ziquan Fu"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Jiangjie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiangjie Chen"
                },
                "author": "Jiangjie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03325v2",
                "updated": "2024-09-12T14:18:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    14,
                    18,
                    26,
                    3,
                    256,
                    0
                ],
                "published": "2024-04-04T09:52:22Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    9,
                    52,
                    22,
                    3,
                    95,
                    0
                ],
                "title": "Embodied Neuromorphic Artificial Intelligence for Robotics:\n  Perspectives, Challenges, and Research Development Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Neuromorphic Artificial Intelligence for Robotics:\n  Perspectives, Challenges, and Research Development Stack"
                },
                "summary": "Robotic technologies have been an indispensable part for improving human\nproductivity since they have been helping humans in completing diverse,\ncomplex, and intensive tasks in a fast yet accurate and efficient way.\nTherefore, robotic technologies have been deployed in a wide range of\napplications, ranging from personal to industrial use-cases. However, current\nrobotic technologies and their computing paradigm still lack embodied\nintelligence to efficiently interact with operational environments, respond\nwith correct/expected actions, and adapt to changes in the environments. Toward\nthis, recent advances in neuromorphic computing with Spiking Neural Networks\n(SNN) have demonstrated the potential to enable the embodied intelligence for\nrobotics through bio-plausible computing paradigm that mimics how the\nbiological brain works, known as \"neuromorphic artificial intelligence (AI)\".\nHowever, the field of neuromorphic AI-based robotics is still at an early\nstage, therefore its development and deployment for solving real-world problems\nexpose new challenges in different design aspects, such as accuracy,\nadaptability, efficiency, reliability, and security. To address these\nchallenges, this paper will discuss how we can enable embodied neuromorphic AI\nfor robotic systems through our perspectives: (P1) Embodied intelligence based\non effective learning rule, training mechanism, and adaptability; (P2)\nCross-layer optimizations for energy-efficient neuromorphic computing; (P3)\nRepresentative and fair benchmarks; (P4) Low-cost reliability and safety\nenhancements; (P5) Security and privacy for neuromorphic computing; and (P6) A\nsynergistic development for energy-efficient and robust neuromorphic-based\nrobotics. Furthermore, this paper identifies research challenges and\nopportunities, as well as elaborates our vision for future research development\ntoward embodied neuromorphic AI for robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic technologies have been an indispensable part for improving human\nproductivity since they have been helping humans in completing diverse,\ncomplex, and intensive tasks in a fast yet accurate and efficient way.\nTherefore, robotic technologies have been deployed in a wide range of\napplications, ranging from personal to industrial use-cases. However, current\nrobotic technologies and their computing paradigm still lack embodied\nintelligence to efficiently interact with operational environments, respond\nwith correct/expected actions, and adapt to changes in the environments. Toward\nthis, recent advances in neuromorphic computing with Spiking Neural Networks\n(SNN) have demonstrated the potential to enable the embodied intelligence for\nrobotics through bio-plausible computing paradigm that mimics how the\nbiological brain works, known as \"neuromorphic artificial intelligence (AI)\".\nHowever, the field of neuromorphic AI-based robotics is still at an early\nstage, therefore its development and deployment for solving real-world problems\nexpose new challenges in different design aspects, such as accuracy,\nadaptability, efficiency, reliability, and security. To address these\nchallenges, this paper will discuss how we can enable embodied neuromorphic AI\nfor robotic systems through our perspectives: (P1) Embodied intelligence based\non effective learning rule, training mechanism, and adaptability; (P2)\nCross-layer optimizations for energy-efficient neuromorphic computing; (P3)\nRepresentative and fair benchmarks; (P4) Low-cost reliability and safety\nenhancements; (P5) Security and privacy for neuromorphic computing; and (P6) A\nsynergistic development for energy-efficient and robust neuromorphic-based\nrobotics. Furthermore, this paper identifies research challenges and\nopportunities, as well as elaborates our vision for future research development\ntoward embodied neuromorphic AI for robotics."
                },
                "authors": [
                    {
                        "name": "Rachmad Vidya Wicaksana Putra"
                    },
                    {
                        "name": "Alberto Marchisio"
                    },
                    {
                        "name": "Fakhreddine Zayer"
                    },
                    {
                        "name": "Jorge Dias"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "arxiv_comment": "To appear at the 18th International Conference on Control,\n  Automation, Robotics and Vision (ICARCV), December 2024, Dubai, UAE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08043v1",
                "updated": "2024-09-12T13:47:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    47,
                    18,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T13:47:18Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    47,
                    18,
                    3,
                    256,
                    0
                ],
                "title": "External Memories of PDP Switches for In-Network Implementable Functions\n  Placement: Deep Learning Based Reconfiguration of SFCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "External Memories of PDP Switches for In-Network Implementable Functions\n  Placement: Deep Learning Based Reconfiguration of SFCs"
                },
                "summary": "Network function virtualization leverages programmable data plane switches to\ndeploy in-network implementable functions, to improve QoS. The memories of\nswitches can be extended through remote direct memory access to access external\nmemories. This paper exploits the switches external memories to place VNFs at\ntime intervals with ultra-low latency and high bandwidth demands. The\nreconfiguration decision is modeled as an optimization to minimize the\ndeployment and reconfiguration cost, while meeting the SFCs deadlines. A DRL\nbased method is proposed to reconfigure service chains adoptable with dynamic\nnetwork and traffic characteristics. To deal with slow convergence due to the\ncomplexity of deployment scenarios, static and dynamic filters are used in\npolicy networks construction to diminish unfeasible placement exploration.\nResults illustrate improvement in convergence, acceptance ratio and cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network function virtualization leverages programmable data plane switches to\ndeploy in-network implementable functions, to improve QoS. The memories of\nswitches can be extended through remote direct memory access to access external\nmemories. This paper exploits the switches external memories to place VNFs at\ntime intervals with ultra-low latency and high bandwidth demands. The\nreconfiguration decision is modeled as an optimization to minimize the\ndeployment and reconfiguration cost, while meeting the SFCs deadlines. A DRL\nbased method is proposed to reconfigure service chains adoptable with dynamic\nnetwork and traffic characteristics. To deal with slow convergence due to the\ncomplexity of deployment scenarios, static and dynamic filters are used in\npolicy networks construction to diminish unfeasible placement exploration.\nResults illustrate improvement in convergence, acceptance ratio and cost."
                },
                "authors": [
                    {
                        "name": "Somayeh Kianpisheh"
                    },
                    {
                        "name": "Tarik Taleb"
                    }
                ],
                "author_detail": {
                    "name": "Tarik Taleb"
                },
                "author": "Tarik Taleb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08038v1",
                "updated": "2024-09-12T13:40:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    40,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T13:40:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    40,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "Towards Scalable Quantum Key Distribution: A Machine Learning-Based\n  Cascade Protocol Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable Quantum Key Distribution: A Machine Learning-Based\n  Cascade Protocol Approach"
                },
                "summary": "Quantum Key Distribution (QKD) is a pivotal technology in the quest for\nsecure communication, harnessing the power of quantum mechanics to ensure\nrobust data protection. However, scaling QKD to meet the demands of high-speed,\nreal-world applications remains a significant challenge. Traditional key rate\ndetermination methods, dependent on complex mathematical models, often fall\nshort in efficiency and scalability. In this paper, we propose an approach that\ninvolves integrating machine learning (ML) techniques with the Cascade error\ncorrection protocol to enhance the scalability and efficiency of QKD systems.\nOur ML-based approach utilizes an autoencoder framework to predict the Quantum\nBit Error Rate (QBER) and final key length with over 99\\% accuracy. This method\nsignificantly reduces error correction time, maintaining a consistently low\ncomputation time even with large input sizes, such as data rates up to 156\nMbps. In contrast, traditional methods exhibit exponentially increasing\ncomputation times as input sizes grow, highlighting the superior scalability of\nour ML-based solution. Through comprehensive simulations, we demonstrate that\nour method not only accelerates the error correction process but also optimizes\nresource utilization, making it more cost-effective and practical for\nreal-world deployment. The Cascade protocol's integration further enhances\nsystem security by dynamically adjusting error correction based on real-time\nQBER observations, providing robust protection against potential eavesdropping.\n  Our research establishes a new benchmark for scalable, high-throughput QKD\nsystems, proving that machine learning can significantly advance the field of\nquantum cryptography. This work continues the evolution towards truly scalable\nquantum communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) is a pivotal technology in the quest for\nsecure communication, harnessing the power of quantum mechanics to ensure\nrobust data protection. However, scaling QKD to meet the demands of high-speed,\nreal-world applications remains a significant challenge. Traditional key rate\ndetermination methods, dependent on complex mathematical models, often fall\nshort in efficiency and scalability. In this paper, we propose an approach that\ninvolves integrating machine learning (ML) techniques with the Cascade error\ncorrection protocol to enhance the scalability and efficiency of QKD systems.\nOur ML-based approach utilizes an autoencoder framework to predict the Quantum\nBit Error Rate (QBER) and final key length with over 99\\% accuracy. This method\nsignificantly reduces error correction time, maintaining a consistently low\ncomputation time even with large input sizes, such as data rates up to 156\nMbps. In contrast, traditional methods exhibit exponentially increasing\ncomputation times as input sizes grow, highlighting the superior scalability of\nour ML-based solution. Through comprehensive simulations, we demonstrate that\nour method not only accelerates the error correction process but also optimizes\nresource utilization, making it more cost-effective and practical for\nreal-world deployment. The Cascade protocol's integration further enhances\nsystem security by dynamically adjusting error correction based on real-time\nQBER observations, providing robust protection against potential eavesdropping.\n  Our research establishes a new benchmark for scalable, high-throughput QKD\nsystems, proving that machine learning can significantly advance the field of\nquantum cryptography. This work continues the evolution towards truly scalable\nquantum communication."
                },
                "authors": [
                    {
                        "name": "Hasan Abbas Al-Mohammed"
                    },
                    {
                        "name": "Saif Al-Kuwari"
                    },
                    {
                        "name": "Hashir Kuniyil"
                    },
                    {
                        "name": "Ahmed Farouk"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Farouk"
                },
                "author": "Ahmed Farouk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08027v1",
                "updated": "2024-09-12T13:18:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    18,
                    41,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T13:18:41Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    13,
                    18,
                    41,
                    3,
                    256,
                    0
                ],
                "title": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework\n  for Student Performance Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework\n  for Student Performance Feedback"
                },
                "summary": "Recent advances in eXplainable AI (XAI) for education have highlighted a\ncritical challenge: ensuring that explanations for state-of-the-art AI models\nare understandable for non-technical users such as educators and students. In\nresponse, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI\npipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is\ndesigned to deliver theory-driven, actionable feedback to students in online\ncourses. iLLuMinaTE navigates three main stages - causal connection,\nexplanation selection, and explanation presentation - with variations drawing\nfrom eight social science theories (e.g. Abnormal Conditions, Pearl's Model of\nExplanation, Necessity and Robustness Selection, Contrastive Explanation). We\nextensively evaluate 21,915 natural language explanations of iLLuMinaTE\nextracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different\nunderlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from\nthree diverse online courses. Our evaluation involves analyses of explanation\nalignment to the social science theory, understandability of the explanation,\nand a real-world user preference study with 114 university students containing\na novel actionability simulation. We find that students prefer iLLuMinaTE\nexplanations over traditional explainers 89.52% of the time. Our work provides\na robust, ready-to-use framework for effectively communicating hybrid\nXAI-driven insights in education, with significant generalization potential for\nother human-centric fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in eXplainable AI (XAI) for education have highlighted a\ncritical challenge: ensuring that explanations for state-of-the-art AI models\nare understandable for non-technical users such as educators and students. In\nresponse, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI\npipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is\ndesigned to deliver theory-driven, actionable feedback to students in online\ncourses. iLLuMinaTE navigates three main stages - causal connection,\nexplanation selection, and explanation presentation - with variations drawing\nfrom eight social science theories (e.g. Abnormal Conditions, Pearl's Model of\nExplanation, Necessity and Robustness Selection, Contrastive Explanation). We\nextensively evaluate 21,915 natural language explanations of iLLuMinaTE\nextracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different\nunderlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from\nthree diverse online courses. Our evaluation involves analyses of explanation\nalignment to the social science theory, understandability of the explanation,\nand a real-world user preference study with 114 university students containing\na novel actionability simulation. We find that students prefer iLLuMinaTE\nexplanations over traditional explainers 89.52% of the time. Our work provides\na robust, ready-to-use framework for effectively communicating hybrid\nXAI-driven insights in education, with significant generalization potential for\nother human-centric fields."
                },
                "authors": [
                    {
                        "name": "Vinitra Swamy"
                    },
                    {
                        "name": "Davide Romano"
                    },
                    {
                        "name": "Bhargav Srinivasa Desikan"
                    },
                    {
                        "name": "Oana-Maria Camburu"
                    },
                    {
                        "name": "Tanja Käser"
                    }
                ],
                "author_detail": {
                    "name": "Tanja Käser"
                },
                "author": "Tanja Käser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08014v1",
                "updated": "2024-09-12T12:57:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    57,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T12:57:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    57,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "An Evaluation Framework for Attributed Information Retrieval using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Evaluation Framework for Attributed Information Retrieval using Large\n  Language Models"
                },
                "summary": "With the growing success of Large Language models (LLMs) in\ninformation-seeking scenarios, search engines are now adopting generative\napproaches to provide answers along with in-line citations as attribution.\nWhile existing work focuses mainly on attributed question answering, in this\npaper, we target information-seeking scenarios which are often more challenging\ndue to the open-ended nature of the queries and the size of the label space in\nterms of the diversity of candidate-attributed answers per query. We propose a\nreproducible framework to evaluate and benchmark attributed information\nseeking, using any backbone LLM, and different architectural designs: (1)\nGenerate (2) Retrieve then Generate, and (3) Generate then Retrieve.\nExperiments using HAGRID, an attributed information-seeking dataset, show the\nimpact of different scenarios on both the correctness and attributability of\nanswers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing success of Large Language models (LLMs) in\ninformation-seeking scenarios, search engines are now adopting generative\napproaches to provide answers along with in-line citations as attribution.\nWhile existing work focuses mainly on attributed question answering, in this\npaper, we target information-seeking scenarios which are often more challenging\ndue to the open-ended nature of the queries and the size of the label space in\nterms of the diversity of candidate-attributed answers per query. We propose a\nreproducible framework to evaluate and benchmark attributed information\nseeking, using any backbone LLM, and different architectural designs: (1)\nGenerate (2) Retrieve then Generate, and (3) Generate then Retrieve.\nExperiments using HAGRID, an attributed information-seeking dataset, show the\nimpact of different scenarios on both the correctness and attributability of\nanswers."
                },
                "authors": [
                    {
                        "name": "Hanane Djeddal"
                    },
                    {
                        "name": "Pierre Erbacher"
                    },
                    {
                        "name": "Raouf Toukal"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Karen Pinel-Sauvagnat"
                    },
                    {
                        "name": "Sophia Katrenko"
                    },
                    {
                        "name": "Lynda Tamine"
                    }
                ],
                "author_detail": {
                    "name": "Lynda Tamine"
                },
                "author": "Lynda Tamine",
                "arxiv_doi": "10.1145/3627673.3679172",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3627673.3679172",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.08014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07985v1",
                "updated": "2024-09-12T12:30:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    30,
                    7,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T12:30:07Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    30,
                    7,
                    3,
                    256,
                    0
                ],
                "title": "Games for AI Control: Models of Safety Evaluations of AI Deployment\n  Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Games for AI Control: Models of Safety Evaluations of AI Deployment\n  Protocols"
                },
                "summary": "To evaluate the safety and usefulness of deployment protocols for untrusted\nAIs, AI Control uses a red-teaming exercise played between a protocol designer\nand an adversary. This paper introduces AI-Control Games, a formal\ndecision-making model of the red-teaming exercise as a multi-objective,\npartially observable, stochastic game. We also introduce methods for finding\noptimal protocols in AI-Control Games, by reducing them to a set of zero-sum\npartially observable stochastic games. We apply our formalism to model,\nevaluate and synthesise protocols for deploying untrusted language models as\nprogramming assistants, focusing on Trusted Monitoring protocols, which use\nweaker language models and limited human assistance. Finally, we demonstrate\nthe utility of our formalism by showcasing improvements over empirical studies\nin existing settings, evaluating protocols in new settings, and analysing how\nmodelling assumptions affect the safety and usefulness of protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To evaluate the safety and usefulness of deployment protocols for untrusted\nAIs, AI Control uses a red-teaming exercise played between a protocol designer\nand an adversary. This paper introduces AI-Control Games, a formal\ndecision-making model of the red-teaming exercise as a multi-objective,\npartially observable, stochastic game. We also introduce methods for finding\noptimal protocols in AI-Control Games, by reducing them to a set of zero-sum\npartially observable stochastic games. We apply our formalism to model,\nevaluate and synthesise protocols for deploying untrusted language models as\nprogramming assistants, focusing on Trusted Monitoring protocols, which use\nweaker language models and limited human assistance. Finally, we demonstrate\nthe utility of our formalism by showcasing improvements over empirical studies\nin existing settings, evaluating protocols in new settings, and analysing how\nmodelling assumptions affect the safety and usefulness of protocols."
                },
                "authors": [
                    {
                        "name": "Charlie Griffin"
                    },
                    {
                        "name": "Louis Thomson"
                    },
                    {
                        "name": "Buck Shlegeris"
                    },
                    {
                        "name": "Alessandro Abate"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Abate"
                },
                "author": "Alessandro Abate",
                "arxiv_comment": "7 pages, with appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11386v2",
                "updated": "2024-09-12T12:08:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    8,
                    4,
                    3,
                    256,
                    0
                ],
                "published": "2024-08-21T07:30:11Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    7,
                    30,
                    11,
                    2,
                    234,
                    0
                ],
                "title": "Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for\n  Business Process Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for\n  Business Process Management"
                },
                "summary": "To promote sustainable business practices, and to achieve climate neutrality\nby 2050, the EU has developed the taxonomy of sustainable activities, which\ndescribes when exactly business practices can be considered sustainable. While\nthe taxonomy has only been recently established, progressively more companies\nwill have to report how much of their revenue was created via sustainably\nexecuted business processes. To help companies prepare to assess whether their\nbusiness processes comply with the constraints outlined in the taxonomy, we\ninvestigate in how far these criteria can be used for conformance checking,\nthat is, assessing in a data-driven manner, whether business process executions\nadhere to regulatory constraints. For this, we develop a few-shot learning\npipeline to characterize the constraints of the taxonomy with the help of an\nLLM as to the process dimensions they relate to. We find that many constraints\nof the taxonomy are useable for conformance checking, particularly in the\nsectors of energy, manufacturing, and transport. This will aid companies in\npreparing to monitor regulatory compliance with the taxonomy automatically, by\ncharacterizing what kind of information they need to extract, and by providing\na better understanding of sectors where such an assessment is feasible and\nwhere it is not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To promote sustainable business practices, and to achieve climate neutrality\nby 2050, the EU has developed the taxonomy of sustainable activities, which\ndescribes when exactly business practices can be considered sustainable. While\nthe taxonomy has only been recently established, progressively more companies\nwill have to report how much of their revenue was created via sustainably\nexecuted business processes. To help companies prepare to assess whether their\nbusiness processes comply with the constraints outlined in the taxonomy, we\ninvestigate in how far these criteria can be used for conformance checking,\nthat is, assessing in a data-driven manner, whether business process executions\nadhere to regulatory constraints. For this, we develop a few-shot learning\npipeline to characterize the constraints of the taxonomy with the help of an\nLLM as to the process dimensions they relate to. We find that many constraints\nof the taxonomy are useable for conformance checking, particularly in the\nsectors of energy, manufacturing, and transport. This will aid companies in\npreparing to monitor regulatory compliance with the taxonomy automatically, by\ncharacterizing what kind of information they need to extract, and by providing\na better understanding of sectors where such an assessment is feasible and\nwhere it is not."
                },
                "authors": [
                    {
                        "name": "Finn Klessascheck"
                    },
                    {
                        "name": "Stephan A. Fahrenkrog-Petersen"
                    },
                    {
                        "name": "Jan Mendling"
                    },
                    {
                        "name": "Luise Pufahl"
                    }
                ],
                "author_detail": {
                    "name": "Luise Pufahl"
                },
                "author": "Luise Pufahl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05639v2",
                "updated": "2024-09-12T12:00:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    12,
                    0,
                    26,
                    3,
                    256,
                    0
                ],
                "published": "2024-06-09T04:42:19Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    4,
                    42,
                    19,
                    6,
                    161,
                    0
                ],
                "title": "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on\n  Automated Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Parameter-Efficient Fine-Tuning of Large Language Model on\n  Automated Program Repair"
                },
                "summary": "Automated Program Repair (APR) aims to fix bugs by generating patches. And\nexisting work has demonstrated that \"pre-training and fine-tuning\" paradigm\nenables Large Language Models (LLMs) improve fixing capabilities on APR.\nHowever, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR\nand limited research has been conducted on the execution-based evaluation of\nParameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can\nreduce computing resource consumption without compromising performance and has\nbeen widely adopted to other software engineering tasks.\n  To fill this gap, we enhance the existing APR dataset by employing prompt\nengineering to create an instruction dataset, APR-INSTRUCTION, at first.\nSecondly, we fine-tune four pre-trained LLMs using four different PEFT methods\nwith APR-INSTRUCTION. The best fine-tuned model fixes 58% more bugs than the\nstate-of-the-art LLM-based APR techniques. The results also show that $(IA)^3$\nimproves the creativity of LLMs more effectively through fine-tuning and\nachieves the highest fixing capability compared to the other three PEFT\nmethods. Thirdly, we explore the optimal configuration of PEFT hyperparameters,\nand assess the impact of instruction dataset size, showing that a larger number\nof parameters and a larger training dataset do not necessarily result in better\nperformance for PEFT. Lastly, we analyze peak memory usage and trainable\nparameters to show the efficiency of PEFT.\n  This work provides a comprehensive exploration of PEFT on APR and suggests\npotentially promising directions for extension to other software engineering\ndownstream tasks. APR-INSTRUCTION, PEFT weights, and the fine-tuning code are\npublicly available as open-source resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair (APR) aims to fix bugs by generating patches. And\nexisting work has demonstrated that \"pre-training and fine-tuning\" paradigm\nenables Large Language Models (LLMs) improve fixing capabilities on APR.\nHowever, existing work mainly focuses on Full-Model Fine-Tuning (FMFT) for APR\nand limited research has been conducted on the execution-based evaluation of\nParameter-Efficient Fine-Tuning (PEFT) for APR. Comparing to FMFT, PEFT can\nreduce computing resource consumption without compromising performance and has\nbeen widely adopted to other software engineering tasks.\n  To fill this gap, we enhance the existing APR dataset by employing prompt\nengineering to create an instruction dataset, APR-INSTRUCTION, at first.\nSecondly, we fine-tune four pre-trained LLMs using four different PEFT methods\nwith APR-INSTRUCTION. The best fine-tuned model fixes 58% more bugs than the\nstate-of-the-art LLM-based APR techniques. The results also show that $(IA)^3$\nimproves the creativity of LLMs more effectively through fine-tuning and\nachieves the highest fixing capability compared to the other three PEFT\nmethods. Thirdly, we explore the optimal configuration of PEFT hyperparameters,\nand assess the impact of instruction dataset size, showing that a larger number\nof parameters and a larger training dataset do not necessarily result in better\nperformance for PEFT. Lastly, we analyze peak memory usage and trainable\nparameters to show the efficiency of PEFT.\n  This work provides a comprehensive exploration of PEFT on APR and suggests\npotentially promising directions for extension to other software engineering\ndownstream tasks. APR-INSTRUCTION, PEFT weights, and the fine-tuning code are\npublicly available as open-source resources."
                },
                "authors": [
                    {
                        "name": "Guochang Li"
                    },
                    {
                        "name": "Chen Zhi"
                    },
                    {
                        "name": "Jialiang Chen"
                    },
                    {
                        "name": "Junxiao Han"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "ASE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03302v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03302v4",
                "updated": "2024-09-12T11:51:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    11,
                    51,
                    51,
                    3,
                    256,
                    0
                ],
                "published": "2024-04-04T08:52:30Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    8,
                    52,
                    30,
                    3,
                    95,
                    0
                ],
                "title": "How Easily do Irrelevant Inputs Skew the Responses of Large Language\n  Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Easily do Irrelevant Inputs Skew the Responses of Large Language\n  Models?"
                },
                "summary": "By leveraging the retrieval of information from external knowledge databases,\nLarge Language Models (LLMs) exhibit enhanced capabilities for accomplishing\nmany knowledge-intensive tasks. However, due to the inherent flaws of current\nretrieval systems, there might exist irrelevant information within those\nretrieving top-ranked passages. In this work, we present a comprehensive\ninvestigation into the robustness of LLMs to different types of irrelevant\ninformation under various conditions. We initially introduce a framework to\nconstruct high-quality irrelevant information that ranges from semantically\nunrelated, partially related, and related to questions. Furthermore, our\nanalysis demonstrates that the constructed irrelevant information not only\nscores highly on similarity metrics, being highly retrieved by existing\nsystems, but also bears semantic connections to the context. Our investigation\nreveals that current LLMs still face challenges in discriminating highly\nsemantically related information and can be easily distracted by these\nirrelevant yet misleading content. Besides, we also find that current solutions\nfor handling irrelevant information have limitations in improving the\nrobustness of LLMs to such distractions. All the resources are available on\nGitHub at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By leveraging the retrieval of information from external knowledge databases,\nLarge Language Models (LLMs) exhibit enhanced capabilities for accomplishing\nmany knowledge-intensive tasks. However, due to the inherent flaws of current\nretrieval systems, there might exist irrelevant information within those\nretrieving top-ranked passages. In this work, we present a comprehensive\ninvestigation into the robustness of LLMs to different types of irrelevant\ninformation under various conditions. We initially introduce a framework to\nconstruct high-quality irrelevant information that ranges from semantically\nunrelated, partially related, and related to questions. Furthermore, our\nanalysis demonstrates that the constructed irrelevant information not only\nscores highly on similarity metrics, being highly retrieved by existing\nsystems, but also bears semantic connections to the context. Our investigation\nreveals that current LLMs still face challenges in discriminating highly\nsemantically related information and can be easily distracted by these\nirrelevant yet misleading content. Besides, we also find that current solutions\nfor handling irrelevant information have limitations in improving the\nrobustness of LLMs to such distractions. All the resources are available on\nGitHub at https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information."
                },
                "authors": [
                    {
                        "name": "Siye Wu"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Tinghui Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03302v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03302v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07964v1",
                "updated": "2024-09-12T11:48:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    11,
                    48,
                    1,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T11:48:01Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    11,
                    48,
                    1,
                    3,
                    256,
                    0
                ],
                "title": "WirelessAgent: Large Language Model Agents for Intelligent Wireless\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WirelessAgent: Large Language Model Agents for Intelligent Wireless\n  Networks"
                },
                "summary": "Wireless networks are increasingly facing challenges due to their expanding\nscale and complexity. These challenges underscore the need for advanced\nAI-driven strategies, particularly in the upcoming 6G networks. In this\narticle, we introduce WirelessAgent, a novel approach leveraging large language\nmodels (LLMs) to develop AI agents capable of managing complex tasks in\nwireless networks. It can effectively improve network performance through\nadvanced reasoning, multimodal data processing, and autonomous decision making.\nThereafter, we demonstrate the practical applicability and benefits of\nWirelessAgent for network slicing management. The experimental results show\nthat WirelessAgent is capable of accurately understanding user intent,\neffectively allocating slice resources, and consistently maintaining optimal\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless networks are increasingly facing challenges due to their expanding\nscale and complexity. These challenges underscore the need for advanced\nAI-driven strategies, particularly in the upcoming 6G networks. In this\narticle, we introduce WirelessAgent, a novel approach leveraging large language\nmodels (LLMs) to develop AI agents capable of managing complex tasks in\nwireless networks. It can effectively improve network performance through\nadvanced reasoning, multimodal data processing, and autonomous decision making.\nThereafter, we demonstrate the practical applicability and benefits of\nWirelessAgent for network slicing management. The experimental results show\nthat WirelessAgent is capable of accurately understanding user intent,\neffectively allocating slice resources, and consistently maintaining optimal\nperformance."
                },
                "authors": [
                    {
                        "name": "Jingwen Tong"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Zijian Li"
                    },
                    {
                        "name": "Zehong Lin"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07873v1",
                "updated": "2024-09-12T09:32:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    9,
                    32,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T09:32:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    9,
                    32,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "A Lagrangian shape and topology optimization framework based on\n  semi-discrete optimal transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lagrangian shape and topology optimization framework based on\n  semi-discrete optimal transport"
                },
                "summary": "This article revolves around shape and topology optimization, in the\napplicative context where the objective and constraint functionals depend on\nthe solution to a physical boundary value problem posed on the optimized\ndomain. We introduce a novel framework based on modern concepts from\ncomputational geometry, optimal transport and numerical analysis. Its pivotal\nfeature is a representation of the optimized shape by the cells of an adapted\nversion of a Laguerre diagram. Although such objects are originally described\nby a collection of seed points and weights, recent results from optimal\ntransport theory suggest a more intuitive parametrization in terms of the seed\npoints and measures of the associated cells. The polygonal mesh of the shape\ninduced by this diagram serves as support for the deployment of the Virtual\nElement Method for the numerical solution of the physical boundary value\nproblem at play and the calculation of the objective and constraint\nfunctionals. The sensitivities of the latter are derived next; at first, we\ncalculate their derivatives with respect to the positions of the vertices of\nthe Laguerre diagram by shape calculus techniques; a suitable adjoint\nmethodology is then developed to express them in terms of the seed points and\ncell measures of the diagram. The evolution of the shape is realized by first\nupdating the design variables according to these sensitivities and then\nreconstructing the diagram with efficient algorithms from computational\ngeometry. Our shape optimization strategy is versatile: it can be applied to a\nwide gammut of physical situations. It is Lagrangian by essence, and it thereby\nbenefits from all the assets of a consistently meshed representation of the\nshape. Yet, it naturally handles dramatic motions, including topological\nchanges, in a very robust fashion. These features, among others, are\nillustrated by a series of 2d numerical examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article revolves around shape and topology optimization, in the\napplicative context where the objective and constraint functionals depend on\nthe solution to a physical boundary value problem posed on the optimized\ndomain. We introduce a novel framework based on modern concepts from\ncomputational geometry, optimal transport and numerical analysis. Its pivotal\nfeature is a representation of the optimized shape by the cells of an adapted\nversion of a Laguerre diagram. Although such objects are originally described\nby a collection of seed points and weights, recent results from optimal\ntransport theory suggest a more intuitive parametrization in terms of the seed\npoints and measures of the associated cells. The polygonal mesh of the shape\ninduced by this diagram serves as support for the deployment of the Virtual\nElement Method for the numerical solution of the physical boundary value\nproblem at play and the calculation of the objective and constraint\nfunctionals. The sensitivities of the latter are derived next; at first, we\ncalculate their derivatives with respect to the positions of the vertices of\nthe Laguerre diagram by shape calculus techniques; a suitable adjoint\nmethodology is then developed to express them in terms of the seed points and\ncell measures of the diagram. The evolution of the shape is realized by first\nupdating the design variables according to these sensitivities and then\nreconstructing the diagram with efficient algorithms from computational\ngeometry. Our shape optimization strategy is versatile: it can be applied to a\nwide gammut of physical situations. It is Lagrangian by essence, and it thereby\nbenefits from all the assets of a consistently meshed representation of the\nshape. Yet, it naturally handles dramatic motions, including topological\nchanges, in a very robust fashion. These features, among others, are\nillustrated by a series of 2d numerical examples."
                },
                "authors": [
                    {
                        "name": "Charles Dapogny"
                    },
                    {
                        "name": "Bruno Levy"
                    },
                    {
                        "name": "Edouard Oudet"
                    }
                ],
                "author_detail": {
                    "name": "Edouard Oudet"
                },
                "author": "Edouard Oudet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07871v1",
                "updated": "2024-09-12T09:28:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    9,
                    28,
                    34,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T09:28:34Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    9,
                    28,
                    34,
                    3,
                    256,
                    0
                ],
                "title": "Objection Overruled! Lay People can Distinguish Large Language Models\n  from Lawyers, but still Favour Advice from an LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objection Overruled! Lay People can Distinguish Large Language Models\n  from Lawyers, but still Favour Advice from an LLM"
                },
                "summary": "Large Language Models (LLMs) are seemingly infiltrating every domain, and the\nlegal context is no exception. In this paper, we present the results of three\nexperiments (total N=288) that investigated lay people's willingness to act\nupon, and their ability to discriminate between, LLM- and lawyer-generated\nlegal advice. In Experiment 1, participants judged their willingness to act on\nlegal advice when the source of the advice was either known or unknown. When\nthe advice source was unknown, participants indicated that they were\nsignificantly more willing to act on the LLM-generated advice. This result was\nreplicated in Experiment 2. Intriguingly, despite participants indicating\nhigher willingness to act on LLM-generated advice in Experiments 1 and 2,\nparticipants discriminated between the LLM- and lawyer-generated texts\nsignificantly above chance-level in Experiment 3. Lastly, we discuss potential\nexplanations and risks of our findings, limitations and future work, and the\nimportance of language complexity and real-world comparability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are seemingly infiltrating every domain, and the\nlegal context is no exception. In this paper, we present the results of three\nexperiments (total N=288) that investigated lay people's willingness to act\nupon, and their ability to discriminate between, LLM- and lawyer-generated\nlegal advice. In Experiment 1, participants judged their willingness to act on\nlegal advice when the source of the advice was either known or unknown. When\nthe advice source was unknown, participants indicated that they were\nsignificantly more willing to act on the LLM-generated advice. This result was\nreplicated in Experiment 2. Intriguingly, despite participants indicating\nhigher willingness to act on LLM-generated advice in Experiments 1 and 2,\nparticipants discriminated between the LLM- and lawyer-generated texts\nsignificantly above chance-level in Experiment 3. Lastly, we discuss potential\nexplanations and risks of our findings, limitations and future work, and the\nimportance of language complexity and real-world comparability."
                },
                "authors": [
                    {
                        "name": "Eike Schneiders"
                    },
                    {
                        "name": "Tina Seabrooke"
                    },
                    {
                        "name": "Joshua Krook"
                    },
                    {
                        "name": "Richard Hyde"
                    },
                    {
                        "name": "Natalie Leesakul"
                    },
                    {
                        "name": "Jeremie Clos"
                    },
                    {
                        "name": "Joel Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Joel Fischer"
                },
                "author": "Joel Fischer",
                "arxiv_comment": "13 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14529v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14529v2",
                "updated": "2024-09-12T09:23:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    9,
                    23,
                    32,
                    3,
                    256,
                    0
                ],
                "published": "2024-05-23T13:15:13Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    13,
                    15,
                    13,
                    3,
                    144,
                    0
                ],
                "title": "AnomalyDINO: Boosting Patch-based Few-shot Anomaly Detection with DINOv2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnomalyDINO: Boosting Patch-based Few-shot Anomaly Detection with DINOv2"
                },
                "summary": "Recent advances in multimodal foundation models have set new standards in\nfew-shot anomaly detection. This paper explores whether high-quality visual\nfeatures alone are sufficient to rival existing state-of-the-art\nvision-language models. We affirm this by adapting DINOv2 for one-shot and\nfew-shot anomaly detection, with a focus on industrial applications. We show\nthat this approach does not only rival existing techniques but can even\noutmatch them in many settings. Our proposed vision-only approach, AnomalyDINO,\nis based on patch similarities and enables both image-level anomaly prediction\nand pixel-level anomaly segmentation. The approach is methodologically simple\nand training-free and, thus, does not require any additional data for\nfine-tuning or meta-learning. Despite its simplicity, AnomalyDINO achieves\nstate-of-the-art results in one- and few-shot anomaly detection (e.g., pushing\nthe one-shot performance on MVTec-AD from an AUROC of 93.1% to 96.6%). The\nreduced overhead, coupled with its outstanding few-shot performance, makes\nAnomalyDINO a strong candidate for fast deployment, e.g., in industrial\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal foundation models have set new standards in\nfew-shot anomaly detection. This paper explores whether high-quality visual\nfeatures alone are sufficient to rival existing state-of-the-art\nvision-language models. We affirm this by adapting DINOv2 for one-shot and\nfew-shot anomaly detection, with a focus on industrial applications. We show\nthat this approach does not only rival existing techniques but can even\noutmatch them in many settings. Our proposed vision-only approach, AnomalyDINO,\nis based on patch similarities and enables both image-level anomaly prediction\nand pixel-level anomaly segmentation. The approach is methodologically simple\nand training-free and, thus, does not require any additional data for\nfine-tuning or meta-learning. Despite its simplicity, AnomalyDINO achieves\nstate-of-the-art results in one- and few-shot anomaly detection (e.g., pushing\nthe one-shot performance on MVTec-AD from an AUROC of 93.1% to 96.6%). The\nreduced overhead, coupled with its outstanding few-shot performance, makes\nAnomalyDINO a strong candidate for fast deployment, e.g., in industrial\ncontexts."
                },
                "authors": [
                    {
                        "name": "Simon Damm"
                    },
                    {
                        "name": "Mike Laszkiewicz"
                    },
                    {
                        "name": "Johannes Lederer"
                    },
                    {
                        "name": "Asja Fischer"
                    }
                ],
                "author_detail": {
                    "name": "Asja Fischer"
                },
                "author": "Asja Fischer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14529v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05344v2",
                "updated": "2024-09-12T08:49:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    8,
                    49,
                    20,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-09T06:02:17Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    6,
                    2,
                    17,
                    0,
                    253,
                    0
                ],
                "title": "GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GOPT: Generalizable Online 3D Bin Packing via Transformer-based Deep\n  Reinforcement Learning"
                },
                "summary": "Robotic object packing has broad practical applications in the logistics and\nautomation industry, often formulated by researchers as the online 3D Bin\nPacking Problem (3D-BPP). However, existing DRL-based methods primarily focus\non enhancing performance in limited packing environments while neglecting the\nability to generalize across multiple environments characterized by different\nbin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin\nPacking approach via Transformer-based deep reinforcement learning (DRL).\nFirst, we design a Placement Generator module to yield finite subspaces as\nplacement candidates and the representation of the bin. Second, we propose a\nPacking Transformer, which fuses the features of the items and bin, to identify\nthe spatial correlation between the item to be packed and available sub-spaces\nwithin the bin. Coupling these two components enables GOPT's ability to perform\ninference on bins of varying dimensions. We conduct extensive experiments and\ndemonstrate that GOPT not only achieves superior performance against the\nbaselines, but also exhibits excellent generalization capabilities.\nFurthermore, the deployment with a robot showcases the practical applicability\nof our method in the real world. The source code will be publicly available at\nhttps://github.com/Xiong5Heng/GOPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic object packing has broad practical applications in the logistics and\nautomation industry, often formulated by researchers as the online 3D Bin\nPacking Problem (3D-BPP). However, existing DRL-based methods primarily focus\non enhancing performance in limited packing environments while neglecting the\nability to generalize across multiple environments characterized by different\nbin dimensions. To this end, we propose GOPT, a generalizable online 3D Bin\nPacking approach via Transformer-based deep reinforcement learning (DRL).\nFirst, we design a Placement Generator module to yield finite subspaces as\nplacement candidates and the representation of the bin. Second, we propose a\nPacking Transformer, which fuses the features of the items and bin, to identify\nthe spatial correlation between the item to be packed and available sub-spaces\nwithin the bin. Coupling these two components enables GOPT's ability to perform\ninference on bins of varying dimensions. We conduct extensive experiments and\ndemonstrate that GOPT not only achieves superior performance against the\nbaselines, but also exhibits excellent generalization capabilities.\nFurthermore, the deployment with a robot showcases the practical applicability\nof our method in the real world. The source code will be publicly available at\nhttps://github.com/Xiong5Heng/GOPT."
                },
                "authors": [
                    {
                        "name": "Heng Xiong"
                    },
                    {
                        "name": "Changrong Guo"
                    },
                    {
                        "name": "Jian Peng"
                    },
                    {
                        "name": "Kai Ding"
                    },
                    {
                        "name": "Wenjie Chen"
                    },
                    {
                        "name": "Xuchong Qiu"
                    },
                    {
                        "name": "Long Bai"
                    },
                    {
                        "name": "Jianfeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Xu"
                },
                "author": "Jianfeng Xu",
                "arxiv_comment": "8 pages, 6 figures. This paper has been accepted by IEEE Robotics and\n  Automation Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13696v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13696v2",
                "updated": "2024-09-12T08:36:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    8,
                    36,
                    47,
                    3,
                    256,
                    0
                ],
                "published": "2024-07-18T17:00:23Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    17,
                    0,
                    23,
                    3,
                    200,
                    0
                ],
                "title": "Do These LLM Benchmarks Agree? Fixing Benchmark Evaluation with\n  BenchBench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do These LLM Benchmarks Agree? Fixing Benchmark Evaluation with\n  BenchBench"
                },
                "summary": "Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: github.com/IBM/BenchBench\n  Leaderboard: hf.co/spaces/IBM/BenchBench",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Language Models (LMs) have catalyzed the creation of\nmultiple benchmarks, designed to assess these models' general capabilities. A\ncrucial task, however, is assessing the validity of the benchmarks themselves.\nThis is most commonly done via Benchmark Agreement Testing (BAT), where new\nbenchmarks are validated against established ones using some agreement metric\n(e.g., rank correlation). Despite the crucial role of BAT for benchmark\nbuilders and consumers, there are no standardized procedures for such agreement\ntesting. This deficiency can lead to invalid conclusions, fostering mistrust in\nbenchmarks and upending the ability to properly choose the appropriate\nbenchmark to use. By analyzing over 40 prominent benchmarks, we demonstrate how\nsome overlooked methodological choices can significantly influence BAT results,\npotentially undermining the validity of conclusions. To address these\ninconsistencies, we propose a set of best practices for BAT and demonstrate how\nutilizing these methodologies greatly improves BAT robustness and validity. To\nfoster adoption and facilitate future research,, we introduce BenchBench, a\npython package for BAT, and release the BenchBench-leaderboard, a\nmeta-benchmark designed to evaluate benchmarks using their peers. Our findings\nunderscore the necessity for standardized BAT, ensuring the robustness and\nvalidity of benchmark evaluations in the evolving landscape of language model\nresearch.\n  BenchBench Package: github.com/IBM/BenchBench\n  Leaderboard: hf.co/spaces/IBM/BenchBench"
                },
                "authors": [
                    {
                        "name": "Yotam Perlitz"
                    },
                    {
                        "name": "Ariel Gera"
                    },
                    {
                        "name": "Ofir Arviv"
                    },
                    {
                        "name": "Asaf Yehudai"
                    },
                    {
                        "name": "Elron Bandel"
                    },
                    {
                        "name": "Eyal Shnarch"
                    },
                    {
                        "name": "Michal Shmueli-Scheuer"
                    },
                    {
                        "name": "Leshem Choshen"
                    }
                ],
                "author_detail": {
                    "name": "Leshem Choshen"
                },
                "author": "Leshem Choshen",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13696v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13696v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07830v1",
                "updated": "2024-09-12T08:26:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    8,
                    26,
                    33,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T08:26:33Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    8,
                    26,
                    33,
                    3,
                    256,
                    0
                ],
                "title": "ReGentS: Real-World Safety-Critical Driving Scenario Generation Made\n  Stable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReGentS: Real-World Safety-Critical Driving Scenario Generation Made\n  Stable"
                },
                "summary": "Machine learning based autonomous driving systems often face challenges with\nsafety-critical scenarios that are rare in real-world data, hindering their\nlarge-scale deployment. While increasing real-world training data coverage\ncould address this issue, it is costly and dangerous. This work explores\ngenerating safety-critical driving scenarios by modifying complex real-world\nregular scenarios through trajectory optimization. We propose ReGentS, which\nstabilizes generated trajectories and introduces heuristics to avoid obvious\ncollisions and optimization problems. Our approach addresses unrealistic\ndiverging trajectories and unavoidable collision scenarios that are not useful\nfor training robust planner. We also extend the scenario generation framework\nto handle real-world data with up to 32 agents. Additionally, by using a\ndifferentiable simulator, our approach simplifies gradient descent-based\noptimization involving a simulator, paving the way for future advancements. The\ncode is available at https://github.com/valeoai/ReGentS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning based autonomous driving systems often face challenges with\nsafety-critical scenarios that are rare in real-world data, hindering their\nlarge-scale deployment. While increasing real-world training data coverage\ncould address this issue, it is costly and dangerous. This work explores\ngenerating safety-critical driving scenarios by modifying complex real-world\nregular scenarios through trajectory optimization. We propose ReGentS, which\nstabilizes generated trajectories and introduces heuristics to avoid obvious\ncollisions and optimization problems. Our approach addresses unrealistic\ndiverging trajectories and unavoidable collision scenarios that are not useful\nfor training robust planner. We also extend the scenario generation framework\nto handle real-world data with up to 32 agents. Additionally, by using a\ndifferentiable simulator, our approach simplifies gradient descent-based\noptimization involving a simulator, paving the way for future advancements. The\ncode is available at https://github.com/valeoai/ReGentS."
                },
                "authors": [
                    {
                        "name": "Yuan Yin"
                    },
                    {
                        "name": "Pegah Khayatan"
                    },
                    {
                        "name": "Éloi Zablocki"
                    },
                    {
                        "name": "Alexandre Boulch"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "Accepted to ECCV 2024 W-CODA Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07829v1",
                "updated": "2024-09-12T08:25:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    8,
                    25,
                    33,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T08:25:33Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    8,
                    25,
                    33,
                    3,
                    256,
                    0
                ],
                "title": "Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs:\n  A Case Study in WeChat",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Cost-Effective UI Automation Testing with Retrieval-Based LLMs:\n  A Case Study in WeChat"
                },
                "summary": "UI automation tests play a crucial role in ensuring the quality of mobile\napplications. Despite the growing popularity of machine learning techniques to\ngenerate these tests, they still face several challenges, such as the mismatch\nof UI elements. The recent advances in Large Language Models (LLMs) have\naddressed these issues by leveraging their semantic understanding capabilities.\nHowever, a significant gap remains in applying these models to industrial-level\napp testing, particularly in terms of cost optimization and knowledge\nlimitation. To address this, we introduce CAT to create cost-effective UI\nautomation tests for industry apps by combining machine learning and LLMs with\nbest practices. Given the task description, CAT employs Retrieval Augmented\nGeneration (RAG) to source examples of industrial app usage as the few-shot\nlearning context, assisting LLMs in generating the specific sequence of\nactions. CAT then employs machine learning techniques, with LLMs serving as a\ncomplementary optimizer, to map the target element on the UI screen. Our\nevaluations on the WeChat testing dataset demonstrate the CAT's performance and\ncost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming\nthe state-of-the-art. We have also integrated our approach into the real-world\nWeChat testing platform, demonstrating its usefulness in detecting 141 bugs and\nenhancing the developers' testing process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UI automation tests play a crucial role in ensuring the quality of mobile\napplications. Despite the growing popularity of machine learning techniques to\ngenerate these tests, they still face several challenges, such as the mismatch\nof UI elements. The recent advances in Large Language Models (LLMs) have\naddressed these issues by leveraging their semantic understanding capabilities.\nHowever, a significant gap remains in applying these models to industrial-level\napp testing, particularly in terms of cost optimization and knowledge\nlimitation. To address this, we introduce CAT to create cost-effective UI\nautomation tests for industry apps by combining machine learning and LLMs with\nbest practices. Given the task description, CAT employs Retrieval Augmented\nGeneration (RAG) to source examples of industrial app usage as the few-shot\nlearning context, assisting LLMs in generating the specific sequence of\nactions. CAT then employs machine learning techniques, with LLMs serving as a\ncomplementary optimizer, to map the target element on the UI screen. Our\nevaluations on the WeChat testing dataset demonstrate the CAT's performance and\ncost-effectiveness, achieving 90% UI automation with $0.34 cost, outperforming\nthe state-of-the-art. We have also integrated our approach into the real-world\nWeChat testing platform, demonstrating its usefulness in detecting 141 bugs and\nenhancing the developers' testing process."
                },
                "authors": [
                    {
                        "name": "Sidong Feng"
                    },
                    {
                        "name": "Haochuan Lu"
                    },
                    {
                        "name": "Jianqin Jiang"
                    },
                    {
                        "name": "Ting Xiong"
                    },
                    {
                        "name": "Likun Huang"
                    },
                    {
                        "name": "Yinglin Liang"
                    },
                    {
                        "name": "Xiaoqin Li"
                    },
                    {
                        "name": "Yuetang Deng"
                    },
                    {
                        "name": "Aldeida Aleti"
                    }
                ],
                "author_detail": {
                    "name": "Aldeida Aleti"
                },
                "author": "Aldeida Aleti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07813v1",
                "updated": "2024-09-12T07:46:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    7,
                    46,
                    58,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T07:46:58Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    7,
                    46,
                    58,
                    3,
                    256,
                    0
                ],
                "title": "What is YOLOv9: An In-Depth Exploration of the Internal Features of the\n  Next-Generation Object Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What is YOLOv9: An In-Depth Exploration of the Internal Features of the\n  Next-Generation Object Detector"
                },
                "summary": "This study provides a comprehensive analysis of the YOLOv9 object detection\nmodel, focusing on its architectural innovations, training methodologies, and\nperformance improvements over its predecessors. Key advancements, such as the\nGeneralized Efficient Layer Aggregation Network GELAN and Programmable Gradient\nInformation PGI, significantly enhance feature extraction and gradient flow,\nleading to improved accuracy and efficiency. By incorporating Depthwise\nConvolutions and the lightweight C3Ghost architecture, YOLOv9 reduces\ncomputational complexity while maintaining high precision. Benchmark tests on\nMicrosoft COCO demonstrate its superior mean Average Precision mAP and faster\ninference times, outperforming YOLOv8 across multiple metrics. The model\nversatility is highlighted by its seamless deployment across various hardware\nplatforms, from edge devices to high performance GPUs, with built in support\nfor PyTorch and TensorRT integration. This paper provides the first in depth\nexploration of YOLOv9s internal features and their real world applicability,\nestablishing it as a state of the art solution for real time object detection\nacross industries, from IoT devices to large scale industrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study provides a comprehensive analysis of the YOLOv9 object detection\nmodel, focusing on its architectural innovations, training methodologies, and\nperformance improvements over its predecessors. Key advancements, such as the\nGeneralized Efficient Layer Aggregation Network GELAN and Programmable Gradient\nInformation PGI, significantly enhance feature extraction and gradient flow,\nleading to improved accuracy and efficiency. By incorporating Depthwise\nConvolutions and the lightweight C3Ghost architecture, YOLOv9 reduces\ncomputational complexity while maintaining high precision. Benchmark tests on\nMicrosoft COCO demonstrate its superior mean Average Precision mAP and faster\ninference times, outperforming YOLOv8 across multiple metrics. The model\nversatility is highlighted by its seamless deployment across various hardware\nplatforms, from edge devices to high performance GPUs, with built in support\nfor PyTorch and TensorRT integration. This paper provides the first in depth\nexploration of YOLOv9s internal features and their real world applicability,\nestablishing it as a state of the art solution for real time object detection\nacross industries, from IoT devices to large scale industrial applications."
                },
                "authors": [
                    {
                        "name": "Muhammad Yaseen"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Yaseen"
                },
                "author": "Muhammad Yaseen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12935v2",
                "updated": "2024-09-12T07:45:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    7,
                    45,
                    50,
                    3,
                    256,
                    0
                ],
                "published": "2024-08-23T09:33:48Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    9,
                    33,
                    48,
                    4,
                    236,
                    0
                ],
                "title": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural\n  Framework for AI Safety with Challenges and Mitigations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural\n  Framework for AI Safety with Challenges and Mitigations"
                },
                "summary": "AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Safety is an emerging area of critical importance to the safe adoption and\ndeployment of AI systems. With the rapid proliferation of AI and especially\nwith the recent advancement of Generative AI (or GAI), the technology ecosystem\nbehind the design, development, adoption, and deployment of AI systems has\ndrastically changed, broadening the scope of AI Safety to address impacts on\npublic safety and national security. In this paper, we propose a novel\narchitectural framework for understanding and analyzing AI Safety; defining its\ncharacteristics from three perspectives: Trustworthy AI, Responsible AI, and\nSafe AI. We provide an extensive review of current research and advancements in\nAI safety from these perspectives, highlighting their key challenges and\nmitigation approaches. Through examples from state-of-the-art technologies,\nparticularly Large Language Models (LLMs), we present innovative mechanism,\nmethodologies, and techniques for designing and testing AI safety. Our goal is\nto promote advancement in AI safety research, and ultimately enhance people's\ntrust in digital transformation."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Ziyao Liu"
                    },
                    {
                        "name": "Weifeng Jiang"
                    },
                    {
                        "name": "Si Qi Goh"
                    },
                    {
                        "name": "Kwok-Yan Lam"
                    }
                ],
                "author_detail": {
                    "name": "Kwok-Yan Lam"
                },
                "author": "Kwok-Yan Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10843v2",
                "updated": "2024-09-12T07:39:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    7,
                    39,
                    49,
                    3,
                    256,
                    0
                ],
                "published": "2024-08-19T11:42:54Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    42,
                    54,
                    0,
                    232,
                    0
                ],
                "title": "Detecting Wildfires on UAVs with Real-time Segmentation Trained by\n  Larger Teacher Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Wildfires on UAVs with Real-time Segmentation Trained by\n  Larger Teacher Models"
                },
                "summary": "Early detection of wildfires is essential to prevent large-scale fires\nresulting in extensive environmental, structural, and societal damage. Uncrewed\naerial vehicles (UAVs) can cover large remote areas effectively with quick\ndeployment requiring minimal infrastructure and equipping them with small\ncameras and computers enables autonomous real-time detection. In remote areas,\nhowever, the UAVs are limited to on-board computing for detection due to the\nlack of high-bandwidth mobile networks. This limits the detection to methods\nwhich are light enough for the on-board computer alone. For accurate\ncamera-based localisation, segmentation of the detected smoke is essential but\ntraining data for deep learning-based wildfire smoke segmentation is limited.\nThis study shows how small specialised segmentation models can be trained using\nonly bounding box labels, leveraging zero-shot foundation model supervision.\nThe method offers the advantages of needing only fairly easily obtainable\nbounding box labels and requiring training solely for the smaller student\nnetwork. The proposed method achieved 63.3% mIoU on a manually annotated and\ndiverse wildfire dataset. The used model can perform in real-time at ~25 fps\nwith a UAV-carried NVIDIA Jetson Orin NX computer while reliably recognising\nsmoke, demonstrated at real-world forest burning events. Code is available at\nhttps://gitlab.com/fgi_nls/public/wildfire-real-time-segmentation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early detection of wildfires is essential to prevent large-scale fires\nresulting in extensive environmental, structural, and societal damage. Uncrewed\naerial vehicles (UAVs) can cover large remote areas effectively with quick\ndeployment requiring minimal infrastructure and equipping them with small\ncameras and computers enables autonomous real-time detection. In remote areas,\nhowever, the UAVs are limited to on-board computing for detection due to the\nlack of high-bandwidth mobile networks. This limits the detection to methods\nwhich are light enough for the on-board computer alone. For accurate\ncamera-based localisation, segmentation of the detected smoke is essential but\ntraining data for deep learning-based wildfire smoke segmentation is limited.\nThis study shows how small specialised segmentation models can be trained using\nonly bounding box labels, leveraging zero-shot foundation model supervision.\nThe method offers the advantages of needing only fairly easily obtainable\nbounding box labels and requiring training solely for the smaller student\nnetwork. The proposed method achieved 63.3% mIoU on a manually annotated and\ndiverse wildfire dataset. The used model can perform in real-time at ~25 fps\nwith a UAV-carried NVIDIA Jetson Orin NX computer while reliably recognising\nsmoke, demonstrated at real-world forest burning events. Code is available at\nhttps://gitlab.com/fgi_nls/public/wildfire-real-time-segmentation"
                },
                "authors": [
                    {
                        "name": "Julius Pesonen"
                    },
                    {
                        "name": "Teemu Hakala"
                    },
                    {
                        "name": "Väinö Karjalainen"
                    },
                    {
                        "name": "Niko Koivumäki"
                    },
                    {
                        "name": "Lauri Markelin"
                    },
                    {
                        "name": "Anna-Maria Raita-Hakola"
                    },
                    {
                        "name": "Juha Suomalainen"
                    },
                    {
                        "name": "Ilkka Pölönen"
                    },
                    {
                        "name": "Eija Honkavaara"
                    }
                ],
                "author_detail": {
                    "name": "Eija Honkavaara"
                },
                "author": "Eija Honkavaara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12093v2",
                "updated": "2024-09-12T07:18:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    7,
                    18,
                    0,
                    3,
                    256,
                    0
                ],
                "published": "2024-08-22T03:03:04Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    3,
                    3,
                    4,
                    3,
                    235,
                    0
                ],
                "title": "LLM-enhanced Scene Graph Learning for Household Rearrangement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-enhanced Scene Graph Learning for Household Rearrangement"
                },
                "summary": "The household rearrangement task involves spotting misplaced objects in a\nscene and accommodate them with proper places. It depends both on common-sense\nknowledge on the objective side and human user preference on the subjective\nside. In achieving such task, we propose to mine object functionality with user\npreference alignment directly from the scene itself, without relying on human\nintervention. To do so, we work with scene graph representation and propose\nLLM-enhanced scene graph learning which transforms the input scene graph into\nan affordance-enhanced graph (AEG) with information-enhanced nodes and newly\ndiscovered edges (relations). In AEG, the nodes corresponding to the receptacle\nobjects are augmented with context-induced affordance which encodes what kind\nof carriable objects can be placed on it. New edges are discovered with newly\ndiscovered non-local relations. With AEG, we perform task planning for scene\nrearrangement by detecting misplaced carriables and determining a proper\nplacement for each of them. We test our method by implementing a tiding robot\nin simulator and perform evaluation on a new benchmark we build. Extensive\nevaluations demonstrate that our method achieves state-of-the-art performance\non misplacement detection and the following rearrangement planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The household rearrangement task involves spotting misplaced objects in a\nscene and accommodate them with proper places. It depends both on common-sense\nknowledge on the objective side and human user preference on the subjective\nside. In achieving such task, we propose to mine object functionality with user\npreference alignment directly from the scene itself, without relying on human\nintervention. To do so, we work with scene graph representation and propose\nLLM-enhanced scene graph learning which transforms the input scene graph into\nan affordance-enhanced graph (AEG) with information-enhanced nodes and newly\ndiscovered edges (relations). In AEG, the nodes corresponding to the receptacle\nobjects are augmented with context-induced affordance which encodes what kind\nof carriable objects can be placed on it. New edges are discovered with newly\ndiscovered non-local relations. With AEG, we perform task planning for scene\nrearrangement by detecting misplaced carriables and determining a proper\nplacement for each of them. We test our method by implementing a tiding robot\nin simulator and perform evaluation on a new benchmark we build. Extensive\nevaluations demonstrate that our method achieves state-of-the-art performance\non misplacement detection and the following rearrangement planning."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Zhiyuan Yu"
                    },
                    {
                        "name": "Qijin She"
                    },
                    {
                        "name": "Zhinan Yu"
                    },
                    {
                        "name": "Yuqing Lan"
                    },
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Ruizhen Hu"
                    },
                    {
                        "name": "Kai Xu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Xu"
                },
                "author": "Kai Xu",
                "arxiv_comment": "SIGGRAPH ASIA 2024 conference accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07796v1",
                "updated": "2024-09-12T06:56:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    56,
                    52,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T06:56:52Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    56,
                    52,
                    3,
                    256,
                    0
                ],
                "title": "In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for\n  Efficient Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Situ Fine-Tuning of Wildlife Models in IoT-Enabled Camera Traps for\n  Efficient Adaptation"
                },
                "summary": "Wildlife monitoring via camera traps has become an essential tool in ecology,\nbut the deployment of machine learning models for on-device animal\nclassification faces significant challenges due to domain shifts and resource\nconstraints. This paper introduces WildFit, a novel approach that reconciles\nthe conflicting goals of achieving high domain generalization performance and\nensuring efficient inference for camera trap applications. WildFit leverages\ncontinuous background-aware model fine-tuning to deploy ML models tailored to\nthe current location and time window, allowing it to maintain robust\nclassification accuracy in the new environment without requiring significant\ncomputational resources. This is achieved by background-aware data synthesis,\nwhich generates training images representing the new domain by blending\nbackground images with animal images from the source domain. We further enhance\nfine-tuning effectiveness through background drift detection and class\ndistribution drift detection, which optimize the quality of synthesized data\nand improve generalization performance. Our extensive evaluation across\nmultiple camera trap datasets demonstrates that WildFit achieves significant\nimprovements in classification accuracy and computational efficiency compared\nto traditional approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wildlife monitoring via camera traps has become an essential tool in ecology,\nbut the deployment of machine learning models for on-device animal\nclassification faces significant challenges due to domain shifts and resource\nconstraints. This paper introduces WildFit, a novel approach that reconciles\nthe conflicting goals of achieving high domain generalization performance and\nensuring efficient inference for camera trap applications. WildFit leverages\ncontinuous background-aware model fine-tuning to deploy ML models tailored to\nthe current location and time window, allowing it to maintain robust\nclassification accuracy in the new environment without requiring significant\ncomputational resources. This is achieved by background-aware data synthesis,\nwhich generates training images representing the new domain by blending\nbackground images with animal images from the source domain. We further enhance\nfine-tuning effectiveness through background drift detection and class\ndistribution drift detection, which optimize the quality of synthesized data\nand improve generalization performance. Our extensive evaluation across\nmultiple camera trap datasets demonstrates that WildFit achieves significant\nimprovements in classification accuracy and computational efficiency compared\nto traditional approaches."
                },
                "authors": [
                    {
                        "name": "Mohammad Mehdi Rastikerdar"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Hui Guan"
                    },
                    {
                        "name": "Deepak Ganesan"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Ganesan"
                },
                "author": "Deepak Ganesan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07790v1",
                "updated": "2024-09-12T06:50:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    50,
                    45,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T06:50:45Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    50,
                    45,
                    3,
                    256,
                    0
                ],
                "title": "Full-text Error Correction for Chinese Speech Recognition with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-text Error Correction for Chinese Speech Recognition with Large\n  Language Model"
                },
                "summary": "Large Language Models (LLMs) have demonstrated substantial potential for\nerror correction in Automatic Speech Recognition (ASR). However, most research\nfocuses on utterances from short-duration speech recordings, which are the\npredominant form of speech data for supervised ASR training. This paper\ninvestigates the effectiveness of LLMs for error correction in full-text\ngenerated by ASR systems from longer speech recordings, such as transcripts\nfrom podcasts, news broadcasts, and meetings. First, we develop a Chinese\ndataset for full-text error correction, named ChFT, utilizing a pipeline that\ninvolves text-to-speech synthesis, ASR, and error-correction pair extractor.\nThis dataset enables us to correct errors across contexts, including both\nfull-text and segment, and to address a broader range of error types, such as\npunctuation restoration and inverse text normalization, thus making the\ncorrection process comprehensive. Second, we fine-tune a pre-trained LLM on the\nconstructed dataset using a diverse set of prompts and target formats, and\nevaluate its performance on full-text error correction. Specifically, we design\nprompts based on full-text and segment, considering various output formats,\nsuch as directly corrected text and JSON-based error-correction pairs. Through\nvarious test settings, including homogeneous, up-to-date, and hard test sets,\nwe find that the fine-tuned LLMs perform well in the full-text setting with\ndifferent prompts, each presenting its own strengths and weaknesses. This\nestablishes a promising baseline for further research. The dataset is available\non the website.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated substantial potential for\nerror correction in Automatic Speech Recognition (ASR). However, most research\nfocuses on utterances from short-duration speech recordings, which are the\npredominant form of speech data for supervised ASR training. This paper\ninvestigates the effectiveness of LLMs for error correction in full-text\ngenerated by ASR systems from longer speech recordings, such as transcripts\nfrom podcasts, news broadcasts, and meetings. First, we develop a Chinese\ndataset for full-text error correction, named ChFT, utilizing a pipeline that\ninvolves text-to-speech synthesis, ASR, and error-correction pair extractor.\nThis dataset enables us to correct errors across contexts, including both\nfull-text and segment, and to address a broader range of error types, such as\npunctuation restoration and inverse text normalization, thus making the\ncorrection process comprehensive. Second, we fine-tune a pre-trained LLM on the\nconstructed dataset using a diverse set of prompts and target formats, and\nevaluate its performance on full-text error correction. Specifically, we design\nprompts based on full-text and segment, considering various output formats,\nsuch as directly corrected text and JSON-based error-correction pairs. Through\nvarious test settings, including homogeneous, up-to-date, and hard test sets,\nwe find that the fine-tuned LLMs perform well in the full-text setting with\ndifferent prompts, each presenting its own strengths and weaknesses. This\nestablishes a promising baseline for further research. The dataset is available\non the website."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Tang"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Shen Huang"
                    },
                    {
                        "name": "Shidong Shang"
                    }
                ],
                "author_detail": {
                    "name": "Shidong Shang"
                },
                "author": "Shidong Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07772v1",
                "updated": "2024-09-12T06:10:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    10,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T06:10:15Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    6,
                    10,
                    15,
                    3,
                    256,
                    0
                ],
                "title": "Alignment with Preference Optimization Is All You Need for LLM Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment with Preference Optimization Is All You Need for LLM Safety"
                },
                "summary": "We demonstrate that preference optimization methods can effectively enhance\nLLM safety. Applying various alignment techniques to the Falcon 11B model using\nsafety datasets, we achieve a significant boost in global safety score (from\n$57.64\\%$ to $99.90\\%$) as measured by LlamaGuard 3 8B, competing with\nstate-of-the-art models. On toxicity benchmarks, average scores in adversarial\nsettings dropped from over $0.6$ to less than $0.07$. However, this safety\nimprovement comes at the cost of reduced general capabilities, particularly in\nmath, suggesting a trade-off. We identify noise contrastive alignment\n(Safe-NCA) as an optimal method for balancing safety and performance. Our study\nultimately shows that alignment techniques can be sufficient for building safe\nand robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that preference optimization methods can effectively enhance\nLLM safety. Applying various alignment techniques to the Falcon 11B model using\nsafety datasets, we achieve a significant boost in global safety score (from\n$57.64\\%$ to $99.90\\%$) as measured by LlamaGuard 3 8B, competing with\nstate-of-the-art models. On toxicity benchmarks, average scores in adversarial\nsettings dropped from over $0.6$ to less than $0.07$. However, this safety\nimprovement comes at the cost of reduced general capabilities, particularly in\nmath, suggesting a trade-off. We identify noise contrastive alignment\n(Safe-NCA) as an optimal method for balancing safety and performance. Our study\nultimately shows that alignment techniques can be sufficient for building safe\nand robust models."
                },
                "authors": [
                    {
                        "name": "Reda Alami"
                    },
                    {
                        "name": "Ali Khalifa Almansoori"
                    },
                    {
                        "name": "Ahmed Alzubaidi"
                    },
                    {
                        "name": "Mohamed El Amine Seddik"
                    },
                    {
                        "name": "Mugariya Farooq"
                    },
                    {
                        "name": "Hakim Hacid"
                    }
                ],
                "author_detail": {
                    "name": "Hakim Hacid"
                },
                "author": "Hakim Hacid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07756v1",
                "updated": "2024-09-12T05:18:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    5,
                    18,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T05:18:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    5,
                    18,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "DiTAS: Quantizing Diffusion Transformers via Enhanced Activation\n  Smoothing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTAS: Quantizing Diffusion Transformers via Enhanced Activation\n  Smoothing"
                },
                "summary": "Diffusion Transformers (DiTs) have recently attracted significant interest\nfrom both industry and academia due to their enhanced capabilities in visual\ngeneration, surpassing the performance of traditional diffusion models that\nemploy U-Net. However, the improved performance of DiTs comes at the expense of\nhigher parameter counts and implementation costs, which significantly limits\ntheir deployment on resource-constrained devices like mobile phones. We propose\nDiTAS, a data-free post-training quantization (PTQ) method for efficient DiT\ninference. DiTAS relies on the proposed temporal-aggregated smoothing\ntechniques to mitigate the impact of the channel-wise outliers within the input\nactivations, leading to much lower quantization error under extremely low\nbitwidth. To further enhance the performance of the quantized DiT, we adopt the\nlayer-wise grid search strategy to optimize the smoothing factor. Experimental\nresults demonstrate that our approach enables 4-bit weight, 8-bit activation\n(W4A8) quantization for DiTs while maintaining comparable performance as the\nfull-precision model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have recently attracted significant interest\nfrom both industry and academia due to their enhanced capabilities in visual\ngeneration, surpassing the performance of traditional diffusion models that\nemploy U-Net. However, the improved performance of DiTs comes at the expense of\nhigher parameter counts and implementation costs, which significantly limits\ntheir deployment on resource-constrained devices like mobile phones. We propose\nDiTAS, a data-free post-training quantization (PTQ) method for efficient DiT\ninference. DiTAS relies on the proposed temporal-aggregated smoothing\ntechniques to mitigate the impact of the channel-wise outliers within the input\nactivations, leading to much lower quantization error under extremely low\nbitwidth. To further enhance the performance of the quantized DiT, we adopt the\nlayer-wise grid search strategy to optimize the smoothing factor. Experimental\nresults demonstrate that our approach enables 4-bit weight, 8-bit activation\n(W4A8) quantization for DiTs while maintaining comparable performance as the\nfull-precision model."
                },
                "authors": [
                    {
                        "name": "Zhenyuan Dong"
                    },
                    {
                        "name": "Sai Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sai Qian Zhang"
                },
                "author": "Sai Qian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03426v2",
                "updated": "2024-09-12T04:47:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    47,
                    33,
                    3,
                    256,
                    0
                ],
                "published": "2024-01-07T09:06:58Z",
                "published_parsed": [
                    2024,
                    1,
                    7,
                    9,
                    6,
                    58,
                    6,
                    7,
                    0
                ],
                "title": "On Leveraging Large Language Models for Enhancing Entity Resolution: A\n  Cost-efficient Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Leveraging Large Language Models for Enhancing Entity Resolution: A\n  Cost-efficient Approach"
                },
                "summary": "Entity resolution, the task of identifying and merging records that refer to\nthe same real-world entity, is crucial in sectors like e-commerce, healthcare,\nand law enforcement. Large Language Models (LLMs) introduce an innovative\napproach to this task, capitalizing on their advanced linguistic capabilities\nand a ``pay-as-you-go'' model that provides significant advantages to those\nwithout extensive data science expertise. However, current LLMs are costly due\nto per-API request billing. Existing methods often either lack quality or\nbecome prohibitively expensive at scale. To address these problems, we propose\nan uncertainty reduction framework using LLMs to improve entity resolution\nresults. We first initialize possible partitions of the entity cluster, refer\nto the same entity, and define the uncertainty of the result. Then, we reduce\nthe uncertainty by selecting a few valuable matching questions for LLM\nverification. Upon receiving the answers, we update the probability\ndistribution of the possible partitions. To further reduce costs, we design an\nefficient algorithm to judiciously select the most valuable matching pairs to\nquery. Additionally, we create error-tolerant techniques to handle LLM mistakes\nand a dynamic adjustment method to reach truly correct partitions. Experimental\nresults show that our method is efficient and effective, offering promising\napplications in real-world tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity resolution, the task of identifying and merging records that refer to\nthe same real-world entity, is crucial in sectors like e-commerce, healthcare,\nand law enforcement. Large Language Models (LLMs) introduce an innovative\napproach to this task, capitalizing on their advanced linguistic capabilities\nand a ``pay-as-you-go'' model that provides significant advantages to those\nwithout extensive data science expertise. However, current LLMs are costly due\nto per-API request billing. Existing methods often either lack quality or\nbecome prohibitively expensive at scale. To address these problems, we propose\nan uncertainty reduction framework using LLMs to improve entity resolution\nresults. We first initialize possible partitions of the entity cluster, refer\nto the same entity, and define the uncertainty of the result. Then, we reduce\nthe uncertainty by selecting a few valuable matching questions for LLM\nverification. Upon receiving the answers, we update the probability\ndistribution of the possible partitions. To further reduce costs, we design an\nefficient algorithm to judiciously select the most valuable matching pairs to\nquery. Additionally, we create error-tolerant techniques to handle LLM mistakes\nand a dynamic adjustment method to reach truly correct partitions. Experimental\nresults show that our method is efficient and effective, offering promising\napplications in real-world tasks."
                },
                "authors": [
                    {
                        "name": "Huahang Li"
                    },
                    {
                        "name": "Longyu Feng"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Fei Hao"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Yuanfeng Song"
                    }
                ],
                "author_detail": {
                    "name": "Yuanfeng Song"
                },
                "author": "Yuanfeng Song",
                "arxiv_comment": "9 pages, preprint under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07737v1",
                "updated": "2024-09-12T04:06:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    6,
                    31,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T04:06:31Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    4,
                    6,
                    31,
                    3,
                    256,
                    0
                ],
                "title": "Ruri: Japanese General Text Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ruri: Japanese General Text Embeddings"
                },
                "summary": "We report the development of Ruri, a series of Japanese general text\nembedding models. While the development of general-purpose text embedding\nmodels in English and multilingual contexts has been active in recent years,\nmodel development in Japanese remains insufficient. The primary reasons for\nthis are the lack of datasets and the absence of necessary expertise. In this\nreport, we provide a detailed account of the development process of Ruri.\nSpecifically, we discuss the training of embedding models using synthesized\ndatasets generated by LLMs, the construction of the reranker for dataset\nfiltering and knowledge distillation, and the performance evaluation of the\nresulting general-purpose text embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the development of Ruri, a series of Japanese general text\nembedding models. While the development of general-purpose text embedding\nmodels in English and multilingual contexts has been active in recent years,\nmodel development in Japanese remains insufficient. The primary reasons for\nthis are the lack of datasets and the absence of necessary expertise. In this\nreport, we provide a detailed account of the development process of Ruri.\nSpecifically, we discuss the training of embedding models using synthesized\ndatasets generated by LLMs, the construction of the reranker for dataset\nfiltering and knowledge distillation, and the performance evaluation of the\nresulting general-purpose text embedding models."
                },
                "authors": [
                    {
                        "name": "Hayato Tsukagoshi"
                    },
                    {
                        "name": "Ryohei Sasano"
                    }
                ],
                "author_detail": {
                    "name": "Ryohei Sasano"
                },
                "author": "Ryohei Sasano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07732v1",
                "updated": "2024-09-12T03:41:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    3,
                    41,
                    39,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T03:41:39Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    3,
                    41,
                    39,
                    3,
                    256,
                    0
                ],
                "title": "Large Language Models are Pattern Matchers: Editing Semi-Structured and\n  Structured Documents with ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Pattern Matchers: Editing Semi-Structured and\n  Structured Documents with ChatGPT"
                },
                "summary": "Large Language Models (LLMs) offer numerous applications, the full extent of\nwhich is not yet understood. This paper investigates if LLMs can be applied for\nediting structured and semi-structured documents with minimal effort. Using a\nqualitative research approach, we conduct two case studies with ChatGPT and\nthoroughly analyze the results. Our experiments indicate that LLMs can\neffectively edit structured and semi-structured documents when provided with\nbasic, straightforward prompts. ChatGPT demonstrates a strong ability to\nrecognize and process the structure of annotated documents. This suggests that\nexplicitly structuring tasks and data in prompts might enhance an LLM's ability\nto understand and solve tasks. Furthermore, the experiments also reveal\nimpressive pattern matching skills in ChatGPT. This observation deserves\nfurther investigation, as it may contribute to understanding the processes\nleading to hallucinations in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) offer numerous applications, the full extent of\nwhich is not yet understood. This paper investigates if LLMs can be applied for\nediting structured and semi-structured documents with minimal effort. Using a\nqualitative research approach, we conduct two case studies with ChatGPT and\nthoroughly analyze the results. Our experiments indicate that LLMs can\neffectively edit structured and semi-structured documents when provided with\nbasic, straightforward prompts. ChatGPT demonstrates a strong ability to\nrecognize and process the structure of annotated documents. This suggests that\nexplicitly structuring tasks and data in prompts might enhance an LLM's ability\nto understand and solve tasks. Furthermore, the experiments also reveal\nimpressive pattern matching skills in ChatGPT. This observation deserves\nfurther investigation, as it may contribute to understanding the processes\nleading to hallucinations in LLMs."
                },
                "authors": [
                    {
                        "name": "Irene Weber"
                    }
                ],
                "author_detail": {
                    "name": "Irene Weber"
                },
                "author": "Irene Weber",
                "arxiv_doi": "10.18420/AKWI2024-001",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18420/AKWI2024-001",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.07732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "AKWI Jahrestagung 2024, Lecture Notes in Informatics (LNI) Bd. 357\n  (2024)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14206v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14206v3",
                "updated": "2024-09-12T02:57:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    57,
                    7,
                    3,
                    256,
                    0
                ],
                "published": "2023-12-21T08:15:02Z",
                "published_parsed": [
                    2023,
                    12,
                    21,
                    8,
                    15,
                    2,
                    3,
                    355,
                    0
                ],
                "title": "LLM4VG: Large Language Models Evaluation for Video Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4VG: Large Language Models Evaluation for Video Grounding"
                },
                "summary": "Recently, researchers have attempted to investigate the capability of LLMs in\nhandling videos and proposed several video LLM models. However, the ability of\nLLMs to handle video grounding (VG), which is an important time-related video\ntask requiring the model to precisely locate the start and end timestamps of\ntemporal moments in videos that match the given textual queries, still remains\nunclear and unexplored in literature. To fill the gap, in this paper, we\npropose the LLM4VG benchmark, which systematically evaluates the performance of\ndifferent LLMs on video grounding tasks. Based on our proposed LLM4VG, we\ndesign extensive experiments to examine two groups of video LLM models on video\ngrounding: (i) the video LLMs trained on the text-video pairs (denoted as\nVidLLM), and (ii) the LLMs combined with pretrained visual description models\nsuch as the video/image captioning model. We propose prompt methods to\nintegrate the instruction of VG and description from different kinds of\ngenerators, including caption-based generators for direct visual description\nand VQA-based generators for information enhancement. We also provide\ncomprehensive comparisons of various VidLLMs and explore the influence of\ndifferent choices of visual models, LLMs, prompt designs, etc, as well. Our\nexperimental evaluations lead to two conclusions: (i) the existing VidLLMs are\nstill far away from achieving satisfactory video grounding performance, and\nmore time-related video tasks should be included to further fine-tune these\nmodels, and (ii) the combination of LLMs and visual models shows preliminary\nabilities for video grounding with considerable potential for improvement by\nresorting to more reliable models and further guidance of prompt instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, researchers have attempted to investigate the capability of LLMs in\nhandling videos and proposed several video LLM models. However, the ability of\nLLMs to handle video grounding (VG), which is an important time-related video\ntask requiring the model to precisely locate the start and end timestamps of\ntemporal moments in videos that match the given textual queries, still remains\nunclear and unexplored in literature. To fill the gap, in this paper, we\npropose the LLM4VG benchmark, which systematically evaluates the performance of\ndifferent LLMs on video grounding tasks. Based on our proposed LLM4VG, we\ndesign extensive experiments to examine two groups of video LLM models on video\ngrounding: (i) the video LLMs trained on the text-video pairs (denoted as\nVidLLM), and (ii) the LLMs combined with pretrained visual description models\nsuch as the video/image captioning model. We propose prompt methods to\nintegrate the instruction of VG and description from different kinds of\ngenerators, including caption-based generators for direct visual description\nand VQA-based generators for information enhancement. We also provide\ncomprehensive comparisons of various VidLLMs and explore the influence of\ndifferent choices of visual models, LLMs, prompt designs, etc, as well. Our\nexperimental evaluations lead to two conclusions: (i) the existing VidLLMs are\nstill far away from achieving satisfactory video grounding performance, and\nmore time-related video tasks should be included to further fine-tune these\nmodels, and (ii) the combination of LLMs and visual models shows preliminary\nabilities for video grounding with considerable potential for improvement by\nresorting to more reliable models and further guidance of prompt instructions."
                },
                "authors": [
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Zeyang Zhang"
                    },
                    {
                        "name": "Houlun Chen"
                    },
                    {
                        "name": "Zihan Song"
                    },
                    {
                        "name": "Yuwei Zhou"
                    },
                    {
                        "name": "Yuekui Yang"
                    },
                    {
                        "name": "Haiyang Wu"
                    },
                    {
                        "name": "Wenwu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wenwu Zhu"
                },
                "author": "Wenwu Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14206v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14206v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07703v1",
                "updated": "2024-09-12T02:08:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    8,
                    0,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:08:00Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    8,
                    0,
                    3,
                    256,
                    0
                ],
                "title": "DSBench: How Far Are Data Science Agents to Becoming Data Science\n  Experts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSBench: How Far Are Data Science Agents to Becoming Data Science\n  Experts?"
                },
                "summary": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have\ndemonstrated impressive language/vision reasoning abilities, igniting the\nrecent trend of building agents for targeted applications such as shopping\nassistants or AI software engineers. Recently, many data science benchmarks\nhave been proposed to investigate their performance in the data science domain.\nHowever, existing data science benchmarks still fall short when compared to\nreal-world data science applications due to their simplified settings. To\nbridge this gap, we introduce DSBench, a comprehensive benchmark designed to\nevaluate data science agents with realistic tasks. This benchmark includes 466\ndata analysis tasks and 74 data modeling tasks, sourced from Eloquence and\nKaggle competitions. DSBench offers a realistic setting by encompassing long\ncontexts, multimodal task backgrounds, reasoning with large data files and\nmulti-table structures, and performing end-to-end data modeling tasks. Our\nevaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle\nwith most tasks, with the best agent solving only 34.12% of data analysis tasks\nand achieving a 34.74% Relative Performance Gap (RPG). These findings\nunderscore the need for further advancements in developing more practical,\nintelligent, and autonomous data science agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have\ndemonstrated impressive language/vision reasoning abilities, igniting the\nrecent trend of building agents for targeted applications such as shopping\nassistants or AI software engineers. Recently, many data science benchmarks\nhave been proposed to investigate their performance in the data science domain.\nHowever, existing data science benchmarks still fall short when compared to\nreal-world data science applications due to their simplified settings. To\nbridge this gap, we introduce DSBench, a comprehensive benchmark designed to\nevaluate data science agents with realistic tasks. This benchmark includes 466\ndata analysis tasks and 74 data modeling tasks, sourced from Eloquence and\nKaggle competitions. DSBench offers a realistic setting by encompassing long\ncontexts, multimodal task backgrounds, reasoning with large data files and\nmulti-table structures, and performing end-to-end data modeling tasks. Our\nevaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle\nwith most tasks, with the best agent solving only 34.12% of data analysis tasks\nand achieving a 34.74% Relative Performance Gap (RPG). These findings\nunderscore the need for further advancements in developing more practical,\nintelligent, and autonomous data science agents."
                },
                "authors": [
                    {
                        "name": "Liqiang Jing"
                    },
                    {
                        "name": "Zhehui Huang"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Wenlin Yao"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Kaixin Ma"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Xinya Du"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07694v1",
                "updated": "2024-09-12T01:58:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    1,
                    58,
                    6,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T01:58:06Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    1,
                    58,
                    6,
                    3,
                    256,
                    0
                ],
                "title": "Learn from Balance: Rectifying Knowledge Transfer for Long-Tailed\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn from Balance: Rectifying Knowledge Transfer for Long-Tailed\n  Scenarios"
                },
                "summary": "Knowledge Distillation (KD) transfers knowledge from a large pre-trained\nteacher network to a compact and efficient student network, making it suitable\nfor deployment on resource-limited media terminals. However, traditional KD\nmethods require balanced data to ensure robust training, which is often\nunavailable in practical applications. In such scenarios, a few head categories\noccupy a substantial proportion of examples. This imbalance biases the trained\nteacher network towards the head categories, resulting in severe performance\ndegradation on the less represented tail categories for both the teacher and\nstudent networks. In this paper, we propose a novel framework called Knowledge\nRectification Distillation (KRDistill) to address the imbalanced knowledge\ninherited in the teacher network through the incorporation of the balanced\ncategory priors. Furthermore, we rectify the biased predictions produced by the\nteacher network, particularly focusing on the tail categories. Consequently,\nthe teacher network can provide balanced and accurate knowledge to train a\nreliable student network. Intensive experiments conducted on various\nlong-tailed datasets demonstrate that our KRDistill can effectively train\nreliable student networks in realistic scenarios of data imbalance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation (KD) transfers knowledge from a large pre-trained\nteacher network to a compact and efficient student network, making it suitable\nfor deployment on resource-limited media terminals. However, traditional KD\nmethods require balanced data to ensure robust training, which is often\nunavailable in practical applications. In such scenarios, a few head categories\noccupy a substantial proportion of examples. This imbalance biases the trained\nteacher network towards the head categories, resulting in severe performance\ndegradation on the less represented tail categories for both the teacher and\nstudent networks. In this paper, we propose a novel framework called Knowledge\nRectification Distillation (KRDistill) to address the imbalanced knowledge\ninherited in the teacher network through the incorporation of the balanced\ncategory priors. Furthermore, we rectify the biased predictions produced by the\nteacher network, particularly focusing on the tail categories. Consequently,\nthe teacher network can provide balanced and accurate knowledge to train a\nreliable student network. Intensive experiments conducted on various\nlong-tailed datasets demonstrate that our KRDistill can effectively train\nreliable student networks in realistic scenarios of data imbalance."
                },
                "authors": [
                    {
                        "name": "Xinlei Huang"
                    },
                    {
                        "name": "Jialiang Tang"
                    },
                    {
                        "name": "Xubin Zheng"
                    },
                    {
                        "name": "Jinjia Zhou"
                    },
                    {
                        "name": "Wenxin Yu"
                    },
                    {
                        "name": "Ning Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Ning Jiang"
                },
                "author": "Ning Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07693v1",
                "updated": "2024-09-12T01:55:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    1,
                    55,
                    8,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T01:55:08Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    1,
                    55,
                    8,
                    3,
                    256,
                    0
                ],
                "title": "Cooperative Inference with Interleaved Operator Partitioning for CNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Inference with Interleaved Operator Partitioning for CNNs"
                },
                "summary": "Deploying deep learning models on Internet of Things (IoT) devices often\nfaces challenges due to limited memory resources and computing capabilities.\nCooperative inference is an important method for addressing this issue,\nrequiring the partitioning and distributive deployment of an intelligent model.\nTo perform horizontal partitions, existing cooperative inference methods take\neither the output channel of operators or the height and width of feature maps\nas the partition dimensions. In this manner, since the activation of operators\nis distributed, they have to be concatenated together before being fed to the\nnext operator, which incurs the delay for cooperative inference. In this paper,\nwe propose the Interleaved Operator Partitioning (IOP) strategy for CNN models.\nBy partitioning an operator based on the output channel dimension and its\nsuccessive operator based on the input channel dimension, activation\nconcatenation becomes unnecessary, thereby reducing the number of communication\nconnections, which consequently reduces cooperative inference de-lay. Based on\nIOP, we further present a model segmentation algorithm for minimizing\ncooperative inference time, which greedily selects operators for IOP pairing\nbased on the inference delay benefit harvested. Experimental results\ndemonstrate that compared with the state-of-the-art partition approaches used\nin CoEdge, the IOP strategy achieves 6.39% ~ 16.83% faster acceleration and\nreduces peak memory footprint by 21.22% ~ 49.98% for three classical image\nclassification models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying deep learning models on Internet of Things (IoT) devices often\nfaces challenges due to limited memory resources and computing capabilities.\nCooperative inference is an important method for addressing this issue,\nrequiring the partitioning and distributive deployment of an intelligent model.\nTo perform horizontal partitions, existing cooperative inference methods take\neither the output channel of operators or the height and width of feature maps\nas the partition dimensions. In this manner, since the activation of operators\nis distributed, they have to be concatenated together before being fed to the\nnext operator, which incurs the delay for cooperative inference. In this paper,\nwe propose the Interleaved Operator Partitioning (IOP) strategy for CNN models.\nBy partitioning an operator based on the output channel dimension and its\nsuccessive operator based on the input channel dimension, activation\nconcatenation becomes unnecessary, thereby reducing the number of communication\nconnections, which consequently reduces cooperative inference de-lay. Based on\nIOP, we further present a model segmentation algorithm for minimizing\ncooperative inference time, which greedily selects operators for IOP pairing\nbased on the inference delay benefit harvested. Experimental results\ndemonstrate that compared with the state-of-the-art partition approaches used\nin CoEdge, the IOP strategy achieves 6.39% ~ 16.83% faster acceleration and\nreduces peak memory footprint by 21.22% ~ 49.98% for three classical image\nclassification models."
                },
                "authors": [
                    {
                        "name": "Zhibang Liu"
                    },
                    {
                        "name": "Chaonong Xu"
                    },
                    {
                        "name": "Zhizhuo Liu"
                    },
                    {
                        "name": "Lekai Huang"
                    },
                    {
                        "name": "Jiachen Wei"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06852v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06852v4",
                "updated": "2024-09-12T00:27:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    0,
                    27,
                    6,
                    3,
                    256,
                    0
                ],
                "published": "2024-06-10T23:54:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    23,
                    54,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "A Survey of Backdoor Attacks and Defenses on Large Language Models:\n  Implications for Security Measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Backdoor Attacks and Defenses on Large Language Models:\n  Implications for Security Measures"
                },
                "summary": "Large Language Models (LLMs), which bridge the gap between human language\nunderstanding and complex problem-solving, achieve state-of-the-art performance\non several NLP tasks, particularly in few-shot and zero-shot settings. Despite\nthe demonstrable efficacy of LLMs, due to constraints on computational\nresources, users have to engage with open-source language models or outsource\nthe entire training process to third-party platforms. However, research has\ndemonstrated that language models are susceptible to potential security\nvulnerabilities, particularly in backdoor attacks. Backdoor attacks are\ndesigned to introduce targeted vulnerabilities into language models by\npoisoning training samples or model weights, allowing attackers to manipulate\nmodel responses through malicious triggers. While existing surveys on backdoor\nattacks provide a comprehensive overview, they lack an in-depth examination of\nbackdoor attacks specifically targeting LLMs. To bridge this gap and grasp the\nlatest trends in the field, this paper presents a novel perspective on backdoor\nattacks for LLMs by focusing on fine-tuning methods. Specifically, we\nsystematically classify backdoor attacks into three categories: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on\ninsights from a substantial review, we also discuss crucial issues for future\nresearch on backdoor attacks, such as further exploring attack algorithms that\ndo not require fine-tuning, or developing more covert attack algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), which bridge the gap between human language\nunderstanding and complex problem-solving, achieve state-of-the-art performance\non several NLP tasks, particularly in few-shot and zero-shot settings. Despite\nthe demonstrable efficacy of LLMs, due to constraints on computational\nresources, users have to engage with open-source language models or outsource\nthe entire training process to third-party platforms. However, research has\ndemonstrated that language models are susceptible to potential security\nvulnerabilities, particularly in backdoor attacks. Backdoor attacks are\ndesigned to introduce targeted vulnerabilities into language models by\npoisoning training samples or model weights, allowing attackers to manipulate\nmodel responses through malicious triggers. While existing surveys on backdoor\nattacks provide a comprehensive overview, they lack an in-depth examination of\nbackdoor attacks specifically targeting LLMs. To bridge this gap and grasp the\nlatest trends in the field, this paper presents a novel perspective on backdoor\nattacks for LLMs by focusing on fine-tuning methods. Specifically, we\nsystematically classify backdoor attacks into three categories: full-parameter\nfine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on\ninsights from a substantial review, we also discuss crucial issues for future\nresearch on backdoor attacks, such as further exploring attack algorithms that\ndo not require fine-tuning, or developing more covert attack algorithms."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Meihuizi Jia"
                    },
                    {
                        "name": "Zhongliang Guo"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Xiaoyu Xu"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Yichao Feng"
                    },
                    {
                        "name": "Fengjun Pan"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    }
                ],
                "author_detail": {
                    "name": "Luu Anh Tuan"
                },
                "author": "Luu Anh Tuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06852v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06852v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03906v2",
                "updated": "2024-09-11T23:37:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    23,
                    37,
                    25,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-05T20:49:35Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    49,
                    35,
                    3,
                    249,
                    0
                ],
                "title": "Analytical Optimized Traffic Flow Recovery for Large-scale Urban\n  Transportation Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analytical Optimized Traffic Flow Recovery for Large-scale Urban\n  Transportation Network"
                },
                "summary": "The implementation of intelligent transportation systems (ITS) has enhanced\ndata collection in urban transportation through advanced traffic sensing\ndevices. However, the high costs associated with installation and maintenance\nresult in sparse traffic data coverage. To obtain complete, accurate, and\nhigh-resolution network-wide traffic flow data, this study introduces the\nAnalytical Optimized Recovery (AOR) approach that leverages abundant GPS speed\ndata alongside sparse flow data to estimate traffic flow in large-scale urban\nnetworks. The method formulates a constrained optimization framework that\nutilizes a quadratic objective function with l2 norm regularization terms to\naddress the traffic flow recovery problem effectively and incorporates a\nLagrangian relaxation technique to maintain non-negativity constraints. The\neffectiveness of this approach was validated in a large urban network in\nShenzhen's Futian District using the Simulation of Urban MObility (SUMO)\nplatform. Analytical results indicate that the method achieves low estimation\nerrors, affirming its suitability for comprehensive traffic analysis in urban\nsettings with limited sensor deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of intelligent transportation systems (ITS) has enhanced\ndata collection in urban transportation through advanced traffic sensing\ndevices. However, the high costs associated with installation and maintenance\nresult in sparse traffic data coverage. To obtain complete, accurate, and\nhigh-resolution network-wide traffic flow data, this study introduces the\nAnalytical Optimized Recovery (AOR) approach that leverages abundant GPS speed\ndata alongside sparse flow data to estimate traffic flow in large-scale urban\nnetworks. The method formulates a constrained optimization framework that\nutilizes a quadratic objective function with l2 norm regularization terms to\naddress the traffic flow recovery problem effectively and incorporates a\nLagrangian relaxation technique to maintain non-negativity constraints. The\neffectiveness of this approach was validated in a large urban network in\nShenzhen's Futian District using the Simulation of Urban MObility (SUMO)\nplatform. Analytical results indicate that the method achieves low estimation\nerrors, affirming its suitability for comprehensive traffic analysis in urban\nsettings with limited sensor deployment."
                },
                "authors": [
                    {
                        "name": "Sicheng Fu"
                    },
                    {
                        "name": "Haotian Shi"
                    },
                    {
                        "name": "Shixiao Liang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Bin Ran"
                    }
                ],
                "author_detail": {
                    "name": "Bin Ran"
                },
                "author": "Bin Ran",
                "arxiv_comment": "27 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05941v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05941v3",
                "updated": "2024-09-11T21:53:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    53,
                    59,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-01T17:42:40Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    42,
                    40,
                    0,
                    183,
                    0
                ],
                "title": "Pruning One More Token is Enough: Leveraging Latency-Workload\n  Non-Linearities for Vision Transformers on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning One More Token is Enough: Leveraging Latency-Workload\n  Non-Linearities for Vision Transformers on the Edge"
                },
                "summary": "This paper investigates how to efficiently deploy vision transformers on edge\ndevices for small workloads. Recent methods reduce the latency of transformer\nneural networks by removing or merging tokens, with small accuracy degradation.\nHowever, these methods are not designed with edge device deployment in mind:\nthey do not leverage information about the latency-workload trends to improve\nefficiency. We address this shortcoming in our work. First, we identify factors\nthat affect ViT latency-workload relationships. Second, we determine token\npruning schedule by leveraging non-linear latency-workload relationships.\nThird, we demonstrate a training-free, token pruning method utilizing this\nschedule. We show other methods may increase latency by 2-30%, while we reduce\nlatency by 9-26%. For similar latency (within 5.2% or 7ms) across devices we\nachieve 78.6%-84.5% ImageNet1K accuracy, while the state-of-the-art, Token\nMerging, achieves 45.8%-85.4%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates how to efficiently deploy vision transformers on edge\ndevices for small workloads. Recent methods reduce the latency of transformer\nneural networks by removing or merging tokens, with small accuracy degradation.\nHowever, these methods are not designed with edge device deployment in mind:\nthey do not leverage information about the latency-workload trends to improve\nefficiency. We address this shortcoming in our work. First, we identify factors\nthat affect ViT latency-workload relationships. Second, we determine token\npruning schedule by leveraging non-linear latency-workload relationships.\nThird, we demonstrate a training-free, token pruning method utilizing this\nschedule. We show other methods may increase latency by 2-30%, while we reduce\nlatency by 9-26%. For similar latency (within 5.2% or 7ms) across devices we\nachieve 78.6%-84.5% ImageNet1K accuracy, while the state-of-the-art, Token\nMerging, achieves 45.8%-85.4%."
                },
                "authors": [
                    {
                        "name": "Nick John Eliopoulos"
                    },
                    {
                        "name": "Purvish Jajal"
                    },
                    {
                        "name": "James Davis"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "George K. Thiravathukal"
                    },
                    {
                        "name": "Yung-Hsiang Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yung-Hsiang Lu"
                },
                "author": "Yung-Hsiang Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05941v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05941v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07641v1",
                "updated": "2024-09-11T21:53:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    53,
                    20,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T21:53:20Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    53,
                    20,
                    2,
                    255,
                    0
                ],
                "title": "SimulBench: Evaluating Language Models with Creative Simulation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimulBench: Evaluating Language Models with Creative Simulation Tasks"
                },
                "summary": "We introduce SimulBench, a benchmark designed to evaluate large language\nmodels (LLMs) across a diverse collection of creative simulation scenarios,\nsuch as acting as a Linux terminal or playing text games with users. While\nthese simulation tasks serve as effective measures of an LLM's general\nintelligence, they are seldom incorporated into existing benchmarks. A major\nchallenge is to develop an evaluation framework for testing different LLMs\nfairly while preserving the multi-round interactive nature of simulation tasks\nbetween users and AI. To tackle this issue, we suggest using a fixed LLM as a\nuser agent to engage with an LLM to collect dialogues first under different\ntasks. Then, challenging dialogue scripts are extracted for evaluating\ndifferent target LLMs. To facilitate automatic assessment on \\DataName{}, GPT-4\nis employed as the evaluator, tasked with reviewing the quality of the final\nresponse generated by the target LLMs given multi-turn dialogue scripts. Our\ncomprehensive experiments indicate that these simulation tasks continue to pose\na significant challenge with their unique natures and show the gap between\nproprietary models and the most advanced open LLMs. For example, GPT-4-turbo\noutperforms LLaMA-3-70b-Chat on 18.55\\% more cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SimulBench, a benchmark designed to evaluate large language\nmodels (LLMs) across a diverse collection of creative simulation scenarios,\nsuch as acting as a Linux terminal or playing text games with users. While\nthese simulation tasks serve as effective measures of an LLM's general\nintelligence, they are seldom incorporated into existing benchmarks. A major\nchallenge is to develop an evaluation framework for testing different LLMs\nfairly while preserving the multi-round interactive nature of simulation tasks\nbetween users and AI. To tackle this issue, we suggest using a fixed LLM as a\nuser agent to engage with an LLM to collect dialogues first under different\ntasks. Then, challenging dialogue scripts are extracted for evaluating\ndifferent target LLMs. To facilitate automatic assessment on \\DataName{}, GPT-4\nis employed as the evaluator, tasked with reviewing the quality of the final\nresponse generated by the target LLMs given multi-turn dialogue scripts. Our\ncomprehensive experiments indicate that these simulation tasks continue to pose\na significant challenge with their unique natures and show the gap between\nproprietary models and the most advanced open LLMs. For example, GPT-4-turbo\noutperforms LLaMA-3-70b-Chat on 18.55\\% more cases."
                },
                "authors": [
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Bill Yuchen Lin"
                },
                "author": "Bill Yuchen Lin",
                "arxiv_comment": "Website: https://simulbench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03166v2",
                "updated": "2024-09-11T21:52:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    52,
                    22,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-05T01:51:54Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    51,
                    54,
                    3,
                    249,
                    0
                ],
                "title": "Continual Skill and Task Learning via Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Skill and Task Learning via Dialogue"
                },
                "summary": "Continual and interactive robot learning is a challenging problem as the\nrobot is present with human users who expect the robot to learn novel skills to\nsolve novel tasks perpetually with sample efficiency. In this work we present a\nframework for robots to query and learn visuo-motor robot skills and task\nrelevant information via natural language dialog interactions with human users.\nPrevious approaches either focus on improving the performance of instruction\nfollowing agents, or passively learn novel skills or concepts. Instead, we used\ndialog combined with a language-skill grounding embedding to query or confirm\nskills and/or tasks requested by a user. To achieve this goal, we developed and\nintegrated three different components for our agent. Firstly, we propose a\nnovel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA),\nwhich enables the existing SoTA ACT model to perform few-shot continual\nlearning. Secondly, we develop an alignment model that projects demonstrations\nacross skill embodiments into a shared embedding allowing us to know when to\nask questions and/or demonstrations from users. Finally, we integrated an\nexisting LLM to interact with a human user to perform grounded interactive\ncontinual skill learning to solve a task. Our ACT-LoRA model learns novel\nfine-tuned skills with a 100% accuracy when trained with only five\ndemonstrations for a novel skill while still maintaining a 74.75% accuracy on\npre-trained skills in the RLBench dataset where other models fall significantly\nshort. We also performed a human-subjects study with 8 subjects to demonstrate\nthe continual learning capabilities of our combined framework. We achieve a\nsuccess rate of 75% in the task of sandwich making with the real robot learning\nfrom participant data demonstrating that robots can learn novel skills or task\nknowledge from dialogue with non-expert users using our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual and interactive robot learning is a challenging problem as the\nrobot is present with human users who expect the robot to learn novel skills to\nsolve novel tasks perpetually with sample efficiency. In this work we present a\nframework for robots to query and learn visuo-motor robot skills and task\nrelevant information via natural language dialog interactions with human users.\nPrevious approaches either focus on improving the performance of instruction\nfollowing agents, or passively learn novel skills or concepts. Instead, we used\ndialog combined with a language-skill grounding embedding to query or confirm\nskills and/or tasks requested by a user. To achieve this goal, we developed and\nintegrated three different components for our agent. Firstly, we propose a\nnovel visual-motor control policy ACT with Low Rank Adaptation (ACT-LoRA),\nwhich enables the existing SoTA ACT model to perform few-shot continual\nlearning. Secondly, we develop an alignment model that projects demonstrations\nacross skill embodiments into a shared embedding allowing us to know when to\nask questions and/or demonstrations from users. Finally, we integrated an\nexisting LLM to interact with a human user to perform grounded interactive\ncontinual skill learning to solve a task. Our ACT-LoRA model learns novel\nfine-tuned skills with a 100% accuracy when trained with only five\ndemonstrations for a novel skill while still maintaining a 74.75% accuracy on\npre-trained skills in the RLBench dataset where other models fall significantly\nshort. We also performed a human-subjects study with 8 subjects to demonstrate\nthe continual learning capabilities of our combined framework. We achieve a\nsuccess rate of 75% in the task of sandwich making with the real robot learning\nfrom participant data demonstrating that robots can learn novel skills or task\nknowledge from dialogue with non-expert users using our approach."
                },
                "authors": [
                    {
                        "name": "Weiwei Gu"
                    },
                    {
                        "name": "Suresh Kondepudi"
                    },
                    {
                        "name": "Lixiao Huang"
                    },
                    {
                        "name": "Nakul Gopalan"
                    }
                ],
                "author_detail": {
                    "name": "Nakul Gopalan"
                },
                "author": "Nakul Gopalan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07638v1",
                "updated": "2024-09-11T21:48:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    48,
                    33,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T21:48:33Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    48,
                    33,
                    2,
                    255,
                    0
                ],
                "title": "Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4\n  Capabilities"
                },
                "summary": "In this paper we explore evaluation of LLM capabilities. We present\nmeasurements of GPT-4 performance on several deterministic tasks; each task\ninvolves a basic calculation and takes as input parameter some element drawn\nfrom a large well-defined population (e.g., count elements in a list, multiply\ntwo k-digit numbers, etc). We examine several conditions per-task and perform\nenough trials so that statistically significant differences can be detected.\nThis allows us to investigate the sensitivity of task-accuracy both to query\nphrasing and input parameter population. We find that seemingly trivial\nmodifications in the task-prompt or input population can yield differences far\nlarger than can be explained by sampling effects. For example, performance on a\nsimple list-counting task varies with query-phrasing and list-length, but also\nwith list composition (i.e., the thing-to-be-counted) and object frequency\n(e.g., success when an element accounts for $\\approx$ 50\\% of a list is\ndifferent from when it accounts for $\\approx$ 70\\% etc).\n  We conclude that efforts to quantify LLM capabilities easily succumb to the\nlanguage-as-fixed-effect fallacy, where experimental observations are\nimproperly generalized beyond what the data supports. A consequence appears to\nbe that intuitions that have been formed based on interactions with humans form\na very unreliable guide as to which input modifications should ``make no\ndifference'' to LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we explore evaluation of LLM capabilities. We present\nmeasurements of GPT-4 performance on several deterministic tasks; each task\ninvolves a basic calculation and takes as input parameter some element drawn\nfrom a large well-defined population (e.g., count elements in a list, multiply\ntwo k-digit numbers, etc). We examine several conditions per-task and perform\nenough trials so that statistically significant differences can be detected.\nThis allows us to investigate the sensitivity of task-accuracy both to query\nphrasing and input parameter population. We find that seemingly trivial\nmodifications in the task-prompt or input population can yield differences far\nlarger than can be explained by sampling effects. For example, performance on a\nsimple list-counting task varies with query-phrasing and list-length, but also\nwith list composition (i.e., the thing-to-be-counted) and object frequency\n(e.g., success when an element accounts for $\\approx$ 50\\% of a list is\ndifferent from when it accounts for $\\approx$ 70\\% etc).\n  We conclude that efforts to quantify LLM capabilities easily succumb to the\nlanguage-as-fixed-effect fallacy, where experimental observations are\nimproperly generalized beyond what the data supports. A consequence appears to\nbe that intuitions that have been formed based on interactions with humans form\na very unreliable guide as to which input modifications should ``make no\ndifference'' to LLM performance."
                },
                "authors": [
                    {
                        "name": "Thomas Ball"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Cormac Herley"
                    }
                ],
                "author_detail": {
                    "name": "Cormac Herley"
                },
                "author": "Cormac Herley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.07619v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.07619v5",
                "updated": "2024-09-11T21:23:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    21,
                    23,
                    4,
                    2,
                    255,
                    0
                ],
                "published": "2023-04-15T19:22:37Z",
                "published_parsed": [
                    2023,
                    4,
                    15,
                    19,
                    22,
                    37,
                    5,
                    105,
                    0
                ],
                "title": "Can ChatGPT Forecast Stock Price Movements? Return Predictability and\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can ChatGPT Forecast Stock Price Movements? Return Predictability and\n  Large Language Models"
                },
                "summary": "We document the capability of large language models (LLMs) like ChatGPT to\npredict stock price movements using news headlines, even without direct\nfinancial training. ChatGPT scores significantly predict out-of-sample daily\nstock returns, subsuming traditional methods, and predictability is stronger\namong smaller stocks and following negative news. To explain these findings, we\ndevelop a theoretical model incorporating information capacity constraints,\nunderreaction, limits-to-arbitrage, and LLMs. The model generates several key\npredictions, which we empirically test: (i) it establishes a critical threshold\nin AI capabilities necessary for profitable predictions, (ii) it demonstrates\nthat only advanced LLMs can effectively interpret complex information, and\n(iii) it predicts that widespread LLM adoption can enhance market efficiency.\nOur results suggest that sophisticated return forecasting is an emerging\ncapability of AI systems and that these technologies can alter information\ndiffusion and decision-making processes in financial markets. Finally, we\nintroduce an interpretability framework to evaluate LLMs' reasoning,\ncontributing to AI transparency and economic decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We document the capability of large language models (LLMs) like ChatGPT to\npredict stock price movements using news headlines, even without direct\nfinancial training. ChatGPT scores significantly predict out-of-sample daily\nstock returns, subsuming traditional methods, and predictability is stronger\namong smaller stocks and following negative news. To explain these findings, we\ndevelop a theoretical model incorporating information capacity constraints,\nunderreaction, limits-to-arbitrage, and LLMs. The model generates several key\npredictions, which we empirically test: (i) it establishes a critical threshold\nin AI capabilities necessary for profitable predictions, (ii) it demonstrates\nthat only advanced LLMs can effectively interpret complex information, and\n(iii) it predicts that widespread LLM adoption can enhance market efficiency.\nOur results suggest that sophisticated return forecasting is an emerging\ncapability of AI systems and that these technologies can alter information\ndiffusion and decision-making processes in financial markets. Finally, we\nintroduce an interpretability framework to evaluate LLMs' reasoning,\ncontributing to AI transparency and economic decision-making."
                },
                "authors": [
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Yuehua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yuehua Tang"
                },
                "author": "Yuehua Tang",
                "arxiv_comment": "Previously posted in SSRN\n  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4412788",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.07619v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.07619v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.13751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.13751v2",
                "updated": "2024-09-11T20:55:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    55,
                    32,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-24T19:12:37Z",
                "published_parsed": [
                    2024,
                    1,
                    24,
                    19,
                    12,
                    37,
                    2,
                    24,
                    0
                ],
                "title": "A Training Rate and Survival Heuristic for Inference and Robustness\n  Evaluation (TRASHFIRE)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training Rate and Survival Heuristic for Inference and Robustness\n  Evaluation (TRASHFIRE)"
                },
                "summary": "Machine learning models -- deep neural networks in particular -- have\nperformed remarkably well on benchmark datasets across a wide variety of\ndomains. However, the ease of finding adversarial counter-examples remains a\npersistent problem when training times are measured in hours or days and the\ntime needed to find a successful adversarial counter-example is measured in\nseconds. Much work has gone into generating and defending against these\nadversarial counter-examples, however the relative costs of attacks and\ndefences are rarely discussed. Additionally, machine learning research is\nalmost entirely guided by test/train metrics, but these would require billions\nof samples to meet industry standards. The present work addresses the problem\nof understanding and predicting how particular model hyper-parameters influence\nthe performance of a model in the presence of an adversary. The proposed\napproach uses survival models, worst-case examples, and a cost-aware analysis\nto precisely and accurately reject a particular model change during routine\nmodel training procedures rather than relying on real-world deployment,\nexpensive formal verification methods, or accurate simulations of very\ncomplicated systems (\\textit{e.g.}, digitally recreating every part of a car or\na plane). Through an evaluation of many pre-processing techniques, adversarial\ncounter-examples, and neural network configurations, the conclusion is that\ndeeper models do offer marginal gains in survival times compared to more\nshallow counterparts. However, we show that those gains are driven more by the\nmodel inference time than inherent robustness properties. Using the proposed\nmethodology, we show that ResNet is hopelessly insecure against even the\nsimplest of white box attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models -- deep neural networks in particular -- have\nperformed remarkably well on benchmark datasets across a wide variety of\ndomains. However, the ease of finding adversarial counter-examples remains a\npersistent problem when training times are measured in hours or days and the\ntime needed to find a successful adversarial counter-example is measured in\nseconds. Much work has gone into generating and defending against these\nadversarial counter-examples, however the relative costs of attacks and\ndefences are rarely discussed. Additionally, machine learning research is\nalmost entirely guided by test/train metrics, but these would require billions\nof samples to meet industry standards. The present work addresses the problem\nof understanding and predicting how particular model hyper-parameters influence\nthe performance of a model in the presence of an adversary. The proposed\napproach uses survival models, worst-case examples, and a cost-aware analysis\nto precisely and accurately reject a particular model change during routine\nmodel training procedures rather than relying on real-world deployment,\nexpensive formal verification methods, or accurate simulations of very\ncomplicated systems (\\textit{e.g.}, digitally recreating every part of a car or\na plane). Through an evaluation of many pre-processing techniques, adversarial\ncounter-examples, and neural network configurations, the conclusion is that\ndeeper models do offer marginal gains in survival times compared to more\nshallow counterparts. However, we show that those gains are driven more by the\nmodel inference time than inherent robustness properties. Using the proposed\nmethodology, we show that ResNet is hopelessly insecure against even the\nsimplest of white box attacks."
                },
                "authors": [
                    {
                        "name": "Charles Meyers"
                    },
                    {
                        "name": "Mohammad Reza Saleh Sedghpour"
                    },
                    {
                        "name": "Tommy Löfstedt"
                    },
                    {
                        "name": "Erik Elmroth"
                    }
                ],
                "author_detail": {
                    "name": "Erik Elmroth"
                },
                "author": "Erik Elmroth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.13751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.13751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07615v1",
                "updated": "2024-09-11T20:55:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    55,
                    12,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T20:55:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    55,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Zero-Shot Machine-Generated Text Detection Using Mixture of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Machine-Generated Text Detection Using Mixture of Large\n  Language Models"
                },
                "summary": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities has vastly increased the\nthreats posed by generative AI technologies by reducing the cost of producing\nharmful, toxic, faked or forged content. In response, various proposals have\nbeen made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a classification problem.\nMost approaches evaluate an input document by a well-chosen detector LLM,\nassuming that low-perplexity scores reliably signal machine-made content. As\nusing one single detector can induce brittleness of performance, we instead\nconsider several and derive a new, theoretically grounded approach to combine\ntheir respective strengths. Our experiments, using a variety of generator LLMs,\nsuggest that our method effectively increases the robustness of detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dissemination of Large Language Models (LLMs), trained at scale, and\nendowed with powerful text-generating abilities has vastly increased the\nthreats posed by generative AI technologies by reducing the cost of producing\nharmful, toxic, faked or forged content. In response, various proposals have\nbeen made to automatically discriminate artificially generated from\nhuman-written texts, typically framing the problem as a classification problem.\nMost approaches evaluate an input document by a well-chosen detector LLM,\nassuming that low-perplexity scores reliably signal machine-made content. As\nusing one single detector can induce brittleness of performance, we instead\nconsider several and derive a new, theoretically grounded approach to combine\ntheir respective strengths. Our experiments, using a variety of generator LLMs,\nsuggest that our method effectively increases the robustness of detection."
                },
                "authors": [
                    {
                        "name": "Matthieu Dubois"
                    },
                    {
                        "name": "François Yvon"
                    },
                    {
                        "name": "Pablo Piantanida"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Piantanida"
                },
                "author": "Pablo Piantanida",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07609v1",
                "updated": "2024-09-11T20:43:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    43,
                    59,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T20:43:59Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    43,
                    59,
                    2,
                    255,
                    0
                ],
                "title": "A Cost-Aware Approach to Adversarial Robustness in Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cost-Aware Approach to Adversarial Robustness in Neural Networks"
                },
                "summary": "Considering the growing prominence of production-level AI and the threat of\nadversarial attacks that can evade a model at run-time, evaluating the\nrobustness of models to these evasion attacks is of critical importance.\nAdditionally, testing model changes likely means deploying the models to (e.g.\na car or a medical imaging device), or a drone to see how it affects\nperformance, making un-tested changes a public problem that reduces development\nspeed, increases cost of development, and makes it difficult (if not\nimpossible) to parse cause from effect. In this work, we used survival analysis\nas a cloud-native, time-efficient and precise method for predicting model\nperformance in the presence of adversarial noise. For neural networks in\nparticular, the relationships between the learning rate, batch size, training\ntime, convergence time, and deployment cost are highly complex, so researchers\ngenerally rely on benchmark datasets to assess the ability of a model to\ngeneralize beyond the training data. To address this, we propose using\naccelerated failure time models to measure the effect of hardware choice, batch\nsize, number of epochs, and test-set accuracy by using adversarial attacks to\ninduce failures on a reference model architecture before deploying the model to\nthe real world. We evaluate several GPU types and use the Tree Parzen Estimator\nto maximize model robustness and minimize model run-time simultaneously. This\nprovides a way to evaluate the model and optimise it in a single step, while\nsimultaneously allowing us to model the effect of model parameters on training\ntime, prediction time, and accuracy. Using this technique, we demonstrate that\nnewer, more-powerful hardware does decrease the training time, but with a\nmonetary and power cost that far outpaces the marginal gains in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considering the growing prominence of production-level AI and the threat of\nadversarial attacks that can evade a model at run-time, evaluating the\nrobustness of models to these evasion attacks is of critical importance.\nAdditionally, testing model changes likely means deploying the models to (e.g.\na car or a medical imaging device), or a drone to see how it affects\nperformance, making un-tested changes a public problem that reduces development\nspeed, increases cost of development, and makes it difficult (if not\nimpossible) to parse cause from effect. In this work, we used survival analysis\nas a cloud-native, time-efficient and precise method for predicting model\nperformance in the presence of adversarial noise. For neural networks in\nparticular, the relationships between the learning rate, batch size, training\ntime, convergence time, and deployment cost are highly complex, so researchers\ngenerally rely on benchmark datasets to assess the ability of a model to\ngeneralize beyond the training data. To address this, we propose using\naccelerated failure time models to measure the effect of hardware choice, batch\nsize, number of epochs, and test-set accuracy by using adversarial attacks to\ninduce failures on a reference model architecture before deploying the model to\nthe real world. We evaluate several GPU types and use the Tree Parzen Estimator\nto maximize model robustness and minimize model run-time simultaneously. This\nprovides a way to evaluate the model and optimise it in a single step, while\nsimultaneously allowing us to model the effect of model parameters on training\ntime, prediction time, and accuracy. Using this technique, we demonstrate that\nnewer, more-powerful hardware does decrease the training time, but with a\nmonetary and power cost that far outpaces the marginal gains in accuracy."
                },
                "authors": [
                    {
                        "name": "Charles Meyers"
                    },
                    {
                        "name": "Mohammad Reza Saleh Sedghpour"
                    },
                    {
                        "name": "Tommy Löfstedt"
                    },
                    {
                        "name": "Erik Elmroth"
                    }
                ],
                "author_detail": {
                    "name": "Erik Elmroth"
                },
                "author": "Erik Elmroth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07604v1",
                "updated": "2024-09-11T20:31:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    31,
                    42,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T20:31:42Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    31,
                    42,
                    2,
                    255,
                    0
                ],
                "title": "Multilingual Prompts in LLM-Based Recommenders: Performance Across\n  Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Prompts in LLM-Based Recommenders: Performance Across\n  Languages"
                },
                "summary": "Large language models (LLMs) are increasingly used in natural language\nprocessing tasks. Recommender systems traditionally use methods such as\ncollaborative filtering and matrix factorization, as well as advanced\ntechniques like deep learning and reinforcement learning. Although language\nmodels have been applied in recommendation, the recent trend have focused on\nleveraging the generative capabilities of LLMs for more personalized\nsuggestions. While current research focuses on English due to its resource\nrichness, this work explores the impact of non-English prompts on\nrecommendation performance. Using OpenP5, a platform for developing and\nevaluating LLM-based recommendations, we expanded its English prompt templates\nto include Spanish and Turkish. Evaluation on three real-world datasets, namely\nML1M, LastFM, and Amazon-Beauty, showed that usage of non-English prompts\ngenerally reduce performance, especially in less-resourced languages like\nTurkish. We also retrained an LLM-based recommender model with multilingual\nprompts to analyze performance variations. Retraining with multilingual prompts\nresulted in more balanced performance across languages, but slightly reduced\nEnglish performance. This work highlights the need for diverse language support\nin LLM-based recommenders and suggests future research on creating evaluation\ndatasets, using newer models and additional languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in natural language\nprocessing tasks. Recommender systems traditionally use methods such as\ncollaborative filtering and matrix factorization, as well as advanced\ntechniques like deep learning and reinforcement learning. Although language\nmodels have been applied in recommendation, the recent trend have focused on\nleveraging the generative capabilities of LLMs for more personalized\nsuggestions. While current research focuses on English due to its resource\nrichness, this work explores the impact of non-English prompts on\nrecommendation performance. Using OpenP5, a platform for developing and\nevaluating LLM-based recommendations, we expanded its English prompt templates\nto include Spanish and Turkish. Evaluation on three real-world datasets, namely\nML1M, LastFM, and Amazon-Beauty, showed that usage of non-English prompts\ngenerally reduce performance, especially in less-resourced languages like\nTurkish. We also retrained an LLM-based recommender model with multilingual\nprompts to analyze performance variations. Retraining with multilingual prompts\nresulted in more balanced performance across languages, but slightly reduced\nEnglish performance. This work highlights the need for diverse language support\nin LLM-based recommenders and suggests future research on creating evaluation\ndatasets, using newer models and additional languages."
                },
                "authors": [
                    {
                        "name": "Makbule Gulcin Ozsoy"
                    }
                ],
                "author_detail": {
                    "name": "Makbule Gulcin Ozsoy"
                },
                "author": "Makbule Gulcin Ozsoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02839v3",
                "updated": "2024-09-11T20:06:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    20,
                    6,
                    26,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-04T16:09:28Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    16,
                    9,
                    28,
                    2,
                    248,
                    0
                ],
                "title": "Jäger: Automated Telephone Call Traceback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jäger: Automated Telephone Call Traceback"
                },
                "summary": "Unsolicited telephone calls that facilitate fraud or unlawful telemarketing\ncontinue to overwhelm network users and the regulators who prosecute them. The\nfirst step in prosecuting phone abuse is traceback -- identifying the call\noriginator. This fundamental investigative task currently requires hours of\nmanual effort per call. In this paper, we introduce J\\\"ager, a distributed\nsecure call traceback system. J\\\"ager can trace a call in a few seconds, even\nwith partial deployment, while cryptographically preserving the privacy of call\nparties, carrier trade secrets like peers and call volume, and limiting the\nthreat of bulk analysis. We establish definitions and requirements of secure\ntraceback, then develop a suite of protocols that meet these requirements using\nwitness encryption, oblivious pseudorandom functions, and group signatures. We\nprove these protocols secure in the universal composibility framework. We then\ndemonstrate that J\\\"ager has low compute and bandwidth costs per call, and\nthese costs scale linearly with call volume. J\\\"ager provides an efficient,\nsecure, privacy-preserving system to revolutionize telephone abuse\ninvestigation with minimal costs to operators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsolicited telephone calls that facilitate fraud or unlawful telemarketing\ncontinue to overwhelm network users and the regulators who prosecute them. The\nfirst step in prosecuting phone abuse is traceback -- identifying the call\noriginator. This fundamental investigative task currently requires hours of\nmanual effort per call. In this paper, we introduce J\\\"ager, a distributed\nsecure call traceback system. J\\\"ager can trace a call in a few seconds, even\nwith partial deployment, while cryptographically preserving the privacy of call\nparties, carrier trade secrets like peers and call volume, and limiting the\nthreat of bulk analysis. We establish definitions and requirements of secure\ntraceback, then develop a suite of protocols that meet these requirements using\nwitness encryption, oblivious pseudorandom functions, and group signatures. We\nprove these protocols secure in the universal composibility framework. We then\ndemonstrate that J\\\"ager has low compute and bandwidth costs per call, and\nthese costs scale linearly with call volume. J\\\"ager provides an efficient,\nsecure, privacy-preserving system to revolutionize telephone abuse\ninvestigation with minimal costs to operators."
                },
                "authors": [
                    {
                        "name": "David Adei"
                    },
                    {
                        "name": "Varun Madathil"
                    },
                    {
                        "name": "Sathvik Prasad"
                    },
                    {
                        "name": "Bradley Reaves"
                    },
                    {
                        "name": "Alessandra Scafuro"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Scafuro"
                },
                "author": "Alessandra Scafuro",
                "arxiv_doi": "10.1145/3658644.3690290",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690290",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 2024 ACM SIGSAC Conference on Computer and\n  Communications Security (CCS '24), October 14---18, 2024, Salt Lake City, UT,\n  USA. ACM, New York, NY, USA, 24 pages",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07591v1",
                "updated": "2024-09-11T19:47:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    19,
                    47,
                    2,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T19:47:02Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    19,
                    47,
                    2,
                    2,
                    255,
                    0
                ],
                "title": "CAVERNAUTE: a design and manufacturing pipeline of a rigid but foldable\n  indoor airship aerial system for cave exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAVERNAUTE: a design and manufacturing pipeline of a rigid but foldable\n  indoor airship aerial system for cave exploration"
                },
                "summary": "Airships, best recognized for their unique quality of payload/energy ratio,\npresent a fascinating challenge for the field of engineering. Their\nconstruction and operation require a delicate balance of materials and rules,\nmaking them a compelling object of study. They embody a distinct intersection\nof physics, design, and innovation, offering a wide array of possibilities for\nfuture transportation and exploration. Thanks to their long-flight endurance,\nthey are suited for long-term missions. To operate in complex environments such\nas indoor cluttered spaces, their membrane and mechatronics need to be\nprotected from impacts. This paper presents a new indoor airship design\ninspired by origami and the Kresling pattern. The airship structure combines a\ncarbon fiber exoskeleton and UV resin micro-lattices for shock absorption. Our\ndesign strengthens the robot while granting the ability to access narrow spaces\nby folding the structure - up to a volume expansion ratio of 19.8. To optimize\nthe numerous parameters of the airship, we present a pipeline for design,\nmanufacture, and assembly. It takes into account manufacturing constraints,\ndimensions of the target deployment area, and aerostatics, allowing for easy\nand quick testing of new configurations. We also present unique features made\npossible by combining origami with airship design, which reduces the chances of\nmission-compromising failures. We demonstrate the potential of the design with\na complete simulation including an effective control strategy leveraging\nlightweight mechatronics to optimize flight autonomy in exploration missions of\nunstructured environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Airships, best recognized for their unique quality of payload/energy ratio,\npresent a fascinating challenge for the field of engineering. Their\nconstruction and operation require a delicate balance of materials and rules,\nmaking them a compelling object of study. They embody a distinct intersection\nof physics, design, and innovation, offering a wide array of possibilities for\nfuture transportation and exploration. Thanks to their long-flight endurance,\nthey are suited for long-term missions. To operate in complex environments such\nas indoor cluttered spaces, their membrane and mechatronics need to be\nprotected from impacts. This paper presents a new indoor airship design\ninspired by origami and the Kresling pattern. The airship structure combines a\ncarbon fiber exoskeleton and UV resin micro-lattices for shock absorption. Our\ndesign strengthens the robot while granting the ability to access narrow spaces\nby folding the structure - up to a volume expansion ratio of 19.8. To optimize\nthe numerous parameters of the airship, we present a pipeline for design,\nmanufacture, and assembly. It takes into account manufacturing constraints,\ndimensions of the target deployment area, and aerostatics, allowing for easy\nand quick testing of new configurations. We also present unique features made\npossible by combining origami with airship design, which reduces the chances of\nmission-compromising failures. We demonstrate the potential of the design with\na complete simulation including an effective control strategy leveraging\nlightweight mechatronics to optimize flight autonomy in exploration missions of\nunstructured environments."
                },
                "authors": [
                    {
                        "name": "Catar Louis"
                    },
                    {
                        "name": "Tabiai Ilyass"
                    },
                    {
                        "name": "St-Onge David"
                    }
                ],
                "author_detail": {
                    "name": "St-Onge David"
                },
                "author": "St-Onge David",
                "arxiv_comment": "36 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07587v1",
                "updated": "2024-09-11T19:33:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    19,
                    33,
                    44,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T19:33:44Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    19,
                    33,
                    44,
                    2,
                    255,
                    0
                ],
                "title": "Exploring LLMs for Malware Detection: Review, Framework Design, and\n  Countermeasure Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLMs for Malware Detection: Review, Framework Design, and\n  Countermeasure Approaches"
                },
                "summary": "The rising use of Large Language Models (LLMs) to create and disseminate\nmalware poses a significant cybersecurity challenge due to their ability to\ngenerate and distribute attacks with ease. A single prompt can initiate a wide\narray of malicious activities. This paper addresses this critical issue through\na multifaceted approach. First, we provide a comprehensive overview of LLMs and\ntheir role in malware detection from diverse sources. We examine five specific\napplications of LLMs: Malware honeypots, identification of text-based threats,\ncode analysis for detecting malicious intent, trend analysis of malware, and\ndetection of non-standard disguised malware. Our review includes a detailed\nanalysis of the existing literature and establishes guiding principles for the\nsecure use of LLMs. We also introduce a classification scheme to categorize the\nrelevant literature. Second, we propose performance metrics to assess the\neffectiveness of LLMs in these contexts. Third, we present a risk mitigation\nframework designed to prevent malware by leveraging LLMs. Finally, we evaluate\nthe performance of our proposed risk mitigation strategies against various\nfactors and demonstrate their effectiveness in countering LLM-enabled malware.\nThe paper concludes by suggesting future advancements and areas requiring\ndeeper exploration in this fascinating field of artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rising use of Large Language Models (LLMs) to create and disseminate\nmalware poses a significant cybersecurity challenge due to their ability to\ngenerate and distribute attacks with ease. A single prompt can initiate a wide\narray of malicious activities. This paper addresses this critical issue through\na multifaceted approach. First, we provide a comprehensive overview of LLMs and\ntheir role in malware detection from diverse sources. We examine five specific\napplications of LLMs: Malware honeypots, identification of text-based threats,\ncode analysis for detecting malicious intent, trend analysis of malware, and\ndetection of non-standard disguised malware. Our review includes a detailed\nanalysis of the existing literature and establishes guiding principles for the\nsecure use of LLMs. We also introduce a classification scheme to categorize the\nrelevant literature. Second, we propose performance metrics to assess the\neffectiveness of LLMs in these contexts. Third, we present a risk mitigation\nframework designed to prevent malware by leveraging LLMs. Finally, we evaluate\nthe performance of our proposed risk mitigation strategies against various\nfactors and demonstrate their effectiveness in countering LLM-enabled malware.\nThe paper concludes by suggesting future advancements and areas requiring\ndeeper exploration in this fascinating field of artificial intelligence."
                },
                "authors": [
                    {
                        "name": "Jamal Al-Karaki"
                    },
                    {
                        "name": "Muhammad Al-Zafar Khan"
                    },
                    {
                        "name": "Marwan Omar"
                    }
                ],
                "author_detail": {
                    "name": "Marwan Omar"
                },
                "author": "Marwan Omar",
                "arxiv_comment": "26 pages, 7 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15672v2",
                "updated": "2024-09-11T19:33:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    19,
                    33,
                    11,
                    2,
                    255,
                    0
                ],
                "published": "2024-03-23T01:40:04Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    1,
                    40,
                    4,
                    5,
                    83,
                    0
                ],
                "title": "Exploring the Ecosystem of DNS HTTPS Resource Records: An End-to-End\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Ecosystem of DNS HTTPS Resource Records: An End-to-End\n  Perspective"
                },
                "summary": "The DNS HTTPS resource record is a new DNS record type designed for the\ndelivery of configuration information and parameters required to initiate\nconnections to HTTPS network services. In addition, it is a key enabler for TLS\nEncrypted ClientHello (ECH) by providing the cryptographic keying material\nneeded to encrypt the initial exchange. To understand the adoption of this new\nDNS HTTPS record, we perform a longitudinal study on the server-side deployment\nof DNS HTTPS for Tranco top million domains, as well as an analysis of the\nclient-side support for DNS HTTPS through snapshots from major browsers. To the\nbest of our knowledge, our work is the first longitudinal study on DNS HTTPS\nserver deployment, and the first known study on client-side support for DNS\nHTTPS. Despite the rapidly growing trend of DNS HTTPS adoption, our study\nhighlights challenges and concerns in the deployment by both servers and\nclients, such as the complexity in properly maintaining HTTPS records and\nconnection failure in browsers when the HTTPS record is not properly\nconfigured.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS HTTPS resource record is a new DNS record type designed for the\ndelivery of configuration information and parameters required to initiate\nconnections to HTTPS network services. In addition, it is a key enabler for TLS\nEncrypted ClientHello (ECH) by providing the cryptographic keying material\nneeded to encrypt the initial exchange. To understand the adoption of this new\nDNS HTTPS record, we perform a longitudinal study on the server-side deployment\nof DNS HTTPS for Tranco top million domains, as well as an analysis of the\nclient-side support for DNS HTTPS through snapshots from major browsers. To the\nbest of our knowledge, our work is the first longitudinal study on DNS HTTPS\nserver deployment, and the first known study on client-side support for DNS\nHTTPS. Despite the rapidly growing trend of DNS HTTPS adoption, our study\nhighlights challenges and concerns in the deployment by both servers and\nclients, such as the complexity in properly maintaining HTTPS records and\nconnection failure in browsers when the HTTPS record is not properly\nconfigured."
                },
                "authors": [
                    {
                        "name": "Hongying Dong"
                    },
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Hyeonmin Lee"
                    },
                    {
                        "name": "Shumon Huque"
                    },
                    {
                        "name": "Yixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Sun"
                },
                "arxiv_affiliation": "University of Virginia",
                "author": "Yixin Sun",
                "arxiv_doi": "10.1145/3646547.3688410",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3646547.3688410",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.15672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in ACM Internet Measurement Conference 2024. 18 pages, 14\n  figures",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07578v1",
                "updated": "2024-09-11T19:10:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    19,
                    10,
                    29,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T19:10:29Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    19,
                    10,
                    29,
                    2,
                    255,
                    0
                ],
                "title": "A Novel Mathematical Framework for Objective Evaluation of Ideas using a\n  Conversational AI (CAI) System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Mathematical Framework for Objective Evaluation of Ideas using a\n  Conversational AI (CAI) System"
                },
                "summary": "The demand for innovation in product design necessitates a prolific ideation\nphase. Conversational AI (CAI) systems that use Large Language Models (LLMs)\nsuch as GPT (Generative Pre-trained Transformer) have been shown to be fruitful\nin augmenting human creativity, providing numerous novel and diverse ideas.\nDespite the success in ideation quantity, the qualitative assessment of these\nideas remains challenging and traditionally reliant on expert human evaluation.\nThis method suffers from limitations such as human judgment errors, bias, and\noversight. Addressing this gap, our study introduces a comprehensive\nmathematical framework for automated analysis to objectively evaluate the\nplethora of ideas generated by CAI systems and/or humans. This framework is\nparticularly advantageous for novice designers who lack experience in selecting\npromising ideas. By converting the ideas into higher dimensional vectors and\nquantitatively measuring the diversity between them using tools such as UMAP,\nDBSCAN and PCA, the proposed method provides a reliable and objective way of\nselecting the most promising ideas, thereby enhancing the efficiency of the\nideation phase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for innovation in product design necessitates a prolific ideation\nphase. Conversational AI (CAI) systems that use Large Language Models (LLMs)\nsuch as GPT (Generative Pre-trained Transformer) have been shown to be fruitful\nin augmenting human creativity, providing numerous novel and diverse ideas.\nDespite the success in ideation quantity, the qualitative assessment of these\nideas remains challenging and traditionally reliant on expert human evaluation.\nThis method suffers from limitations such as human judgment errors, bias, and\noversight. Addressing this gap, our study introduces a comprehensive\nmathematical framework for automated analysis to objectively evaluate the\nplethora of ideas generated by CAI systems and/or humans. This framework is\nparticularly advantageous for novice designers who lack experience in selecting\npromising ideas. By converting the ideas into higher dimensional vectors and\nquantitatively measuring the diversity between them using tools such as UMAP,\nDBSCAN and PCA, the proposed method provides a reliable and objective way of\nselecting the most promising ideas, thereby enhancing the efficiency of the\nideation phase."
                },
                "authors": [
                    {
                        "name": "B. Sankar"
                    },
                    {
                        "name": "Dibakar Sen"
                    }
                ],
                "author_detail": {
                    "name": "Dibakar Sen"
                },
                "author": "Dibakar Sen",
                "arxiv_comment": "20 pages, 12 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "53A45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07511v1",
                "updated": "2024-09-11T18:00:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    18,
                    0,
                    0,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T18:00:00Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    18,
                    0,
                    0,
                    2,
                    255,
                    0
                ],
                "title": "Initial performance of the Radar Echo Telescope for Cosmic Rays, RET-CR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Initial performance of the Radar Echo Telescope for Cosmic Rays, RET-CR"
                },
                "summary": "The Radar Echo Telescope for Cosmic Rays (RET-CR), a pathfinder instrument\nfor the radar echo method of ultrahigh energy (UHE) neutrino detection, was\ninitially deployed near Summit Station, Greenland, in May 2023. After a 4 week\ncommissioning period, 9 days of data were taken before the instrument went\noffline. In this article, we describe the instrument as it was deployed, and\nthe initial performance of the detector. We show that the technical aspects of\nrunning a radar based particle cascade detector in the ice have been\ndemonstrated. Analysis of the 2023 data informed improvements that were\nincorporated into the May-August 2024 deployment, which has just concluded at\ntime of writing. Results from the 2024 run will be presented in forthcoming\npublications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Radar Echo Telescope for Cosmic Rays (RET-CR), a pathfinder instrument\nfor the radar echo method of ultrahigh energy (UHE) neutrino detection, was\ninitially deployed near Summit Station, Greenland, in May 2023. After a 4 week\ncommissioning period, 9 days of data were taken before the instrument went\noffline. In this article, we describe the instrument as it was deployed, and\nthe initial performance of the detector. We show that the technical aspects of\nrunning a radar based particle cascade detector in the ice have been\ndemonstrated. Analysis of the 2023 data informed improvements that were\nincorporated into the May-August 2024 deployment, which has just concluded at\ntime of writing. Results from the 2024 run will be presented in forthcoming\npublications."
                },
                "authors": [
                    {
                        "name": "P. Allison"
                    },
                    {
                        "name": "J. Beatty"
                    },
                    {
                        "name": "D. Besson"
                    },
                    {
                        "name": "A. Connolly"
                    },
                    {
                        "name": "A. Cummings"
                    },
                    {
                        "name": "C. Deaconu"
                    },
                    {
                        "name": "S. De Kockere"
                    },
                    {
                        "name": "K. D. de Vries"
                    },
                    {
                        "name": "D. Frikken"
                    },
                    {
                        "name": "C. Hast"
                    },
                    {
                        "name": "E. Huesca Santiago"
                    },
                    {
                        "name": "C. -Y. Kuo"
                    },
                    {
                        "name": "A. Kyriacou"
                    },
                    {
                        "name": "U. A. Latif"
                    },
                    {
                        "name": "J. Loonen"
                    },
                    {
                        "name": "I. Loudon"
                    },
                    {
                        "name": "V. Lukic"
                    },
                    {
                        "name": "C. McLennan"
                    },
                    {
                        "name": "K. Mulrey"
                    },
                    {
                        "name": "J. Nam"
                    },
                    {
                        "name": "K. Nivedita"
                    },
                    {
                        "name": "A. Nozdrina"
                    },
                    {
                        "name": "E. Oberla"
                    },
                    {
                        "name": "S. Prohira"
                    },
                    {
                        "name": "J. P. Ralston"
                    },
                    {
                        "name": "M. F. H. Seikh"
                    },
                    {
                        "name": "R. S. Stanley"
                    },
                    {
                        "name": "S. Toscano"
                    },
                    {
                        "name": "D. Van den Broeck"
                    },
                    {
                        "name": "N. van Eijndhoven"
                    },
                    {
                        "name": "S. Wissel"
                    }
                ],
                "author_detail": {
                    "name": "S. Wissel"
                },
                "author": "S. Wissel",
                "arxiv_comment": "To be submitted to PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07453v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07453v1",
                "updated": "2024-09-11T17:59:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    59,
                    1,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T17:59:01Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    59,
                    1,
                    2,
                    255,
                    0
                ],
                "title": "\"My Grade is Wrong!\": A Contestable AI Framework for Interactive\n  Feedback in Evaluating Student Essays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"My Grade is Wrong!\": A Contestable AI Framework for Interactive\n  Feedback in Evaluating Student Essays"
                },
                "summary": "Interactive feedback, where feedback flows in both directions between teacher\nand student, is more effective than traditional one-way feedback. However, it\nis often too time-consuming for widespread use in educational practice. While\nLarge Language Models (LLMs) have potential for automating feedback, they\nstruggle with reasoning and interaction in an interactive setting. This paper\nintroduces CAELF, a Contestable AI Empowered LLM Framework for automating\ninteractive feedback. CAELF allows students to query, challenge, and clarify\ntheir feedback by integrating a multi-agent system with computational\nargumentation. Essays are first assessed by multiple Teaching-Assistant Agents\n(TA Agents), and then a Teacher Agent aggregates the evaluations through formal\nreasoning to generate feedback and grades. Students can further engage with the\nfeedback to refine their understanding. A case study on 500 critical thinking\nessays with user studies demonstrates that CAELF significantly improves\ninteractive feedback, enhancing the reasoning and interaction capabilities of\nLLMs. This approach offers a promising solution to overcoming the time and\nresource barriers that have limited the adoption of interactive feedback in\neducational settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive feedback, where feedback flows in both directions between teacher\nand student, is more effective than traditional one-way feedback. However, it\nis often too time-consuming for widespread use in educational practice. While\nLarge Language Models (LLMs) have potential for automating feedback, they\nstruggle with reasoning and interaction in an interactive setting. This paper\nintroduces CAELF, a Contestable AI Empowered LLM Framework for automating\ninteractive feedback. CAELF allows students to query, challenge, and clarify\ntheir feedback by integrating a multi-agent system with computational\nargumentation. Essays are first assessed by multiple Teaching-Assistant Agents\n(TA Agents), and then a Teacher Agent aggregates the evaluations through formal\nreasoning to generate feedback and grades. Students can further engage with the\nfeedback to refine their understanding. A case study on 500 critical thinking\nessays with user studies demonstrates that CAELF significantly improves\ninteractive feedback, enhancing the reasoning and interaction capabilities of\nLLMs. This approach offers a promising solution to overcoming the time and\nresource barriers that have limited the adoption of interactive feedback in\neducational settings."
                },
                "authors": [
                    {
                        "name": "Shengxin Hong"
                    },
                    {
                        "name": "Chang Cai"
                    },
                    {
                        "name": "Sixuan Du"
                    },
                    {
                        "name": "Haiyue Feng"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Xiuyi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Xiuyi Fan"
                },
                "author": "Xiuyi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07453v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07453v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07440v1",
                "updated": "2024-09-11T17:37:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    37,
                    48,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T17:37:48Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    37,
                    48,
                    2,
                    255,
                    0
                ],
                "title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research\n  Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research\n  Repositories"
                },
                "summary": "Given that Large Language Models (LLMs) have made significant progress in\nwriting code, can they now be used to autonomously reproduce results from\nresearch repositories? Such a capability would be a boon to the research\ncommunity, helping researchers validate, understand, and extend prior work. To\nadvance towards this goal, we introduce SUPER, the first benchmark designed to\nevaluate the capability of LLMs in setting up and executing tasks from research\nrepositories. SUPERaims to capture the realistic challenges faced by\nresearchers working with Machine Learning (ML) and Natural Language Processing\n(NLP) research repositories. Our benchmark comprises three distinct problem\nsets: 45 end-to-end problems with annotated expert solutions, 152 sub problems\nderived from the expert set that focus on specific challenges (e.g.,\nconfiguring a trainer), and 602 automatically generated problems for\nlarger-scale development. We introduce various evaluation measures to assess\nboth task success and progress, utilizing gold solutions when available or\napproximations otherwise. We show that state-of-the-art approaches struggle to\nsolve these problems with the best model (GPT-4o) solving only 16.3% of the\nend-to-end set, and 46.1% of the scenarios. This illustrates the challenge of\nthis task, and suggests that SUPER can serve as a valuable resource for the\ncommunity to make and measure progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given that Large Language Models (LLMs) have made significant progress in\nwriting code, can they now be used to autonomously reproduce results from\nresearch repositories? Such a capability would be a boon to the research\ncommunity, helping researchers validate, understand, and extend prior work. To\nadvance towards this goal, we introduce SUPER, the first benchmark designed to\nevaluate the capability of LLMs in setting up and executing tasks from research\nrepositories. SUPERaims to capture the realistic challenges faced by\nresearchers working with Machine Learning (ML) and Natural Language Processing\n(NLP) research repositories. Our benchmark comprises three distinct problem\nsets: 45 end-to-end problems with annotated expert solutions, 152 sub problems\nderived from the expert set that focus on specific challenges (e.g.,\nconfiguring a trainer), and 602 automatically generated problems for\nlarger-scale development. We introduce various evaluation measures to assess\nboth task success and progress, utilizing gold solutions when available or\napproximations otherwise. We show that state-of-the-art approaches struggle to\nsolve these problems with the best model (GPT-4o) solving only 16.3% of the\nend-to-end set, and 46.1% of the scenarios. This illustrates the challenge of\nthis task, and suggests that SUPER can serve as a valuable resource for the\ncommunity to make and measure progress."
                },
                "authors": [
                    {
                        "name": "Ben Bogin"
                    },
                    {
                        "name": "Kejuan Yang"
                    },
                    {
                        "name": "Shashank Gupta"
                    },
                    {
                        "name": "Kyle Richardson"
                    },
                    {
                        "name": "Erin Bransom"
                    },
                    {
                        "name": "Peter Clark"
                    },
                    {
                        "name": "Ashish Sabharwal"
                    },
                    {
                        "name": "Tushar Khot"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Khot"
                },
                "author": "Tushar Khot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07424v1",
                "updated": "2024-09-11T17:10:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    10,
                    20,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T17:10:20Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    17,
                    10,
                    20,
                    2,
                    255,
                    0
                ],
                "title": "Towards Fairer Health Recommendations: finding informative unbiased\n  samples via Word Sense Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fairer Health Recommendations: finding informative unbiased\n  samples via Word Sense Disambiguation"
                },
                "summary": "There have been growing concerns around high-stake applications that rely on\nmodels trained with biased data, which consequently produce biased predictions,\noften harming the most vulnerable. In particular, biased medical data could\ncause health-related applications and recommender systems to create outputs\nthat jeopardize patient care and widen disparities in health outcomes. A recent\nframework titled Fairness via AI posits that, instead of attempting to correct\nmodel biases, researchers must focus on their root causes by using AI to debias\ndata. Inspired by this framework, we tackle bias detection in medical curricula\nusing NLP models, including LLMs, and evaluate them on a gold standard dataset\ncontaining 4,105 excerpts annotated by medical experts for bias from a large\ncorpus. We build on previous work by coauthors which augments the set of\nnegative samples with non-annotated text containing social identifier terms.\nHowever, some of these terms, especially those related to race and ethnicity,\ncan carry different meanings (e.g., \"white matter of spinal cord\"). To address\nthis issue, we propose the use of Word Sense Disambiguation models to refine\ndataset quality by removing irrelevant sentences. We then evaluate fine-tuned\nvariations of BERT models as well as GPT models with zero- and few-shot\nprompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for\nbias detection, while fine-tuned BERT models generally perform well across all\nevaluated metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There have been growing concerns around high-stake applications that rely on\nmodels trained with biased data, which consequently produce biased predictions,\noften harming the most vulnerable. In particular, biased medical data could\ncause health-related applications and recommender systems to create outputs\nthat jeopardize patient care and widen disparities in health outcomes. A recent\nframework titled Fairness via AI posits that, instead of attempting to correct\nmodel biases, researchers must focus on their root causes by using AI to debias\ndata. Inspired by this framework, we tackle bias detection in medical curricula\nusing NLP models, including LLMs, and evaluate them on a gold standard dataset\ncontaining 4,105 excerpts annotated by medical experts for bias from a large\ncorpus. We build on previous work by coauthors which augments the set of\nnegative samples with non-annotated text containing social identifier terms.\nHowever, some of these terms, especially those related to race and ethnicity,\ncan carry different meanings (e.g., \"white matter of spinal cord\"). To address\nthis issue, we propose the use of Word Sense Disambiguation models to refine\ndataset quality by removing irrelevant sentences. We then evaluate fine-tuned\nvariations of BERT models as well as GPT models with zero- and few-shot\nprompting. We found LLMs, considered SOTA on many NLP tasks, unsuitable for\nbias detection, while fine-tuned BERT models generally perform well across all\nevaluated metrics."
                },
                "authors": [
                    {
                        "name": "Gavin Butts"
                    },
                    {
                        "name": "Pegah Emdad"
                    },
                    {
                        "name": "Jethro Lee"
                    },
                    {
                        "name": "Shannon Song"
                    },
                    {
                        "name": "Chiman Salavati"
                    },
                    {
                        "name": "Willmar Sosa Diaz"
                    },
                    {
                        "name": "Shiri Dori-Hacohen"
                    },
                    {
                        "name": "Fabricio Murai"
                    }
                ],
                "author_detail": {
                    "name": "Fabricio Murai"
                },
                "author": "Fabricio Murai",
                "arxiv_comment": "Accepted for long presentation at the FAcctRec @ Recsys 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3; K.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.12573v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.12573v3",
                "updated": "2024-09-11T16:52:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    52,
                    44,
                    2,
                    255,
                    0
                ],
                "published": "2023-11-21T12:38:05Z",
                "published_parsed": [
                    2023,
                    11,
                    21,
                    12,
                    38,
                    5,
                    1,
                    325,
                    0
                ],
                "title": "Moderating Model Marketplaces: Platform Governance Puzzles for AI\n  Intermediaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moderating Model Marketplaces: Platform Governance Puzzles for AI\n  Intermediaries"
                },
                "summary": "The AI development community is increasingly making use of hosting\nintermediaries such as Hugging Face provide easy access to user-uploaded models\nand training data. These model marketplaces lower technical deployment barriers\nfor hundreds of thousands of users, yet can be used in numerous potentially\nharmful and illegal ways. In this article, we explain ways in which AI systems,\nwhich can both `contain' content and be open-ended tools, present one of the\ntrickiest platform governance challenges seen to date. We provide case studies\nof several incidents across three illustrative platforms -- Hugging Face,\nGitHub and Civitai -- to examine how model marketplaces moderate models.\nBuilding on this analysis, we outline important (and yet nevertheless limited)\npractices that industry has been developing to respond to moderation demands:\nlicensing, access and use restrictions, automated content moderation, and open\npolicy development. While the policy challenge at hand is a considerable one,\nwe conclude with some ideas as to how platforms could better mobilize resources\nto act as a careful, fair, and proportionate regulatory access point.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI development community is increasingly making use of hosting\nintermediaries such as Hugging Face provide easy access to user-uploaded models\nand training data. These model marketplaces lower technical deployment barriers\nfor hundreds of thousands of users, yet can be used in numerous potentially\nharmful and illegal ways. In this article, we explain ways in which AI systems,\nwhich can both `contain' content and be open-ended tools, present one of the\ntrickiest platform governance challenges seen to date. We provide case studies\nof several incidents across three illustrative platforms -- Hugging Face,\nGitHub and Civitai -- to examine how model marketplaces moderate models.\nBuilding on this analysis, we outline important (and yet nevertheless limited)\npractices that industry has been developing to respond to moderation demands:\nlicensing, access and use restrictions, automated content moderation, and open\npolicy development. While the policy challenge at hand is a considerable one,\nwe conclude with some ideas as to how platforms could better mobilize resources\nto act as a careful, fair, and proportionate regulatory access point."
                },
                "authors": [
                    {
                        "name": "Robert Gorwa"
                    },
                    {
                        "name": "Michael Veale"
                    }
                ],
                "author_detail": {
                    "name": "Michael Veale"
                },
                "author": "Michael Veale",
                "arxiv_doi": "10.1080/17579961.2024.2388914",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/17579961.2024.2388914",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.12573v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.12573v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "(2024) 16(2) Law Innovation and Technology",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07407v1",
                "updated": "2024-09-11T16:49:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    49,
                    46,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T16:49:46Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    49,
                    46,
                    2,
                    255,
                    0
                ],
                "title": "CLNX: Bridging Code and Natural Language for C/C++\n  Vulnerability-Contributing Commits Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLNX: Bridging Code and Natural Language for C/C++\n  Vulnerability-Contributing Commits Identification"
                },
                "summary": "Large Language Models (LLMs) have shown great promise in vulnerability\nidentification. As C/C++ comprises half of the Open-Source Software (OSS)\nvulnerabilities over the past decade and updates in OSS mainly occur through\ncommits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing\nCommits (VCCs) is essential. However, current studies primarily focus on\nfurther pre-training LLMs on massive code datasets, which is resource-intensive\nand poses efficiency challenges. In this paper, we enhance the ability of\nBERT-based LLMs to identify C/C++ VCCs in a lightweight manner. We propose\nCodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++\nprograms and LLMs. Based on commits, CLNX efficiently converts the source code\ninto a more natural representation while preserving key details. Specifically,\nCLNX first applies structure-level naturalization to decompose complex\nprograms, followed by token-level naturalization to interpret complex symbols.\nWe evaluate CLNX on public datasets of 25,872 C/C++ functions with their\ncommits. The results show that CLNX significantly enhances the performance of\nLLMs on identifying C/C++ VCCs. Moreover, CLNX-equipped CodeBERT achieves new\nstate-of-the-art and identifies 38 OSS vulnerabilities in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown great promise in vulnerability\nidentification. As C/C++ comprises half of the Open-Source Software (OSS)\nvulnerabilities over the past decade and updates in OSS mainly occur through\ncommits, enhancing LLMs' ability to identify C/C++ Vulnerability-Contributing\nCommits (VCCs) is essential. However, current studies primarily focus on\nfurther pre-training LLMs on massive code datasets, which is resource-intensive\nand poses efficiency challenges. In this paper, we enhance the ability of\nBERT-based LLMs to identify C/C++ VCCs in a lightweight manner. We propose\nCodeLinguaNexus (CLNX) as a bridge facilitating communication between C/C++\nprograms and LLMs. Based on commits, CLNX efficiently converts the source code\ninto a more natural representation while preserving key details. Specifically,\nCLNX first applies structure-level naturalization to decompose complex\nprograms, followed by token-level naturalization to interpret complex symbols.\nWe evaluate CLNX on public datasets of 25,872 C/C++ functions with their\ncommits. The results show that CLNX significantly enhances the performance of\nLLMs on identifying C/C++ VCCs. Moreover, CLNX-equipped CodeBERT achieves new\nstate-of-the-art and identifies 38 OSS vulnerabilities in the real world."
                },
                "authors": [
                    {
                        "name": "Zeqing Qin"
                    },
                    {
                        "name": "Yiwei Wu"
                    },
                    {
                        "name": "Lansheng Han"
                    }
                ],
                "author_detail": {
                    "name": "Lansheng Han"
                },
                "author": "Lansheng Han",
                "arxiv_comment": "8 pages, 2 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07394v1",
                "updated": "2024-09-11T16:35:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    35,
                    18,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T16:35:18Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    35,
                    18,
                    2,
                    255,
                    0
                ],
                "title": "AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and\n  Parametric Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and\n  Parametric Knowledge"
                },
                "summary": "Knowledge conflict arises from discrepancies between information in the\ncontext of a large language model (LLM) and the knowledge stored in its\nparameters. This can hurt performance when using standard decoding techniques,\nwhich tend to ignore the context. Existing test-time contrastive methods seek\nto address this by comparing the LLM's output distribution with and without the\ncontext and adjust the model according to the contrast between them. However,\nwe find that these methods frequently misjudge the degree of conflict and\nstruggle to handle instances that vary in their amount of conflict, with static\nmethods over-adjusting when conflict is absent. We propose a fine-grained,\ninstance-level approach called AdaCAD, which dynamically infers the weight of\nadjustment based on the degree of conflict, as measured by the Jensen-Shannon\ndivergence between distributions representing contextual and parametric\nknowledge. Our experiments across four models on six diverse question-answering\n(QA) datasets and three summarization tasks demonstrate that our training-free\nadaptive method consistently outperforms other decoding methods on QA, with\naverage accuracy gains of 14.21% (absolute) over a static contrastive baseline,\nand improves the factuality of summaries by 5.59 (AlignScore). Furthermore, our\nanalysis shows that while decoding with contrastive baselines hurts performance\nwhen conflict is absent, AdaCAD mitigates these losses, making it more\napplicable to real-world datasets in which some examples have conflict and\nothers do not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge conflict arises from discrepancies between information in the\ncontext of a large language model (LLM) and the knowledge stored in its\nparameters. This can hurt performance when using standard decoding techniques,\nwhich tend to ignore the context. Existing test-time contrastive methods seek\nto address this by comparing the LLM's output distribution with and without the\ncontext and adjust the model according to the contrast between them. However,\nwe find that these methods frequently misjudge the degree of conflict and\nstruggle to handle instances that vary in their amount of conflict, with static\nmethods over-adjusting when conflict is absent. We propose a fine-grained,\ninstance-level approach called AdaCAD, which dynamically infers the weight of\nadjustment based on the degree of conflict, as measured by the Jensen-Shannon\ndivergence between distributions representing contextual and parametric\nknowledge. Our experiments across four models on six diverse question-answering\n(QA) datasets and three summarization tasks demonstrate that our training-free\nadaptive method consistently outperforms other decoding methods on QA, with\naverage accuracy gains of 14.21% (absolute) over a static contrastive baseline,\nand improves the factuality of summaries by 5.59 (AlignScore). Furthermore, our\nanalysis shows that while decoding with contrastive baselines hurts performance\nwhen conflict is absent, AdaCAD mitigates these losses, making it more\napplicable to real-world datasets in which some examples have conflict and\nothers do not."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "16 pages, Code: https://github.com/HanNight/AdaCAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02076v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02076v3",
                "updated": "2024-09-11T16:35:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    35,
                    0,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-03T17:25:54Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    25,
                    54,
                    1,
                    247,
                    0
                ],
                "title": "LongGenbench: Benchmarking Long-Form Generation in Long Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongGenbench: Benchmarking Long-Form Generation in Long Context LLMs"
                },
                "summary": "The abilities of long-context language models (LMs) are often evaluated using\nthe \"Needle-in-a-Haystack\" (NIAH) test, which comprises tasks designed to\nassess a model's ability to identify specific information (\"needle\") within\nlarge text sequences (\"haystack\"). While these benchmarks measure how well\nmodels understand long-context input sequences, they do not effectively gauge\nthe quality of long-form text generation--a critical aspect for applications\nsuch as design proposals and creative writing. To address this gap, we have\nintroduced a new long-form text evaluation benchmark, LongGenbench, which tests\nmodels' ability to identify specific events within generated long text\nsequences. In this benchmark, we prompt long-context LMs to create long-form\ntext that must include particular events or constraints and evaluate their\nability to incorporate these elements. We evaluated ten long-context LMs across\nfour distinct scenarios, three types of prompt instructions, and two different\ngeneration-length settings (16K and 32K). Although these models perform well on\nNIAH benchmarks, none demonstrated satisfactory performance on the\nLongGenbench, raising concerns about their ability to generate coherent\nlong-form text that follows instructions. Additionally, as the length of the\ngenerated text increases, all models exhibit a significant drop in performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The abilities of long-context language models (LMs) are often evaluated using\nthe \"Needle-in-a-Haystack\" (NIAH) test, which comprises tasks designed to\nassess a model's ability to identify specific information (\"needle\") within\nlarge text sequences (\"haystack\"). While these benchmarks measure how well\nmodels understand long-context input sequences, they do not effectively gauge\nthe quality of long-form text generation--a critical aspect for applications\nsuch as design proposals and creative writing. To address this gap, we have\nintroduced a new long-form text evaluation benchmark, LongGenbench, which tests\nmodels' ability to identify specific events within generated long text\nsequences. In this benchmark, we prompt long-context LMs to create long-form\ntext that must include particular events or constraints and evaluate their\nability to incorporate these elements. We evaluated ten long-context LMs across\nfour distinct scenarios, three types of prompt instructions, and two different\ngeneration-length settings (16K and 32K). Although these models perform well on\nNIAH benchmarks, none demonstrated satisfactory performance on the\nLongGenbench, raising concerns about their ability to generate coherent\nlong-form text that follows instructions. Additionally, as the length of the\ngenerated text increases, all models exhibit a significant drop in performance."
                },
                "authors": [
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Ming Shan Hee"
                    },
                    {
                        "name": "Zhiqing Hu"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "arxiv_comment": "work in progress. arXiv admin note: text overlap with\n  arXiv:2404.06654 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02076v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02076v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07372v1",
                "updated": "2024-09-11T16:03:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    3,
                    9,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T16:03:09Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    16,
                    3,
                    9,
                    2,
                    255,
                    0
                ],
                "title": "Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring\n  System via Language Model Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring\n  System via Language Model Coordination"
                },
                "summary": "The vast pre-existing slides serve as rich and important materials to carry\nlecture knowledge. However, effectively leveraging lecture slides to serve\nstudents is difficult due to the multi-modal nature of slide content and the\nheterogeneous teaching actions. We study the problem of discovering effective\ndesigns that convert a slide into an interactive lecture. We develop\nSlide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring\nsystem that can (1) effectively convert an input lecture slide into a\nstructured teaching agenda consisting of a set of heterogeneous teaching\nactions; (2) create and manage an interactive lecture that generates responsive\ninteractions catering to student learning demands while regulating the\ninteractions to follow teaching actions. Slide2Lecture contains a complete\npipeline for learners to obtain an interactive classroom experience to learn\nthe slide. For teachers and developers, Slide2Lecture enables customization to\ncater to personalized demands. The evaluation rated by annotators and students\nshows that Slide2Lecture is effective in outperforming the remaining\nimplementation. Slide2Lecture's online deployment has made more than 200K\ninteraction with students in the 3K lecture sessions. We open source\nSlide2Lecture's implementation in\nhttps://anonymous.4open.science/r/slide2lecture-4210/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast pre-existing slides serve as rich and important materials to carry\nlecture knowledge. However, effectively leveraging lecture slides to serve\nstudents is difficult due to the multi-modal nature of slide content and the\nheterogeneous teaching actions. We study the problem of discovering effective\ndesigns that convert a slide into an interactive lecture. We develop\nSlide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring\nsystem that can (1) effectively convert an input lecture slide into a\nstructured teaching agenda consisting of a set of heterogeneous teaching\nactions; (2) create and manage an interactive lecture that generates responsive\ninteractions catering to student learning demands while regulating the\ninteractions to follow teaching actions. Slide2Lecture contains a complete\npipeline for learners to obtain an interactive classroom experience to learn\nthe slide. For teachers and developers, Slide2Lecture enables customization to\ncater to personalized demands. The evaluation rated by annotators and students\nshows that Slide2Lecture is effective in outperforming the remaining\nimplementation. Slide2Lecture's online deployment has made more than 200K\ninteraction with students in the 3K lecture sessions. We open source\nSlide2Lecture's implementation in\nhttps://anonymous.4open.science/r/slide2lecture-4210/."
                },
                "authors": [
                    {
                        "name": "Daniel Zhang-Li"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Joy Lim Jia Yin"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Linlu Gong"
                    },
                    {
                        "name": "Haohua Wang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Huiqin Liu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07368v1",
                "updated": "2024-09-11T15:56:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    56,
                    15,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:56:15Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    56,
                    15,
                    2,
                    255,
                    0
                ],
                "title": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code"
                },
                "summary": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: http://3.131.141.63:8501/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: http://3.131.141.63:8501/."
                },
                "authors": [
                    {
                        "name": "Khiem Ton"
                    },
                    {
                        "name": "Nhi Nguyen"
                    },
                    {
                        "name": "Mahmoud Nazzal"
                    },
                    {
                        "name": "Abdallah Khreishah"
                    },
                    {
                        "name": "Cristian Borcea"
                    },
                    {
                        "name": "NhatHai Phan"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Issa Khalil"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13764v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13764v4",
                "updated": "2024-09-11T15:47:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    47,
                    11,
                    2,
                    255,
                    0
                ],
                "published": "2024-02-21T12:38:59Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    12,
                    38,
                    59,
                    2,
                    52,
                    0
                ],
                "title": "CriticEval: Evaluating Large Language Model as Critic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CriticEval: Evaluating Large Language Model as Critic"
                },
                "summary": "Critique ability, i.e., the capability of Large Language Models (LLMs) to\nidentify and rectify flaws in responses, is crucial for their applications in\nself-improvement and scalable oversight. While numerous studies have been\nproposed to evaluate critique ability of LLMs, their comprehensiveness and\nreliability are still limited. To overcome this problem, we introduce\nCriticEval, a novel benchmark designed to comprehensively and reliably evaluate\ncritique ability of LLMs. Specifically, to ensure the comprehensiveness,\nCriticEval evaluates critique ability from four dimensions across nine diverse\ntask scenarios. It evaluates both scalar-valued and textual critiques,\ntargeting responses of varying quality. To ensure the reliability, a large\nnumber of critiques are annotated to serve as references, enabling GPT-4 to\nevaluate textual critiques reliably. Extensive evaluations of open-source and\nclosed-source LLMs first validate the reliability of evaluation in CriticEval.\nThen, experimental results demonstrate the promising potential of open-source\nLLMs, the effectiveness of critique datasets and several intriguing\nrelationships between the critique ability and some critical factors, including\ntask types, response qualities and critique dimensions. Datasets and evaluation\ntoolkit for CriticEval will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critique ability, i.e., the capability of Large Language Models (LLMs) to\nidentify and rectify flaws in responses, is crucial for their applications in\nself-improvement and scalable oversight. While numerous studies have been\nproposed to evaluate critique ability of LLMs, their comprehensiveness and\nreliability are still limited. To overcome this problem, we introduce\nCriticEval, a novel benchmark designed to comprehensively and reliably evaluate\ncritique ability of LLMs. Specifically, to ensure the comprehensiveness,\nCriticEval evaluates critique ability from four dimensions across nine diverse\ntask scenarios. It evaluates both scalar-valued and textual critiques,\ntargeting responses of varying quality. To ensure the reliability, a large\nnumber of critiques are annotated to serve as references, enabling GPT-4 to\nevaluate textual critiques reliably. Extensive evaluations of open-source and\nclosed-source LLMs first validate the reliability of evaluation in CriticEval.\nThen, experimental results demonstrate the promising potential of open-source\nLLMs, the effectiveness of critique datasets and several intriguing\nrelationships between the critique ability and some critical factors, including\ntask types, response qualities and critique dimensions. Datasets and evaluation\ntoolkit for CriticEval will be publicly released."
                },
                "authors": [
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Xian-ling Mao"
                    }
                ],
                "author_detail": {
                    "name": "Xian-ling Mao"
                },
                "author": "Xian-ling Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13764v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13764v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07355v1",
                "updated": "2024-09-11T15:40:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    40,
                    7,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:40:07Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    40,
                    7,
                    2,
                    255,
                    0
                ],
                "title": "Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud\n  Outcomes for Effective Text Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud\n  Outcomes for Effective Text Evaluation"
                },
                "summary": "This study introduces \\textbf{InteractEval}, a framework that integrates\nhuman expertise and Large Language Models (LLMs) using the Think-Aloud (TA)\nmethod to generate attributes for checklist-based text evaluation. By combining\nhuman flexibility and reasoning with LLM consistency, InteractEval outperforms\ntraditional non-LLM-based and LLM-based baselines across four distinct\ndimensions, consisting of Coherence, Fluency, Consistency, and Relevance. The\nexperiment also investigates the effectiveness of the TA method, showing that\nit promotes divergent thinking in both humans and LLMs, leading to the\ngeneration of a wider range of relevant attributes and enhance text evaluation\nperformance. Comparative analysis reveals that humans excel at identifying\nattributes related to internal quality (Coherence and Fluency), but LLMs\nperform better at those attributes related to external alignment (Consistency\nand Relevance). Consequently, leveraging both humans and LLMs together produces\nthe best evaluation outcomes. In other words, this study emphasizes the\nnecessity of effectively combining humans and LLMs in an automated\nchecklist-based text evaluation framework. The code is available at\n\\textbf{\\url{https://github.com/BBeeChu/InteractEval.git}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces \\textbf{InteractEval}, a framework that integrates\nhuman expertise and Large Language Models (LLMs) using the Think-Aloud (TA)\nmethod to generate attributes for checklist-based text evaluation. By combining\nhuman flexibility and reasoning with LLM consistency, InteractEval outperforms\ntraditional non-LLM-based and LLM-based baselines across four distinct\ndimensions, consisting of Coherence, Fluency, Consistency, and Relevance. The\nexperiment also investigates the effectiveness of the TA method, showing that\nit promotes divergent thinking in both humans and LLMs, leading to the\ngeneration of a wider range of relevant attributes and enhance text evaluation\nperformance. Comparative analysis reveals that humans excel at identifying\nattributes related to internal quality (Coherence and Fluency), but LLMs\nperform better at those attributes related to external alignment (Consistency\nand Relevance). Consequently, leveraging both humans and LLMs together produces\nthe best evaluation outcomes. In other words, this study emphasizes the\nnecessity of effectively combining humans and LLMs in an automated\nchecklist-based text evaluation framework. The code is available at\n\\textbf{\\url{https://github.com/BBeeChu/InteractEval.git}}."
                },
                "authors": [
                    {
                        "name": "SeongYeub Chu"
                    },
                    {
                        "name": "JongWoo Kim"
                    },
                    {
                        "name": "MunYong Yi"
                    }
                ],
                "author_detail": {
                    "name": "MunYong Yi"
                },
                "author": "MunYong Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07353v1",
                "updated": "2024-09-11T15:39:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    39,
                    42,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:39:42Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    39,
                    42,
                    2,
                    255,
                    0
                ],
                "title": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak\n  and Adversarial Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Vision-Language Models with a Robust Encoder Against Jailbreak\n  and Adversarial Attacks"
                },
                "summary": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets,\nhave significantly advanced AI by excelling in vision-language tasks. However,\nthese models remain vulnerable to adversarial attacks, particularly jailbreak\nattacks, which bypass safety protocols and cause the model to generate\nmisleading or harmful responses. This vulnerability stems from both the\ninherent susceptibilities of LLMs and the expanded attack surface introduced by\nthe visual modality. We propose Sim-CLIP+, a novel defense mechanism that\nadversarially fine-tunes the CLIP vision encoder by leveraging a Siamese\narchitecture. This approach maximizes cosine similarity between perturbed and\nclean samples, facilitating resilience against adversarial manipulations.\nSim-CLIP+ offers a plug-and-play solution, allowing seamless integration into\nexisting LVLM architectures as a robust vision encoder. Unlike previous\ndefenses, our method requires no structural modifications to the LVLM and\nincurs minimal computational overhead. Sim-CLIP+ demonstrates effectiveness\nagainst both gradient-based adversarial attacks and various jailbreak\ntechniques. We evaluate Sim-CLIP+ against three distinct jailbreak attack\nstrategies and perform clean evaluations using standard downstream datasets,\nincluding COCO for image captioning and OKVQA for visual question answering.\nExtensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracy\nwhile substantially improving robustness against both gradient-based\nadversarial attacks and jailbreak techniques. Our code and robust vision\nencoders are available at\nhttps://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs), trained on multimodal big datasets,\nhave significantly advanced AI by excelling in vision-language tasks. However,\nthese models remain vulnerable to adversarial attacks, particularly jailbreak\nattacks, which bypass safety protocols and cause the model to generate\nmisleading or harmful responses. This vulnerability stems from both the\ninherent susceptibilities of LLMs and the expanded attack surface introduced by\nthe visual modality. We propose Sim-CLIP+, a novel defense mechanism that\nadversarially fine-tunes the CLIP vision encoder by leveraging a Siamese\narchitecture. This approach maximizes cosine similarity between perturbed and\nclean samples, facilitating resilience against adversarial manipulations.\nSim-CLIP+ offers a plug-and-play solution, allowing seamless integration into\nexisting LVLM architectures as a robust vision encoder. Unlike previous\ndefenses, our method requires no structural modifications to the LVLM and\nincurs minimal computational overhead. Sim-CLIP+ demonstrates effectiveness\nagainst both gradient-based adversarial attacks and various jailbreak\ntechniques. We evaluate Sim-CLIP+ against three distinct jailbreak attack\nstrategies and perform clean evaluations using standard downstream datasets,\nincluding COCO for image captioning and OKVQA for visual question answering.\nExtensive experiments demonstrate that Sim-CLIP+ maintains high clean accuracy\nwhile substantially improving robustness against both gradient-based\nadversarial attacks and jailbreak techniques. Our code and robust vision\nencoders are available at\nhttps://github.com/speedlab-git/Robust-Encoder-against-Jailbreak-attack.git."
                },
                "authors": [
                    {
                        "name": "Md Zarif Hossain"
                    },
                    {
                        "name": "Ahmed Imteaj"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Imteaj"
                },
                "author": "Ahmed Imteaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07314v1",
                "updated": "2024-09-11T14:44:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    44,
                    51,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T14:44:51Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    44,
                    51,
                    2,
                    255,
                    0
                ],
                "title": "MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDIC: Towards a Comprehensive Framework for Evaluating LLMs in Clinical\n  Applications"
                },
                "summary": "The rapid development of Large Language Models (LLMs) for healthcare\napplications has spurred calls for holistic evaluation beyond frequently-cited\nbenchmarks like USMLE, to better reflect real-world performance. While\nreal-world assessments are valuable indicators of utility, they often lag\nbehind the pace of LLM evolution, likely rendering findings obsolete upon\ndeployment. This temporal disconnect necessitates a comprehensive upfront\nevaluation that can guide model selection for specific clinical applications.\nWe introduce MEDIC, a framework assessing LLMs across five critical dimensions\nof clinical competence: medical reasoning, ethics and bias, data and language\nunderstanding, in-context learning, and clinical safety. MEDIC features a novel\ncross-examination framework quantifying LLM performance across areas like\ncoverage and hallucination detection, without requiring reference outputs. We\napply MEDIC to evaluate LLMs on medical question-answering, safety,\nsummarization, note generation, and other tasks. Our results show performance\ndisparities across model sizes, baseline vs medically finetuned models, and\nhave implications on model selection for applications requiring specific model\nstrengths, such as low hallucination or lower cost of inference. MEDIC's\nmultifaceted evaluation reveals these performance trade-offs, bridging the gap\nbetween theoretical capabilities and practical implementation in healthcare\nsettings, ensuring that the most promising models are identified and adapted\nfor diverse healthcare applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) for healthcare\napplications has spurred calls for holistic evaluation beyond frequently-cited\nbenchmarks like USMLE, to better reflect real-world performance. While\nreal-world assessments are valuable indicators of utility, they often lag\nbehind the pace of LLM evolution, likely rendering findings obsolete upon\ndeployment. This temporal disconnect necessitates a comprehensive upfront\nevaluation that can guide model selection for specific clinical applications.\nWe introduce MEDIC, a framework assessing LLMs across five critical dimensions\nof clinical competence: medical reasoning, ethics and bias, data and language\nunderstanding, in-context learning, and clinical safety. MEDIC features a novel\ncross-examination framework quantifying LLM performance across areas like\ncoverage and hallucination detection, without requiring reference outputs. We\napply MEDIC to evaluate LLMs on medical question-answering, safety,\nsummarization, note generation, and other tasks. Our results show performance\ndisparities across model sizes, baseline vs medically finetuned models, and\nhave implications on model selection for applications requiring specific model\nstrengths, such as low hallucination or lower cost of inference. MEDIC's\nmultifaceted evaluation reveals these performance trade-offs, bridging the gap\nbetween theoretical capabilities and practical implementation in healthcare\nsettings, ensuring that the most promising models are identified and adapted\nfor diverse healthcare applications."
                },
                "authors": [
                    {
                        "name": "Praveen K Kanithi"
                    },
                    {
                        "name": "Clément Christophe"
                    },
                    {
                        "name": "Marco AF Pimentel"
                    },
                    {
                        "name": "Tathagata Raha"
                    },
                    {
                        "name": "Nada Saadi"
                    },
                    {
                        "name": "Hamza Javed"
                    },
                    {
                        "name": "Svetlana Maslenkova"
                    },
                    {
                        "name": "Nasir Hayat"
                    },
                    {
                        "name": "Ronnie Rajan"
                    },
                    {
                        "name": "Shadab Khan"
                    }
                ],
                "author_detail": {
                    "name": "Shadab Khan"
                },
                "author": "Shadab Khan",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05993v2",
                "updated": "2024-09-11T14:42:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    42,
                    29,
                    2,
                    255,
                    0
                ],
                "published": "2024-04-09T03:54:28Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    3,
                    54,
                    28,
                    1,
                    100,
                    0
                ],
                "title": "AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM\n  Experts"
                },
                "summary": "As Large Language Models (LLMs) and generative AI become more widespread, the\ncontent safety risks associated with their use also increase. We find a notable\ndeficiency in high-quality content safety datasets and benchmarks that\ncomprehensively cover a wide range of critical safety areas. To address this,\nwe define a broad content safety risk taxonomy, comprising 13 critical risk and\n9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new\ndataset of approximately 26, 000 human-LLM interaction instances, complete with\nhuman annotations adhering to the taxonomy. We plan to release this dataset to\nthe community to further research and to help benchmark LLM models for safety.\nTo demonstrate the effectiveness of the dataset, we instruction-tune multiple\nLLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS),\nnot only surpass or perform competitively with the state-of-the-art LLM-based\nsafety models and general purpose LLMs, but also exhibit robustness across\nmultiple jail-break attack categories. We also show how using\nAEGISSAFETYDATASET during the LLM alignment phase does not negatively impact\nthe performance of the aligned models on MT Bench scores. Furthermore, we\npropose AEGIS, a novel application of a no-regret online adaptation framework\nwith strong theoretical guarantees, to perform content moderation with an\nensemble of LLM content safety experts in deployment",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) and generative AI become more widespread, the\ncontent safety risks associated with their use also increase. We find a notable\ndeficiency in high-quality content safety datasets and benchmarks that\ncomprehensively cover a wide range of critical safety areas. To address this,\nwe define a broad content safety risk taxonomy, comprising 13 critical risk and\n9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new\ndataset of approximately 26, 000 human-LLM interaction instances, complete with\nhuman annotations adhering to the taxonomy. We plan to release this dataset to\nthe community to further research and to help benchmark LLM models for safety.\nTo demonstrate the effectiveness of the dataset, we instruction-tune multiple\nLLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS),\nnot only surpass or perform competitively with the state-of-the-art LLM-based\nsafety models and general purpose LLMs, but also exhibit robustness across\nmultiple jail-break attack categories. We also show how using\nAEGISSAFETYDATASET during the LLM alignment phase does not negatively impact\nthe performance of the aligned models on MT Bench scores. Furthermore, we\npropose AEGIS, a novel application of a no-regret online adaptation framework\nwith strong theoretical guarantees, to perform content moderation with an\nensemble of LLM content safety experts in deployment"
                },
                "authors": [
                    {
                        "name": "Shaona Ghosh"
                    },
                    {
                        "name": "Prasoon Varshney"
                    },
                    {
                        "name": "Erick Galinkin"
                    },
                    {
                        "name": "Christopher Parisien"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Parisien"
                },
                "author": "Christopher Parisien",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07284v1",
                "updated": "2024-09-11T14:12:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    12,
                    44,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T14:12:44Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    12,
                    44,
                    2,
                    255,
                    0
                ],
                "title": "TLD-READY: Traffic Light Detection -- Relevance Estimation and\n  Deployment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TLD-READY: Traffic Light Detection -- Relevance Estimation and\n  Deployment Analysis"
                },
                "summary": "Effective traffic light detection is a critical component of the perception\nstack in autonomous vehicles. This work introduces a novel deep-learning\ndetection system while addressing the challenges of previous work. Utilizing a\ncomprehensive dataset amalgamation, including the Bosch Small Traffic Lights\nDataset, LISA, the DriveU Traffic Light Dataset, and a proprietary dataset from\nKarlsruhe, we ensure a robust evaluation across varied scenarios. Furthermore,\nwe propose a relevance estimation system that innovatively uses directional\narrow markings on the road, eliminating the need for prior map creation. On the\nDriveU dataset, this approach results in 96% accuracy in relevance estimation.\nFinally, a real-world evaluation is performed to evaluate the deployment and\ngeneralizing abilities of these models. For reproducibility and to facilitate\nfurther research, we provide the model weights and code:\nhttps://github.com/KASTEL-MobilityLab/traffic-light-detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective traffic light detection is a critical component of the perception\nstack in autonomous vehicles. This work introduces a novel deep-learning\ndetection system while addressing the challenges of previous work. Utilizing a\ncomprehensive dataset amalgamation, including the Bosch Small Traffic Lights\nDataset, LISA, the DriveU Traffic Light Dataset, and a proprietary dataset from\nKarlsruhe, we ensure a robust evaluation across varied scenarios. Furthermore,\nwe propose a relevance estimation system that innovatively uses directional\narrow markings on the road, eliminating the need for prior map creation. On the\nDriveU dataset, this approach results in 96% accuracy in relevance estimation.\nFinally, a real-world evaluation is performed to evaluate the deployment and\ngeneralizing abilities of these models. For reproducibility and to facilitate\nfurther research, we provide the model weights and code:\nhttps://github.com/KASTEL-MobilityLab/traffic-light-detection."
                },
                "authors": [
                    {
                        "name": "Nikolai Polley"
                    },
                    {
                        "name": "Svetlana Pavlitska"
                    },
                    {
                        "name": "Yacin Boualili"
                    },
                    {
                        "name": "Patrick Rohrbeck"
                    },
                    {
                        "name": "Paul Stiller"
                    },
                    {
                        "name": "Ashok Kumar Bangaru"
                    },
                    {
                        "name": "J. Marius Zöllner"
                    }
                ],
                "author_detail": {
                    "name": "J. Marius Zöllner"
                },
                "author": "J. Marius Zöllner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07276v2",
                "updated": "2024-09-13T04:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    4,
                    16,
                    55,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-11T13:49:48Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    49,
                    48,
                    2,
                    255,
                    0
                ],
                "title": "STORE: Streamlining Semantic Tokenization and Generative Recommendation\n  with A Single LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STORE: Streamlining Semantic Tokenization and Generative Recommendation\n  with A Single LLM"
                },
                "summary": "Traditional recommendation models often rely on unique item identifiers (IDs)\nto distinguish between items, which can hinder their ability to effectively\nleverage item content information and generalize to long-tail or cold-start\nitems. Recently, semantic tokenization has been proposed as a promising\nsolution that aims to tokenize each item's semantic representation into a\nsequence of discrete tokens. In this way, it preserves the item's semantics\nwithin these tokens and ensures that semantically similar items are represented\nby similar tokens. These semantic tokens have become fundamental in training\ngenerative recommendation models. However, existing generative recommendation\nmethods typically involve multiple sub-models for embedding, quantization, and\nrecommendation, leading to an overly complex system. In this paper, we propose\nto streamline the semantic tokenization and generative recommendation process\nwith a unified framework, dubbed STORE, which leverages a single large language\nmodel (LLM) for both tasks. Specifically, we formulate semantic tokenization as\na text-to-token task and generative recommendation as a token-to-token task,\nsupplemented by a token-to-text reconstruction task and a text-to-token\nauxiliary task. All these tasks are framed in a generative manner and trained\nusing a single LLM backbone. Extensive experiments have been conducted to\nvalidate the effectiveness of our STORE framework across various recommendation\ntasks and datasets. We will release the source code and configurations for\nreproducible research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional recommendation models often rely on unique item identifiers (IDs)\nto distinguish between items, which can hinder their ability to effectively\nleverage item content information and generalize to long-tail or cold-start\nitems. Recently, semantic tokenization has been proposed as a promising\nsolution that aims to tokenize each item's semantic representation into a\nsequence of discrete tokens. In this way, it preserves the item's semantics\nwithin these tokens and ensures that semantically similar items are represented\nby similar tokens. These semantic tokens have become fundamental in training\ngenerative recommendation models. However, existing generative recommendation\nmethods typically involve multiple sub-models for embedding, quantization, and\nrecommendation, leading to an overly complex system. In this paper, we propose\nto streamline the semantic tokenization and generative recommendation process\nwith a unified framework, dubbed STORE, which leverages a single large language\nmodel (LLM) for both tasks. Specifically, we formulate semantic tokenization as\na text-to-token task and generative recommendation as a token-to-token task,\nsupplemented by a token-to-text reconstruction task and a text-to-token\nauxiliary task. All these tasks are framed in a generative manner and trained\nusing a single LLM backbone. Extensive experiments have been conducted to\nvalidate the effectiveness of our STORE framework across various recommendation\ntasks and datasets. We will release the source code and configurations for\nreproducible research."
                },
                "authors": [
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Lu Fan"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ming Wu"
                },
                "author": "Xiao-Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07267v1",
                "updated": "2024-09-11T13:43:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    43,
                    1,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T13:43:01Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    43,
                    1,
                    2,
                    255,
                    0
                ],
                "title": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving"
                },
                "summary": "Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters."
                },
                "authors": [
                    {
                        "name": "Enming Zhang"
                    },
                    {
                        "name": "Xingyuan Dai"
                    },
                    {
                        "name": "Yisheng Lv"
                    },
                    {
                        "name": "Qianghai Miao"
                    }
                ],
                "author_detail": {
                    "name": "Qianghai Miao"
                },
                "author": "Qianghai Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20441v2",
                "updated": "2024-09-11T13:11:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    11,
                    16,
                    2,
                    255,
                    0
                ],
                "published": "2024-05-30T19:35:06Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    19,
                    35,
                    6,
                    3,
                    151,
                    0
                ],
                "title": "SECURE: Benchmarking Large Language Models for Cybersecurity Advisory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SECURE: Benchmarking Large Language Models for Cybersecurity Advisory"
                },
                "summary": "Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated potential in cybersecurity\napplications but have also caused lower confidence due to problems like\nhallucinations and a lack of truthfulness. Existing benchmarks provide general\nevaluations but do not sufficiently address the practical and applied aspects\nof LLM performance in cybersecurity-specific tasks. To address this gap, we\nintroduce the SECURE (Security Extraction, Understanding \\& Reasoning\nEvaluation), a benchmark designed to assess LLMs performance in realistic\ncybersecurity scenarios. SECURE includes six datasets focussed on the\nIndustrial Control System sector to evaluate knowledge extraction,\nunderstanding, and reasoning based on industry-standard sources. Our study\nevaluates seven state-of-the-art models on these tasks, providing insights into\ntheir strengths and weaknesses in cybersecurity contexts, and offer\nrecommendations for improving LLMs reliability as cyber advisory tools."
                },
                "authors": [
                    {
                        "name": "Dipkamal Bhusal"
                    },
                    {
                        "name": "Md Tanvirul Alam"
                    },
                    {
                        "name": "Le Nguyen"
                    },
                    {
                        "name": "Ashim Mahara"
                    },
                    {
                        "name": "Zachary Lightcap"
                    },
                    {
                        "name": "Rodney Frazier"
                    },
                    {
                        "name": "Romy Fieblinger"
                    },
                    {
                        "name": "Grace Long Torales"
                    },
                    {
                        "name": "Nidhi Rastogi"
                    }
                ],
                "author_detail": {
                    "name": "Nidhi Rastogi"
                },
                "author": "Nidhi Rastogi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07246v1",
                "updated": "2024-09-11T13:04:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    4,
                    34,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T13:04:34Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    4,
                    34,
                    2,
                    255,
                    0
                ],
                "title": "Propaganda to Hate: A Multimodal Analysis of Arabic Memes with\n  Multi-Agent LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Propaganda to Hate: A Multimodal Analysis of Arabic Memes with\n  Multi-Agent LLMs"
                },
                "summary": "In the past decade, social media platforms have been used for information\ndissemination and consumption. While a major portion of the content is posted\nto promote citizen journalism and public awareness, some content is posted to\nmislead users. Among different content types such as text, images, and videos,\nmemes (text overlaid on images) are particularly prevalent and can serve as\npowerful vehicles for propaganda, hate, and humor. In the current literature,\nthere have been efforts to individually detect such content in memes. However,\nthe study of their intersection is very limited. In this study, we explore the\nintersection between propaganda and hate in memes using a multi-agent LLM-based\napproach. We extend the propagandistic meme dataset with coarse and\nfine-grained hate labels. Our finding suggests that there is an association\nbetween propaganda and hate in memes. We provide detailed experimental results\nthat can serve as a baseline for future studies. We will make the experimental\nresources publicly available to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past decade, social media platforms have been used for information\ndissemination and consumption. While a major portion of the content is posted\nto promote citizen journalism and public awareness, some content is posted to\nmislead users. Among different content types such as text, images, and videos,\nmemes (text overlaid on images) are particularly prevalent and can serve as\npowerful vehicles for propaganda, hate, and humor. In the current literature,\nthere have been efforts to individually detect such content in memes. However,\nthe study of their intersection is very limited. In this study, we explore the\nintersection between propaganda and hate in memes using a multi-agent LLM-based\napproach. We extend the propagandistic meme dataset with coarse and\nfine-grained hate labels. Our finding suggests that there is an association\nbetween propaganda and hate in memes. We provide detailed experimental results\nthat can serve as a baseline for future studies. We will make the experimental\nresources publicly available to the community."
                },
                "authors": [
                    {
                        "name": "Firoj Alam"
                    },
                    {
                        "name": "Md. Rafiul Biswas"
                    },
                    {
                        "name": "Uzair Shah"
                    },
                    {
                        "name": "Wajdi Zaghouani"
                    },
                    {
                        "name": "Georgios Mikros"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Mikros"
                },
                "author": "Georgios Mikros",
                "arxiv_comment": "propaganda, hate-speech, disinformation, misinformation, fake news,\n  LLMs, GPT-4, multimodality, multimodal LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07239v1",
                "updated": "2024-09-11T12:53:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    53,
                    7,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:53:07Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    53,
                    7,
                    2,
                    255,
                    0
                ],
                "title": "PiTe: Pixel-Temporal Alignment for Large Video-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiTe: Pixel-Temporal Alignment for Large Video-Language Model"
                },
                "summary": "Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models\n(LVLMs) have emerged as a pivotal advancement, bridging the gap between image\nand text. However, video making it challenging for LVLMs to perform adequately\ndue to the complexity of the relationship between language and spatial-temporal\ndata structure. Recent Large Video-Language Models (LVidLMs) align feature of\nstatic visual data like image into latent space of language feature, by general\nmulti-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we\nexplore fine-grained alignment approach via object trajectory for different\nmodalities across both spatial and temporal dimensions simultaneously. Thus, we\npropose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed\nPiTe, that exhibits promising applicable model property. To achieve\nfine-grained video-language alignment, we curate a multi-modal pre-training\ndataset PiTe-143k, the dataset provision of moving trajectories in pixel level\nfor all individual objects, that appear and mention in the video and caption\nboth, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates\nastounding capabilities on myriad video-related multi-modal tasks through beat\nthe state-of-the-art methods by a large margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fueled by the Large Language Models (LLMs) wave, Large Visual-Language Models\n(LVLMs) have emerged as a pivotal advancement, bridging the gap between image\nand text. However, video making it challenging for LVLMs to perform adequately\ndue to the complexity of the relationship between language and spatial-temporal\ndata structure. Recent Large Video-Language Models (LVidLMs) align feature of\nstatic visual data like image into latent space of language feature, by general\nmulti-modal tasks to leverage abilities of LLMs sufficiently. In this paper, we\nexplore fine-grained alignment approach via object trajectory for different\nmodalities across both spatial and temporal dimensions simultaneously. Thus, we\npropose a novel LVidLM by trajectory-guided Pixel-Temporal Alignment, dubbed\nPiTe, that exhibits promising applicable model property. To achieve\nfine-grained video-language alignment, we curate a multi-modal pre-training\ndataset PiTe-143k, the dataset provision of moving trajectories in pixel level\nfor all individual objects, that appear and mention in the video and caption\nboth, by our automatic annotation pipeline. Meanwhile, PiTe demonstrates\nastounding capabilities on myriad video-related multi-modal tasks through beat\nthe state-of-the-art methods by a large margin."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00374v3",
                "updated": "2024-09-11T12:48:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    48,
                    42,
                    2,
                    255,
                    0
                ],
                "published": "2023-12-01T06:36:17Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    6,
                    36,
                    17,
                    4,
                    335,
                    0
                ],
                "title": "The Philosopher's Stone: Trojaning Plugins of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Philosopher's Stone: Trojaning Plugins of Large Language Models"
                },
                "summary": "Open-source Large Language Models (LLMs) have recently gained popularity\nbecause of their comparable performance to proprietary LLMs. To efficiently\nfulfill domain-specialized tasks, open-source LLMs can be refined, without\nexpensive accelerators, using low-rank adapters. However, it is still unknown\nwhether low-rank adapters can be exploited to control LLMs. To address this\ngap, we demonstrate that an infected adapter can induce, on specific\ntriggers,an LLM to output content defined by an adversary and to even\nmaliciously use tools. To train a Trojan adapter, we propose two novel attacks,\nPOLISHED and FUSION, that improve over prior approaches. POLISHED uses a\nsuperior LLM to align na\\\"ively poisoned data based on our insight that it can\nbetter inject poisoning knowledge during training. In contrast, FUSION\nleverages a novel over-poisoning procedure to transform a benign adapter into a\nmalicious one by magnifying the attention between trigger and target in model\nweights. In our experiments, we first conduct two case studies to demonstrate\nthat a compromised LLM agent can use malware to control the system (e.g., a\nLLM-driven robot) or to launch a spear-phishing attack. Then, in terms of\ntargeted misinformation, we show that our attacks provide higher attack\neffectiveness than the existing baseline and, for the purpose of attracting\ndownloads, preserve or improve the adapter's utility. Finally, we designed and\nevaluated three potential defenses. However, none proved entirely effective in\nsafeguarding against our attacks, highlighting the need for more robust\ndefenses supporting a secure LLM supply chain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source Large Language Models (LLMs) have recently gained popularity\nbecause of their comparable performance to proprietary LLMs. To efficiently\nfulfill domain-specialized tasks, open-source LLMs can be refined, without\nexpensive accelerators, using low-rank adapters. However, it is still unknown\nwhether low-rank adapters can be exploited to control LLMs. To address this\ngap, we demonstrate that an infected adapter can induce, on specific\ntriggers,an LLM to output content defined by an adversary and to even\nmaliciously use tools. To train a Trojan adapter, we propose two novel attacks,\nPOLISHED and FUSION, that improve over prior approaches. POLISHED uses a\nsuperior LLM to align na\\\"ively poisoned data based on our insight that it can\nbetter inject poisoning knowledge during training. In contrast, FUSION\nleverages a novel over-poisoning procedure to transform a benign adapter into a\nmalicious one by magnifying the attention between trigger and target in model\nweights. In our experiments, we first conduct two case studies to demonstrate\nthat a compromised LLM agent can use malware to control the system (e.g., a\nLLM-driven robot) or to launch a spear-phishing attack. Then, in terms of\ntargeted misinformation, we show that our attacks provide higher attack\neffectiveness than the existing baseline and, for the purpose of attracting\ndownloads, preserve or improve the adapter's utility. Finally, we designed and\nevaluated three potential defenses. However, none proved entirely effective in\nsafeguarding against our attacks, highlighting the need for more robust\ndefenses supporting a secure LLM supply chain."
                },
                "authors": [
                    {
                        "name": "Tian Dong"
                    },
                    {
                        "name": "Minhui Xue"
                    },
                    {
                        "name": "Guoxing Chen"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Shaofeng Li"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Haojin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Haojin Zhu"
                },
                "author": "Haojin Zhu",
                "arxiv_comment": "Accepted by NDSS Symposium 2025. Please cite this paper as \"Tian\n  Dong, Minhui Xue, Guoxing Chen, Rayne Holland, Yan Meng, Shaofeng Li, Zhen\n  Liu, Haojin Zhu. The Philosopher's Stone: Trojaning Plugins of Large Language\n  Models. In the 32nd Annual Network and Distributed System Security Symposium\n  (NDSS 2025).\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07507v1",
                "updated": "2024-09-11T12:27:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    27,
                    41,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:27:41Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    27,
                    41,
                    2,
                    255,
                    0
                ],
                "title": "Traceable LLM-based validation of statements in knowledge graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traceable LLM-based validation of statements in knowledge graphs"
                },
                "summary": "This article presents a method for verifying RDF triples using LLMs, with an\nemphasis on providing traceable arguments. Because the LLMs cannot currently\nreliably identify the origin of the information used to construct the response\nto the user query, our approach is to avoid using internal LLM factual\nknowledge altogether. Instead, verified RDF statements are compared to chunks\nof external documents retrieved through a web search or Wikipedia. To assess\nthe possible application of this workflow on biosciences content, we evaluated\n1,719 positive statements from the BioRED dataset and the same number of newly\ngenerated negative statements. The resulting precision is 88%, and recall is\n44%. This indicates that the method requires human oversight. We demonstrate\nthe method on Wikidata, where a SPARQL query is used to automatically retrieve\nstatements needing verification. Overall, the results suggest that LLMs could\nbe used for large-scale verification of statements in KGs, a task previously\nunfeasible due to human annotation costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article presents a method for verifying RDF triples using LLMs, with an\nemphasis on providing traceable arguments. Because the LLMs cannot currently\nreliably identify the origin of the information used to construct the response\nto the user query, our approach is to avoid using internal LLM factual\nknowledge altogether. Instead, verified RDF statements are compared to chunks\nof external documents retrieved through a web search or Wikipedia. To assess\nthe possible application of this workflow on biosciences content, we evaluated\n1,719 positive statements from the BioRED dataset and the same number of newly\ngenerated negative statements. The resulting precision is 88%, and recall is\n44%. This indicates that the method requires human oversight. We demonstrate\nthe method on Wikidata, where a SPARQL query is used to automatically retrieve\nstatements needing verification. Overall, the results suggest that LLMs could\nbe used for large-scale verification of statements in KGs, a task previously\nunfeasible due to human annotation costs."
                },
                "authors": [
                    {
                        "name": "Daniel Adam"
                    },
                    {
                        "name": "Tomáš Kliegr"
                    }
                ],
                "author_detail": {
                    "name": "Tomáš Kliegr"
                },
                "author": "Tomáš Kliegr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13555v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13555v2",
                "updated": "2024-09-11T12:19:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    19,
                    14,
                    2,
                    255,
                    0
                ],
                "published": "2024-06-19T13:44:56Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    13,
                    44,
                    56,
                    2,
                    171,
                    0
                ],
                "title": "BiLD: Bi-directional Logits Difference Loss for Large Language Model\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiLD: Bi-directional Logits Difference Loss for Large Language Model\n  Distillation"
                },
                "summary": "In recent years, large language models (LLMs) have shown exceptional\ncapabilities across various natural language processing (NLP) tasks. However,\nsuch impressive performance often comes with the trade-off of an increased\nparameter size, posing significant challenges for widespread deployment.\nKnowledge distillation (KD) provides a solution by transferring knowledge from\na large teacher model to a smaller student model. In this paper, we explore the\ntask-specific distillation of LLMs at the logit level. Our investigation\nreveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail\ndistribution than those from vision models, with hidden \"noise\" in the long\ntail affecting distillation performance. Furthermore, existing logits\ndistillation methods often struggle to effectively utilize the internal ranking\ninformation from the logits. To address these, we propose the Bi-directional\nLogits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by\nutilizing only top-$k$ teacher and student logits, and leverages the internal\nlogits ranking information by constructing logits differences. To evaluate BiLD\nloss, we conduct comprehensive experiments on 13 datasets using two types of\nLLMs. Our results show that the BiLD loss, with only the top-8 logits,\noutperforms supervised fine-tuning (SFT), vanilla KL loss, and five other\ndistillation methods from both NLP and CV fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have shown exceptional\ncapabilities across various natural language processing (NLP) tasks. However,\nsuch impressive performance often comes with the trade-off of an increased\nparameter size, posing significant challenges for widespread deployment.\nKnowledge distillation (KD) provides a solution by transferring knowledge from\na large teacher model to a smaller student model. In this paper, we explore the\ntask-specific distillation of LLMs at the logit level. Our investigation\nreveals that the logits of fine-tuned LLMs exhibit a more extreme long-tail\ndistribution than those from vision models, with hidden \"noise\" in the long\ntail affecting distillation performance. Furthermore, existing logits\ndistillation methods often struggle to effectively utilize the internal ranking\ninformation from the logits. To address these, we propose the Bi-directional\nLogits Difference (BiLD) loss. The BiLD loss filters out the long-tail noise by\nutilizing only top-$k$ teacher and student logits, and leverages the internal\nlogits ranking information by constructing logits differences. To evaluate BiLD\nloss, we conduct comprehensive experiments on 13 datasets using two types of\nLLMs. Our results show that the BiLD loss, with only the top-8 logits,\noutperforms supervised fine-tuning (SFT), vanilla KL loss, and five other\ndistillation methods from both NLP and CV fields."
                },
                "authors": [
                    {
                        "name": "Minchong Li"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Xiaohui Song"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Song"
                },
                "author": "Xiaohui Song",
                "arxiv_comment": "Submitted to COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13555v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13555v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02616v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02616v5",
                "updated": "2024-09-11T11:59:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    59,
                    25,
                    2,
                    255,
                    0
                ],
                "published": "2024-06-03T09:41:42Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    9,
                    41,
                    42,
                    0,
                    155,
                    0
                ],
                "title": "Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A\n  Model-Based Reinforcement Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Layer Splitting for Wireless LLM Inference in Edge Computing: A\n  Model-Based Reinforcement Learning Approach"
                },
                "summary": "Optimizing the deployment of large language models (LLMs) in edge computing\nenvironments is critical for enhancing privacy and computational efficiency.\nToward efficient wireless LLM inference in edge computing, this study\ncomprehensively analyzes the impact of different splitting points in mainstream\nopen-source LLMs. On this basis, this study introduces a framework taking\ninspiration from model-based reinforcement learning (MBRL) to determine the\noptimal splitting point across the edge and user equipment (UE). By\nincorporating a reward surrogate model, our approach significantly reduces the\ncomputational cost of frequent performance evaluations. Extensive simulations\ndemonstrate that this method effectively balances inference performance and\ncomputational load under varying network conditions, providing a robust\nsolution for LLM deployment in decentralized settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the deployment of large language models (LLMs) in edge computing\nenvironments is critical for enhancing privacy and computational efficiency.\nToward efficient wireless LLM inference in edge computing, this study\ncomprehensively analyzes the impact of different splitting points in mainstream\nopen-source LLMs. On this basis, this study introduces a framework taking\ninspiration from model-based reinforcement learning (MBRL) to determine the\noptimal splitting point across the edge and user equipment (UE). By\nincorporating a reward surrogate model, our approach significantly reduces the\ncomputational cost of frequent performance evaluations. Extensive simulations\ndemonstrate that this method effectively balances inference performance and\ncomputational load under varying network conditions, providing a robust\nsolution for LLM deployment in decentralized settings."
                },
                "authors": [
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Xiaoxue Yu"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02616v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02616v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07178v1",
                "updated": "2024-09-11T10:41:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    41,
                    5,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T10:41:05Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    41,
                    5,
                    2,
                    255,
                    0
                ],
                "title": "Identify Design Problems Through Questioning: Exploring Role-playing\n  Interactions with Large Language Models to Foster Design Questioning Skills",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Design Problems Through Questioning: Exploring Role-playing\n  Interactions with Large Language Models to Foster Design Questioning Skills"
                },
                "summary": "Identifying design problems is a crucial step for creating plausible\nsolutions, but it is challenging for design novices due to their limited\nknowledge and experience. Questioning is a promising skill that enables\nstudents to independently identify design problems without being passive or\nrelying on instructors. This study explores role-playing interactions with\nLarge Language Model (LLM)-powered Conversational Agents (CAs) to foster the\nquestioning skills of novice design students. We proposed an LLM-powered CA\nprototype and conducted a preliminary study with 16 novice design students\nengaged in a real-world design class to observe the interactions between\nstudents and the LLM-powered CAs. Our findings indicate that while the CAs\nstimulated questioning and reduced pressure to ask questions, it also\ninadvertently led to over-reliance on LLM responses. We proposed design\nconsiderations and future works for LLM-powered CA to foster questioning\nskills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying design problems is a crucial step for creating plausible\nsolutions, but it is challenging for design novices due to their limited\nknowledge and experience. Questioning is a promising skill that enables\nstudents to independently identify design problems without being passive or\nrelying on instructors. This study explores role-playing interactions with\nLarge Language Model (LLM)-powered Conversational Agents (CAs) to foster the\nquestioning skills of novice design students. We proposed an LLM-powered CA\nprototype and conducted a preliminary study with 16 novice design students\nengaged in a real-world design class to observe the interactions between\nstudents and the LLM-powered CAs. Our findings indicate that while the CAs\nstimulated questioning and reduced pressure to ask questions, it also\ninadvertently led to over-reliance on LLM responses. We proposed design\nconsiderations and future works for LLM-powered CA to foster questioning\nskills."
                },
                "authors": [
                    {
                        "name": "Hyunseung Lim"
                    },
                    {
                        "name": "Dasom Choi"
                    },
                    {
                        "name": "Hwajung Hong"
                    }
                ],
                "author_detail": {
                    "name": "Hwajung Hong"
                },
                "author": "Hwajung Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07165v1",
                "updated": "2024-09-11T10:24:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    24,
                    43,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T10:24:43Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    24,
                    43,
                    2,
                    255,
                    0
                ],
                "title": "Linear Time Complexity Conformers with SummaryMixing for Streaming\n  Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Time Complexity Conformers with SummaryMixing for Streaming\n  Speech Recognition"
                },
                "summary": "Automatic speech recognition (ASR) with an encoder equipped with\nself-attention, whether streaming or non-streaming, takes quadratic time in the\nlength of the speech utterance. This slows down training and decoding, increase\ntheir cost, and limit the deployment of the ASR in constrained devices.\nSummaryMixing is a promising linear-time complexity alternative to\nself-attention for non-streaming speech recognition that, for the first time,\npreserves or outperforms the accuracy of self-attention models. Unfortunately,\nthe original definition of SummaryMixing is not suited to streaming speech\nrecognition. Hence, this work extends SummaryMixing to a Conformer Transducer\nthat works in both a streaming and an offline mode. It shows that this new\nlinear-time complexity speech encoder outperforms self-attention in both\nscenarios while requiring less compute and memory during training and decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic speech recognition (ASR) with an encoder equipped with\nself-attention, whether streaming or non-streaming, takes quadratic time in the\nlength of the speech utterance. This slows down training and decoding, increase\ntheir cost, and limit the deployment of the ASR in constrained devices.\nSummaryMixing is a promising linear-time complexity alternative to\nself-attention for non-streaming speech recognition that, for the first time,\npreserves or outperforms the accuracy of self-attention models. Unfortunately,\nthe original definition of SummaryMixing is not suited to streaming speech\nrecognition. Hence, this work extends SummaryMixing to a Conformer Transducer\nthat works in both a streaming and an offline mode. It shows that this new\nlinear-time complexity speech encoder outperforms self-attention in both\nscenarios while requiring less compute and memory during training and decoding."
                },
                "authors": [
                    {
                        "name": "Titouan Parcollet"
                    },
                    {
                        "name": "Rogier van Dalen"
                    },
                    {
                        "name": "Shucong Zhang"
                    },
                    {
                        "name": "Sourav Batthacharya"
                    }
                ],
                "author_detail": {
                    "name": "Sourav Batthacharya"
                },
                "author": "Sourav Batthacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07162v1",
                "updated": "2024-09-11T10:21:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    21,
                    13,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T10:21:13Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    21,
                    13,
                    2,
                    255,
                    0
                ],
                "title": "A Fine-grained Sentiment Analysis of App Reviews using Large Language\n  Models: An Evaluation Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fine-grained Sentiment Analysis of App Reviews using Large Language\n  Models: An Evaluation Study"
                },
                "summary": "Analyzing user reviews for sentiment towards app features can provide\nvaluable insights into users' perceptions of app functionality and their\nevolving needs. Given the volume of user reviews received daily, an automated\nmechanism to generate feature-level sentiment summaries of user reviews is\nneeded. Recent advances in Large Language Models (LLMs) such as ChatGPT have\nshown impressive performance on several new tasks without updating the model's\nparameters i.e. using zero or a few labeled examples. Despite these\nadvancements, LLMs' capabilities to perform feature-specific sentiment analysis\nof user reviews remain unexplored. This study compares the performance of\nstate-of-the-art LLMs, including GPT-4, ChatGPT, and LLama-2-chat variants, for\nextracting app features and associated sentiments under 0-shot, 1-shot, and\n5-shot scenarios. Results indicate the best-performing GPT-4 model outperforms\nrule-based approaches by 23.6% in f1-score with zero-shot feature extraction;\n5-shot further improving it by 6%. GPT-4 achieves a 74% f1-score for predicting\npositive sentiment towards correctly predicted app features, with 5-shot\nenhancing it by 7%. Our study suggests that LLM models are promising for\ngenerating feature-specific sentiment summaries of user reviews.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing user reviews for sentiment towards app features can provide\nvaluable insights into users' perceptions of app functionality and their\nevolving needs. Given the volume of user reviews received daily, an automated\nmechanism to generate feature-level sentiment summaries of user reviews is\nneeded. Recent advances in Large Language Models (LLMs) such as ChatGPT have\nshown impressive performance on several new tasks without updating the model's\nparameters i.e. using zero or a few labeled examples. Despite these\nadvancements, LLMs' capabilities to perform feature-specific sentiment analysis\nof user reviews remain unexplored. This study compares the performance of\nstate-of-the-art LLMs, including GPT-4, ChatGPT, and LLama-2-chat variants, for\nextracting app features and associated sentiments under 0-shot, 1-shot, and\n5-shot scenarios. Results indicate the best-performing GPT-4 model outperforms\nrule-based approaches by 23.6% in f1-score with zero-shot feature extraction;\n5-shot further improving it by 6%. GPT-4 achieves a 74% f1-score for predicting\npositive sentiment towards correctly predicted app features, with 5-shot\nenhancing it by 7%. Our study suggests that LLM models are promising for\ngenerating feature-specific sentiment summaries of user reviews."
                },
                "authors": [
                    {
                        "name": "Faiz Ali Shah"
                    },
                    {
                        "name": "Ahmed Sabir"
                    },
                    {
                        "name": "Rajesh Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Sharma"
                },
                "author": "Rajesh Sharma",
                "arxiv_comment": "The summary of the project is available at\n  https://ahmed.jp/project_page/App_LLMs_2024/app_llms.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2109.08783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2109.08783v2",
                "updated": "2024-09-11T10:10:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    10,
                    13,
                    2,
                    255,
                    0
                ],
                "published": "2021-09-17T23:38:02Z",
                "published_parsed": [
                    2021,
                    9,
                    17,
                    23,
                    38,
                    2,
                    4,
                    260,
                    0
                ],
                "title": "From the Beginning: Key Transitions in the First 15 Years of DNSSEC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From the Beginning: Key Transitions in the First 15 Years of DNSSEC"
                },
                "summary": "When the global rollout of the DNS Security Extensions (DNSSEC) began in\n2005, a first-of-its-kind trial started: The complexity of a core Internet\nprotocol was magnified in favor of better security for the overall Internet.\nThereby, the scale of the loosely-federated delegation in DNS became an\nunprecedented cryptographic key management challenge. Though fundamental for\ncurrent and future operational success, our community lacks a clear notion of\nhow to empirically evaluate the process of securely transitioning keys.\n  In this paper, we propose two building blocks to formally characterize and\nassess key transitions. First, the anatomy of key transitions, i.e., measurable\nand well-defined properties of key changes; and second, a novel classification\nmodel based on this anatomy for describing key transition practices in abstract\nterms. This abstraction allows for classifying operational behavior. We apply\nour proposed transition anatomy and transition classes to describe the global\nDNSSEC deployment. Specifically, we use measurements from the first 15 years of\nthe DNSSEC rollout to detect and understand which key transitions have been\nused to what degree and which rates of errors and warnings occurred. In\ncontrast to prior work, we consider all possible transitions and not only 1:1\nkey rollovers. Our results show measurable gaps between prescribed key\nmanagement processes and key transitions in the wild. We also find evidence\nthat such noncompliant transitions are needed in operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When the global rollout of the DNS Security Extensions (DNSSEC) began in\n2005, a first-of-its-kind trial started: The complexity of a core Internet\nprotocol was magnified in favor of better security for the overall Internet.\nThereby, the scale of the loosely-federated delegation in DNS became an\nunprecedented cryptographic key management challenge. Though fundamental for\ncurrent and future operational success, our community lacks a clear notion of\nhow to empirically evaluate the process of securely transitioning keys.\n  In this paper, we propose two building blocks to formally characterize and\nassess key transitions. First, the anatomy of key transitions, i.e., measurable\nand well-defined properties of key changes; and second, a novel classification\nmodel based on this anatomy for describing key transition practices in abstract\nterms. This abstraction allows for classifying operational behavior. We apply\nour proposed transition anatomy and transition classes to describe the global\nDNSSEC deployment. Specifically, we use measurements from the first 15 years of\nthe DNSSEC rollout to detect and understand which key transitions have been\nused to what degree and which rates of errors and warnings occurred. In\ncontrast to prior work, we consider all possible transitions and not only 1:1\nkey rollovers. Our results show measurable gaps between prescribed key\nmanagement processes and key transitions in the wild. We also find evidence\nthat such noncompliant transitions are needed in operations."
                },
                "authors": [
                    {
                        "name": "Eric Osterweil"
                    },
                    {
                        "name": "Pouyan Fotouhi Tehrani"
                    },
                    {
                        "name": "Thomas C. Schmidt"
                    },
                    {
                        "name": "Matthias Wählisch"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Wählisch"
                },
                "author": "Matthias Wählisch",
                "arxiv_doi": "10.1109/TNSM.2022.3195406",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNSM.2022.3195406",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2109.08783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2109.08783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Network and Service Management, Vol. 19, No.\n  4, pp. 5265-5283, Dec. 2022",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11346v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11346v3",
                "updated": "2024-09-11T10:05:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    10,
                    5,
                    37,
                    2,
                    255,
                    0
                ],
                "published": "2024-06-17T09:08:30Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    9,
                    8,
                    30,
                    0,
                    169,
                    0
                ],
                "title": "WaDec: Decompiling WebAssembly Using Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaDec: Decompiling WebAssembly Using Large Language Model"
                },
                "summary": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web\ndevelopment, offering a compact binary format that allows high-performance\napplications to run at near-native speeds in web browsers. Despite its\nadvantages, Wasm's binary nature presents significant challenges for developers\nand researchers, particularly regarding readability when debugging or analyzing\nweb applications. Therefore, effective decompilation becomes crucial.\nUnfortunately, traditional decompilers often struggle with producing readable\noutputs. While some large language model (LLM)-based decompilers have shown\ngood compatibility with general binary files, they still face specific\nchallenges when dealing with Wasm. In this paper, we introduce a novel\napproach, WaDec, which is the first use of a fine-tuned LLM to interpret and\ndecompile Wasm binary code into a higher-level, more comprehensible source code\nrepresentation. The LLM was meticulously fine-tuned using a specialized dataset\nof wat-c code snippets, employing self-supervised learning techniques. This\nenables WaDec to effectively decompile not only complete wat functions but also\nfiner-grained wat code snippets. Our experiments demonstrate that WaDec\nmarkedly outperforms current state-of-the-art tools, offering substantial\nimprovements across several metrics. It achieves a code inflation rate of only\n3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%.\nUnlike baselines' output that cannot be directly compiled or executed, WaDec\nmaintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and\nan output consistency of 27.15%. Additionally, it significantly exceeds\nstate-of-the-art performance in AST edit distance similarity by 185%,\ncyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average\ncode similarity above 50%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web\ndevelopment, offering a compact binary format that allows high-performance\napplications to run at near-native speeds in web browsers. Despite its\nadvantages, Wasm's binary nature presents significant challenges for developers\nand researchers, particularly regarding readability when debugging or analyzing\nweb applications. Therefore, effective decompilation becomes crucial.\nUnfortunately, traditional decompilers often struggle with producing readable\noutputs. While some large language model (LLM)-based decompilers have shown\ngood compatibility with general binary files, they still face specific\nchallenges when dealing with Wasm. In this paper, we introduce a novel\napproach, WaDec, which is the first use of a fine-tuned LLM to interpret and\ndecompile Wasm binary code into a higher-level, more comprehensible source code\nrepresentation. The LLM was meticulously fine-tuned using a specialized dataset\nof wat-c code snippets, employing self-supervised learning techniques. This\nenables WaDec to effectively decompile not only complete wat functions but also\nfiner-grained wat code snippets. Our experiments demonstrate that WaDec\nmarkedly outperforms current state-of-the-art tools, offering substantial\nimprovements across several metrics. It achieves a code inflation rate of only\n3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%.\nUnlike baselines' output that cannot be directly compiled or executed, WaDec\nmaintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and\nan output consistency of 27.15%. Additionally, it significantly exceeds\nstate-of-the-art performance in AST edit distance similarity by 185%,\ncyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average\ncode similarity above 50%."
                },
                "authors": [
                    {
                        "name": "Xinyu She"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "This paper was accepted by ASE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11346v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11346v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07150v1",
                "updated": "2024-09-11T09:54:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    54,
                    45,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:54:45Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    54,
                    45,
                    2,
                    255,
                    0
                ],
                "title": "ZKFault: Fault attack analysis on zero-knowledge based post-quantum\n  digital signature schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZKFault: Fault attack analysis on zero-knowledge based post-quantum\n  digital signature schemes"
                },
                "summary": "Computationally hard problems based on coding theory, such as the syndrome\ndecoding problem, have been used for constructing secure cryptographic schemes\nfor a long time. Schemes based on these problems are also assumed to be secure\nagainst quantum computers. However, these schemes are often considered\nimpractical for real-world deployment due to large key sizes and inefficient\ncomputation time. In the recent call for standardization of additional\npost-quantum digital signatures by the National Institute of Standards and\nTechnology, several code-based candidates have been proposed, including LESS,\nCROSS, and MEDS. These schemes are designed on the relatively new\nzero-knowledge framework. Although several works analyze the hardness of these\nschemes, there is hardly any work that examines the security of these schemes\nin the presence of physical attacks.\n  In this work, we analyze these signature schemes from the perspective of\nfault attacks. All these schemes use a similar tree-based construction to\ncompress the signature size. We attack this component of these schemes.\nTherefore, our attack is applicable to all of these schemes. In this work, we\nfirst analyze the LESS signature scheme and devise our attack. Furthermore, we\nshowed how this attack can be extended to the CROSS signature scheme. Our\nattacks are built on very simple fault assumptions. Our results show that we\ncan recover the entire secret key of LESS and CROSS using as little as a single\nfault. Finally, we propose various countermeasures to prevent these kinds of\nattacks and discuss their efficiency and shortcomings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computationally hard problems based on coding theory, such as the syndrome\ndecoding problem, have been used for constructing secure cryptographic schemes\nfor a long time. Schemes based on these problems are also assumed to be secure\nagainst quantum computers. However, these schemes are often considered\nimpractical for real-world deployment due to large key sizes and inefficient\ncomputation time. In the recent call for standardization of additional\npost-quantum digital signatures by the National Institute of Standards and\nTechnology, several code-based candidates have been proposed, including LESS,\nCROSS, and MEDS. These schemes are designed on the relatively new\nzero-knowledge framework. Although several works analyze the hardness of these\nschemes, there is hardly any work that examines the security of these schemes\nin the presence of physical attacks.\n  In this work, we analyze these signature schemes from the perspective of\nfault attacks. All these schemes use a similar tree-based construction to\ncompress the signature size. We attack this component of these schemes.\nTherefore, our attack is applicable to all of these schemes. In this work, we\nfirst analyze the LESS signature scheme and devise our attack. Furthermore, we\nshowed how this attack can be extended to the CROSS signature scheme. Our\nattacks are built on very simple fault assumptions. Our results show that we\ncan recover the entire secret key of LESS and CROSS using as little as a single\nfault. Finally, we propose various countermeasures to prevent these kinds of\nattacks and discuss their efficiency and shortcomings."
                },
                "authors": [
                    {
                        "name": "Puja Mondal"
                    },
                    {
                        "name": "Supriya Adhikary"
                    },
                    {
                        "name": "Suparna Kundu"
                    },
                    {
                        "name": "Angshuman Karmakar"
                    }
                ],
                "author_detail": {
                    "name": "Angshuman Karmakar"
                },
                "author": "Angshuman Karmakar",
                "arxiv_comment": "35 pages including appendix and bibliography",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07136v1",
                "updated": "2024-09-11T09:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    31,
                    44,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    31,
                    44,
                    2,
                    255,
                    0
                ],
                "title": "Leveraging Unstructured Text Data for Federated Instruction Tuning of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Unstructured Text Data for Federated Instruction Tuning of\n  Large Language Models"
                },
                "summary": "Federated instruction tuning enables multiple clients to collaboratively\nfine-tune a shared large language model (LLM) that can follow humans'\ninstructions without directly sharing raw data. However, existing literature\nimpractically requires that all the clients readily hold instruction-tuning\ndata (i.e., structured instruction-response pairs), which necessitates massive\nhuman annotations since clients' data is usually unstructured text instead.\nAddressing this, we propose a novel and flexible framework FedIT-U2S, which can\nautomatically transform unstructured corpus into structured data for federated\ninstruction tuning. FedIT-U2S consists two key steps: (1) few-shot\ninstruction-tuning data generation, where each unstructured data piece together\nwith several examples is combined to prompt an LLM in generating an\ninstruction-response pair. To further enhance the flexibility, a\nretrieval-based example selection technique is proposed, where the examples are\nautomatically selected based on the relatedness between the client's data piece\nand example pool, bypassing the need of determining examples in advance. (2) A\ntypical federated instruction tuning process based on the generated data.\nOverall, FedIT-U2S can be applied to diverse scenarios as long as the client\nholds valuable text corpus, broadening the application scope of federated\ninstruction tuning. We conduct a series of experiments on three domains\n(medicine, knowledge, and math), showing that our proposed FedIT-U2S can\nconsistently and significantly brings improvement over the base LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated instruction tuning enables multiple clients to collaboratively\nfine-tune a shared large language model (LLM) that can follow humans'\ninstructions without directly sharing raw data. However, existing literature\nimpractically requires that all the clients readily hold instruction-tuning\ndata (i.e., structured instruction-response pairs), which necessitates massive\nhuman annotations since clients' data is usually unstructured text instead.\nAddressing this, we propose a novel and flexible framework FedIT-U2S, which can\nautomatically transform unstructured corpus into structured data for federated\ninstruction tuning. FedIT-U2S consists two key steps: (1) few-shot\ninstruction-tuning data generation, where each unstructured data piece together\nwith several examples is combined to prompt an LLM in generating an\ninstruction-response pair. To further enhance the flexibility, a\nretrieval-based example selection technique is proposed, where the examples are\nautomatically selected based on the relatedness between the client's data piece\nand example pool, bypassing the need of determining examples in advance. (2) A\ntypical federated instruction tuning process based on the generated data.\nOverall, FedIT-U2S can be applied to diverse scenarios as long as the client\nholds valuable text corpus, broadening the application scope of federated\ninstruction tuning. We conduct a series of experiments on three domains\n(medicine, knowledge, and math), showing that our proposed FedIT-U2S can\nconsistently and significantly brings improvement over the base LLM."
                },
                "authors": [
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Rui Ge"
                    },
                    {
                        "name": "Yuchi Fengting"
                    },
                    {
                        "name": "Jingyi Chai"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "11 pages, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07132v1",
                "updated": "2024-09-11T09:29:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    29,
                    28,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:29:28Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    29,
                    28,
                    2,
                    255,
                    0
                ],
                "title": "LLM-based feature generation from text for interpretable machine\n  learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based feature generation from text for interpretable machine\n  learning"
                },
                "summary": "Existing text representations such as embeddings and bag-of-words are not\nsuitable for rule learning due to their high dimensionality and absent or\nquestionable feature-level interpretability. This article explores whether\nlarge language models (LLMs) could address this by extracting a small number of\ninterpretable features from text. We demonstrate this process on two datasets\n(CORD-19 and M17+) containing several thousand scientific articles from\nmultiple disciplines and a target being a proxy for research impact. An\nevaluation based on testing for the statistically significant correlation with\nresearch impact has shown that LLama 2-generated features are semantically\nmeaningful. We consequently used these generated features in text\nclassification to predict the binary target variable representing the citation\nrate for the CORD-19 dataset and the ordinal 5-class target representing an\nexpert-awarded grade in the M17+ dataset. Machine-learning models trained on\nthe LLM-generated features provided similar predictive performance to the\nstate-of-the-art embedding model SciBERT for scientific text. The LLM used only\n62 features compared to 768 features in SciBERT embeddings, and these features\nwere directly interpretable, corresponding to notions such as article\nmethodological rigor, novelty, or grammatical correctness. As the final step,\nwe extract a small number of well-interpretable action rules. Consistently\ncompetitive results obtained with the same LLM feature set across both\nthematically diverse datasets show that this approach generalizes across\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing text representations such as embeddings and bag-of-words are not\nsuitable for rule learning due to their high dimensionality and absent or\nquestionable feature-level interpretability. This article explores whether\nlarge language models (LLMs) could address this by extracting a small number of\ninterpretable features from text. We demonstrate this process on two datasets\n(CORD-19 and M17+) containing several thousand scientific articles from\nmultiple disciplines and a target being a proxy for research impact. An\nevaluation based on testing for the statistically significant correlation with\nresearch impact has shown that LLama 2-generated features are semantically\nmeaningful. We consequently used these generated features in text\nclassification to predict the binary target variable representing the citation\nrate for the CORD-19 dataset and the ordinal 5-class target representing an\nexpert-awarded grade in the M17+ dataset. Machine-learning models trained on\nthe LLM-generated features provided similar predictive performance to the\nstate-of-the-art embedding model SciBERT for scientific text. The LLM used only\n62 features compared to 768 features in SciBERT embeddings, and these features\nwere directly interpretable, corresponding to notions such as article\nmethodological rigor, novelty, or grammatical correctness. As the final step,\nwe extract a small number of well-interpretable action rules. Consistently\ncompetitive results obtained with the same LLM feature set across both\nthematically diverse datasets show that this approach generalizes across\ndomains."
                },
                "authors": [
                    {
                        "name": "Vojtěch Balek"
                    },
                    {
                        "name": "Lukáš Sýkora"
                    },
                    {
                        "name": "Vilém Sklenák"
                    },
                    {
                        "name": "Tomáš Kliegr"
                    }
                ],
                "author_detail": {
                    "name": "Tomáš Kliegr"
                },
                "author": "Tomáš Kliegr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07131v1",
                "updated": "2024-09-11T09:27:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    27,
                    50,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:27:50Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    27,
                    50,
                    2,
                    255,
                    0
                ],
                "title": "Reranking Laws for Language Generation: A Communication-Theoretic\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking Laws for Language Generation: A Communication-Theoretic\n  Perspective"
                },
                "summary": "To ensure large language models (LLMs) are used safely, one must reduce their\npropensity to hallucinate or to generate unacceptable answers. A simple and\noften used strategy is to first let the LLM generate multiple hypotheses and\nthen employ a reranker to choose the best one. In this paper, we draw a\nparallel between this strategy and the use of redundancy to decrease the error\nrate in noisy communication channels. We conceptualize the generator as a\nsender transmitting multiple descriptions of a message through parallel noisy\nchannels. The receiver decodes the message by ranking the (potentially\ncorrupted) descriptions and selecting the one found to be most reliable. We\nprovide conditions under which this protocol is asymptotically error-free\n(i.e., yields an acceptable answer almost surely) even in scenarios where the\nreranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the\nchannel distributions are statistically dependent. We use our framework to\nobtain reranking laws which we validate empirically on two real-world tasks\nusing LLMs: text-to-code generation with DeepSeek-Coder 7B and machine\ntranslation of medical data with TowerInstruct 13B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To ensure large language models (LLMs) are used safely, one must reduce their\npropensity to hallucinate or to generate unacceptable answers. A simple and\noften used strategy is to first let the LLM generate multiple hypotheses and\nthen employ a reranker to choose the best one. In this paper, we draw a\nparallel between this strategy and the use of redundancy to decrease the error\nrate in noisy communication channels. We conceptualize the generator as a\nsender transmitting multiple descriptions of a message through parallel noisy\nchannels. The receiver decodes the message by ranking the (potentially\ncorrupted) descriptions and selecting the one found to be most reliable. We\nprovide conditions under which this protocol is asymptotically error-free\n(i.e., yields an acceptable answer almost surely) even in scenarios where the\nreranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the\nchannel distributions are statistically dependent. We use our framework to\nobtain reranking laws which we validate empirically on two real-world tasks\nusing LLMs: text-to-code generation with DeepSeek-Coder 7B and machine\ntranslation of medical data with TowerInstruct 13B."
                },
                "authors": [
                    {
                        "name": "António Farinhas"
                    },
                    {
                        "name": "Haau-Sing Li"
                    },
                    {
                        "name": "André F. T. Martins"
                    }
                ],
                "author_detail": {
                    "name": "André F. T. Martins"
                },
                "author": "André F. T. Martins",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07123v1",
                "updated": "2024-09-11T09:21:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    21,
                    20,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T09:21:20Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    9,
                    21,
                    20,
                    2,
                    255,
                    0
                ],
                "title": "Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Refine: Improving Natural Language Explanation Generation by\n  Learning in Tandem"
                },
                "summary": "Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language explanations (NLEs) are vital for elucidating the reasoning\nbehind large language model (LLM) decisions. Many techniques have been\ndeveloped to generate NLEs using LLMs. However, like humans, LLMs might not\nalways produce optimal NLEs on first attempt. Inspired by human learning\nprocesses, we introduce Cross-Refine, which employs role modeling by deploying\ntwo LLMs as generator and critic, respectively. The generator outputs a first\nNLE and then refines this initial explanation using feedback and suggestions\nprovided by the critic. Cross-Refine does not require any supervised training\ndata or additional training. We validate Cross-Refine across three NLP tasks\nusing three state-of-the-art open-source LLMs through automatic and human\nevaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which\nonly utilizes self-feedback to refine the explanations. Our findings from\nautomatic evaluation and a user study indicate that Cross-Refine outperforms\nSelf-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful\nLLMs, whereas Self-Refine only yields strong results with ChatGPT.\nAdditionally, we conduct an ablation study to assess the importance of feedback\nand suggestions. Both of them play an important role in refining explanations.\nWe further evaluate Cross-Refine on a bilingual dataset in English and German."
                },
                "authors": [
                    {
                        "name": "Qianli Wang"
                    },
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Sebastian Möller"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "17 pages; under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]